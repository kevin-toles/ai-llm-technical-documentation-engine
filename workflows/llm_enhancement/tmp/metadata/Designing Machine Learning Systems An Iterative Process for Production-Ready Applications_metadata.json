[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 2-9)",
    "start_page": 2,
    "end_page": 9,
    "summary": "The book provides a detailed guide for people building end-to-end machine learning systems.\nChip Huyen has produced an important addition to the canon of machine learning literature—one that is deeply literate in ML fundamentals, but has a much more concrete and practical approach than most.\nThis book will resonate with engineers getting started with ML and with others in any part of the organization trying to understand how ML works.\nDesigning Machine Learning Systems An Iterative Process for Production-Ready Applications\nDesigning Machine Learning Systems\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Designing Machine Learning Systems, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nEver since the first machine learning course I taught at Stanford in 2017, many people have asked me for advice on how to deploy ML models at their organizations.\nThese questions can also be specific, such as “I’m convinced that switching from batch prediction to online prediction will give our model a performance boost, but how do I convince my manager to let me do so?” or “I’m the most senior data scientist at my company and I’ve recently been tasked with setting up our first machine learning platform; where do I start?”\nThey are complex because they consist of many different components (ML algorithms, data, business logic, evaluation metrics, underlying infrastructure, etc.) and involve many different stakeholders (data scientists, ML engineers, business leaders, users, even society at large).\nML systems are unique because they are data dependent, and data varies wildly from one use case to the next.\nFor example, two companies might be in the same domain (ecommerce) and have the same problem that they want ML to solve (recommender system), but their resulting ML systems can have different model architecture, use different sets of features, be evaluated on different metrics, and bring different returns on investment.\nThis book takes a holistic approach to ML systems.\nThe content in this book is illustrated using actual case studies, many of which I’ve personally worked on, backed by ample references, and reviewed by ML practitioners in both academia and industry.\nThis book is for anyone who wants to leverage ML to solve real-world problems.\nML in this book refers to both deep learning and classical algorithms, with a leaning toward ML systems at scale, such as those seen at medium to large enterprises and fast-growing startups.\nBecause my background is engineering, the language of this book is geared toward engineers, including ML engineers, data scientists, data engineers, ML platform engineers, and engineering managers.\nEach ML use case in your organization has been deployed using its own workflow, and you want to lay down the foundation (e.g., model store, feature store, monitoring tools) that can be shared and reused across use cases.",
    "keywords": [
      "Machine Learning Systems",
      "Machine Learning",
      "Designing Machine Learning",
      "Learning Systems",
      "applied machine learning",
      "Chip Huyen",
      "Learning",
      "Machine",
      "machine learning applications",
      "Reliable Machine Learning",
      "book",
      "Systems",
      "Machine Learning OceanofPDF.com",
      "Designing Machine",
      "Lab Chip Huyen"
    ],
    "concepts": [
      "data",
      "learning",
      "learn",
      "learned",
      "engineers",
      "engineering",
      "engineer",
      "process",
      "processing",
      "processes"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 10-17)",
    "start_page": 10,
    "end_page": 17,
    "summary": "What This Book Is Not\nThis book is not an introduction to ML.\nThere are many books, courses, and resources available for ML theories, and therefore, this book shies away from these concepts to focus on the practical aspects of ML.\nthis book has few code snippets and instead focuses on providing a lot of discussion around trade-offs, pros and cons, and concrete examples.\nNavigating This Book\nThe chapters in this book are organized to reflect the problems data scientists might encounter as they progress through the lifecycle of an ML project.\nChapters 4 to 6 cover the pre-deployment phase of an ML project: from creating the training data and engineering features to developing and evaluating your models in a development environment.\nI debated for a long time on how deep to go into data systems and where to introduce it in the book.\nearly will help us get on the same page to discuss data matters in the rest of the book.\nWhile we cover many technical aspects of an ML system in this book, ML systems are built by people, for people, and can have outsized impact on the life of many.\nIt’d be remiss to write a book on ML production without a chapter on the human side of it, which is the focus of Chapter 11, the last chapter.\nIn this book, we use “data scientist” as an umbrella term to include anyone who works developing and deploying ML models, including people whose job titles might be ML engineers, data engineers, data analysts, etc.\nCode snippets used in this book\nConventions Used in This Book\nThis book is here to help you get your job done.\nIn general, if example code is offered with this book, you may use it in your programs and documentation.\nFor example, writing a program that uses several chunks of code from this book does not require permission.\nSelling or distributing examples from O’Reilly books does require permission.\nIncorporating a significant amount of example code from this book into your product’s documentation does require permission.\nWe have a web page for this book, where we list errata, examples, and any additional information.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\nFor news and information about our books and courses, visit https://oreilly.com.\nThe system also includes the business requirements that gave birth to the ML project in the first place, the interface where users and developers interact with your system, the data stack, and the logic for developing, monitoring, and updating your models, as well as the infrastructure that enables the delivery of that logic.\nFigure 1-1 shows you the different components of an ML system and in which chapters of this book they will be covered.",
    "keywords": [
      "Book",
      "data",
      "data systems",
      "Machine Learning Systems",
      "code",
      "systems",
      "Learning",
      "system",
      "chapters",
      "Machine Learning",
      "books",
      "Learning Systems",
      "project",
      "concepts",
      "Online Learning"
    ],
    "concepts": [
      "book",
      "books",
      "examples",
      "learn",
      "learning",
      "learned",
      "including",
      "include",
      "includes",
      "systems"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 18-25)",
    "start_page": 18,
    "end_page": 25,
    "summary": "Machine learning is an approach to (1) learn (2) complex patterns from (3) existing data and use these patterns to make (4) predictions on (5) unseen data.\nFor an ML system to learn, there must be something for it to learn from.\nIn most cases, ML systems learn from data.\nFor example, if you want to build an ML system to learn to predict the rental price for Airbnb listings, you need to provide a dataset where each input is a listing with relevant characteristics (square footage, number of rooms, neighborhood, amenities, rating of that listing, etc.) and the associated output is the rental price of that listing.\nOnce learned, this ML system should be able to predict the price of a new listing given its characteristics.\nML solutions are only useful when there are patterns to learn.\nSane people don’t invest money into building an ML system to predict the next outcome of a fair die because there’s no pattern in how these outcomes are generated.\nHowever, there are patterns in how stocks are priced, and therefore companies have invested billions of dollars in building ML systems to learn those patterns.\nInstead of telling your system how to calculate the price from a list of characteristics, you can provide prices and characteristics, and let your ML system figure out the pattern.\nInstead of requiring hand-specified patterns to calculate outputs, ML solutions learn patterns from inputs and outputs\nBecause ML learns from data, there must be data for it to learn from.\nIn the zero-shot learning (sometimes known as zero-data learning) context, it’s possible for an ML system to make good predictions for a task without having been trained on data for that task.\nHowever, this ML system was previously trained on data for other tasks, often related to the task in consideration.\nIt’s also possible to launch an ML system without data.\nFor example, in the context of continual learning, ML models can be deployed without having been trained on any data, but they will learn from incoming data\nWithout data and without continual learning, many companies follow a “fake-it-til-you make it” approach: launching a product that serves predictions made by humans, instead of ML models, with the hope of using the generated data to train ML models later.\nML models make predictions, so they can only solve problems that require predictive answers.\nAs predictive machines (e.g., ML models) are becoming more effective, more and more problems are being reframed as predictive problems.\nInstead of computing the exact outcome of a process, which might be even more computationally costly and time-consuming than ML, you can frame the problem as: “What would the outcome of this process look like?” and approximate it using an ML model.\nDue to the way most ML algorithms today learn, ML solutions will especially shine if your problem has these additional following characteristics:\nDespite exciting progress in few-shot learning research, most ML algorithms still require many examples to learn a pattern.\nML solutions often require nontrivial up-front investment on data, compute, infrastructure, and talent, so it’d make sense if we can use these solutions a lot.\nHaving a problem at scale also means that there’s a lot of data for you to collect, which is useful for training ML models.",
    "keywords": [
      "Developments and Operations",
      "SYSTEMS DESIGN Ops",
      "data",
      "system",
      "learn",
      "patterns",
      "DESIGN Ops",
      "short for Developments",
      "SYSTEMS",
      "n’t",
      "predictions",
      "pattern",
      "solutions",
      "model",
      "models"
    ],
    "concepts": [
      "predictions",
      "predict",
      "predictive",
      "prediction",
      "predicting",
      "predicts",
      "learning",
      "learn",
      "learned",
      "learns"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 26-33)",
    "start_page": 26,
    "end_page": 33,
    "summary": "Because ML learns from data, you can update your ML model with new data without having to figure out how the data has changed.\nThe list of use cases can go on and on, and it’ll grow even longer as ML adoption matures in the industry.\nWe’ll go over one case study where the use of ML algorithms can be argued as unethical in the section “Case study I: Automated grader’s biases”.\nFor example, if you can’t build a chatbot to answer all your customers’ queries, it might be possible to build an ML model to predict whether a query matches one of the frequently asked questions.\nML has found increasing usage in both enterprise and consumer applications.\nSince the mid-2010s, there has been an explosion of applications that leverage ML to deliver superior or previously impossible services to consumers.\nWith the explosion of information and services, it would have been very challenging for us to find what we want without the help of ML, manifested in either a search engine or a recommender system.\nTyping on your phone is made easier with predictive typing, an ML system that gives you suggestions on what you might want to say next.\nThe ML use case that drew me into the field was machine translation, automatically translating from one language to another.\nEven though the market for consumer ML applications is booming, the majority of ML use cases are still in the enterprise world.\nEnterprise ML applications tend to have vastly different requirements and considerations from consumer applications.\nFor people interested in building companies out of ML applications, consumer apps might be easier to distribute but much harder to monetize.\nAccording to Algorithmia’s 2020 state of enterprise machine learning survey, ML applications in enterprises are diverse, serving both internal use cases (reducing costs, generating customer insights and intelligence, internal processing automation) and external use cases (improving customer experience, retaining customers, interacting with customers) as shown in Figure 1-3.8\nFraud detection is among the oldest applications of ML in the enterprise world.\nBy leveraging ML solutions for anomaly detection, you can have systems that learn from historical fraud transactions and predict whether a future transaction is fraudulent.\nML-based pricing optimization is most suitable for cases with a large number of transactions where demand fluctuates and consumers are willing to pay a dynamic price—for example, internet ads, flight tickets, accommodation bookings, ride-sharing, and events.\nThis can be done through better identifying potential customers, showing better-targeted ads, giving discounts at the right time, etc.—all of which are suitable tasks for ML.\nAn ML system can analyze the ticket content and predict where it should go, which can shorten the response time and improve customer satisfaction.\nAnother popular use case of ML in enterprise is brand monitoring.\nA set of ML use cases that has generated much excitement recently is in health care.\nIn this section, we’ll go over how ML systems are different from both ML in research (or as often taught in school) and traditional software, which motivates the need for this book.\nAs ML usage in the industry is still fairly new, most people with ML expertise have gained it through academia: taking courses, doing research, reading academic papers.\nML in production is very different from ML in research.",
    "keywords": [
      "data",
      "Machine Learning",
      "cases",
      "n’t",
      "system",
      "customer",
      "enterprise",
      "Learning",
      "enterprise machine learning",
      "applications",
      "customers",
      "’ll",
      "systems",
      "Machine Learning Systems",
      "research"
    ],
    "concepts": [
      "customers",
      "customer",
      "different",
      "differences",
      "learning",
      "learn",
      "requires",
      "requirements",
      "case",
      "enterprise"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 34-41)",
    "start_page": 34,
    "end_page": 41,
    "summary": "Having different, often conflicting, requirements can make it difficult to design, develop, and select an ML model that satisfies all the requirements.\nWant a model that recommends restaurants that users will most likely order from, and they believe they can do so by using a more complex model with more data.\nNotices that every increase in latency leads to a drop in orders through the service, so they want a model that can return the recommended restaurants in less than 100 milliseconds.\n“Recommending the restaurants that users are most likely to click on” and “recommending the restaurants that will bring in the most money for the app” are two different objectives, and in the section “Decoupling objectives”, we’ll discuss how to develop an ML system that satisfies different objectives.\nEnsembling combines “multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.” While it can give your ML system a small performance improvement, ensembling tends to make a system too complex to be useful in production, e.g., slower to make predictions or harder to interpret the results.\nDuring the model development process, you might train many different models, and each model does multiple passes over the training data.\nIn this book, to simplify the discussion and to be consistent with the terminology used in the ML community, we use latency to refer to the response time, so the latency of a request measures the time from when the request is sent to the time a response is received.\nFor example, the average latency of Google Translate is the average time it takes from when a user clicks Translate to when the translation is shown, and the throughput is how many queries it processes and serves a second.\nIf your system always processes one query at a time, higher latency means lower throughput.\nIf the average latency is 10 ms, which means it takes 10 ms to process a query, the throughput is 100 queries/second.\nIf the average latency is 100 ms, the throughput is 10 queries/second.\nHowever, because most modern distributed systems batch queries to process them together, often concurrently, higher latency might also mean higher throughput.\nIf you process 10 queries at a time and it takes 10 ms to run a batch, the average latency is still 10 ms but the throughput is now 10 times higher—1,000 queries/second.\nIf you process 50 queries at a time and it takes 20 ms to run a batch, the average latency now is 20 ms and the throughput is 2,500 queries/second.\nThe difference in latency and throughput trade-off for processing queries one at a time and processing queries in batches is illustrated in Figure 1-4.\nWhen processing queries one at a time, higher latency means lower throughput.\nWhen processing queries in batches, however, higher latency might also mean higher throughput.\nBatching requires your system to wait for enough queries to arrive in a batch before processing them, which further increases latency.\nIn research, you care more about how many samples you can process in a second (throughput) and less about how long it takes for each sample to be processed (latency).\nTo reduce latency in production, you might have to reduce the number of queries you can process on the same hardware at a time.\nIt’s tempting to simplify this distribution by using a single number like the average (arithmetic mean) latency of all the requests within a time window, but this number can be misleading.\nIt’s a common practice to use high percentiles to specify the performance requirements for your system; for example, a product manager might specify that the 90th percentile or 99.9th percentile latency of a system must be below a certain number.\nDuring the research phase, a model is not yet used on people, so it’s easy for researchers to put off fairness as an afterthought: “Let’s try to get state of the art first and worry about fairness when we get to production.” When it gets to production, it’s too late.",
    "keywords": [
      "model",
      "latency",
      "time",
      "data",
      "queries",
      "throughput",
      "average latency",
      "system",
      "models",
      "production",
      "users",
      "restaurants",
      "higher latency",
      "research",
      "process"
    ],
    "concepts": [
      "latency",
      "latencies",
      "data",
      "model",
      "models",
      "production",
      "product",
      "time",
      "times",
      "requirements"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 42-49)",
    "start_page": 42,
    "end_page": 49,
    "summary": "ML algorithms don’t predict the future, but encode the past, thus perpetuating the biases in the data and more.\nSince most ML research is still evaluated on a single objective, model performance, researchers aren’t incentivized to work on model interpretability.\nAs ML research and off-the-shelf models become more accessible, more people and organizations would want to find applications for them, which increases the demand for ML in production.\nOn the contrary, ML systems are part code, part data, and part artifacts created from the two.\nInstead of focusing on improving ML algorithms, most companies will focus on improving their data.\nBecause data can change quickly, ML applications need to be adaptive to the changing environment, which might require faster development and deployment cycles.\nWith ML, we have to test and version our data too, and that’s the hard part.\nThe size of ML models is another challenge.\nAs of 2022, it’s common for ML models to have hundreds of millions, if not billions, of parameters, which requires gigabytes of random-access memory (RAM) to load them into memory.\nThis chapter also highlighted the differences between ML in research and ML in production.\nWe also discussed how ML systems differ from traditional software systems, which motivated the need for this book.\nData scientists and ML engineers working with ML systems in production will likely find that focusing only on the ML algorithms part is far from enough.\nThis book takes a system approach to developing ML systems, which means that we’ll consider all components of a system holistically instead of just looking at ML algorithms.\n1 Mike Schuster, Melvin Johnson, and Nikhil Thorat, “Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System,” Google AI Blog, November 22, 2016, https://oreil.ly/2R1CB.\n8 “2020 State of Enterprise Machine Learning,” Algorithmia, 2020, https://oreil.ly/wKMZB.\n2019, by User Action and Operating System,” Statista, 2019, https://oreil.ly/2pTCH.\n13 Marty Swant, “The World’s 20 Most Valuable Brands,” Forbes, 2020, https://oreil.ly/4uS5i.\nSee also Sejuti Das’s analysis “How Data Scientists Are Also Susceptible to the Layoffs Amid Crisis,” Analytics India Magazine, May 21, 2020, https://oreil.ly/jobmz.\n“Ensemble learning,” https://oreil.ly/5qkgp.\n16 Julia Evans, “Machine Learning Isn’t Kaggle Competitions,” 2014, https://oreil.ly/p8mZq.\nLeaderboards,” EMNLP, 2020, https://oreil.ly/4Ud8P.\nModels: 6 Lessons Learned at Booking.com,” KDD ’19, August 4–8, 2019, Anchorage, AK, https://oreil.ly/G5QNA.\n22 “Consumer Insights,” Think with Google, https://oreil.ly/JCp6Z.\nFind,” CBS News, November 15, 2019, https://oreil.ly/UiHUB.\non Deep Learning Systems Using Data Poisoning,” arXiv, December 15, 2017, https://oreil.ly/OkAjb.\n33 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv, October 11, 2018, https://oreil.ly/TG3ZW.\nTo reiterate from the first chapter, ML systems design takes a system approach to MLOps, which means that we’ll consider an ML system holistically to ensure that all the components—the business requirements, the data stack, infrastructure, deployment, monitoring, etc.—and their stakeholders can work together to satisfy the specified objectives and requirements.\nIf this system is built for a business, it must be driven by business objectives, which will need to be translated into ML objectives to guide the development of ML models.\nOnce everyone is on board with the objectives for our ML system, we’ll need to set out some requirements to guide the development of this system.\nYou might wonder: with all these objectives, requirements, and processes in place, can I finally start building my ML model yet?\nWe’ll continue this chapter with how to frame your ML problems.",
    "keywords": [
      "Machine Learning Systems",
      "data",
      "system",
      "systems",
      "Machine Learning",
      "Learning Systems",
      "models",
      "model",
      "n’t",
      "Deep Learning Systems",
      "Machine Learning Models",
      "applications",
      "Learning",
      "part",
      "’ll"
    ],
    "concepts": [
      "models",
      "model",
      "data",
      "applicants",
      "applications",
      "application",
      "researchers",
      "research",
      "changing",
      "change"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 50-57)",
    "start_page": 50,
    "end_page": 57,
    "summary": "Business and ML Objectives\nWhen working on an ML project, data scientists tend to care about the ML objectives: the metrics they can measure about the performance of their ML models such as accuracy, F1 score, inference latency, etc.\nBut the truth is: most companies don’t care about the fancy ML metrics.\nA pattern I see in many short-lived ML projects is that the data scientists become too focused on hacking ML metrics without paying attention to business metrics.\nTheir managers, however, only care about business metrics and, after failing to see how an ML project can help push their business metrics, kill the projects prematurely (and possibly let go of the data science team involved).\nWhat business performance metrics is the new ML system supposed to influence, e.g., the amount of ads revenue, the number of monthly active users?\nOne of the reasons why predicting ad click-through rates and fraud detection are among the most popular use cases for ML today is that it’s easy to map ML models’ performance to business metrics: every increase in click-through rate results in actual ad revenue, and every fraudulent transaction stopped results in actual money saved.\nMany companies create their own metrics to map business metrics to ML metrics.\nThe effect of an ML project on business objectives can be hard to reason about.\nFor example, an ML model that gives customers more personalized solutions can make them happier, which makes them spend more money on\nThe same ML model can also solve their problems faster, which makes them spend less money on your services.\nTo gain a definite answer on the question of how ML metrics influence business metrics, experiments are often needed.\nMany companies do that with experiments like A/B testing and choose the model that leads to better business metrics, regardless of whether this model has better ML metrics.\nYet, even rigorous experiments might not be sufficient to understand the relationship between an ML model’s outputs and business metrics.\nImagine you work for a cybersecurity company that detects and stops security threats, and ML is just a component in their complex process.\nAn ML model is used to detect anomalies in the traffic pattern.\nWhen this process fails to stop a threat, it might be impossible to figure out whether the ML component has anything to do with it.\nAccording to a 2020 survey by Algorithmia, among companies that are more sophisticated in their ML adoption (having had models in production for over five years), almost 75% can deploy a model in under 30 days.\nAmong those just getting started with their ML pipeline, 60% take over 30 days to deploy a model (see Figure 2-1).\nHow long it takes for a company to bring a model to production is proportional to how long it has used ML.\nRequirements for ML Systems\nWe’ll discuss how ML systems fail in production in Chapter 8.\nHowever, as your company’s user base grows, the number of prediction requests your ML system serves daily fluctuates between 1 million and 10 million.\nAn ML system might grow in ML model count.\nBecause scalability is such an important topic throughout the ML project workflow, we’ll discuss it in different parts of the book.\nBecause ML systems are part code, part data, and data can change quickly, ML systems need to be able to evolve quickly.\nFor example, here is one workflow that you might encounter when building an ML model to predict whether an ad should be shown when users enter a search query:",
    "keywords": [
      "model",
      "system",
      "business metrics",
      "metrics",
      "data",
      "Business",
      "systems",
      "recommender system",
      "models",
      "’ll",
      "performance",
      "companies",
      "n’t",
      "business performance metrics",
      "section"
    ],
    "concepts": [
      "data",
      "business",
      "businesses",
      "process",
      "processing",
      "prediction",
      "predictive",
      "predict",
      "predictions",
      "predicts"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 58-71)",
    "start_page": 58,
    "end_page": 71,
    "summary": "Figure 2-2 shows an oversimplified representation of what the iterative process for developing ML systems in production looks like from the perspective of a data scientist or an ML engineer.\nThis process looks different from the perspective of an ML platform engineer or a DevOps engineer, as they might not have as much context into model development and might spend a lot more time on setting up infrastructure.\nWe already discussed different stakeholders and some of the foci for ML projects in production in Chapter 1.\nA vast majority of ML models today learn from data, so developing ML models starts with engineering data.\nML model development\nWith the initial set of training data, we’ll need to extract features and develop initial models leveraging these features.\nIn Chapter 6, we’ll discuss model selection, training, and evaluation.\nAfter a model is developed, it needs to be made accessible to users.\nWe’ll discuss different ways to deploy an ML model in Chapter 7.\nFraming ML Problems\nAn ML problem is defined by inputs, outputs, and the objective function that guides the learning process—none of these three components are obvious from your boss’s request.\nIt’s your job, as a seasoned ML engineer, to use your knowledge of what problems ML can solve to frame this request as an ML problem.\nYou can alleviate this bottleneck by developing an ML model to predict which of these four departments a request should go to.\nWe’ll discuss extensively how to extract features from raw data to input into your ML model in Chapter 5.\nIn this section, we’ll focus on two aspects: the output of your model and the objective function that guides the learning process.\nThe output of your model dictates the task type of your ML problem.\nThe most general types of ML tasks are classification and regression.\nClassification models classify inputs into different categories.\nAn example is a house prediction model that outputs the price of a given house.\nA regression model can easily be framed as a classification model and vice versa.\nThe email classification model can become a regression model if we make it output values between 0 and 1, and decide on a threshold to determine which values should be SPAM (for example, if the value is above 0.5, the email is spam), as shown in Figure 2-4.\nWhen there are more than two classes, the problem becomes multiclass classification.\nIn my experience, ML models typically need at least 100 examples for each class to learn to classify that class.\nIn both binary and multiclass classification, each example belongs to exactly one class.\nWhen an example can belong to multiple classes, we have a multilabel classification problem.\nIn multiclass classification, if there are four possible classes [tech, entertainment, finance, politics] and the label for an example is entertainment, you represent this label with the vector [0, 1, 0, 0].\nFor the article classification problem, you can have four models corresponding to four topics, each model outputting whether an article is in that topic or not.\nA naive setup would be to frame this as a multiclass classification task—use the user’s and environment’s features (user demographic information, time, location, previous apps used) as input, and output a probability distribution for every single app on the user’s phone.\nGiven the problem of predicting the app a user will most likely open next, you can frame it as a classification problem.\nIn this framing, for a given user at a given time, there are N predictions to make, one for each app, but each prediction is just a number.\nGiven the problem of predicting the app a user will most likely open next, you can frame it as a regression problem.\nIn this new framing, whenever there’s a new app you want to consider recommending to a user, you simply need to use new inputs with this new app’s feature instead of having to retrain your model or part of your model from scratch.\nTo learn, an ML model needs an objective function to guide the learning process.\nFor supervised ML, this loss can be computed by comparing the model’s outputs with the ground truth labels using a measurement like root mean squared error (RMSE) or cross entropy.\nthis model, given this example, is the cross entropy of [0.45, 0.2, 0.02, 0.33] relative to [0, 0, 0, 1].\nComing up with meaningful objective functions requires algebra knowledge, so most ML engineers just use common loss functions like RMSE or MAE (mean absolute error) for regression, logistic loss (also log loss) for binary classification, and cross entropy for multiclass classification.\nFraming ML problems can be tricky when you want to minimize multiple objective functions.\nYou want to minimize engagement_loss: the difference between each post’s predicted clicks and its actual number of clicks.\nA problem with this approach is that each time you tune α and β—for example, if the quality of your users’ newsfeeds goes up but users’ engagement goes down, you might want to decrease α and increase β— you’ll have to retrain your model.\nAnother approach is to train two different models, each optimizing one loss.\nSo you have two models:\nengagement_model\nMinimizes engagement_loss and outputs the predicted number of clicks of each post\nIn general, when there are multiple objectives, it’s a good idea to decouple them first because it makes model development and maintenance easier.",
    "keywords": [
      "model",
      "classification",
      "data",
      "problem",
      "models",
      "classification problem",
      "multiclass classification",
      "objective function",
      "user",
      "loss",
      "objective",
      "Problems",
      "quality",
      "binary classification problems",
      "classification task"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "classification",
      "classifications",
      "step",
      "steps",
      "users",
      "user",
      "different"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 72-79)",
    "start_page": 72,
    "end_page": 79,
    "summary": "We just have more data.”\nIf you want to use data science, a discipline of which ML is a part of, to improve your products or processes, you need to start with building out your data, both in terms of quality and quantity.\nWe ended the chapter on a philosophical discussion of the role of data in ML systems.\nHowever, the success of systems including AlexNet, BERT, and GPT showed that the progress of ML in the last decade relies on having access to a large amount of data.\nNow that we’ve covered the high-level overview of an ML system in production, we’ll zoom in to its building blocks in the following chapters, starting with the fundamentals of data engineering in the next chapter.\n“Pareto optimization,” https://oreil.ly/NdApy. While you’re at it, you might also want to read Jin and Sendhoff’s great paper on applying Pareto optimization for ML, in which the authors claimed that “machine learning is inherently a multiobjective task” (Yaochu Jin and Bernhard Sendhoff, “Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies,” IEEE Transactions on Systems, Man, and Cybernetics—Part C: Applications and Reviews 38, no.\n21 Alon Halevy, Peter Norvig, and Fernando Pereira, “The Unreasonable Effectiveness of Data,”\n23 Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson, “One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,” arXiv, December 11, 2013, https://oreil.ly/1AdO6.\nLarge data systems, even without ML, are complex.\nIn this chapter, we’ll cover the basics of data engineering that will, hopefully, give you a steady piece of land to stand on as you explore the landscape for your own needs.\nWe’ll start with different sources of data that you might work with in a typical ML project.\nWe’ll continue to discuss the formats in which data can be stored.\nWe’ll continue to discuss data storage engines, also known as databases, for the two major types of processing: transactional and analytical.\nWhen working with data in production, you usually work with data across multiple processes and services.\nIn the following section of the chapter, we’ll discuss different modes of data passing across processes.\nDuring the discussion of different modes of data passing, we’ll learn about two distinct types of data: historical data in data storage engines, and streaming data in real-time transports.\nThese two different types of data require different processing paradigms, which we’ll discuss in the section “Batch Processing Versus Stream Processing”.\nKnowing how to collect, process, store, retrieve, and process an increasingly growing amount of data is essential to people who want to build ML systems in production.\nIf you’re already familiar with data systems, you might want to move directly to Chapter 4 to learn more about how to sample and generate labels to create training data.\nData Sources\nAn ML system can work with data from many different sources.\nIf it’s even remotely possible for users to input wrong data, they are going to do it.\nUser input data requires more heavy-duty checking and processing.\nTherefore, user input data tends to require fast processing.\nAnother source is system-generated data.\nThis is the data generated by different components of your systems, which include various types of logs and system outputs such as model predictions.\nThey can record the results of different jobs, including large batch jobs for data processing and model training.\nBecause logs are system generated, they are much less likely to be malformatted the way user input data is.\nOverall, logs don’t need to be processed as soon as they arrive, the way you would want to process user input data.",
    "keywords": [
      "data",
      "user input data",
      "system",
      "input data",
      "systems",
      "Seeking an improvement",
      "logs",
      "data science",
      "data engineering",
      "data systems",
      "Monica Rogati",
      "user input",
      "Processing",
      "’ll",
      "input"
    ],
    "concepts": [
      "data",
      "processes",
      "process",
      "processing",
      "processed",
      "difference",
      "different",
      "log",
      "logs",
      "systems"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 80-87)",
    "start_page": 80,
    "end_page": 87,
    "summary": "Data Formats\nIt’s important to think about how the data will be used in the future so that the format you use will make sense.\nHow do I store multimodal data, e.g., a sample that might contain both images and texts?\nWhere do I store my data so that it’s cheap and still fast to access?\nThe process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later is data serialization.\nThere are many, many data serialization formats.\nWhen considering a format to work with, you might want to consider different characteristics such as human readability, access patterns, and whether it’s based on text or binary, which influences the size of its files.\nFor example, your data can be stored in a structured format like the following:\nThe same data can also be stored in an unstructured blob of text like the following:\nOnce you’ve committed the data in your JSON files to a schema, it’s pretty painful to retrospectively go back\nRow-Major Versus Column-Major Format\nBecause modern computers process sequential data more efficiently than nonsequential data, if a table is row-major, accessing its rows will be faster than accessing its columns in expectation.\nThis means that for row-major formats, accessing data by rows is expected to be faster than accessing data by columns.\nIf we consider each example as a row and each feature as a column, as is often the case in ML, then the row- major formats like CSV are better for accessing examples, e.g., accessing all the examples collected today.\nColumn-major formats like Parquet are better for accessing features, e.g., accessing the timestamps of all your examples.\nColumn-major formats allow flexible column-based reads, especially if your data is large with thousands, if not millions, of features.\nRow-major formats allow faster data writes.\nFor each individual example, it’d be much faster to write it to a file where your data is already in a row-major format.\nPeople coming to pandas from NumPy tend to treat DataFrame the way they would ndarray, e.g., trying to access data by rows, and find DataFrame slow.\nI use CSV as an example of the row-major format because it’s popular and generally recognizable by everyone I’ve talked to in tech.\nHowever, some of the early reviewers of this book pointed out that they believe CSV to be a horrible data format.\nCSV and JSON are text files, whereas Parquet files are binary files.\nA program has to know exactly how the data inside the binary file is laid out to make use of the file.\nAs an illustration, I use interviews.csv, which is a CSV file (text format) of 17,654 rows and 10 columns.\nWhen I converted it to a binary format (Parquet), the file size went from 14 MB to 6 MB, as shown in Figure 3-3.\nWhen stored in CSV format, my interview file is 14 MB.\nData Models\nThis is another data model for cars.",
    "keywords": [
      "data",
      "format",
      "binary",
      "Formats",
      "text",
      "files",
      "CSV",
      "binary files",
      "Versus Binary Format",
      "Parquet",
      "file",
      "text files",
      "Binary Format CSV",
      "JSON",
      "Binary Format"
    ],
    "concepts": [
      "data",
      "formats",
      "format",
      "texts",
      "text",
      "access",
      "accessing",
      "binary",
      "advertiser",
      "advertisers"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 88-98)",
    "start_page": 88,
    "end_page": 98,
    "summary": "Relational Model\nIn this model, data is organized into relations; each relation is a set of tuples.\nData following the relational model is usually stored in file formats like CSV or Parquet.\nYou can join the data from different relations back together, but joining can be expensive for large tables.\nDatabases built around the relational data model are relational databases.\nOnce you’ve put data in your databases, you’ll want a way to retrieve it.\nThe language that you can use to specify the data that you want from a database is called a query language.\nEven though inspired by the relational model, the data model behind SQL has deviated from the original relational model.\nWith an SQL database, you specify the pattern of data you want—the tables you want the data from, the conditions the results must meet, the basic data transformations such as join, sort, group, aggregate, etc.—but not how to retrieve the data.\ndatabase systems, and normalization means that data is spread out on multiple relations, which makes joining it together even harder.\nYou give the system your data (inputs and outputs) and specify the number of models you want to experiment.\nThe relational data model has been able to generalize to a lot of use cases, from ecommerce to finance to social networks.\nThe latest movement against the relational data model is NoSQL.\nOriginally started as a hashtag for a meetup to discuss nonrelational databases, NoSQL has been retroactively reinterpreted as Not Only SQL, Two major types of nonrelational models are the document model and the graph model.\nThe document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare.\nThe graph model goes in the opposite direction, targeting use cases where relationships between data items are common and important.\nas many NoSQL data systems also support relational models.\nDocument model\nA collection of documents could be considered analogous to a table in a relational database, and a document analogous to a row.\nFor example, you can convert the book data in Tables 3-3 and 3-4 into three JSON documents as shown in Examples 3-1, 3-2, and 3-3.\nAll rows in a table must follow the same schema (e.g., have the same sequence of columns), while documents in the same collection can have completely different schemas.\n{ \"Title\": \"Harry Potter\", \"Author\": \"J .K. Rowling\", \"Publisher\": \"Banana Press\", \"Country\": \"UK\", \"Sold as\": [ {\"Format\": \"Paperback\", \"Price\": \"$20\"}, {\"Format\": \"E-book\", \"Price\": \"$10\"} ] }\n{ \"Title\": \"Sherlock Holmes\", \"Author\": \"Conan Doyle\", \"Publisher\": \"Guava Press\", \"Country\": \"US\", \"Sold as\": [ {\"Format\": \"Paperback\", \"Price\": \"$30\"}, {\"Format\": \"E-book\", \"Price\": \"$15\"}\n{ \"Title\": \"The Hobbit\", \"Author\": \"J.R.R. Tolkien\", \"Publisher\": \"Banana Press\", \"Country\": \"UK\", \"Sold as\": [ {\"Format\": \"Paperback\", \"Price\": \"$30\"}, ] }\nBecause the document model doesn’t enforce a schema, it’s often referred to as schemaless.\nDocument databases just shift the responsibility of assuming structures from the application that writes the data to the application that reads the data.\nThe document model has better locality than the relational model.\nConsider the book data example in Tables 3-3 and 3-4 where the information about a book is spread across both the Book table and the Publisher table (and potentially also the Format table).\nIn the document model, all information about a book can be stored in a document, making it much easier to retrieve.\nHowever, compared to the relational model, it’s harder and less efficient to execute joins across documents compared to across tables.\nBecause of the different strengths of the document and relational data models, it’s common to use both models for different tasks in the same database systems.\nA database that uses graph structures to store its data is called a graph database.\nIf in document databases, the content of each document is the priority, then in graph databases, the relationships between data items are the priority.\nBecause the relationships are modeled explicitly in graph models, it’s faster to retrieve data based on relationships.\nGiven this graph, you can start from the node USA and traverse the graph following the edges “within” and “born_in” to find all the nodes of the type “person.” Now, imagine that instead of using the graph model to represent this data, we use the relational model.\nMany queries that are easy to do in one data model are harder to do in another data model.\nPicking the right data model for your application can make your life so much easier.\nStructured data follows a predefined data model, also known as a data schema.\nFor example, the data model might specify that each data item consists of two values: the first value, “name,” is a string of at most 50 characters, and the second value, “age,” is an 8-bit integer in the range between 0 and 200.\nOne of the strangest bugs one of my colleagues encountered was when they could no longer use users’ ages with their transactions, and their data schema replaced all the null ages with 0, and their ML model thought the transactions were made by people 0 years old.\nFor example, a text file of logs generated by your ML model is unstructured data.\nFor example, if your storage follows a schema, you can only store data following that schema.\nBut if your storage doesn’t follow a schema, you can store any type of data.",
    "keywords": [
      "data",
      "Model",
      "data model",
      "Press Banana Press",
      "Paperback Paperback Publisher",
      "Paperback Paperback Paperback",
      "Banana Press",
      "relational data model",
      "document model",
      "Press Guava Press",
      "document",
      "Relational Model",
      "Banana Press Guava",
      "Doyle Format Paperback",
      "Paperback E-book Paperback"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "modeled",
      "different",
      "difference",
      "differences",
      "query",
      "queried",
      "queries"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 99-107)",
    "start_page": 99,
    "end_page": 107,
    "summary": "Data Storage Engines and Processing\nEven though these different transactions involve different types of data, the way they’re processed is similar across applications.\nThis also means that transactional databases might not be efficient for questions such as “What’s the average price for all the rides in September in San Francisco?” This kind of analytical question requires aggregating data in columns across multiple rows of data.\nSecond, in the traditional OLTP or OLAP paradigms, storage and processing are tightly coupled —how data is stored is also how data is processed.\nThis may result in the same data being stored in multiple databases and using different processing engines to solve different types of queries.\nAccording to Wikipedia, online processing means data is immediately available for input/output.\nETL refers to the general purpose processing and aggregating of data into the shape and the format that you want.\nTransform is the meaty part of the process, where most of the data processing is done.\nWhichever application needs data can just pull out raw data from there and process it.” This process of loading data into storage first then processing it later is sometimes called ELT (extract, load, transform).\nThis paradigm allows for the fast arrival of data since there’s little processing needed before data is stored.\nA question arises: how do we pass data between different processes that don’t share memory?\nWhen data is passed from one process to another, we say that the data flows from one process to another, which gives us a dataflow.\nData passing through databases\nData passing through services using requests such as the requests provided by REST and RPC APIs (e.g., POST/GET requests)\nData passing through a real-time transport like Apache Kafka and Amazon Kinesis\nData Passing Through Databases\nThe easiest way to pass data between two processes is through databases, which we’ve discussed in the section “Data Storage Engines and Processing”.\nFor example, to pass data from\nSecond, it requires both processes to access data from databases, and read/write from databases can be slow, making it unsuitable for applications with strict latency requirements—e.g., almost all consumer-facing applications.\nData Passing Through Services\nOne way to pass data between two processes is to send data directly through a network that connects these two processes.\nTo pass data from process B to process A, process A first sends a request to process B that specifies the data A needs, and B returns the requested data through the same network.\nThis mode of data passing is tightly coupled with the service-oriented architecture.\nFor B to be able to request data from A, A will also need to be exposed to B as a service.\nBecause the price depends on supply (the available drivers) and demand (the requested rides), the price optimization service needs data from both the driver management and ride management services.\nTheir detailed analysis is beyond the scope of this book, but one major difference is that REST was designed for requests over networks, whereas RPC “tries to make a request to a remote network service look the same as calling a function or method in your programming language.” Because of this, “REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organization, typically within the same data center.”\nData Passing Through Real-Time Transport\nIn the last section, we discussed how the price optimization service needs data from the ride and driver management services to predict the optimal price for each ride.\nSimilarly, the ride management service might also want data from the driver management and price optimization services.\nIf we pass data through services as discussed in the previous section, each of these services needs to send requests to the other two services, as shown in Figure 3-8.\nWith only three services, data passing is already getting complicated.\nRequest-driven data passing is synchronous: the target service has to listen to the request for the request to go through.\nIf the price optimization service requests data from the driver management service and the driver management service is down, the price optimization service will keep resending the request until it times out.\nA service that is down can cause all services that require data from it to be down.\nWhat if there’s a broker that coordinates data passing among services?\nInstead of having services request data directly from each other and creating a web of complex interservice data passing, each service only has to communicate with the broker, as shown in Figure 3-9.\nWhichever service wants data from the driver management service can check that broker for the most recent predicted number of drivers.\nTechnically, a database can be a broker—each service can write data to a database and other services that need the data can read from that database.\nReal-time transports can be thought of as in-memory storage for data passing among services.",
    "keywords": [
      "data",
      "Data passing",
      "service",
      "services",
      "Price optimization service",
      "databases",
      "Driver management service",
      "management service",
      "Processing",
      "process",
      "optimization service",
      "Driver management",
      "price",
      "price optimization",
      "passing"
    ],
    "concepts": [
      "data",
      "service",
      "services",
      "processing",
      "process",
      "processed",
      "processes",
      "different",
      "difference",
      "transaction"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 108-117)",
    "start_page": 108,
    "end_page": 117,
    "summary": "Historical data is often processed in batch jobs—jobs that are kicked off periodically.\nWhen data is processed in batch jobs, we refer to it as batch processing.\nBatch processing has been a research subject for many decades, and companies have come up with distributed systems like MapReduce and Spark to process batch data efficiently.\nWhen you have data in real-time transports like Apache Kafka and Amazon Kinesis, we say that you have streaming data.\nStream processing refers to doing computation on streaming data.\nComputation on streaming data can also be kicked off periodically, but the periods are usually much shorter than the periods for batch jobs (e.g., every five minutes instead of every day).\nComputation on streaming data can also be kicked off whenever the need arises.\nFor example, whenever a user requests a ride, you process your data stream to see what drivers are currently available.\nStream processing, when done right, can give low latency because you can process data as soon as data is generated, without having to first write it into databases.\nWith stream processing, it’s possible to continue computing only the new data each day and joining the new data computation with the older data computation, preventing redundancy.\nYou need infrastructure that allows you to process streaming data as well as batch data and join them together to feed into your ML models.\nTo do computation on data streams, you need a stream computation engine (the way Spark and MapReduce are batch computation engines).\nFor simple streaming computation, you might be able to get away with the built-in stream computation capacity of real-time transports like Apache Kafka, but Kafka stream processing is limited in its ability to deal with various data sources.\nStream processing is more difficult because the data amount is unbounded and the data comes in at variable rates and speeds.\nIn this chapter, we learned it’s important to choose the right format to store our data to make it easier to use the data in the future.\nWe continued the chapter with data storage engines and processing.\nWhen discussing data formats, data models, data storage engines, and processing, data is assumed to be within a process.\nHowever, while working in production, you’ll likely work with multiple processes, and you’ll likely need to transfer data between them.\nAs data in real-time transports have different properties from data in databases, they require different processing techniques, as discussed in the section “Batch Processing Versus Stream Processing”.\nData in databases is often processed in batch jobs and produces static features, whereas data in real-time transports is often processed using stream computation engines and produces dynamic features.\nhttps://oreil.ly/utf7z; Suresh H., “Snowflake Architecture and Key Concepts: A Comprehensive Guide,” Hevo blog, January 18, 2019, https://oreil.ly/GyvKl; Preetam Kumar, “Cutting the Cord: Separating Data from Compute in Your Data Lake with Object Storage,” IBM blog, September 21, 2017, https://oreil.ly/Nd3xD; “The Power of Separating Cloud Compute and Cloud Storage,” Teradata, last accessed April 2022, https://oreil.ly/f82gP.\nTraining Data\nDespite the importance of training data in developing and improving ML models, ML curricula are heavily skewed toward modeling, which is considered by many practitioners the “fun” part of the process.\nBut this is precisely the reason why data scientists and ML engineers should learn how to handle data well, saving us time and headache down the road.\nTraining data, in this chapter, encompasses all the data used in the developing phase of ML models, including the different splits used for training, validation, and testing (the train, validation, test splits).\nThis chapter starts with different sampling techniques to select data for training.\nLike other steps in building ML systems, creating training data is an iterative process.\nAs your model evolves through a project lifecycle, your training data will likely also evolve.\nHistorical data might be embedded with human biases, and ML models, trained on this data, can perpetuate them.\nSampling\nSampling happens in many steps of an ML project lifecycle, such as sampling from all possible real-world data to create training data; sampling from a given dataset to create splits for training, validation, and testing; or sampling from all possible events that happen within your ML system for monitoring purposes.\nIn this section, we’ll focus on sampling methods for creating training data, but these sampling methods can also be used for other steps in an ML project lifecycle.\nOne case is when you don’t have access to all possible data in the real world, the data that you use to train your model is a subset of real-world data, created by one sampling method or another.\nAnother case is when it’s infeasible to process all the data that you have access to—because it requires too much time or resources—so you have to sample that data to create a subset that is feasible to process.\nFor example, when considering a new model, you might want to do a quick experiment with a small subset of your data to see if the new model is promising first before training this new model on all your data.\nUnderstanding different sampling methods and how they are being used in our workflow can, first, help us avoid potential sampling biases, and second, help us choose the methods that improve the efficiency of the data we sample.\nNonprobability sampling is when the selection of data isn’t based on any probability criteria.\nSamples of data are selected based on their availability.\nYou select samples based on quotas for certain slices of data without any randomization.\nThe samples selected by nonprobability criteria are not representative of the real-world data and therefore are riddled with selection biases.\nBecause of these biases, you might think that it’s a bad idea to select data to train ML models using this family of sampling methods.\nUnfortunately, in many cases, the selection of data for ML models is still driven by convenience.\nA third example is data for training self-driving cars.\nNonprobability sampling can be a quick and easy way to gather your initial data to get your project off the ground.\nHowever, for reliable models, you might want to use probability-based sampling, which we will cover next.\nIf you randomly select 1% of your data, samples of this rare class will unlikely be selected.\nFor example, to sample 1% of data that has two classes, A and B, you can sample 1% of class A and 1% of class B.\nFor example, if you have three samples, A, B, and C, and want them to be selected with the probabilities of 50%, 30%, and 20% respectively, you can give them the weights 0.5, 0.3, and 0.2.\nFor example, if you know that a certain subpopulation of data, such as more recent data, is more valuable to your model and want it to have a higher chance of being selected, you can give it a higher weight.\nFor example, if in your data, red samples account for 25% and blue samples account for 75%, but you know that in the real world, red and blue have equal probability to happen, you can give red samples weights three times higher than blue samples.\nReservoir Sampling\nReservoir sampling is a fascinating algorithm that is especially useful when you have to deal with streaming data, which is usually what you have in production.\nImagine you have an incoming stream of tweets and you want to sample a certain number, k, of tweets to do analysis or train a model on.\nOne example where importance sampling is used in ML is policy-based reinforcement learning.\nDespite the promise of unsupervised ML, most ML models in production today are supervised, which means that they need labeled data to learn from.\nThe performance of an ML model still depends heavily on the quality and quantity of the labeled data it’s trained on.\nHe responded: “How long do we need an engineering team for?” Data labeling has gone from being an auxiliary task to being a core function of many ML teams in production.\nAnyone who has ever had to work with data in production has probably felt this at a visceral level: acquiring hand labels for your data is difficult for many, many reasons.\nHand labeling means that someone has to look at your data, which isn’t always possible if your data has strict privacy requirements.",
    "keywords": [
      "data",
      "Stream Processing",
      "sampling",
      "Processing",
      "Batch Processing",
      "training data",
      "streaming data",
      "Batch",
      "Stream",
      "data passing",
      "data storage engines",
      "Samples",
      "Apache Kafka",
      "sample",
      "data storage"
    ],
    "concepts": [
      "data",
      "sampling",
      "sample",
      "samples",
      "sampled",
      "label",
      "labels",
      "labeling",
      "labeled",
      "processing"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 118-125)",
    "start_page": 118,
    "end_page": 125,
    "summary": "Often, to obtain enough labeled data, companies have to use data from multiple sources and rely on multiple annotators who have different levels of expertise.\nA model trained on data labeled by annotator 1 will perform very differently from a model trained on data labeled by annotator 2.\nYour ML engineers are confident that more data will improve the model performance, so you spend a lot of money to hire annotators to label another million data samples.\nThe reason is that the new million samples were crowdsourced to annotators who labeled data with much less accuracy than the original data.\nOn more than one occasion, we’ve discovered that the problem wasn’t with our model, but because of the unusually high number of wrong labels in the data that we’d acquired recently.\nNatural Labels\nYou might be lucky enough to work on tasks with natural ground truth labels.\nTasks with natural labels are tasks where the model’s predictions can be automatically evaluated or\nThe canonical example of tasks with natural labels is recommender systems.\nA recommendation that gets clicked on can be presumed to be good (i.e., the label is POSITIVE) and a recommendation that doesn’t get clicked on after a period of time, say 10 minutes, can be presumed to be bad (i.e., the label is NEGATIVE).\nNatural labels that are inferred from user behaviors like clicks and ratings are also known as behavioral labels.\nEven if your task doesn’t inherently have natural labels, it might be possible to set up your system in a way that allows you to collect some feedback on your model.\nNewsfeed ranking is not a task with inherent labels, but by adding the Like button and other reactions to each newsfeed item, Facebook is able to collect feedback on their ranking algorithm.\nIn a survey of 86 companies in my network, I found that 63% of them work with tasks with natural labels, as shown in Figure 4-3.\nThis doesn’t mean that 63% of tasks that can benefit from ML solutions have natural labels.\nWhat is more likely is that companies find it easier and cheaper to first start on tasks that have natural labels.\nSixty-three percent of companies in my network work on tasks with natural labels.\nThe percentages don’t sum to 1 because a company can work with tasks with different label sources.\nIt’s different from explicit labels where users explicitly demonstrate their feedback on a recommendation by giving it a low rating or downvoting it.\nFor tasks with natural ground truth labels, the time it takes from when a prediction is served until when the feedback on it is provided is the feedback loop length.\nTasks with short feedback loops are tasks where labels are generally available within minutes.\nIf you want to extract labels from user feedback, it’s important to note that there are different types of user feedback.\nWhen building a product recommender system, many companies focus on optimizing for clicks, which give them a higher volume of feedback to evaluate their models.\nA short window length means that you can capture labels faster, which allows you to use these labels to detect issues with your model and address those issues as soon as possible.\nHowever, a short window length also means that you might prematurely label a recommendation as bad before it’s clicked on.\nFor tasks with long feedback loops, natural labels might not arrive for weeks or even months.\nLabels with long feedback loops are helpful for reporting a model’s performance on quarterly or yearly business reports.\nLabels data samples that are most useful to your model\nOne of the most popular open source tools for weak supervision is Snorkel, developed at the Stanford AI Lab. which can be developed with subject matter expertise, to label data.",
    "keywords": [
      "Galactic Emperor",
      "labels",
      "Galactic Empire",
      "data",
      "Natural Labels",
      "model",
      "feedback",
      "Galactic",
      "Label",
      "Emperor",
      "tasks",
      "Darth Sidious",
      "Dark Lord",
      "Natural",
      "feedback loops"
    ],
    "concepts": [
      "labeling",
      "label",
      "labeled",
      "labels",
      "data",
      "model",
      "models",
      "likely",
      "likes",
      "different"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 126-134)",
    "start_page": 126,
    "end_page": 134,
    "summary": "In a study with Stanford Medicine, models trained with weakly supervised labels obtained by a single radiologist after eight hours of writing LFs had comparable performance with models trained on data obtained through almost a year of hand labeling, as shown in Figure 4-5.\nFirst, the models continued improving with more unlabeled data even without more LFs. Second, LFs were being reused across tasks.\nComparison of the performance of a model trained on fully supervised labels (FS) and a model trained with programmatic labels (DP) on CXR and EXR tasks.\nMy students often ask that if heuristics work so well to label data, why do we need ML models?\nOne reason is that LFs might not cover all data samples, so we can train ML models on data programmatically labeled with LFs and use this trained model to generate predictions for samples that aren’t covered by any LF.\nYou start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples.\nAssuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set.\nAnother semi-supervision method assumes that data samples that share similar characteristics share the same labels.\nSemi-supervision is the most useful when the number of training labels is limited.\nOne thing to consider when doing semi-supervision with limited data is how much of this limited data should be used to evaluate multiple candidate models and select the best one.\nOn the other hand, if you use a large amount of data for evaluation, the performance boost gained by selecting the best model based on this evaluation set might be less than the boost gained by adding the evaluation set to the limited training set.\nTransfer learning refers to the family of methods where a model developed for a task is reused as the starting point for a model on a second task.\nFirst, the base model is trained for a base task.\nLanguage modeling is a great candidate because it doesn’t require labeled data.\nIn some cases, such as in zero-shot learning scenarios, you might be able to use the base model on a downstream task directly.\nFine-tuning means making small changes to the base model, such as 19 continuing to train the base model or a part of the base model on data from a given downstream task.\nTransfer learning is especially appealing for tasks that don’t have a lot of labeled data.\nEven for tasks that have a lot of labeled data, using a pretrained model as the starting point can often boost the performance significantly compared to training from scratch.\nTransfer learning also lowers the entry barriers into ML, as it helps reduce the up-front cost needed for labeling data to build ML applications.\nActive learning is a method for improving the efficiency of data labels.\nThe hope here is that ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.\nActive learning is sometimes called query learning—though this term is getting increasingly unpopular—because a model (active learner) sends back queries in the form of unlabeled samples to be labeled by annotators (usually humans).\nInstead of randomly labeling data samples, you label the samples that are most helpful to your models according to some metrics or heuristics.\nFor example, in the case of classification problems where your model outputs raw probabilities for different classes, it might choose the data samples with the lowest probabilities for the predicted class.\n(b) A model trained on 30 samples randomly labeled gives an accuracy of 70%.\n(c) A model trained on 30 samples chosen by active learning gives an accuracy of 90%.\nwhich are usually the same model trained with different sets of hyperparameters or the same model trained on different slices of data.\nEach model can make one vote for which samples to label next, and it might vote based on how uncertain it is about the prediction.\nThe samples to be labeled can come from different data regimes.\nstationary distribution where you’ve already collected a lot of unlabeled data and your model chooses samples from this pool to label.\nThey can come from the real-world distribution where you have a stream of data coming in, as in production, and your model chooses samples from this stream of data to label.\nActive learning in this data regime will allow your model to learn more effectively in real time and adapt faster to changing environments.\nClass imbalance typically refers to a problem in classification tasks where there is a substantial difference in the number of samples in each class of the training data.\nClass imbalance can also happen with regression tasks where the labels are continuous.\nTherefore, we might have to train the model to be better at predicting 95th percentile bills, even if it reduces the overall metrics.\nML, especially deep learning, works well in situations when the data distribution is more balanced, and usually not so well when the classes are heavily imbalanced, as illustrated in Figure 4-8.\nThe first reason is that class imbalance often means there’s insufficient signal for your model to learn to detect the minority classes.\nIn the case where there is a small number of instances in the minority class, the problem becomes a few-shot learning problem where your model only gets to see the minority class a few times before having to make a decision on it.\nIn the case where there is no instance of the rare classes in your training set, your model might assume these rare classes don’t exist.\nThe second reason is that class imbalance makes it easier for your model to get stuck in a nonoptimal solution by exploiting a simple heuristic instead of learning anything useful about the underlying pattern of the data.\nIf your model learns to always output the majority class, its accuracy is already 99.99%.\nIn this section, we will cover three approaches to handling class imbalance: choosing the right metrics for your problem; data-level methods, which means changing the data distribution to make it less imbalanced; and algorithm-level methods, which means changing your learning method to make it more robust to class imbalance.\nHowever, these are insufficient metrics for tasks with class imbalance because they treat all classes equally, which means the performance of your model on the majority class will dominate these metrics.\nConsider a task with two labels: CANCER (the positive class) and NORMAL (the negative class), where 90% of the labeled data is NORMAL.",
    "keywords": [
      "Class Imbalance",
      "model",
      "data",
      "models",
      "Imbalance",
      "learning",
      "samples",
      "labels",
      "base model",
      "data samples",
      "task",
      "LFs",
      "model trained",
      "Active learning",
      "Class Imbalance Class"
    ],
    "concepts": [
      "label",
      "labels",
      "labeling",
      "labeled",
      "models",
      "model",
      "modeling",
      "classes",
      "data",
      "learning"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 135-142)",
    "start_page": 135,
    "end_page": 142,
    "summary": "The accuracy of model A on the CANCER class is 10% and the accuracy of model B on the CANCER class is 90%.\nF1, precision, and recall are metrics that measure your model’s performance with respect to the positive class in binary classification problems, as they rely on true positive—an outcome where the model correctly predicts the positive class.35\nFor readers needing a refresh, precision, recall, and F1 scores, for binary tasks, are calculated using the count of true positives, true negatives, false positives, and false negatives.\nRecall = True Positive / (True Positive + False Negative)\nF1, precision, and recall are asymmetric metrics, which means that their values change depending on which class is considered the positive class.\nIn our case, if we consider CANCER the positive class, model A’s F1 is 0.17.\nHowever, if we consider NORMAL the positive class, model A’s F1 is 0.95.\nAccuracy, precision, recall, and F1 scores of model A and model B when CANCER is the positive class are shown in Table 4-7.",
    "keywords": [
      "Actual NORMAL Predicted",
      "Actual CANCER Actual",
      "CANCER Actual NORMAL",
      "NORMAL Predicted CANCER",
      "Predicted Positive Predicted",
      "true positive",
      "Actual NORMAL",
      "Positive Predicted Negative",
      "Predicted Negative Positive",
      "positive",
      "NORMAL Predicted",
      "Predicted NORMAL",
      "Actual CANCER",
      "CANCER Actual",
      "Predicted CANCER"
    ],
    "concepts": [
      "negative",
      "positive",
      "positives",
      "precision",
      "predicted",
      "predictions",
      "predicts",
      "classes",
      "change",
      "metrics"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 143-150)",
    "start_page": 143,
    "end_page": 150,
    "summary": "Like F1 and recall, the ROC curve focuses only on the positive class and doesn’t show how well your model does on the negative class.\nData-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn.\nResampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes.\nThe simplest way to undersample is to randomly remove instances from the majority class, whereas the simplest way to oversample is to randomly make copies of the minority class until you have a ratio that you’re happy with.\nA popular method of oversampling low-dimensional data is SMOTE (synthetic minority oversampling technique).\nexisting data points within the minority class.\nWhen you resample your training data, never evaluate your model on resampled data, since it will cause your model to overfit to that resampled distribution.\nOversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.\nYou first train your model on the resampled data.\nThis resampled data can be achieved by randomly undersampling large classes until each class has only N instances.\nYou then fine-tune your model on the original data.\nAnother technique is dynamic sampling: oversample the low-performing classes and undersample the high- performing classes during the training process.\nIf data-level methods mitigate the challenge of class imbalance by altering the distribution of your training data, algorithm-level methods keep the training data distribution intact but alter the algorithm to make it more robust to class imbalance.\nThe key idea is that if there are two instances, x and x , and the loss resulting from making the wrong prediction on x is higher than x , the model will prioritize making the correct prediction on x over making the correct prediction on x .\nBy giving the training instances we care about higher 2 weight, we can make the model focus more on learning these instances.\nLet L(x;θ) be the loss caused by the instance x for the model with the parameter set θ.\nThe model’s loss is often defined as the average loss caused by all instances.\nBack in 2001, based on the insight that misclassification of different classes incurs different costs, Elkan proposed cost-sensitive learning in which the individual loss function is modified to take into account this varying cost.\nThe loss caused by instance x of class i will become the weighted average of all possible classifications of instance x.\nWhat might happen with a model trained on an imbalanced dataset is that it’ll bias toward majority classes and make wrong predictions on minority classes.\nWhat if we punish the model for making wrong predictions on minority classes to correct this bias?\nThe loss caused by instance x of class i will become as follows, with Loss(x, j) being the loss when x is classified as class j.\nIn our data, some examples are easier to classify than others, and our model might learn to classify them quickly.\nThe model trained with focal loss (FL) shows reduced loss values compared to the model trained with cross entropy loss (CE).\nData augmentation is a family of techniques that are used to increase the amount of training data.\nTraditionally, these techniques are used for tasks that have limited training data, such as in medical imaging.\nHowever, in the last few years, they have shown to be useful even when we have a lot of data—augmented data can make our models more robust to noise and even adversarial attacks.\nIn computer vision, the simplest data augmentation technique is to randomly modify an image while preserving its label.\nPerturbation is also a label-preserving operation, but because sometimes it’s used to trick models into making wrong predictions, I thought it deserves its own section.",
    "keywords": [
      "data",
      "Model",
      "loss",
      "training data",
      "Accuracy Precision Recall",
      "Data Augmentation",
      "positive",
      "loss function",
      "ROC curve",
      "instances",
      "training",
      "positive rate",
      "Accuracy Precision",
      "classes",
      "loss caused"
    ],
    "concepts": [
      "data",
      "classes",
      "loss",
      "sample",
      "samples",
      "sampling",
      "positive",
      "methods",
      "method",
      "curve"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 151-158)",
    "start_page": 151,
    "end_page": 158,
    "summary": "Adding noisy samples to training data can help models recognize the weak spots in their learned decision boundary and improve their performance.\n2 The authors showed that mixup improves models’ generalization, reduces their memorization of corrupt labels, increases their robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\nshowed that by adding images generated using CycleGAN to their original training data, they were able to improve their model’s performance significantly on computed tomography (CT) segmentation tasks.\nMost ML algorithms in use today are supervised ML algorithms, so obtaining labels is an integral part of creating training data.\nWe ended the chapter with a discussion on data augmentation techniques that can be used to improve a model’s performance and generalization for both computer vision and NLP tasks.\nOnce you have your training data, you will want to extract features from it to train your ML models, which we will cover in the next chapter.\n3 Rachel Lerman, “Google Is Testing Its Self-Driving Car in Kirkland,” Seattle Times, February 3, 2016, https://oreil.ly/3IA1V.\n6 “SVM: Weighted Samples,” scikit-learn, https://oreil.ly/BDqbk.\n“Addressing Delayed Feedback for Continuous Training with Neural Networks in CTR Prediction,” arXiv, July 15, 2019, https://oreil.ly/5y2WA.\n3 (2017): 269–82, https://oreil.ly/vFPjk.\n12 Ratner et al., “Snorkel: Rapid Training Data Creation with Weak Supervision.”\n16 Avrim Blum and Tom Mitchell, “Combining Labeled and Unlabeled Data with Co-Training,” in Proceedings of the Eleventh Annual\nConference on Computational Learning Theory (July 1998): 92–100, https://oreil.ly/T79AE.\nLearning Algorithms,” NeurIPS 2018 Proceedings, https://oreil.ly/dRmPV.\nhttps://oreil.ly/DBEbw.\nSurvey of Prompting Methods in Natural Language Processing,” arXiv, July 28, 2021, https://oreil.ly/0lBgn.\nUnderstanding,” arXiv, October 11, 2018, https://oreil.ly/RdIGU; Tom B.\nBrown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al., “Language Models Are Few-Shot Learners,” OpenAI, 2020, https://oreil.ly/YVmrr.\n24 Dana Angluin, “Queries and Concept Learning,” Machine Learning 2 (1988): 319–42, https://oreil.ly/0uKs4.\nhttps://oreil.ly/FSFWS.\n29 The Nilson Report, “Payment Card Fraud Losses Reach $27.85 Billion,” PR Newswire, November 21, 2019, https://oreil.ly/NM5zo.\n30 “Job Market Expert Explains Why Only 2% of Job Seekers Get Interviewed,” WebWire, January 7, 2014, https://oreil.ly/UpL8S.\n31 “Email and Spam Data,” Talos Intelligence, last accessed May 2021, https://oreil.ly/lI5Jr.\n32 Nathalie Japkowciz and Shaju Stephen, “The Class Imbalance Problem: A Systematic Study,” 2002, https://oreil.ly/d7lVu.\n33 Nathalie Japkowicz, “The Class Imbalance Problem: Significance and Strategies,” 2000, https://oreil.ly/Ma50Z.\nImbalanced Class Distribution,” 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2017, https://oreil.ly/WeW6J.\nConference on Machine Learning, 2006, https://oreil.ly/s40F3.\n37 Rafael Alencar, “Resampling Strategies for Imbalanced Datasets,” Kaggle, https://oreil.ly/p8Whs.\n448–52, https://oreil.ly/JCxHZ.\n(Workshop on Learning from Imbalanced Datasets II, ICML, Washington, DC, 2003), https://oreil.ly/qnpra; Miroslav Kubat and Stan Matwin, “Addressing the Curse of Imbalanced Training Sets: One-Sided Selection,” 2000, https://oreil.ly/8pheJ.\n42 Hansang Lee, Minseok Park, and Junmo Kim, “Plankton Classification on Imbalanced Large Scale Database via Convolutional Neural Networks with Transfer Learning,” 2016 IEEE International Conference on Image Processing (ICIP), 2016, https://oreil.ly/YiA8p.\nKaseb, Kent Gauen, Ryan Dailey, et al., “Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification,” 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), 2018, https://oreil.ly/D3Ak5.\nhttps://oreil.ly/Km2dF.\nhttps://oreil.ly/aphzA.\n5 (2019): 828–41, https://oreil.ly/LzN9D.\nhttps://oreil.ly/9v2No; Ian J.\nNetworks,” in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, https://oreil.ly/dYVL8.\nand Semi-Supervised Learning,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, https://oreil.ly/MBQeu.\nhttps://oreil.ly/lIM5E.\n1 (2019): 16884, https://oreil.ly/TDUwm.\nDoesn’t deep learning promise us that we no longer have to engineer features?”\nFigure 5-1 shows an example of classical text processing techniques you can use to handcraft n-gram features for your text.\nAn example of techniques that you can use to handcraft n-gram features for your text",
    "keywords": [
      "training data",
      "data",
      "learning",
      "Deep Learning",
      "training",
      "Rapid Training Data",
      "class imbalance",
      "neural networks",
      "Deep Neural Networks",
      "Training Data Creation",
      "Convolutional Neural Networks",
      "features",
      "create training data",
      "Class Imbalance Problem",
      "n’t deep learning"
    ],
    "concepts": [
      "data",
      "learned",
      "learning",
      "learn",
      "labels",
      "label",
      "labeling",
      "labeled",
      "features",
      "feature"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 159-167)",
    "start_page": 159,
    "end_page": 167,
    "summary": "Your model will hopefully learn to extract useful features from this.\nThere are many possible features to use in your model.\nThe process of choosing what information to use and how to extract this information into a format usable by your ML models is feature engineering.\nSome of the possible features about a comment, a thread, or a user to be included in your model\nIn this section, we will discuss several of the most important operations that you might want to consider while engineering features from your data.\nThey include handling missing values, scaling, discretization, encoding categorical features, and generating the old-school but still very effective cross features as well as the newer and exciting positional features.\nHandling Missing Values\nOne of the first things you might notice when dealing with data in production is that some values are missing.\nHowever, one thing that many ML engineers I’ve interviewed don’t know is that not all types of missing values are equal.\nThere are three types of missing values.\nThe income values are missing for reasons related to the values themselves.\nIn this example, we might notice that age values are often missing for respondents of the gender “A,” which might be because the people of gender A in this survey don’t like disclosing their age.\nIn this example, we might think that the missing values for the column “Job” might be completely random, not because of the job itself and not because of any other variable.\nWhen I ask candidates about how to handle missing values during interviews, many tend to prefer deletion, not because it’s a better method, but because it’s easier to do.\nOne way to delete is column deletion: if a variable has too many missing values, just remove that variable.\nFor example, in the example above, over 50% of the values for the variable “Marital status” are missing, so you might be tempted to remove this variable from your model.\nAnother way to delete is row deletion: if a sample has missing value(s), just remove that sample.\nThis method can work when the missing values are completely at random (MCAR) and the number of examples with missing values is small, such as less than 0.1%.\nHowever, removing rows of data can also remove important information that your model needs to make predictions, especially if the missing values are not at random (MNAR).\nFor example, you don’t want to remove samples of gender B respondents with missing income because the fact that income is missing is information itself (missing income might mean higher income, and thus, more correlated to buying a house) and can be used to make predictions.\nOn top of that, removing rows of data can create biases in your model, especially if the missing values are at random (MAR).\nFor example, if you remove all examples missing age values in the data in Table 5-2, you will remove all respondents with gender A from your data, and your model won’t be able to make good predictions for respondents with gender A.\nIf you don’t want to delete missing values, you will have to impute them, which means “fill them with certain values.” Deciding which “certain values” to use is the hard part.\nOne common practice is to fill in missing values with their defaults.\nFor example, if the temperature value is missing for a data sample whose month value is July, it’s not a bad idea to fill it with the median temperature of July.\nOne time, in one of the projects I was helping with, we discovered that the model was spitting out garbage because the app’s frontend no longer asked users to enter their age, so age values were missing, and the model filled them with 0.\nBut the model never saw the age value of 0 during training, so it couldn’t make reasonable predictions.\nMultiple techniques might be used at the same time or in sequence to handle missing values for a particular set of data.\nRegardless of what techniques you use, one thing is certain: there is no perfect way to handle missing values.\nThe values of the variable Age in our data range from 20 to 40, whereas the values of the variable Annual Income range from 10,000 to 150,000.\nBefore inputting features into models, it’s important to scale them to be similar ranges.\nIf you want your feature to be in an arbitrary range [a, b]—empirically, I find the range [–1, 1] to work better than the range [0, 1]—you can use the following formula: x′= a + (x−min(x))(b−a) max(x)−min(x)\nDuring training, our model has seen the annual income values of “150,000,” “50,000,” “100,000,” and so on.\nOne of the features you want to use is the product brand.\nTo address this, you create a category UNKNOWN with the value of 2,000,000 to catch all the brands your model hasn’t seen during training.\nHowever, your model treats them all the same way it treats unpopular brands in the training data.\nFor example, if you want to predict whether a comment is spam, you might want to use the account that posted this comment as a feature, and new accounts are being created all the time.\nThe gist of this trick is that you use a hash function to generate a hashed value of each category.\nBecause you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many categories there will be.\nFor example, if you choose a hash space of 18 bits, which corresponds to 2 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143.\nIt can be especially useful in continual learning settings where your model learns from incoming examples in production.\nThis technique is useful to model the nonlinear relationships between features.\nFor example, for the task of predicting whether someone will want to buy a house in the next 12 months, you suspect that there might be a nonlinear relationship between marital status and number of children, so you combine them to create a new feature “marriage and children” as in Table 5-3.",
    "keywords": [
      "missing",
      "model",
      "data",
      "features",
      "n’t",
      "feature",
      "brands",
      "income",
      "data leakage",
      "number",
      "variable",
      "feature engineering",
      "categories",
      "Age",
      "missing income"
    ],
    "concepts": [
      "values",
      "value",
      "features",
      "feature",
      "data",
      "model",
      "models",
      "brand",
      "brands",
      "income"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 168-178)",
    "start_page": 168,
    "end_page": 178,
    "summary": "DeepFM and xDeepFM are the family of models that have successfully leveraged explicit feature interactions for recommender systems and click-through- rate prediction.\nYou will need a lot more data for models to learn all these possible values.\nAnother caveat is that because feature crossing increases the number of features models use, it can make models overfit to the training data.\nFourier features have been shown to improve models’ performance for tasks that take in coordinates (or positions) as inputs.\n“Because patients scanned while lying down were more likely to be seriously ill, the model learned to predict serious covid risk from a person’s position.”\nData leakage refers to the phenomenon when a form of the label “leaks” into the set of features used for making predictions, and this same information is not available during inference.\nWhen building models to predict the future stock prices, you want to split your training data by time, such as training your model on data from the first six days and evaluating it on data from the seventh day.\nIf you randomly split your data, prices from the seventh day will be included in your train split and leak into your model the condition of the market on that day.\nTo prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible.\nFor example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test splits as shown in Figure 5-7.\nSplit data by time to prevent future information from leaking into the training process\nOne common mistake is to use the entire training data to generate global statistics before splitting it into different splits, leaking the mean and variance of the test samples into the training process, allowing a model to adjust its predictions for the test samples.\nTo avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits.\nData leakage can happen during many steps, from generating, collecting, sampling, splitting, and processing data to feature engineering.\nDo ablation studies to measure how important a feature or a set of features is to your model.\nIf removing a feature causes the model’s performance to deteriorate significantly, investigate why that feature is so important.\nKeep an eye out for new features added to your model.\nIf adding a new feature significantly improves your model’s performance, either that feature is really good or that feature just contains leaked information about labels.\nIf you use the test split in any way other than to report a model’s final performance, whether to come up with ideas for new features or to tune hyperparameters, you risk leaking information from the future into your training process.\nGenerally, adding more features leads to better model performance.\nIn my experience, the list of features used for a model in production only grows over time.\nHowever, more features doesn’t always mean better model performance.\nHaving too many features can be bad both during training and serving your model for the following reasons:\nThe more features you have, the more opportunities there are for data leakage.\nToo many features can increase memory required to serve a model, which, in turn, might require you to use a more expensive machine/instance to serve your model.\nToo many features can increase inference latency when doing online prediction, especially if you need to extract these features from raw data for predictions online.\nWhenever your data pipeline changes, all the affected features need to be adjusted accordingly.\nIn theory, if a feature doesn’t help a model make good predictions, regularization techniques like L1 regularization should reduce that feature’s weight to 0.\nHowever, in practice, it might help models learn faster if the features that are no longer useful (and even possibly harmful) are removed, prioritizing good features.\nThere are two factors you might want to consider when evaluating whether a feature is good for a model: importance to the model and generalization to unseen data.\nFeature Importance\nThe exact algorithm for feature importance measurement is complex, but intuitively, a feature’s importance to a model is measured by how much that model’s performance deteriorates if that feature or a set of features containing that feature is removed from the model.\nSHAP is great because it not only measures a feature’s importance to an entire model, it also measures each feature’s contribution to a model’s specific prediction.\nFigures 5-8 and 5-9 show how SHAP can help you understand the contribution of each feature to a model’s predictions.\nHow much each feature contributes to a model’s single prediction, measured by SHAP.\nHow much each feature contributes to a model, measured by SHAP.\nOften, a small number of features accounts for a large portion of your model’s feature importance.\nWhen measuring feature importance for a click-through rate prediction model, the ads team at Facebook found out that the top 10 features are responsible for about half of the model’s total feature importance, whereas the last 300 features contribute less than 1% feature importance, as shown in Figure 5-10.\nNot only good for choosing the right features, feature importance techniques are also great for interpretability as they help you understand how your models work under the hood.\nSince the goal of an ML model is to make correct predictions on unseen data, features used for the model should generalize to unseen data.\nFor example, for the task of predicting whether a comment is spam, the identifier of each comment is not generalizable at all and shouldn’t be used as a feature for the model.\nA rough rule of thumb is that if this feature appears in a very small percentage of your data, it’s not going to be very generalizable.\nFor example, if you want to build a model to predict whether someone will buy a house in the next 12 months and you think that the number of children someone has will be a good feature, but you can only get this information for 1% of your data, this feature might not be very useful.\nThis rule of thumb is rough because some features can still be useful even if they are missing in most of your data.\nFor example, if a feature appears only in 1% of your data, but 99% of the examples with this feature have POSITIVE labels, this feature is useful and you should use it.\nCoverage of a feature can differ wildly between different slices of data and even in the same slice of data over time.\nIf the coverage of a feature differs a lot between the train and test split (such as it appears in 90% of the examples in the train split but only in 20% of the examples in the test split), this is an indication that your train and test splits don’t come from the same distribution.\nYou might want to investigate whether the way you split your data makes sense and whether this feature is a cause for data leakage.\nIf the set of values that appears in the seen data (such as the train split) has no overlap with the set of values that appears in the unseen data (such as the test split), this feature might even hurt your model’s performance.\nYou retrain this model every week, and you want to use the data from the last six days to predict the ETAs (estimated time of arrival) for today.\nIf you include this feature in your model without a clever scheme to encode the days, it won’t generalize to the test split, and might harm your model’s performance.\nOn the other hand, HOUR_OF_THE_DAY is a great feature, because the time in the day affects the traffic too, and the range of values for this feature in the train split overlaps with the test split 100%.\nThe best way to learn is through experience: trying out different features and observing how they affect your models’ performance.\nUse statistics from only the train split, instead of the entire data, to scale your features and handle missing values.\nUnderstand feature importance to your model.\nUse features that generalize well.\nRemove no longer useful features from your models.\nWith a set of good features, we’ll move to the next part of the workflow: training ML models.\nBefore we move on, I just want to reiterate that moving to modeling doesn’t mean we’re done with handling data or feature engineering.\nWe are never done with data and features.\nIn most real-world ML projects, the process of collecting data and feature engineering goes on as long as your models are in production.\n4 Feature scaling once boosted my model’s performance by almost 10%.\nPrediction,” Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI, 2017), https://oreil.ly/1Vs3v; Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun, “xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems,” arXiv, 2018, https://oreil.ly/WFmFt.",
    "keywords": [
      "data",
      "feature",
      "features",
      "Data Leakage",
      "model",
      "Single Married Single",
      "Married Single Single",
      "Single Single Married",
      "Feature Importance",
      "split",
      "Single Married",
      "Marriage Single Married",
      "Leakage",
      "test split",
      "train split"
    ],
    "concepts": [
      "feature",
      "features",
      "data",
      "model",
      "models",
      "modeling",
      "splitting",
      "split",
      "splits",
      "prediction"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 179-186)",
    "start_page": 179,
    "end_page": 186,
    "summary": "Model Development and Offline Evaluation\nThe section that follows discusses different aspects of model development, such as debugging, experiment tracking and versioning, distributed training, and AutoML.\nI expect that most readers already have an understanding of common ML algorithms such as linear models, decision trees, k-nearest neighbors, and different types of neural networks.\nModel Development and Training\nIn this section, we’ll discuss necessary aspects to help you develop and train your model, including how to evaluate different ML models for your problem, creating ensembles of models, experiment tracking and versioning, and distributed training, which is necessary for the scale at which models today are usually trained at.\nEvaluating ML Models\nA k-means clustering model might be used to extract features to input into a neural network.\nVice versa, a pretrained neural network (like BERT or GPT-3) might be used to generate embeddings to input into a logistic regression model.\nFor example, if your boss tells you to build a system to detect toxic tweets, you know that this is a text classification problem—given a piece of text, classify whether it’s toxic or not—and common models for text classification include naive Bayes, logistic regression, recurrent neural networks, and transformer-based models such as BERT, GPT, and their variants.\nWhen considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability.\nFor example, a simple logistic regression model might have lower accuracy than a complex neural network, but it requires less labeled data to start, it’s much faster to train, it’s much easier to deploy, and it’s also much easier to explain why it’s making certain predictions.\nWhile helping companies as well as recent graduates get started in ML, I usually have to spend a nontrivial amount of time steering them away from jumping straight into state-of-the-art models.\nResearchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets.\nFor example, pretrained BERT models are complex, but they require little effort to get started with, especially if you use a ready-made implementation like the one in Hugging Face’s Transformer.\nImagine an engineer on your team is assigned the task of evaluating which model is better for your problem: a gradient-boosted tree or a pretrained BERT model.\nPart of the process of evaluating an ML architecture is to experiment with different features and different sets of hyperparameters to find the best model of that architecture.\nIf an engineer is more excited about an architecture, they will likely spend a lot more time experimenting with it, which might result in better-performing models for that architecture.\nBecause the performance of a model architecture depends heavily on the context it’s evaluated in—e.g., the task, the training data, the test data, the hyperparameters, etc.—it’s extremely difficult to make claims that a model architecture is better than another architecture.\nFor example, a tree-based model might work better now because you don’t have a ton of data yet, but two months from now, you might be able to double your amount of training data, and your neural network might perform much better.\nA simple way to estimate how your model’s performance might change with more data is to use learning curves.\nA learning curve of a model is a plot of its performance—e.g., training loss, training accuracy, validation accuracy—against the number of training samples it uses, as shown in Figure 6-1.\nA situation that I’ve encountered is when a team evaluates a simple neural network against a collaborative filtering model for making recommendations.\nThe team decided to deploy both the collaborative filtering model and the simple neural network.\nThey used the collaborative filtering model to make predictions for users, and continually trained the simple neural network in production with new, incoming data.\nAfter two weeks, the simple neural network was able to outperform the collaborative filtering model.\nUnderstanding what’s more important in the performance of your ML system will help you choose the most suitable model.\nIn a task where false positives are more dangerous than false negatives, such as fingerprint unlocking (unauthorized people shouldn’t be classified as authorized and given access), you might prefer a model that makes fewer false positives.\nAnother example of trade-off is compute requirement and accuracy—a more complex model might deliver higher accuracy but might require a more powerful machine, such as a GPU instead of a CPU, to generate predictions with acceptable inference latency.\nUnderstanding what assumptions a model makes and whether our data satisfies those assumptions can help you evaluate which model works best for your use case.\nEvery model that aims to predict an output Y from an input X makes the assumption that it’s possible to predict Y based on X.\nEvery generative model makes the assumption that it’s tractable to compute the probability P(Z|X).\nWhen considering an ML solution to your problem, you might want to start with a system that contains just one model (the process of selecting one model for your problem was discussed earlier in the chapter).\nOne method that has consistently given a performance boost is to use an ensemble of multiple models instead of just an individual model to make predictions.\nEach model in the ensemble is called a base learner.\nthe task of predicting whether an email is SPAM or NOT SPAM, you might have three different models.\nTherefore, it’s common to choose very different types of models for an ensemble.\nFor example, you might create an ensemble that consists of one transformer model, one recurrent neural network, and one gradient-boosted tree.\nGiven a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps.",
    "keywords": [
      "Model",
      "models",
      "algorithms",
      "neural networks",
      "neural",
      "BERT model",
      "pretrained BERT model",
      "ensemble",
      "problem",
      "performance",
      "data",
      "simple neural network",
      "Model Development",
      "training",
      "BERT"
    ],
    "concepts": [
      "model",
      "models",
      "predictions",
      "prediction",
      "predict",
      "predicting",
      "solutions",
      "solution",
      "train",
      "trained"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 187-194)",
    "start_page": 187,
    "end_page": 194,
    "summary": "If the problem is classification, the final prediction is decided by the majority vote of all models.\nAn example of a boosting algorithm is a gradient boosting machine (GBM), which produces a prediction model typically from weak decision trees.\nDuring the model development process, you often have to experiment with many architectures and many different models to choose the best one for your problem.\nAn artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process.\nComparing different experiments can also help you understand how small changes affect your model’s performance, which, in turn, gives you more visibility into how your model works.\nA large part of training an ML model is babysitting the learning processes.\nIt’s important to track what’s going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful.\nThe speed of your model, evaluated by the number of steps per second or, if your data is text, the number of tokens processed per second.\nThe values over time of any parameter and hyperparameter whose changes can affect your model’s performance, such as the learning rate if you use a learning rate schedule; gradient norms (both globally and per layer), especially if you’re clipping your gradient norms; and weight norm, especially if you’re doing weight decay.\nAnother confusion is in how to resolve merge conflicts: if developer 1 uses data version X to train model A and developer 2 uses data version Y to train model B, it doesn’t make sense to merge data versions X and Y to create Z, since there’s no model corresponding with Z.\nThird, if you use user data to train your model, regulations like General Data Protection Regulation (GDPR) might make versioning this data complicated.\nThe way we have to run so many experiments right now to find the best possible model is the result of us treating ML as a black box.\nHowever, I hope that as the field progresses, we’ll gain more understanding into different models and can reason about what model will work best instead of running hundreds or thousands of experiments.\nDEBUGGING ML MODELS\nDebugging is never fun, and debugging ML models can be especially frustrating for the following three reasons.\nHowever, when making changes to an ML model, you might have to retrain the model and wait until it converges to see whether the bug is fixed, which can take hours.\nThird, debugging ML models is hard because of their cross-functional complexity.\nHere are some of the things that might cause an ML model to fail:\nAs discussed previously, each model comes with its own assumptions about the data and the features it uses.\nA model might fail because the data it learns from doesn’t conform to its assumptions.\nFor example, you use a linear model for the data whose decision boundaries aren’t linear.\nThe model might be a good fit for the data, but the bugs are in the implementation of the model.\nThe model is a great fit for your data, and its implementation is correct, but a poor set of hyperparameters might render your model useless.\nThere are many things that could go wrong in data collection and preprocessing that might cause your models to perform poorly, such as data samples and labels being incorrectly paired, noisy labels, features normalized using outdated statistics, and more.\nToo many features might cause your models to overfit to the training data or cause data leakage.\nToo few features might lack predictive power to allow your models to make good predictions.\nHaving the discipline to follow both the best practices and the debugging procedure is crucial in developing, implementing, and deploying ML models.\nCurrently, many people start out by cloning an open source implementation of a state-of-the-art model and plugging in their own data.\nBut if it doesn’t, it’s very hard to debug the system because the problem could have been caused by any of the many components in the model.\nAfter you have a simple implementation of your model, try to overfit a small amount of training data and run evaluation on the same data to make sure that it gets to the smallest possible loss.\nThere are so many factors that contribute to the randomness of your model: weight initialization, dropout, data shuffling, etc.\nRandomness makes it hard to compare results across different experiments —you have no idea if the change in performance is due to a change in the model or a different random seed.\nIt’s common to train a model using data that doesn’t fit into memory.\nIt can also happen with text data if you work for teams that train large language models (cue OpenAI, Google, NVIDIA, Cohere).\ndata is large, e.g., one machine can handle a few samples at a time, you might only be able to work with a small batch size, which leads to instability for gradient descent-based optimization.\nAccording to the authors of the open source package gradient- checkpointing, “For feed-forward models we were able to fit more than 10x larger models onto our GPU, at only a 20% increase in computation time.” Even when a sample fits into memory, using checkpointing can allow you to fit more samples into a batch, which might allow you to train your model faster.\nIt’s now the norm to train ML models on multiple machines.\nThe most common parallelization method supported by modern ML frameworks is data parallelism: you split your data on multiple machines, train your model on all of them, and accumulate gradients.\nAs each machine produces its own gradient, if your model waits for all of them to finish a run—synchronous stochastic gradient descent (SGD)—stragglers will cause the entire system to slow down, wasting time and resources.\nIf your model updates the weight using the gradient from each machine separately—asynchronous SGD— gradient staleness might become a problem because the gradients from one machine have caused the weights to change before the gradients from another machine have come in.\nAnother problem is that spreading your model on multiple machines can cause your batch size to be very big.",
    "keywords": [
      "model",
      "data",
      "models",
      "Experiment",
      "Experiment Tracking",
      "n’t",
      "training",
      "gradient",
      "Tracking",
      "Versioning",
      "SGD",
      "code",
      "Train",
      "training data",
      "machine"
    ],
    "concepts": [
      "models",
      "model",
      "data",
      "gradient",
      "gradients",
      "trained",
      "training",
      "train",
      "version",
      "versions"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 195-202)",
    "start_page": 195,
    "end_page": 202,
    "summary": "Model parallelism\nModel parallelism is when different components of your model are trained on different machines, as shown in Figure 6-7.\nModel parallelism can be misleading because in some cases parallelism doesn’t mean that different parts of the model in different machines are executed in parallel.\nFor example, if your model is a massive matrix and the matrix is split into two halves on two machines, then these two halves might be executed in parallel.\nHowever, if your model is a neural network and you put the first layer on machine 1 and the second layer on machine 2, and layer 2 needs outputs from layer 1 to execute, then machine 2 has to wait for machine 1 to finish first to run.\nPipeline parallelism is a clever technique to make different components of a model on different machines run more in parallel.\nInstead of paying a group of 100 ML researchers/engineers to fiddle with various models and eventually select a suboptimal one, why not use that money on compute to search for the optimal model?\nWith different sets of hyperparameters, the same model can give drastically different performances on the same dataset.\nshowed in their 2018 paper “On the State of the Art of Evaluation in Neural Language Models” that weaker models with well-tuned hyperparameters can outperform stronger, fancier models.\ngoal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space—the performance of each set evaluated on a validation set.\nPopular methods example, scikit-learn with auto-sklearn, for hyperparameter tuning include random search, AutoML: Methods, Systems, Challenges by the AutoML group at the University of Freiburg dedicates its first chapter (which you can read online for free) to hyperparameter optimization.\nChoose the best set of hyperparameters for a model based on its performance on a validation split, then report the model’s final performance on the test split.\nIf you use your test split to tune hyperparameters, you risk overfitting your model to the test split.\nSome teams take hyperparameter tuning to the next level: what if we treat other components of a model or the entire model as hyperparameters.\nThis area of research is known as architectural search, or neural architecture search (NAS) for neural networks, as it searches for the optimal model architecture.\nIn a typical ML training process, you have a model and then a learning procedure, an algorithm that helps your model find the set of parameters that minimize a given objective function for a given set of data.\nThe most common learning procedure for neural networks today is gradient descent, which leverages an optimizer to specify how to update a model’s weights given gradient updates.\nFirst, the resulting architectures and learned optimizers can allow ML algorithms to work off-the-shelf on multiple real-world tasks, saving production time and cost, during both training and inferencing.\nFOUR PHASES OF ML MODEL DEVELOPMENT\nBefore we transition to model training, let’s take a look at the four phases of ML model development.\nSimplest machine learning models\nFor your first ML model, you want to start with a simple algorithm, something that gives you visibility into its working to allow you to validate the usefulness of your problem framing and your data.\nOptimizing simple models\nOnce you have your ML framework in place, you can focus on optimizing the simple ML models with different objective functions, hyperparameter search, feature engineering, more data, and ensembles.\nOne common but quite difficult question I often encounter when helping companies with their ML strategies is: “How do I know that our ML models are any good?” In one case, a company deployed ML to detect intrusions to 100 surveillance drones, but they had no way of measuring how many intrusions their system failed to detect, and they couldn’t decide if one ML algorithm was better than another for their needs.\nFor other tasks, you might not be able to evaluate your model’s performance in production directly and might have to rely on extensive monitoring to detect changes and failures in your ML system’s performance.\nIn this section, we’ll discuss methods to evaluate your model’s performance before it’s deployed.\nWe’ll start with the baselines against which we will evaluate our models.\nHis model might as well be making predictions at random.\nWhen evaluating your model, it’s essential to know the baseline you’re evaluating it against.\nTable 6-2 shows the F1 and accuracy scores of baseline models making predictions at random.",
    "keywords": [
      "model",
      "machine",
      "models",
      "Model parallelism",
      "Hyperparameter",
      "Hyperparameter tuning",
      "machines",
      "parallelism",
      "search",
      "machine learning models",
      "machine learning",
      "neural",
      "layer",
      "neural network",
      "optimizers"
    ],
    "concepts": [
      "model",
      "models",
      "learning",
      "learned",
      "automl",
      "architecture",
      "architectural",
      "architectures",
      "optimal",
      "optimization"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 203-210)",
    "start_page": 203,
    "end_page": 210,
    "summary": "The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.\nFor example, for the task of recommending the app a user is most likely to use next on their phone, the simplest model would be to recommend their most frequently used app.\nIn many cases, the goal of ML is to automate what would have been otherwise done by humans, so it’s useful to know how your model performs compared to human experts.\nHowever, in production, we also want our models to be robust, fair, calibrated, and overall make sense.\nHowever, when they deployed it to actual users, this model’s predictions were close to random.\nTo get a sense of how well your model might perform with noisy data, you can make small changes to your test splits to see how these changes affect your model’s performance.\nModel calibration\nIf a model predicts that team A will beat team B with a 70% probability, and out of the 1,000 times these two teams play together, team A only wins 60% of the time, then we say that this model isn’t calibrated.\nA calibrated model should predict that team A wins with a 60% probability.\nModel calibration is often overlooked by ML practitioners, but it’s one of the most important properties of any predictive system.\nSecond, consider the task of building a model to predict how likely it is that a user will click on an ad.\nYour model predicts that this user will click on ad A with a 10% probability and on ad B with an 8% probability.\nYou don’t need your model to be calibrated to rank ad A above ad B.\nneed your model to be calibrated.\nIf your model predicts that a user will click on ad A with a 10% probability but in reality the ad is only clicked on 5% of the time, your estimated number of clicks will be way off.\nTo measure a model’s calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y.\nIndiscriminately showing all a model’s predictions to users, even the predictions that the model is unsure about, can, at best, cause annoyance and make users lose trust in the system, such as an activity detection system on your smartwatch that thinks you’re running even though you’re just walking a bit fast.\nIf you only want to show the predictions that your model is certain about, how do you measure that certainty?\nSlicing means to separate your data into subsets and look at your model’s performance on each subset separately.\nOne is that their model performs differently on different slices of data when the model should perform the same.\nModel B achieves 95% accuracy on the majority and 95% on the minority, which means its overall accuracy is 95%.\nModel A\nModel B\nFor example, they might decide to improve model A’s performance on the minority subgroup, which leads to improving this model’s performance overall.\nAnother problem is that their model performs the same on different slices of data when the model should perform differently.\nFor example, when you build a model for user churn prediction (predicting when a user will cancel a subscription or a service), paid users are more critical than nonpaid users.\nFocusing on a model’s overall performance might hurt its performance on these critical slices.\nThis means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately.\nConsider model A’s and model B’s performance on group A and group B as shown in Table 6-4.\nModel A\nModel B",
    "keywords": [
      "model",
      "distribution Predicting NEGATIVE",
      "Random distribution Meaning",
      "Simple heuristic Forget",
      "system",
      "distribution Meaning",
      "Model calibration Model",
      "Model calibration",
      "data",
      "performance",
      "models",
      "Uniform random Predicting",
      "Accuracy Uniform random",
      "model predicts",
      "model performs"
    ],
    "concepts": [
      "model",
      "models",
      "user",
      "users",
      "predicting",
      "predictions",
      "predicts",
      "predict",
      "predictable",
      "predicted"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 211-218)",
    "start_page": 211,
    "end_page": 218,
    "summary": "To make informed decisions regarding what model to choose, we need to take into account its performance not only on the entire data, but also on individual slices.\nSlice-based evaluation can give you insights to improve your model’s performance both overall and on critical data and help detect potential biases.\nThere has been research to systemize the process of finding slices, including Chung et al.’s “Slice Finder: Automated Data Slicing for Model Validation” in 2019 and covered in Sumyea Helal’s “Subgroup Discovery Algorithms: A Survey and Empirical Evaluation” (2016).\nWith the initial models, we can bring to life (in the form of predictions) all our hard work in data and feature engineering, and can finally evaluate our hypothesis (i.e., we can predict the outputs given the inputs).\nAs models today are getting bigger and consuming more data, distributed training is becoming an essential skill for ML model developers, and we discussed techniques for parallelism including data parallelism, model parallelism, and pipeline parallelism.\nWe ended the chapter with how to evaluate your models to pick the best one to deploy.\nWe also covered a range of evaluation techniques necessary to sanity check your models before further evaluating your models in a production environment.\nIn the next chapter, we’ll go over how to deploy a model.\nWinner Solution-Gilberto Titericz and Stanislav Semenov,” Kaggle, https://oreil.ly/z5od8).\n4 (July 2012): 463–84, https://oreil.ly/ZBlgE; G.\n5 Leo Breiman, “Bagging Predictors,” Machine Learning 24 (1996): 123–40, https://oreil.ly/adzJu.\n6 “Machine Learning Challenge Winning Solutions,” https://oreil.ly/YjS8d.\n“External memory algorithm,” https://oreil.ly/apv5m).\nPradeep Dubey, “Distributed Deep Learning Using Synchronous Stochastic Gradient Descent,” arXiv, February 22, 2016, https://oreil.ly/ma8Y6.\nJoseph, Randy Katz, and Ion Stoica, “Improving MapReduce Performance in Heterogeneous Environments,” 8th USENIX Symposium on Operating Systems Design and Implementation, https://oreil.ly/FWswd; Aaron Harlap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R.\nDeep Networks,” NIPS 2012, https://oreil.ly/EWPun.\n17 Jim Dowling, “Distributed TensorFlow,” O’Reilly Media, December 19, 2017, https://oreil.ly/VYlOP.\nDescent,” 2011, https://oreil.ly/sAEbv.\nModels Are Few-Shot Learners,” arXiv, May 28, 2020, https://oreil.ly/qjg2S.\n20 Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team, “An Empirical Model of Large-Batch Training,” arXiv, December 14, 2018, https://oreil.ly/mcjbV; Christopher J.\nDahl, “Measuring the Effects of Data Parallelism on Neural Network Training,” Journal of Machine Learning Research 20 (2019): 1–49, https://oreil.ly/YAEOM.\nMicro-Batch Pipeline Parallelism,” arXiv, July 25, 2019, https://oreil.ly/wehkx.\nhttps://oreil.ly/5vEsH; “Debate About Science at Organizations like Google Brain/FAIR/DeepMind,” Reddit, https://oreil.ly/2K77r; “Grad Student Descent,” Science Dryad, January 25, 2014, https://oreil.ly/dIR9r; and Guy Zyskind (@GuyZys), “Grad Student Descent: the preferred #nonlinear #optimization technique #machinelearning,” Twitter, April 27, 2015, https://oreil.ly/SW1or.\nLe, “Neural Architecture Search with Reinforcement Learning,” arXiv, November 5, 2016, https://oreil.ly/FhsuQ; Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V.\nLe, “Regularized Evolution for Image Classifier Architecture Search,” AAAI 2019, https://oreil.ly/FWYjn. 29 You can make the search space continuous to allow differentiation, but the resulting architecture has to be converted into a discrete\nTraining More Effective Learned Optimizers, and Using Them to Train Themselves,” arXiv, September 23, 2020, https://oreil.ly/IH7eT.\n33 Samantha Murphy, “The Evolution of Facebook News Feed,” Mashable, March 12, 2013, https://oreil.ly/1HMXh.\n34 Iveta Ryšavá, “What Mark Zuckerberg’s News Feed Looked Like in 2006,” Newsfeed.org, January 14, 2016, https://oreil.ly/XZT6Q.\n35 Martin Zinkevich, “Rules of Machine Learning: Best Practices for ML Engineering,” Google, 2019, https://oreil.ly/YtEsN.\nIn Chapters 4 through 6, we have discussed the considerations for developing an ML model, from creating training data, extracting features, and developing the model to crafting metrics to evaluate this model.\nThese considerations constitute the logic of the model—instructions on how to go from raw data into an ML model, as shown in Figure 7-1.\nDifferent aspects that make up the ML model logic\nIn this chapter, we’ll discuss another part in the iterative process: deploying your model.\nIn this chapter, we focus on deploying models to production environments.\nFor other teams, production means keeping your models up and running for millions of users a day.",
    "keywords": [
      "Men Applicants",
      "Women Applicants",
      "model",
      "Data",
      "models",
      "Admitted",
      "Applicants",
      "Machine Learning",
      "learning",
      "Machine Learning Research",
      "slices",
      "training data",
      "training",
      "critical slices",
      "evaluation"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "learning",
      "learned",
      "train",
      "likely",
      "slices",
      "slicing",
      "architecture"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 219-228)",
    "start_page": 219,
    "end_page": 228,
    "summary": "In many companies, the responsibility of deploying models falls into the hands of the same people who developed those models.\nIn this chapter, we’ll start off with some common myths about ML deployment that I’ve often heard from people who haven’t deployed ML models.\nWe’ll then discuss the two main ways a model generates and serves its predictions to users: online prediction and batch prediction.\nHow a model serves and computes the predictions influences how it should be designed, the infrastructure it requires, and the behaviors that users encounter.\nAs discussed in Chapter 1, deploying an ML model can be very different from deploying a traditional software program.\nThis difference might cause people who have never deployed a model before to either dread the process or underestimate how much time and effort it will take.\nMyth 1: You Only Deploy One or Two ML Models at a Time\nIn reality, companies have many, many ML models.\nIt needs a model to predict each of the following elements: ride demand, driver availability, estimated time of arrival, dynamic pricing, fraudulent transaction, customer churn, and more.\nWhile many companies still only update their models once a month, or even once a quarter, Weibo’s iteration cycle for updating some of their ML I’ve heard similar numbers at companies like models is 10 minutes.\nBatch Prediction Versus Online Prediction\nOne fundamental decision you’ll have to make that will affect both your end users and developers working on your system is how it generates and serves its predictions to end users: online or batch.\nThe terminologies surrounding batch and online prediction are still quite confusing due to the lack of standardized practices in the industry.\nBatch prediction, which uses only batch features.\nOnline prediction that uses only batch features (e.g., precomputed embeddings).\nOnline prediction that uses both batch features and streaming features.\nTo avoid this confusion, people sometimes prefer the terms “synchronous prediction” and “asynchronous prediction.” However, this distinction isn’t perfect either, because when online prediction leverages a real-time transport to send prediction requests to your model, the requests and predictions technically are asynchronous.\nFigure 7-4 shows a simplified architecture for batch prediction, and Figure 7-5 shows a simplified version of online prediction using only batch features.\nA simplified architecture for online prediction that uses only batch features\nIn batch prediction, only batch features are used.\nprediction, however, it’s possible to use both batch features and streaming features.\nOnline features are more general, as they refer to any feature used for online prediction, including batch features stored in memory.\nA very common type of batch feature used for online prediction, especially session- based recommendations, is item embeddings.\nItem embeddings are usually precomputed in batch and fetched whenever they are needed for online prediction.\nA simplified architecture for online prediction that uses both streaming features and batch features is shown in Figure 7-6.\nSome companies call this kind of prediction “streaming prediction” to distinguish it from the kind of online prediction that doesn’t use streaming features.\nA simplified architecture for online prediction that uses both batch features and streaming features",
    "keywords": [
      "online prediction",
      "prediction",
      "batch prediction",
      "batch features",
      "batch",
      "features",
      "online",
      "model",
      "models",
      "streaming features",
      "streaming",
      "ONLINE FEATURES",
      "n’t",
      "data",
      "’ll"
    ],
    "concepts": [
      "model",
      "predictions",
      "predict",
      "likely",
      "deployed",
      "deploy",
      "deployment",
      "features",
      "feature",
      "time"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 229-236)",
    "start_page": 229,
    "end_page": 236,
    "summary": "In many applications, online prediction and batch prediction are used side by side for different use cases.\nFor example, food ordering apps like DoorDash and UberEats use batch prediction to generate restaurant recommendations—it’d take too long to generate these recommendations online because there are many restaurants.\nMany people believe that online prediction is less efficient, both in terms of cost and performance, than batch prediction because you might not be able to batch inputs together and leverage vectorization or other optimization techniques.\nFrom Batch Prediction to Online Prediction\nYou give your model an input and it generates a prediction as soon as it receives that input.\nA problem with online prediction is that your model might take too long to generate predictions.\nBecause the predictions are precomputed, you don’t have to worry about how long it’ll take your models to generate predictions.\nFor this reason, batch prediction can also be seen as a trick to reduce the inference latency of more complex models—the time it takes to retrieve a prediction is usually less than the time it takes to generate it.\nBatch prediction is good for when you want to generate a lot of predictions and don’t need the results immediately.\nYou don’t have to use all the predictions generated.\nHowever, the problem with batch prediction is that it makes your model less responsive to users’ change preferences.\nAnother problem with batch prediction is that you need to know what requests to generate predictions for in advance.\nFrench, it might be impossible to anticipate every possible English text to be translated—you need to use online prediction to generate predictions as requests arrive.\nA (near) real-time pipeline that can work with incoming data, extract streaming features (if needed), input them into a model, and return a prediction in near real time.\nA model that can generate predictions at a speed acceptable to its end users.\nWhen companies started with ML, they leveraged their existing batch systems to make predictions.\nWhen these companies want to use streaming features for their online prediction, they need to build a separate streaming pipeline.\nFigure 7-8 shows a more detailed but also more complex feature of the data pipeline for ML systems that do online prediction.\nA data pipeline for ML systems that do online prediction\nWe’ve talked about a streaming pipeline that allows an ML system to extract streaming features from incoming data and input them into an ML model in (near) real time.\nIf the model you want to deploy takes too long to generate predictions, there are three main approaches to reduce its inference latency: make it do inference faster, make the model smaller, or make the hardware it’s deployed on run faster.",
    "keywords": [
      "Batch prediction",
      "Online prediction",
      "prediction",
      "generate predictions",
      "Batch",
      "model",
      "Online",
      "generate",
      "Model Compression",
      "pipeline",
      "Pipeline Batch prediction",
      "n’t",
      "inference",
      "data",
      "batch pipeline"
    ],
    "concepts": [
      "model",
      "models",
      "predictions",
      "predict",
      "likely",
      "stream",
      "streaming",
      "pipeline",
      "pipelines",
      "useful"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 237-244)",
    "start_page": 237,
    "end_page": 244,
    "summary": "factorization is compact convolutional filters, where the over-parameterized (having too many parameters) convolution filters are replaced with compact blocks to both reduce the number of parameters and increase speed.\nSource: Adapted from an image by Howard et al.\nHowever, it tends to be specific to certain types of models (e.g., compact convolutional filters are specific to convolutional neural networks) and requires a lot of architectural knowledge to design, so it’s not widely applicable to many use cases yet.\nKnowledge distillation is a method in which a small model (student) is trained to mimic a larger model or ensemble of models (teacher).\nOne example of a distilled network used in production is DistilBERT, which reduces the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster.\nIf you use a pretrained model as the teacher model, training the student network will require less data and will likely be faster.\nOne is to remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters.\nThis helps with reducing the size of a model because pruning makes a neural network more sparse, and sparse architecture tends to require less storage space than dense structure.\nExperiments show that pruning techniques can reduce the nonzero parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising overall accuracy.\nIn some cases, pruning can be useful as an architecture search paradigm, and the pruned architecture should be retrained from scratch as a dense model.\nQuantization reduces a model’s size by using fewer bits to represent its parameters.\nIf a model has 100M parameters and each requires 32 bits to store, it’ll take up 400 MB.\nThis method is also known as “fixed point.” In the extreme case, some have attempted the 1-bit representation of each weight (binary weight neural networks), e.g., BinaryConnect and XNOR-Net. The authors of the XNOR-Net paper spun off Xnor.ai, a startup that focused on model compression.\nSecond, less precision speeds up computation, which further reduces training time and inference latency.\nReducing the number of bits to represent your numbers means that you can represent a smaller range of values.\nQuantization can either happen during training (quantization aware training), where models are trained in single-precision floating point and then quantized for inference.\nUsing quantization during training means that you can use less memory for each parameter, which allows you to train larger models on the same hardware.\nTPUs (tensor processing units) also support training with Bfloat16 (16-bit Brain Floating Point Format), which the company dubbed “the secret to 34 high performance on Cloud TPUs.” Training in fixed-point is not yet as popular but has had a lot of promising results.\nAnother decision you’ll want to consider is where your model’s computation will happen: on the cloud or on the edge.\nThe easiest way is to package your model up and deploy it via a managed cloud service such as AWS or GCP, and this is how many companies deploy when they get started in ML.\nCloud services have done an incredible job to make it easy for companies to bring ML models into production.",
    "keywords": [
      "model",
      "Cloud",
      "models",
      "number",
      "parameters",
      "convolution",
      "Pruning",
      "Low-Rank Factorization",
      "network",
      "training",
      "Quantization",
      "teacher",
      "neural networks",
      "networks",
      "edge"
    ],
    "concepts": [
      "models",
      "model",
      "trained",
      "training",
      "train",
      "cloud",
      "clouds",
      "bits",
      "bit",
      "pruning"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 245-256)",
    "start_page": 245,
    "end_page": 256,
    "summary": "Requiring data transfer over the network (sending data to the model on the cloud to make predictions then sending predictions back to the users) might make some use cases impossible.\nTo move computation to the edge, the edge devices have to be powerful enough to handle the computation, have enough memory to store ML models and load them into memory, as well as have enough battery or be connected to an energy source to power the application for a reasonable amount of time.\nBecause of the many benefits that edge computing has over cloud computing, companies are in a race to develop edge devices optimized for different ML use cases.\nWith so many new offerings for hardware to run ML models on, one question arises: how do we make our model run on arbitrary hardware efficiently?\nIn the following section, we’ll discuss how to compile and optimize a model to run it on a certain hardware backend.\nCompiling and Optimizing Models for Edge Devices\nFor a model built with a certain framework, such as TensorFlow or PyTorch, to run on a hardware backend, that framework has to be supported by the hardware vendor.\nMapping from ML workloads to a hardware backend requires understanding and taking advantage of that hardware’s design, and different hardware backends have different memory layouts and compute primitives, as shown in Figure 7-11.\nDeploying ML models to new hardware requires significant manual effort.\nFrom the original code for a model, compilers generate a series of high- and low-level IRs before generating the code native to a hardware backend so that it can run on that hardware backend, as shown in Figure 7-12.\nA series of high- and low-level IRs between the original model code to machine code that can run on a given hardware backend\nHigh-level IRs are usually computation graphs of your ML models.\nAfter you’ve “lowered” your code to run your models into the hardware of your choice, an issue you might run into is performance.\ndata locality and hardware caches, or it may not leverage advanced features such as vector or parallel operations that could speed code up.\nIn many companies, what usually happens is that data scientists and ML engineers develop models that seem to be working fine in development.\nHowever, when these models are deployed, they turn out to be too slow, so their companies hire optimization engineers to optimize their models for the hardware their models run on.\nThis vision comes together in the AI Engineering team, where our expertise is used to develop AI algorithms and models that are optimized for our hardware, as well as to provide guidance to Mythic’s hardware and compiler teams.\nIn the process of lowering ML model code into machine code, compilers can look at the computation graph of your ML model and the operators it consists of—convolution, loops, cross-entropy—and find a way to speed it up.\nThere are two ways to optimize your ML models: locally and globally.\nLocally is when you optimize an operator or a set of operators of your model.\nThere are standard local optimization techniques that are known to speed up your model, most of them making things run in parallel or reducing memory access on chips.\nnetwork with the computation graph can be fused vertically or horizontally to reduce memory access and speed up the model, as shown in Figure 7-14.\nUsing ML to optimize ML models\nAs hinted by the previous section with the vertical and horizontal fusion for a convolutional neural network, there are many possible ways to execute a given computation graph.\nTraditionally, framework and hardware vendors hire optimization engineers who, based on their experience, come up with heuristics on how to best execute the computation graph of a model.\nThis is complicated by the fact that model optimization is dependent on the operators its computation graph consists of.\nHardware vendors like NVIDIA and Google focus on optimizing popular models like ResNet-50 and BERT for their hardware.\nIf you don’t have ideas for good heuristics, one possible solution might be to try all possible ways to execute a computation graph, record the time they need to run, then pick the best one.\nautoTVM measures the actual time it takes to run each path it goes down, which gives it ground truth data to train a cost model to predict how long a future path will take.\nThe pro of this approach is that because the model is trained using the data generated during runtime, it can adapt to any type of hardware it runs on.\nYou optimize your model once for one hardware backend then run it on multiple devices of that same hardware type.\nThis sort of optimization is ideal when you have a model ready for production and target hardware to run inference on.\nWe’ve been talking about how compilers can help us generate machine- native code run models on certain hardware backends.\nAfter you’ve built your models in scikit-learn, PyTorch, TensorFlow, or whatever frameworks you’ve used, instead of compiling your models to run on specific hardware, you can compile your model to WASM.\nWe’ve discussed different ways to deploy a model, comparing online prediction with batch prediction, and ML on the edge with ML on the cloud.",
    "keywords": [
      "hardware",
      "model",
      "models",
      "run",
      "computation graph",
      "hardware backend",
      "computation",
      "code",
      "graph",
      "data",
      "edge",
      "code run models",
      "WASM",
      "hardware vendors",
      "framework"
    ],
    "concepts": [
      "hardware",
      "models",
      "model",
      "optimized",
      "optimize",
      "optimizing",
      "optimization",
      "running",
      "run",
      "runs"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 257-264)",
    "start_page": 257,
    "end_page": 264,
    "summary": "In the next chapter, we’ll discuss how our models might fail in production, and how to continually monitor models to detect issues and address them as fast as possible.\nfor One Large ML Pipeline,” Google, 2020, video, 19:06, https://oreil.ly/HjQm0.\nLearning Models: 6 Lessons Learned at Booking.com,” KDD ’19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (July 2019): 1743–51, https://oreil.ly/Ea1Ke.\n9 “2021 Enterprise Trends in Machine Learning,” Algorithmia, https://oreil.ly/9kdcw.\n14 “Developer Survey Results,” Stack Overflow, 2019, https://oreil.ly/guYIq.\nML model.\n18 Shuyi Chean and Fabian Hueske, “Streaming SQL to Unify Batch & Stream Processing w/ Apache Flink @Uber,” InfoQ, https://oreil.ly/XoaNu; Yu, “Machine Learning with Flink in Weibo.”\n19 Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang, “A Survey of Model Compression and Acceleration for Deep Neural Networks,” arXiv, June 14, 2020, https://oreil.ly/1eMho. 20 Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman, “Speeding up Convolutional Neural\nNetworks with Low Rank Expansions,” arXiv, May 15, 2014, https://oreil.ly/4Vf4s.\nDally, and Kurt Keutzer, “SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size,” arXiv, November 4, 2016, https://oreil.ly/xs3mi.\nHoward, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam, “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,” arXiv, April 17, 2017, https://oreil.ly/T84fD.\nNetwork,” arXiv, March 9, 2015, https://oreil.ly/OJEPW.\nVersion of BERT: Smaller, Faster, Cheaper and Lighter,” arXiv, October 2, 2019, https://oreil.ly/mQWBv.\nTrainable Neural Networks,” ICLR 2019, https://oreil.ly/ychdl.\nState of Neural Network Pruning?” arXiv, March 6, 2020, https://oreil.ly/VQsC3.\nValue of Network Pruning,” arXiv, March 5, 2019, https://oreil.ly/mB4IZ.\nfor Model Compression,” arXiv, November 13, 2017, https://oreil.ly/KBRjy.\nDeep Neural Networks with Binary Weights During Propagations,” arXiv, November 2, 2015, https://oreil.ly/Fwp2G; Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi, “XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,” arXiv, August 2, 2016, https://oreil.ly/gr3Ay.\nVahid Noroozi, and Ravi Gadde, “Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq,” NVIDIA Devblogs, October 9, 2018, https://oreil.ly/WDT1l.\nTPUs,” Google Cloud Blog, August 23, 2019, https://oreil.ly/ZG5p0.\nActivations,” Journal of Machine Learning Research 18 (2018): 1–30; Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko, “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,” arXiv, December 15, 2017, https://oreil.ly/sUuMT.\nCPUs,” Roblox, May 27, 2020, https://oreil.ly/U01Uj.\nBills,” The Information, February 25, 2019, https://oreil.ly/H9ans; Mats Bauer, “How Much Does Netflix Pay Amazon Web Services Each Month?” Quora, 2020, https://oreil.ly/HtrBk.\n38 “2021 State of Cloud Cost Report,” Anodot, https://oreil.ly/5ZIJK.\n39 “Burnt $72K Testing Firebase and Cloud Run and Almost Went Bankrupt,” Hacker News, December 10, 2020, https://oreil.ly/vsHHC; “How to Burn the Most Money with a Single Click in Azure,” Hacker News, March 29, 2020, https://oreil.ly/QvCiI.\n2025,” Statista, https://oreil.ly/BChLN.\n43 Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, et al., “TVM: An Automated End-to-End Optimizing Compiler for Deep Learning,” arXiv, February 12, 2018, https://oreil.ly/vGnkW.\n48 Shashank Prasanna, Prethvi Kashinkunti, and Fausto Milletari, “TensorRT 3: Faster TensorFlow Inference and Volta Support,” NVIDIA Developer, December 4, 2017, https://oreil.ly/d9h98.\nNative Code,” USENIX, https://oreil.ly/uVzrX.\nWe’ll start by covering reasons why ML models that perform great during development fail in production.\nThen, we’ll take a deep dive into one especially prevalent and thorny issue that affects almost all ML models in production: data distribution shifts.\nIn the next chapter, we’ll cover how to continually update your models in production to adapt to shifts in data distributions.\nperformance expectation violations are harder to detect as doing so requires measuring and monitoring the performance of ML models in production.\nTo effectively detect and fix ML system failures in production, it’s useful to understand why a model, after proving to work well during development, would fail in production.",
    "keywords": [
      "Machine Learning",
      "Machine Learning Models",
      "software system failures",
      "Neural Networks",
      "Convolutional Neural Networks",
      "System Failures",
      "Machine Learning Systems",
      "Deep Neural Networks",
      "System",
      "model",
      "machine translation system",
      "software system",
      "Learning",
      "Training Neural Networks",
      "Machine"
    ],
    "concepts": [
      "data",
      "models",
      "model",
      "learning",
      "learned",
      "failures",
      "failure",
      "companies",
      "company",
      "deployed"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 265-272)",
    "start_page": 265,
    "end_page": 272,
    "summary": "Examples include data collection and processing problems, poor hyperparameters, changes in the training pipeline not correctly replicated in the inference pipeline and vice versa, data distribution shifts that cause a model’s performance to deteriorate over time, edge cases, and degenerate feedback loops.\nIn this chapter, we’ll discuss three new but very common problems that arise after a model has been deployed: production data differing from training data, edge cases, and degenerate feedback loops.\nWhen we say that an ML model learns from the training data, it means that the model learns the underlying distribution of the training data with the goal of leveraging this learned distribution to generate accurate predictions for unseen data—data that it didn’t see during training.\nWhen the model is able to generate accurate predictions for unseen data, 6 we say that this model “generalizes to unseen data.” The test data that we use to evaluate a model during development is supposed to represent unseen data, and the model’s performance on the test data is supposed to give us an idea of how well the model will generalize.\nOne of the first things I learned in ML courses is that it’s essential for the training data and the unseen data to come from a similar distribution.\nIf the unseen data comes from a different distribution, the model might not generalize well.\nCurating a training dataset that can accurately represent the data that a model will encounter in production turns out to be very difficult.\nThere are many different selection and sampling biases, as discussed in Chapter 4, that can happen and make real-world data diverge from training data.\nData distributions shift.\nAnother common failure mode is that a model does great when first deployed, but its performance degrades over time as the data distribution changes.\nWhen I use COVID-19 as an example that causes data shifts, some people have the impression that data shifts only happen because of unusual events, which implies they don’t happen often.\nmissing values incorrectly inputted, inconsistencies between the features extracted during training and inference, features standardized using statistics from the wrong subset of data, wrong model version, or bugs in the app interface that force users to change their behaviors.\nAn ML model that performs well on most cases but fails on a small number of cases might not be usable if these failures cause catastrophic consequences.\nEdge cases are the data samples so extreme that they cause the model to make catastrophic mistakes.\nEven though edge cases generally refer to data samples drawn from the same distribution, if there is a sudden increase in the number of data samples in which your model doesn’t perform well, it could be an indication that the underlying data distribution has shifted.\nAn outlier can cause a model to perform unusually poorly, which makes it an edge case.\nIn many cases, it might be beneficial to remove outliers as it helps your model to learn better decision boundaries and generalize better to unseen data.\nA degenerate feedback loop can happen when the predictions themselves influence the feedback, which, in turn, influences the next iteration of the model.\nIn ML, a system’s predictions can influence how users interact with the system, and because users’ interactions with the system are sometimes used as training data to the same system, degenerate feedback loops can occur and cause unintended consequences.\nDegenerate feedback loops are especially common in tasks with natural labels from users, such as recommender systems and ads click-through-rate prediction.\nIn the beginning, the rankings of two songs, A and B, might be only marginally different, but because A was originally ranked a bit higher, it showed up higher in the recommendation list, making users click on A more, which made the system rank A even Degenerate feedback loops are one reason why higher.\nThe model finds that feature X accurately predicts whether someone is qualified, so it recommends resumes with feature X.\nthe importance of each feature for the model, as discussed in Chapter 5—can help detect the bias toward feature X in this case.\nLeft unattended, degenerate feedback loops can cause your model to perform suboptimally at best.\nFor the task of recommender systems, it’s possible to detect degenerate feedback loops by measuring the popularity diversity of a system’s outputs even when the system is offline.\nWe’ve discussed that degenerate feedback loops can cause a system’s outputs to be more homogeneous over time.\nIn the case of recommender systems, instead of showing the users only the items that the system ranks highly for them, we show users random items and use their feedback to determine the true quality of these items.\nWe’ve also discussed that degenerate feedback loops are caused by users’ feedback on predictions, and users’ feedback on a prediction is biased based on where it is shown.\nYou are unsure whether your model is exceptionally good at picking the top song, or whether users click on any song as long as it’s recommended on top.\nDuring training, you add “whether a song is recommended first” as a feature to your training data, as shown in Table 8-1.\nThis feature allows your model to learn how much being a top recommendation influences how likely a song is clicked on.\nDuring inference, you want to predict whether a user will click on a song regardless of where the song is recommended, so you might want to set the 1st Position feature to be False.\nThe first model predicts the probability that the user will see and consider a recommendation taking into account the position at which that recommendation will be shown.\nThe second model then predicts the probability that the user will click on the item given that they saw and considered it.\nData Distribution Shifts\nIn this section, we’ll zero in onto one especially sticky cause of failures: data distribution shifts, or data shifts for short.\nData distribution shift refers to the phenomenon in supervised learning when the data a model works with changes over time, which causes this model’s predictions to become less accurate as time passes.\nThe distribution of the data the model is trained on is called the source distribution.\nThe distribution of the data the model runs inference on is called the target distribution.\nEven though discussions around data distribution shift have only become common in recent years with the growing adoption of ML in the industry, data distribution shift in systems that learned from data has been studied as early as in 1986.\nTypes of Data Distribution Shifts\nNote that this discussion on different types of data shifts is math-heavy and mostly useful from a research perspective: to develop efficient algorithms to detect and address data shifts requires understanding the causes of those shifts.\nWe know that in supervised learning, the training data can be viewed as a set of samples from the joint distribution P(X, Y), and then ML usually models P(Y|X).\nMathematically, covariate shift is when P(X) changes, but P(Y|X) remains the same, which means that the distribution of the input changes, but the conditional probability of an output given an input remains the same.",
    "keywords": [
      "data distribution shifts",
      "data",
      "data distribution",
      "degenerate feedback loops",
      "training data",
      "model",
      "data shifts",
      "degenerate feedback",
      "feedback loops",
      "distribution",
      "distribution shifts",
      "feedback",
      "system",
      "unseen data",
      "shift"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "distributions",
      "users",
      "user",
      "shift",
      "shifted",
      "items",
      "item"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 273-280)",
    "start_page": 273,
    "end_page": 280,
    "summary": "You know that the risk of breast cancer is higher for women over the age of 40, over the age of 40 in your training data than in your inference data, so the input distributions differ for your training and inference data.\nDuring model development, covariate shifts can happen due to biases during the data selection process, which could result from difficulty in collecting examples for certain classes.\nCovariate shifts can also happen because the training data is artificially altered to make it easier for your model to learn.\nAs discussed in Chapter 4, it’s hard for ML models to learn from imbalanced datasets, so you might want to collect more samples of the rare classes or oversample your data on the rare classes to make it easier for your model to learn the rare classes.\nCovariate shift can also be caused by the model’s learning process, especially through active learning.\nThis means that the training input distribution is altered by the learning process to differ from the real-world input distribution, and covariate shifts are a by-product.\nThe input distribution into your model has changed, but the probability that a user with a given income level will convert remains the same.\nIf you know in advance how the real-world input distribution will differ from your training input distribution, you can leverage techniques such as importance weighting to train your model to work for the real-world data.\nImportance weighting consists of two steps: estimate the density ratio between the real-world input distribution and the training input distribution, then weight the training data according to this ratio and train an ML model on this weighted data.\nHowever, because we don’t know in advance how the distribution will change in the real world, it’s very difficult to preemptively train your models to make them robust to new, unknown distributions.\nThere has been research that attempts to help models learn representations of latent variables that are invariant across data distributions, but I’m not aware of their adoption in the industry.\nRemember that covariate shift is when the input distribution changes.\nWhen the input distribution changes, the output distribution also changes, resulting in both covariate shift and label shift happening at the same time.\nHowever, given a person with breast cancer, the age distribution remains the same, so this is still a case of label shift.\nBecause label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts are similar to covariate shift adaptation methods.\nConcept drift, also known as posterior shift, is when the input distribution remains the same but the conditional distribution of the output given an input changes.\nYou can think of this as “same input, different output.” Consider you’re in charge of a model that predicts the price of a house based on its features.\nGeneral Data Distribution Shifts\nWhen the number of classes changes, your model’s structure might change, both relabel your data and retrain your model from scratch.\nDetecting Data Distribution Shifts\nData distribution shifts are only a problem if they cause your model’s performance to degrade.\nAccuracy-related metrics work by comparing the model’s predictions to ground truth labels.\nIn research, there have been efforts to understand and detect label shifts without labels from the target distribution.\nHowever, in the industry, most drift detection methods focus on detecting changes in the input distribution, especially the distributions of features, as we discuss in detail in this chapter.\nAs of October 2021, even TensorFlow Extended’s built-in data validation tools use only summary statistics to detect the skew between the training and serving data and shifts between different days of training data.\nIf those metrics differ significantly, the inference distribution might have shifted from the training distribution.\nIf you consider the data from yesterday to be the source population and the data from today to be the target population and they are statistically different, it’s likely that the underlying data distribution has shifted between yesterday and today.\nIf your model’s predictions and labels are one-dimensional (scalar numbers), then the KS test is useful to detect label or prediction shifts.\nFor example, shifts happen at different rates, and abrupt changes are easier to detect than slow, gradual changes.\nTo detect temporal shifts, a common approach is to treat input data to ML applications as time-series data.\nWhen dealing with temporal shifts, the time scale window of the data we look at affects the shifts we can detect.\nIf we use data from day 9 to day 14 as the source distribution, then day 15 looks like a shift.\nAs of today, many companies use the distribution of the training data as the base distribution and monitor the production data distribution at a certain granularity level, such as hourly and daily.\nwindow, the faster you’ll be able to detect changes in your data distribution.\nMore advanced monitoring platforms even attempt a root cause analysis (RCA) feature that automatically analyzes statistics across various time window sizes to detect exactly the time window where a change in data happened.\nAddressing Data Distribution Shifts\nAt one end of the spectrum, we have companies that have just started with ML and are still working on getting ML models into production, so they might not have gotten to the point where data shifts are catastrophic to them.\nThey will then need to adapt their models to the shifted distributions or to replace them with other solutions.\nAt the same time, many companies assume that data shifts are inevitable, so they periodically retrain their models —once a month, once a week, or once a day—regardless of the extent of the shift.\nTo make a model work with a new distribution in production, there are three main approaches.\nThe hope here is that if the training dataset is large enough, the model will be able to learn such a comprehensive distribution that whatever data points the model will encounter in production will likely come from this distribution.\nThe second approach, less popular in research, is to adapt a trained model to a target distribution without requiring new labels.\n(2013) used causal interpretations together with kernel embedding of conditional and marginal distributions to correct models’ predictions for both covariate shifts and label shifts without using labels Similarly, Zhao et al.\nThe third approach is what is usually done in the industry today: retrain your model using the labeled data from the target distribution.\nRetraining can mean retraining your model from scratch on both the old and new data or continuing training the existing model on new data.\nSimilarly, if you consider learning a joint distribution P(X, Y) as a task, then adapting a model trained on one joint distribution for another joint distribution can be framed as a form of transfer learning.\nHowever, to adapt your model to a new distribution, you might need to retrain your model from scratch.\nAddressing data distribution shifts doesn’t have to start after the shifts have happened.\nFor example, housing prices might change a lot faster in major cities like San Francisco than in rural Arizona, so a housing price prediction model serving rural Arizona might need to be updated less frequently than a model serving San Francisco.\nWithin ML-specific metrics, there are generally four artifacts to monitor: a model’s accuracy-related metrics, predictions, features, and raw inputs.\nEven if the feedback can’t be used to infer natural labels directly, it can be used to detect changes in your ML model’s performance.\nYou can monitor predictions for distribution shifts.\nBecause predictions are low dimensional, it’s also easier to compute two-sample tests to detect whether the prediction distribution has shifted.",
    "keywords": [
      "Data Distribution Shifts",
      "data",
      "model",
      "distribution",
      "shifts",
      "input distribution",
      "Distribution Shifts",
      "Data Distribution",
      "shift",
      "Label shift",
      "covariate shift",
      "Distribution Shifts Data",
      "Shifts Data distribution",
      "time",
      "data shifts"
    ],
    "concepts": [
      "distribution",
      "shifts",
      "shift",
      "shifted",
      "model",
      "label",
      "labels",
      "labeled",
      "labeling",
      "statistical"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 281-289)",
    "start_page": 281,
    "end_page": 289,
    "summary": "Monitoring features\nML monitoring solutions in the industry focus on tracking changes in features, both the features that a model uses as inputs and the intermediate transformations from raw inputs into final features.\nFeature monitoring is appealing because compared to raw input data, features are well structured following a predefined schema.\nThe first step of feature monitoring is feature validation: ensuring that your features follow an expected schema.\nBeyond basic feature validation, you can also use two-sample tests to detect whether the underlying distribution of a feature or a set of features has shifted.\nWhile tracking features is useful for debugging purposes, it’s not very useful for detecting model performance degradation.\nIn theory, a small distribution shift can cause catastrophic failure, but in practice, an individual feature’s minor changes might not harm the model’s performance at all.\nFeature distributions shift all the time, and most of these changes are benign.\nThe problem of feature monitoring becomes the problem of trying to decide which feature shifts are critical and which are not.\nEven if you detect a harmful change in a feature, it might be impossible to detect whether this change is caused by a change in the underlying input distribution or whether it’s caused by an error in one of the multiple processing steps.\nIf you don’t have a way to version your schemas and map each of your features to its expected schema, the cause of the reported alert might be due to the mismatched schema rather than a change in the data.\nThese concerns are not to dismiss the importance of feature monitoring; changes in the feature space are a useful source of signals to understand the health of your ML systems.\nAs discussed in the previous section, a change in the features might be caused by problems in processing steps and not by changes in data.\nThe raw input data might not be easier to monitor, as it can come from multiple sources in different formats, following multiple structures.\nThe way many ML workflows are set up today also makes it impossible for ML engineers to get direct access to raw input data, as the raw input data is often managed by a data platform team who processes and moves the data to a location like a data warehouse, and the ML engineers can only query for data from that data warehouse where the data is already partially processed.\nTherefore, monitoring raw inputs is often a responsibility of the data platform team, not the data science or ML team.\nSo far, we’ve discussed different types of metrics to monitor, from operational metrics generally used for software systems to ML-specific metrics that help you keep track of the health of your ML models.\nIn the next section, we’ll discuss the toolbox you can use to help with metrics monitoring.\nIt’s common for the industry to herald metrics, logs, and traces as the three pillars of monitoring.\nThey seem to be generated from the perspective of people who develop monitoring systems: traces are a form of logs and metrics can be computed from logs.\nsection, I’d like to focus on the set of tools from the perspective of users of the monitoring systems: logs, dashboards, and alerts.\nAn example use case of ML in log analysis is anomaly detection: to detect abnormal events in your system.\nWhen our monitoring system detects something suspicious, it’s necessary to alert the right people about it.\nSince the mid-2010s, the industry has started embracing the term “observability” instead of “monitoring.” Monitoring makes no assumption about the relationship between the internal state of a system and its outputs.\nThe word “telemetry” comes from the Greek roots tele, meaning “remote,” and metron, meaning “measure.” So telemetry basically means “remote measures.” In the monitoring context, it refers to logs and metrics collected from remote components such as cloud services or applications run on customer devices.\nWhen something goes wrong with an observable system, we should be able to figure out what went wrong by looking at the system’s logs and metrics without having to ship new code to the system.\nFor example, you should be able to query your logs for the answers to questions like: “show me all the users for which model A returned wrong predictions over the last hour, grouped by their zip codes” or “show me the outliers requests in the last 10 minutes” or “show me all the intermediate outputs of this input through the system.” To achieve this, you need to have logged your\nFor example, when a model’s performance degrades over the last hour, being able to interpret which feature contributes the most to all the wrong predictions made over the last hour will help with figuring out what went wrong with the system and how to fix it.\nIn this section, we’ve discussed multiple aspects of monitoring, from what data to monitor and what metrics to keep track of to different tools for monitoring and observability.\nWe discussed three major causes of ML-specific failures: production data differing from training data, edge cases, and degenerate feedback loops.\nTo be able to detect shifts, we need to monitor our deployed systems.\nMonitoring is an important set of practices for any software engineering system in production, not just ML, and it’s an area of ML where we should learn as much as we can from the DevOps world.\nWe discussed different metrics we need to monitor: operational metrics—the metrics that should be monitored with any software systems such as latency, throughput, and CPU utilization—and ML-specific metrics.\nMonitoring can be applied to accuracy-related metrics, predictions, features, and/or raw inputs.\nIt’s easy to build dashboards to show graphs, but it’s much more difficult to understand what a graph means, whether it shows signs of drift, and, if there’s drift, whether it’s caused by an underlying data distribution change or by errors in the pipeline.\n“Soft error,” https://oreil.ly/4cvNg).\n4 Daniel Papasian and Todd Underwood, “How ML Breaks: A Decade of Outages for One Large ML Pipeline,” Google, July 17, 2020, video, 19:06, https://oreil.ly/WGabN.\n8 John Mcquaid, “Limits to Growth: Can AI’s Voracious Appetite for Data Be Tamed?” Undark, October 18, 2021, https://oreil.ly/LSjVD.\ndrivers was 15.8, or 0.0158% (“Fatality Rate per 100,000 Licensed Drivers in the U.S. from 1990 to 2019,” Statista, 2021, https://oreil.ly/w3wYh).\n11 Rodney Brooks, “Edge Cases for Self Driving Cars,” Robots, AI, and Other Stuff, June 17, 2017, https://oreil.ly/Nyp4F; Lance Eliot, “Whether Those Endless Edge or Corner Cases Are the Long-Tail Doom for AI Self-Driving Cars,” Forbes, July 13, 2021, https://oreil.ly/L2Sbp; Kevin McAllister, “Self-Driving Cars Will Be Shaped by Simulated, Location Data,” Protocol, March 25, 2021, https://oreil.ly/tu8hs.\n8 (2011): 1373–86, https://oreil.ly/tGhHi; Daniel Fleder and Kartik Hosanagar, “Blockbuster Culture’s Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity,” Management Science 55, no.\n5 (2009), https://oreil.ly/Zwkh8; Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher, “Managing Popularity Bias in Recommender Systems with Personalized Re-ranking,” arXiv, January 22, 2019, https://oreil.ly/jgYLr.\nwith RecList,” arXiv, November 18, 2021, https://oreil.ly/7GfHk.\nLearning and Evaluation,” arXiv, February 17, 2016, https://oreil.ly/oDPSK.\nMatching,” Journal of Machine Learning Research (2009), https://oreil.ly/s49MI.\nNeurIPS Proceedings 2020, https://oreil.ly/GzJ1r; Gretton et al., “Covariate Shift by Kernel Mean Matching.”\nProceedings of Machine Learning Research 97 (2019): 7523–32, https://oreil.ly/ZxYWD.\n35 Li Bu, Cesare Alippi, and Dongbin Zhao, “A pdf-Free Change Detection Test Based on Density Difference Estimation,” IEEE Transactions on Neural Networks and Learning Systems 29, no.\nMethod,” 2006, https://oreil.ly/Dnv0s.\nDetection on Time-series Data,” arXiv, October 12, 2021, https://oreil.ly/xmdqW.\nProceedings of Machine Learning Research 97 (2019): 7523–32, https://oreil.ly/W78hH.\n45 Some monitoring vendors claim that their solutions are able to detect not only when your model should be retrained, but also what data to retrain\n46 “Amazon Compute Service Level Agreement,” Amazon Web Services, last updated August 24, 2021, https://oreil.ly/5bjx9.\n49 Ian Malpass, “Measure Anything, Measure Everything,” Code as Craft, February 15, 2011, https://oreil.ly/3KF1K.\n50 Andrew Morgan, “Data Engineering in Badoo: Handling 20 Billion Events Per Day,” InfoQ, August 9, 2019, https://oreil.ly/qnnuV.\n51 Charity Majors, “Observability—A 3-Year Retrospective,” The New Stack, August 6, 2019, https://oreil.ly/Logby.\n52 “Log Management Market Size, Share and Global Market Forecast to 2026,” MarketsandMarkets, 2021, https://oreil.ly/q0xgh.\nWe also discussed multiple monitoring techniques and tools to detect data distribution shifts.\nThis chapter is a continuation of this discussion: how do we adapt our models to data distribution shifts?\nAfter you’ve set up your infrastructure to allow you to update your models as frequently as you want, you might want to consider the question that I’ve been asked by almost every single ML engineer I’ve met: “How often should I retrain my models?” This question is the focus of the next section of the book.\nThis process is a way to test your systems with live data in production to ensure that your updated model indeed works without catastrophic consequences.\nIf monitoring means passively keeping track of the outputs of whatever model is being used, test in production means proactively choosing which model to produce outputs so that we can evaluate it.",
    "keywords": [
      "data",
      "system",
      "Machine Learning",
      "Monitoring",
      "Feature",
      "data distribution shifts",
      "systems",
      "Feature monitoring",
      "learning",
      "Machine Learning Research",
      "Machine Learning Systems",
      "metrics",
      "features",
      "raw input data",
      "feature validation"
    ],
    "concepts": [
      "data",
      "monitor",
      "monitored",
      "feature",
      "metrics",
      "metric",
      "logs",
      "log",
      "logged",
      "mean"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 290-297)",
    "start_page": 290,
    "end_page": 297,
    "summary": "When hearing “continual learning,” many people think of the training paradigm where a model updates itself with every incoming sample in production.\nCompanies that employ continual learning in production update their models in micro-batches.\nStill, the term “continual learning” makes people imagine updating models very frequently, such as every 5 or 10 minutes.\nHowever, continual learning isn’t about the retraining frequency, but the manner in which the model is retrained.\nMost companies do stateless retraining—the model is trained from scratch each time.\nmeans also allowing stateful training—the model continues training on new data.\nStateful training allows you to update your model with less data.\nTraining a model from scratch tends to require a lot more data than fine-tuning the same model.\nFor example, if you retrain your model from scratch, you might need to use all data from the last three months.\nIn the traditional stateless retraining, a data sample might be reused during multiple training iterations of a model, which means that data needs to be stored.\nIn the stateful training paradigm, each model update is trained using only the fresh data, so a data sample is used only once for training, as shown in Figure 9- 2.\nThis means that it’s possible to train your model without having to store data in permanent storage, which helps eliminate many concerns about data privacy.\nThe companies that have most successfully used stateful training also occasionally train their model from scratch on a large amount of data to calibrate it.\nAlternatively, they might also train their model from scratch in parallel with stateful training and then combine both updated models using techniques such as parameter server.\nContinual learning is about setting up infrastructure in a way that allows you, a data scientist or ML engineer, to update your models whenever it is needed, whether from scratch or fine-tuning, and to deploy this update quickly.\nAs of today, stateful training is mostly applied for data iteration, as changing your model architecture or adding a new feature still requires training the resulting model from scratch.\nSome people use “online learning” to refer to the specific setting where a model learns from each incoming new sample.\nI also use the term “continual learning” instead of “continuous learning.” Continuous learning refers to the regime in which your model continuously learns with each incoming sample, whereas with continual learning, the learning is done in a series of batches or micro- batches.\nContinuous learning is sometimes used to refer to continuous delivery of ML, which is closely related to continual learning as both help companies to speed up the iteration cycle of their ML models.\nWe discussed that continual learning is about setting up infrastructure so that you can update your models and deploy these changes as fast as you want.\nTo improve performance, your model should learn throughout the day with fresh data.\nThe cold start problem arises when your model has to make predictions for a new user without any historical data.\nIf your model doesn’t adapt quickly enough, it won’t be able to make recommendations relevant to these users until the next time the model is updated.",
    "keywords": [
      "continual learning",
      "learning",
      "model",
      "Stateful Training",
      "continual",
      "training",
      "data",
      "models",
      "Stateful",
      "existing model",
      "update",
      "n’t",
      "continuous learning",
      "online learning",
      "retraining"
    ],
    "concepts": [
      "data",
      "training",
      "trained",
      "train",
      "learned",
      "learns",
      "learn",
      "model",
      "models",
      "continues"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 298-306)",
    "start_page": 298,
    "end_page": 306,
    "summary": "If you want to update your model every hour, you need new data every hour.\nIf your model needs labeled data to update, as most models today do, this data will need to be labeled as well.\nIn many applications, the speed at which a model can be updated is bottlenecked by the speed at which data is labeled.\nLabel computation can be done with batch processing: e.g., waiting for logs to be deposited into data warehouses first before running a batch job to extract all labels from logs at once.\nIf your model’s speed iteration is bottlenecked by labeling speed, it’s also possible to speed up the labeling process by leveraging programmatic labeling tools like Snorkel to generate fast labels with minimal human intervention.\nGiven that tooling around streaming is still nascent, architecting an efficient streaming-first infrastructure for accessing fresh data and extracting fast labels from real-time transports can be engineering-intensive and costly.\nSecond, continual learning makes your models more susceptible to coordinated manipulation and adversarial attack.\nBecause your models learn online from real-world data, it makes it easier for users to input malicious data to trick models into learning wrong things.\nWhen designing the evaluation pipeline for continual learning, keep in mind that evaluation takes time, which can be another bottleneck for model update frequency.\nTo be precise, it only affects matrix-based and tree-based models that want to be updated very fast (e.g., hourly).\nYou can update the neural network model with a data batch of any size.\nHowever, if you want to update the collaborative filtering model, you first need to use\nIt’s much easier to adapt models like neural networks than matrix-based and tree-based models to the continual learning paradigm.\nHowever, there have been algorithms to create tree-based models that can learn from incremental amounts of data, most notably Hoeffding Tree and its variants Hoeffding Window Tree and Hoeffding Adaptive Tree, widespread.\nWhen your model can only see a small subset of data at a time, in theory, you can compute these statistics for each subset of data.\nBecause your team is focusing on developing new models, updating existing models takes a backseat.\nYou update an existing model only when the following two conditions are met: the model’s performance has degraded to the point that it’s doing more harm than good, and your team has time to update it.\nSome of your models are being updated once every six months.\nThe process of updating a model is manual and ad hoc.\nSomeone else cleans this new data, extracts features from it, retrains that model from scratch on both the old and new data, and then exports the updated model into a binary format.\ndeploys the updated model.\nOftentimes, the code encapsulating data, features, and model logic was changed during the retraining process but these changes failed to be replicated to production, causing bugs that are hard to track down.\nYou have anywhere between 5 and 10 models in production.\nThe ad hoc, manual process of updating models mentioned from the previous stage has grown into a pain point too big to be ignored.\nWhen creating scripts to automate the retraining process for your system, you need to take into account that different models in your system might require different retraining schedules.\nThe embedding model might need to be retrained a lot less frequently than the ranking model.\nFor example, because the ranking model depends on the embeddings, when the embeddings change, the ranking model should be updated too.\nIf your company has ML models in production, it’s likely that your company already has most of the infrastructure pieces needed for automated retraining.\nHowever, in general, the three major factors that will affect the feasibility of this script are: scheduler, data, and model store.\nWhen creating training data from new data to update your model, remember that the new data has already gone through the prediction service.\nThis prediction service has already extracted features from this new data to input into models for predictions.\nSome companies reuse these extracted features for model retraining, which both saves computation and allows for consistency between prediction and training.\nIn stage 2, each time you retrain your model, you train it from scratch (stateless retraining).",
    "keywords": [
      "data",
      "model",
      "continual learning",
      "models",
      "learning",
      "data warehouses",
      "continual",
      "fresh data",
      "model store",
      "update",
      "retraining",
      "labels",
      "stage",
      "training data",
      "n’t"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "labels",
      "label",
      "labeling",
      "learn",
      "retraining",
      "retrains",
      "retrained"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 307-316)",
    "start_page": 307,
    "end_page": 316,
    "summary": "The main thing you need in this stage is a change in the mindset: retraining from scratch is such a norm—many companies are so used to data scientists handing off a model to engineers to deploy from scratch each time—that many companies don’t think about setting up their infrastructure to enable stateful training.\nThe main thing you need at this stage is a way to track your data and model lineage.\nThis model is updated with new data to create model version 1.1, and so on to create model 1.2.\nThis model is updated with new data to create model version 2.1.\nYou might want to know how these models evolve over time, which model was used as its base model, and which data was used to update it so that you can reproduce and debug it.\nAt stage 3, your models are still updated based on a fixed schedule set out by developers.\nInstead of relying on a fixed schedule, you might want your models to be automatically updated whenever data distributions shift and the model’s performance plummets.\nYou’ll first need a mechanism to trigger model updates.\nYou’ll also need a solid pipeline to continually evaluate your model updates.\nHow Often to Update Your Models\nNow that your infrastructure has been set up to update a model quickly, you started asking the question that has been haunting ML engineers at companies of all shapes and sizes: “How often should I update my models?” Before attempting to answer that question, we first need to figure out how much gain your model will get from being updated with fresh data.\nThe more gain your model can get from fresher data, the more frequently it should be retrained.\nFor example, if we switch from retraining our model every month to every week, how much performance gain can we get?\nOne way to figure out the gain is by training your model on the data from different time windows in the past and evaluating it on the data from today to see how the performance changes.\nTo measure the value of data freshness, you can experiment with training model version A on the data from January to June 2020, model version B on the data from April to September, and model version C on the data from June to November, then test each of these model\nThe difference in the performance of these versions will give you a sense of the performance gain your model can get from fresher data.\nTo get a sense of the performance gain you can get from fresher data, train your model on data from different time windows in the past and test on data from today to see how the performance changes\nOn the one hand, if you find that iterating on your data doesn’t give you much performance gain, then you should spend your resources on finding a better model.\nOn the other hand, if finding a better model architecture requires 100X compute for training and gives you 1% performance whereas updating the same model on data from the last three hours requires only 1X compute and also gives 1% performance gain, you’ll be better off iterating on data.\nHowever, as your infrastructure matures and the process of updating a model is partially automated and can be done in a matter of hours, if not minutes, the answer to this question is contingent on the answer to the following question: “How much performance gain would I get from fresher data?” It’s important to run experiments to quantify the value of data freshness to your models.\nThe first type of model evaluation you might think about is the good old test splits that you can use to evaluate your models offline, as discussed in Chapter 6.\nIt’ll be hard to compare the test results of two models if they are tested on different test sets.\nHowever, if you update the model to adapt to a new data distribution, it’s not sufficient to evaluate this new model on test splits from the old distribution.\nAssuming that the fresher the data, the more likely it is to come from the current distribution, one idea is to test your model on the most recent data that you have access to.\nSo, after you’ve updated your model on the data from the last day, you might want to test this model on the data from the last hour (assuming that data from the last hour wasn’t included in the data used to update your model).\nThe method of testing a predictive model on data from a specific period of time in the past is known as a backtest.\nBecause data distributions shift, the fact that a model does well on the data from the last hour doesn’t mean that it will continue doing well on the data\nWe’ll use A/B testing to determine which model is better according to some predefined metrics.\nFirst, A/B testing consists of a randomized experiment: the traffic routed to each model has to be truly random.\nThe gist here is that if your A/B test result shows that a model is better than another with statistical significance, you can determine which model is indeed better.\nSay we run a two- sample test and get the result that model A is better than model B with the p-value of p = 0.05 or 5%, and we define statistical significance as p ≤ 0.5.\nIf you’ve run your A/B test with a lot of samples and the difference between the two tested models is statistically insignificant, maybe there isn’t much difference between these two models, and it’s probably OK for you to use either.",
    "keywords": [
      "model",
      "data",
      "existing model",
      "models",
      "model version",
      "candidate model",
      "model updates",
      "n’t",
      "performance",
      "update",
      "fresher data",
      "existing",
      "’ll",
      "version",
      "canary"
    ],
    "concepts": [
      "models",
      "tested",
      "data",
      "statistically",
      "statistical",
      "statistics",
      "updating",
      "updated",
      "updates",
      "retraining"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 317-324)",
    "start_page": 317,
    "end_page": 324,
    "summary": "Each time, a model recommends 10 items users might like.\nEach user will be exposed to the recommendations made by one model.\nWhat if instead of exposing a user to recommendations from a model, we expose that user to recommendations from both models and see which model’s recommendations they will click on?\nWhen we show recommendations from multiple models to users, it’s important to note that the position of a recommendation influences how likely a user will click on it.\nAs of today, the standard method for testing models in production is A/B testing.\nWith A/B testing, you randomly route traffic to each model for\nA/B testing is stateless: you can route traffic to each model without having to know about their current performance.\nWhen you have multiple models to evaluate, each model can be considered a slot machine whose payout (i.e., prediction accuracy) you don’t know.\nBandits allow you to determine how to route traffic to each model for prediction to determine the best model while maximizing prediction accuracy for your users.\nBandit is stateful: before routing a request to a model, you need to calculate all models’ current performance.\nBandits are well-studied in academia and have been shown to be a lot more data-efficient than A/B testing (in many cases, bandits are even optimal).\nBandits require less data to determine which model is the best and, at the same time, reduce opportunity cost as they route traffic to the better model more quickly.\nbandit algorithm (Thompson Sampling) determined that a model was 5% better than the other with less than 12,000 samples.\nHowever, bandits are a lot more difficult to implement than A/B testing because it requires computing and keeping track of models’ payoffs.\nIf bandits for model evaluation are to determine the payout (i.e., prediction accuracy) of each model, contextual bandits are to determine the payout of each action.\nContextual bandits, like other bandits, are an amazing technique to improve the data efficiency of your model.\nSome people also call bandits for model evaluation “contextual bandits.” This makes conversations confusing, so in this book, “contextual bandits” refer to exploration strategies to determine the payout of predictions.\nContextual bandits are algorithms that help you balance between showing users the items they will like and showing the items that you want feedback on.\nIn contextual bandits, you can get bandit feedback right away after an action—e.g., after recommending an ad, you get feedback on whether a user has clicked on that recommendation.\nContextual bandits are well researched and have been shown to improve models’ performance significantly (see reports by Twitter and Google).\nHowever, contextual bandits are even harder to implement than model bandits, since the exploration strategy depends on the ML model’s architecture (e.g., whether it’s a decision tree or a neural network), which makes it less generalizable across use cases.\nData scientists tend to evaluate their new model ad hoc using the sets of tests that they like.\nOne data scientist might perform a set of tests and find that model A is better than model B, while another data scientist might report differently.\nTo mitigate this issue, it’s important for each team to outline clear pipelines on how models should be evaluated: e.g., the tests to run, the order in which they should run, the thresholds they must pass in order to be promoted to the next stage.",
    "keywords": [
      "model",
      "Bandits",
      "Contextual bandits",
      "models",
      "bandit",
      "user",
      "users",
      "items",
      "feedback",
      "Contextual",
      "recommendations",
      "data",
      "Interleaving",
      "bandit algorithms",
      "Learning"
    ],
    "concepts": [
      "models",
      "bandits",
      "bandit",
      "feedback",
      "data",
      "predictions",
      "prediction",
      "recommendations",
      "recommendation",
      "recommend"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 325-333)",
    "start_page": 325,
    "end_page": 333,
    "summary": "Start Problem in e-Commerce Recommender Systems,” arXiv, August 5, 2015, https://oreil.ly/GWUyD.\nBianchi, and Giovanni Cassani, “SIGIR 2021 E-Commerce Workshop Data Challenge,” arXiv, April 19, 2021, https://oreil.ly/8QxmS.\nHooked,” Towards Data Science, June 7, 2020, https://oreil.ly/BDWf8.\nHelp Design and Build the Future of Big Data and Stream Processing,” Snowflake blog, October 26, 2020, https://oreil.ly/Knh2Y.\n$100M,” Materialize, September 30, 2021, https://oreil.ly/dqxRb.\nBrooks, “Disparity in Home Lending Costs Minorities Millions, Researchers Find,” CBS News, November 15, 2019, https://oreil.ly/SpZ1N; Lee Brown, “Tesla Driver Killed in Crash Posted Videos Driving Without His Hands on the Wheel,” New York Post, May 16, 2021, https://oreil.ly/uku9S; “A Tesla Driver Is Charged in a Crash Involving Autopilot That Killed 2 People,” NPR, January 18, 2022, https://oreil.ly/WWaRA.\nLess Than a Day,” The Verge, May 24, 2016, https://oreil.ly/NJEVF.\nthe Sixth International Conference on Knowledge Discovery and Data Mining (Boston: ACM Press, 2000), 71–80; Albert Bifet and Ricard Gavaldà, “Adaptive Parameter-free Learning from Evolving Data Streams,” 2009, https://oreil.ly/XIMpl.\n25 Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Tanxin Shi, et al., “Practical Lessons from Predicting Clicks on Ads at Facebook,” in ADKDD ’14: Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (August 2014): 1–9, https://oreil.ly/oS16J.\n28 Danilo Sato, “CanaryRelease,” June 25, 2014, MartinFowler.com, https://oreil.ly/YtKJE.\n30 Joshua Parks, Juliette Aurisset, and Michael Ramm, “Innovating Faster on Personalization Algorithms at Netflix Using Interleaving,” Netflix Technology Blog, November 29, 2017, https://oreil.ly/lnvDY.\nBandits,” Towards Data Science, January 22, 2020, https://oreil.ly/MsaAK.\nMachine Learning Research 3 (November 2002): 397–422, https://oreil.ly/vp9mI.\nexemplifies the exploration–exploitation trade-off dilemma (s.v., “Multi-armed bandit,” https://oreil.ly/ySjwo).\nMany data scientists have told me that they know the right things to do for their ML systems, but they can’t do them because their infrastructure isn’t set up in a way that enables them to do so.\nIn this chapter, we’ll discuss how to set up infrastructure right for ML systems.\nBefore we dive in, it’s important to note that every company’s infrastructure needs are different.\nto show your friends, you probably won’t need any infrastructure either— you just need an Android-compatible ML framework like TensorFlow Lite.\nThese companies will likely need to develop their own highly specialized infrastructure.\nCompanies in the middle of the spectrum will likely benefit from generalized ML infrastructure that is being increasingly standardized (see Figure 10-1).\nAccording to Wikipedia, in the physical world, “infrastructure is the set of fundamental facilities and systems that support the sustainable functionality of households and firms.” In the ML world, infrastructure is the set of fundamental facilities that support the development and maintenance of ML systems.\nThe compute layer provides the compute needed to run your ML workloads such as training a model, computing features, generating features, etc.\nData and compute are the essential resources needed for any ML project, and thus the storage and compute layer forms the infrastructural foundation for any company that wants to apply ML.\nDifferent layers of infrastructure for ML\nthen we’ll discuss resource management, a contentious topic among data scientists—people are still debating whether a data scientist needs to know about this layer or not.\nAn ML platform requires up-front investment from a company, but if it’s done right, it can make the life of data scientists across business use cases at that company so much easier.\nEven if two companies have the exact same infrastructure needs, their resulting infrastructure might look different depending on their approaches to build versus buy decisions—i.e., what they want to build in-house versus what they want to outsource to other companies.\nWe’ll discuss the build versus buy decisions in the last part of this chapter, where we’ll also discuss the hope for standardized and unified abstractions for ML infrastructure.\nML systems work with a lot of data, and this data needs to be stored somewhere.\nWe’ve covered the data layer intensively in Chapter 3, so in this chapter, we’ll focus on the compute layer.\nThe compute layer refers to all the compute resources a company has access to and the mechanism to determine how these resources can be used.\nA compute unit can also be created to be more “permanent,” aka without being tied to a job, like a virtual machine.\nFor example, computation engines like Spark and Ray use “job” as their unit, and Kubernetes uses “pod,” a wrapper around containers, as its smallest deployable unit.\nTo execute a job, you first need to load the required data into your compute unit’s memory, then execute the required operations—addition, multiplication, division, convolution, etc.—on that data.",
    "keywords": [
      "Alex Egg",
      "Data",
      "compute",
      "compute layer",
      "compute unit",
      "Infrastructure",
      "layer",
      "storage layer",
      "German Big Data",
      "Distributed Machine Learning",
      "companies",
      "Learning",
      "Big Data",
      "Neural Network Surgery",
      "Data Startup Data"
    ],
    "concepts": [
      "data",
      "infrastructure",
      "compute",
      "computing",
      "computation",
      "likely",
      "learning",
      "learn",
      "uses",
      "different"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 334-341)",
    "start_page": 334,
    "end_page": 341,
    "summary": "memory a compute unit has but also how fast it is to load data in and out of memory, so some cloud providers advertise their instances as having “high bandwidth memory” or specify their instances’ I/O bandwidth.\nAs the name suggests, this metric denotes the number of float point operations a compute unit can run per second.\nThis means that instead of setting up their own data centers for storage and compute, companies can pay cloud providers like AWS and Azure for the exact amount of compute they use.\nCloud compute makes it extremely easy for companies to start building without having to worry about the compute layer.\nIt’s convenient to be able to just add more compute or shut down instances as needed—most cloud providers even do that automatically for you—reducing engineering operational overhead.\nMost cloud providers offer limits on the compute resources you can use at a time.\nHaving a lot of compute resources doesn’t mean that it’s always easy to use them, especially if you have to work with spot instances to save cost.\nDue to the cloud’s elasticity and ease of use, more and more companies are choosing to pay for the cloud over building and maintaining their own storage and compute layer.\nSynergy Research Group’s research shows that in 2020, “enterprise spending on cloud infrastructure services [grew] by 35% to reach almost $130 billion” while “enterprise spending on data [centers] dropped by 6% to under $90 billion,” as shown in Figure 10-4.\nWhile leveraging the cloud tends to give companies higher returns than building their own storage and compute layers early on, this becomes less defensible as a company grows.\nThe high cost of the cloud has prompted companies to start moving their workloads back to their own data centers, a process called “cloud repatriation.” Dropbox’s S-1 filing in 2018 shows that the company was able to save $75M over the two years prior to IPO due to their infrastructure optimization overhaul—a large chunk of it consisted of moving their workloads from public cloud to their own data centers.\nMore and more companies are following a hybrid approach: keeping most of their workloads on the cloud but slowly increasing their investment in data centers.\nAnother way for companies to reduce their dependence on any single cloud provider is to follow a multicloud strategy: spreading their workloads on multiple cloud providers.\nAs Josh Wills, one of our early reviewers, put it: “Nobody in their right mind intends to use multicloud.” It’s incredibly hard to move data and orchestrate workloads across clouds.\nThe dev environment is where ML engineers write code, run experiments, and interact with the production environment where champion models are deployed and challenger models evaluated.\nAccording to Ville Tuulos in his book Effective Data Science Infrastructure, “you would be surprised to know how many companies have well-tuned, scalable production infrastructure but the question of how the code is developed, debugged, and tested in the first place is solved in an ad-hoc manner.”\nHe suggested that “if you have time to set up only one piece of infrastructure well, make it the development environment for data scientists.” Because the dev environment is where engineers work, improvements in the dev environment translate directly into improvements in engineering productivity.\nAs of this writing, companies use an ad hoc set of tools to version their ML workflows, such as Git to version control code, DVC to version data, Weights & Biases or Comet.ml to track experiments during development, and MLflow to track artifacts of models when deploying them.\nIDEs can be native apps like VS Code or Vim. IDEs can be browser-based, which means they run in browsers, such as AWS Cloud9.\nWith notebooks, you only need to load your data once —notebooks can retain this data in memory—instead of having to load it each time you want to run your code.",
    "keywords": [
      "cloud",
      "data",
      "compute",
      "compute unit",
      "dev environment",
      "FLOPS",
      "companies",
      "Environment",
      "Data Centers",
      "CPU cores",
      "cloud providers",
      "AWS",
      "Cloud compute",
      "code",
      "cloud infrastructure"
    ],
    "concepts": [
      "clouds",
      "data",
      "companies",
      "company",
      "compute",
      "engineers",
      "engineer",
      "code",
      "operation",
      "operations"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 342-350)",
    "start_page": 342,
    "end_page": 350,
    "summary": "We had a bash file that a new team member could run to create a new virtual environment—in our case, we use conda for virtual environments— and install the required packages needed to run our code.\nAfter seeing him struggling with setting the environment up for a day, we decided to move to a cloud dev environment.\nThis means that we still standardize the virtual environment and tools and packages, but now everyone uses the virtual environment and tools and packages on the same type of machine too, provided by a cloud provider.\nWhen using a cloud dev environment, you can use a cloud dev environment that also comes with a cloud IDE like AWS Cloud9 (which has no built-in notebooks) and Amazon SageMaker Studio (which comes with hosted JupyterLab).\nHowever, most engineers I know who use cloud IDEs do so by installing IDEs of their choice, like Vim, on their cloud instances.\nA much more popular option is to use a cloud dev environment with a local IDE.\nFor example, you can use VS Code installed on your computer and connect the local IDE to the cloud environment using a secure protocol like Secure Shell (SSH).\nAmong them, VS Code is a good choice since it allows easy integration with cloud dev instances.\nAt our startup, we chose GitHub Codespaces as our cloud dev environment, but an AWS EC2 or a GCP instance that you can SSH into is also a good option.\nBefore moving to cloud environments, like many other companies, we were worried about the cost—what if we forgot to shut down our instances when not in use and they kept charging us money?\nBecause engineering time is expensive, if a cloud dev environment can help you save a few hours of engineering time a month, it’s worth it for many companies.\nThird, cloud dev environments can help with security.\nOf course, some companies might not be able to move to cloud dev environments also because of security concerns.\nThe fourth benefit, which I would argue is the biggest benefit for companies that do production on the cloud, is that having your dev environment on the cloud reduces the gap between the dev environment and the production environment.\nOccasionally, a company has to move their dev environments to the cloud not only because of the benefits, but also out of necessity.\nOf course, cloud dev environments might not work for every company due to cost, security, or other concerns.\nSetting up cloud dev environments also requires some initial investments, and you might need to educate your data scientists on cloud hygiene, including establishing secure connections to the\nHowever, standardization of dev environments might make your data scientists’ lives easier and save you money in the long run.\nYou will have to turn on new instances as needed, and these instances will need to be set up with required tools and packages to execute your workloads.\nWhen a new instance is allocated for your workload, you’ll need to install dependencies using a list of predefined instructions.\nA question arises: how do you re-create an environment on any new instance?\nWith Docker, you create a Dockerfile with step-by-step instructions on how to re-create an environment in which your model can run: install this package, download this pretrained model, set environment variables, navigate into a folder, etc.\nIf you run this Docker image, you get back a Docker container.\nFrom this mold, you can create multiple running instances; each is a Docker container.\nFor example, NVIDIA might provide a Docker image that contains TensorFlow and all necessary libraries to optimize TensorFlow for GPUs. If you want to build an application that runs TensorFlow on GPUs, it’s not a bad idea to use this Docker image as your base and install dependencies specific to your application on top of this base image.\nIf you run both parts of the code on the same GPU instances, you’ll need GPU instances with high memory, which can be very expensive.",
    "keywords": [
      "cloud dev environment",
      "dev environment",
      "cloud dev",
      "cloud",
      "Dev",
      "environment",
      "Docker",
      "run",
      "Docker image",
      "instances",
      "Notebooks",
      "Container",
      "code",
      "resources",
      "Containers"
    ],
    "concepts": [
      "cloud",
      "containers",
      "container",
      "contains",
      "resources",
      "resource",
      "notebooks",
      "notebook",
      "run",
      "runs"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 351-358)",
    "start_page": 351,
    "end_page": 358,
    "summary": "In this section, we’ll discuss how to manage resources for ML workflows.\nThere are two key characteristics of ML workflows that influence their resource management: repetitiveness and dependencies.\nIt doesn’t care about the dependencies between the jobs it runs—you can run job A after job B with cron but you can’t schedule anything complicated like run B if A succeeds and run C if A fails.\nSteps in an ML workflow might have complex dependency relationships with each other.\nFor example, an ML workflow might consist of the following steps:\nMost workflow management tools require you to specify your workflows in a form of DAGs.\nIt takes in the DAG of a workflow and schedules each step accordingly.\nThis means that schedulers need to be aware of the resources available and the resources needed to run each job—the resources needed are either specified as options when you schedule a job or estimated by the scheduler.\nFor instance, if a job requires 8 GB of memory and two CPUs, the scheduler needs to find among the resources it manages an instance with 8 GB of memory and two CPUs and wait until the instance is not executing other jobs to run this job on the instance.\nSchedulers should also optimize for resource utilization since they have information on resources available, jobs to run, and resources needed for each job to run.\nDesigning a general-purpose scheduler is hard, since this scheduler will need to be able to manage almost any number of concurrent machines and workflows.\nIf schedulers are concerned with when to run jobs and what resources are needed to run those jobs, orchestrators are concerned with where to get\nOrchestrators such as HashiCorp Nomad and data science–specific orchestrators including Airflow, Argo, Prefect, and Dagster have their own schedulers.\nWe’ve discussed the differences between schedulers and orchestrators and how they can be used to execute workflows in general.\nReaders familiar with workflow management tools aimed especially at data science like Airflow, Argo, Prefect, Kubeflow, Metaflow, etc.\nAlmost all workflow management tools come with some schedulers, and therefore, you can think of them as schedulers that, instead of focusing on individual jobs, focus on the workflow as a whole.\nOnce a workflow is defined, the underlying scheduler usually works with an orchestrator to allocate resources to run the workflow, as shown in Figure 10-8.\nAfter a workflow is defined, the tasks in this workflow are scheduled and orchestrated\nIf two different steps in your workflow have different requirements, you can, in theory, create different containers for them using Airflow’s DockerOperator, but it’s not that easy to do so.\nThe next generation of workflow orchestrators (Argo, Prefect) were created to address different drawbacks of Airflow.\nYou can run each step in a container, but you’ll still have to deal with Dockerfiles and register your docker with your workflows in Prefect.\nEvery step in an Argo workflow is run in its own container.",
    "keywords": [
      "workflow",
      "Airflow",
      "job",
      "workflows",
      "Schedulers",
      "DAG",
      "run",
      "workflow management",
      "resources",
      "workflow management tools",
      "jobs",
      "step",
      "scheduler",
      "Orchestrators",
      "Prefect"
    ],
    "concepts": [
      "schedulers",
      "scheduled",
      "scheduling",
      "schedule",
      "schedules",
      "scheduler",
      "workflows",
      "workflow",
      "orchestrators",
      "orchestrated"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 359-366)",
    "start_page": 359,
    "end_page": 366,
    "summary": "# fitA requires a different version of NumPy compared to fitB @conda(libraries={\"scikit-learn\":\"0.21.1\", \"numpy\":\"1.13.0\"}) @step def fitA(self): self.model = fit(self.data, model=\"A\") self.next(self.ensemble)\n@conda(libraries={\"numpy\":\"0.9.8\"}) # Requires 2 GPU of 16GB memory @batch(gpu=2, memory=16000) @step def fitB(self): self.model = fit(self.data, model=\"B\") self.next(self.ensemble)\n@step def ensemble(self, inputs): self.outputs = ( (inputs.fitA.model.predict(self.data) + inputs.fitB.model.predict(self.data)) / 2 for input in inputs ) self.next(self.end)\nTo deploy their recommender systems, they needed to build out tools such as feature management, model management, monitoring, etc.\nHere, I’ll focus on the components that I most often see in ML platforms, which include model development, model store, and feature store.\nYou’ll need to run and serve your models from a compute layer, and usually tools only support integration with a handful of cloud providers.\nIf it’s managed service, your models and likely some of your data will be on its service, which might not work for certain regulations.\nLet’s start with the first component: model deployment.\nModel Deployment\nOnce a model is trained (and hopefully tested), you want to make its predictive capability accessible to users.\nIn Chapter 7, we talked at length on how a model can serve its predictions: online or batch prediction.\nWe also discussed how the simplest way to deploy a model is to push your model and its dependencies to a location accessible in production then expose your model as an endpoint to your users.\nIf you do online prediction, this endpoint will provoke your model to generate a prediction.\nA deployment service can help with both pushing your models and their dependencies to production and exposing your models as endpoints.\nThere are also a myriad of startups that offer model deployment tools such as MLflow Models, Seldon, Cortex, Ray Serve, and so on.\nAn open problem with model deployment is how to ensure the quality of a model before it’s deployed.\nModel Store\nMany companies dismiss model stores because they sound simple.\nIn the section “Model Deployment”, we talked about how, to deploy a model, you have to package your model and upload it to a location accessible in production.\nThe person who was alerted to the problem is a DevOps engineer, who, after looking into the problem, decided that she needed to inform the data scientist who created this model.\nThe model is correct, the feature list is correct, the featurization code is correct, but something is wrong with the data processing pipeline.\nIn this simple example, we assume that the data scientist responsible still has access to the code used to generate the model.\nModel parameters\nThe data used to train this model might be pointers to the location where the data is stored or the name/version of your data.\nModel generation code\nVery often, data scientists generate models by writing code in notebooks.\nCompanies with more mature pipelines make their data scientists commit the model generation code into their Git repos on GitHub or GitLab. However, in many companies, this process is ad hoc, and data scientists don’t even check in their notebooks.\nIf the data scientist responsible for the model loses the notebook or quits or goes",
    "keywords": [
      "Model",
      "data",
      "model deployment",
      "model store",
      "prediction",
      "models",
      "tools",
      "batch prediction",
      "deployment",
      "data scientists",
      "Batch",
      "Kubeflow",
      "run",
      "model deployment tools",
      "production"
    ],
    "concepts": [
      "models",
      "model",
      "data",
      "deploy",
      "deployment",
      "deploying",
      "deployed",
      "tools",
      "tool",
      "experience"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 367-374)",
    "start_page": 367,
    "end_page": 374,
    "summary": "Because of the lack of a good model store solution, companies like Stitch Fix resolve to build their own model store.\nFeature Store\nits core, there are three main problems that a feature store can help address: feature management, feature transformation, and feature consistency.\nA feature store solution might address one or a combination of these problems:\nFeature management\nA company might have multiple ML models, each model using a lot of 31 features.\nIt’s often the case that features used for one model can be useful for another model.\nA feature store can help teams share and discover features, as well as manage roles and sharing settings for each feature.\nHowever, if the computation is expensive, you might want to execute it only once the first time it is required, then store it for feature uses.\nIn this capacity, a feature store acts like a data warehouse.\nDuring development, data scientists might define features and create models using Python.\nWhile it’s generally agreed that feature stores should manage feature definitions and ensure feature consistency, their exact capacities vary from vendor to vendor.\nSome feature stores only manage feature definitions without computing features from data; some feature stores do both.\nfeature store.\nOut of those who use a feature store, half of them build their own feature store.\nAt one extreme, you can outsource all your ML use cases to a company that provides ML applications end-to-end, and then perhaps the only piece of infrastructure you need is for data movement: moving your data from your applications to your vendor, and moving predictions from that vendor back to your users.\nAt the other extreme, if you’re a company that handles sensitive data that prevents you from using services managed by another company, you might need to build and maintain all your infrastructure in-house, even having your own data centers.\nFor example, your compute might be managed by AWS EC2 and your data warehouse managed by Snowflake, but you have your own feature store and your own monitoring dashboards.\nStefan Krawczyk, manager of the ML platform team at Stitch Fix, explained to me his build versus buy decision: “If it’s something we want to be really good at, we’ll manage that in-house.\nFor example, your team might decide that you need a model store, and you’d have preferred to use a vendor, but there’s no vendor mature enough for your needs, so you have to build your own feature store, perhaps on top of an open source solution.\nAs we’re building out Claypot AI, other founders have actually advised us to avoid selling to big tech companies because, if we do, we’ll get sucked into what they call “integration hell”—spending more time integrating our solution with custom infrastructure instead of building out our core features.\nTo enable data scientists to develop and deploy ML models, it’s crucial to have the right tools and infrastructure set up.\nWe chose to focus on the three sets of tools that are essential for most ML platforms: deployment, model store, and feature store.",
    "keywords": [
      "Feature Store",
      "Feature",
      "store",
      "features",
      "model store",
      "model",
      "Stitch Fix",
      "data",
      "infrastructure",
      "companies",
      "stores",
      "build",
      "company",
      "Stack Overflow",
      "vendor"
    ],
    "concepts": [
      "feature",
      "features",
      "model",
      "models",
      "data",
      "management",
      "manage",
      "managed",
      "manager",
      "infrastructure"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 375-382)",
    "start_page": 375,
    "end_page": 382,
    "summary": "3 The definition for “reasonable scale” was inspired by Jacopo Tagliabue in his paper “You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack,” arXiv, July 15, 2021, https://oreil.ly/YNRZQ.\nEngineering, October 17, 2018, https://oreil.ly/6Ykd3; Kaushik Krishnamurthi, “Building a Big Data Pipeline to Process Clickstream Data,” Zillow, April 6, 2018, https://oreil.ly/SGmNe.\n“Infrastructure,” https://oreil.ly/YaIk8.\nFor example, an m5.xlarge instance type has two CPU cores and two threads per core by default—four vCPUs in total” (“Optimize CPU Options,” Amazon Web Services, last accessed April 2020, https://oreil.ly/eeOtd).\nEnterprise Spending on Data Centers,” March 18, 2021, https://oreil.ly/uPx94.\n2021, https://huyenchip.com/2021/09/13/data-science-infrastructure.html; Neil Conway and David Hershey, “Data Scientists Don’t Care About Kubernetes,” Determined AI, November 30, 2020, https://oreil.ly/FFDQW; I Am Developer on Twitter (@iamdevloper): “I barely understand my own feelings how am I supposed to understand kubernetes,” June 26, 2021, https://oreil.ly/T2eQE.\nJohn Wilkes, “Large-Scale Cluster Management at Google with Borg,” EuroSys ’15: Proceedings of the Tenth European Conference on Computer Systems (April 2015): 18, https://oreil.ly/9TeTM.\nVolz, “Why You Need a Feature Store,” Continual, September 28, 2021, https://oreil.ly/kQPMb; Mike Del Balso, “What Is a Feature Store?” Tecton, October 20, 2020, https://oreil.ly/pzy0I.\nPlatform,” Uber Engineering, September 5, 2017, https://oreil.ly/XteNy. 32 Some people use the term “feature transformation.”\nIn this chapter, we’ll discuss how users and developers of ML systems might interact with these systems.\nWe’ll first consider how user experience might be altered and affected due to the probabilistic nature of ML models.\nSecond, due to this probabilistic nature, ML systems’ predictions are mostly correct, and the hard part is we usually don’t know for what inputs the system will be correct!\nThird, ML systems can also be large and might take an unexpectedly long time to produce a prediction.\nThese differences mean that ML systems can affect user experience differently, especially for users that have so far been used to traditional software.\nIn this section, we’ll discuss three challenges that ML systems pose to good user experience and how to address them.\nML predictions are probabilistic and inconsistent, which means that predictions generated for one user today might be different from what will be generated for the same user the next day, depending on the context of the predictions.\nFor tasks that want to leverage ML to improve users’ experience, the inconsistency in ML predictions can be a hindrance.\nThe applied ML team at Booking.com wanted to use ML to automatically suggest filters that a user might want, based on the filters they’ve used in a given browsing session.\nThe challenge they encountered is that if their ML model kept suggesting different filters each time, users could get confused, especially if they couldn’t find a filter that they had already applied before.\nIn this section, we’ll talk about how, in some cases, we want less consistency and more diversity in a model’s predictions.\nAn advantage of these large language models is that they’re able to generate predictions for a wide range of tasks with little to no task-specific training data required.\nFor example, you can use the requirements for a web page as an input to the model, and it’ll output the React code needed to create that web page, as shown in Figure 11-1.\nHowever, a drawback of these models is that these predictions are not always correct, and it’s very expensive to fine-tune them on task-specific data to improve their predictions.\nIn this case, given a set of requirements input by users, you can have the model produce multiple snippets of React code.\nWe’ve talked at length about the effect of an ML model’s inference latency on user experience in the section “Computational priorities”.\nThis means that you might have a rule that specifies: if the main model takes longer than X milliseconds to generate predictions, use the backup model instead.\nThis less-optimal but fast model might give users worse predictions but might still be preferred in situations where latency is crucial.",
    "keywords": [
      "Model",
      "predictions",
      "Data",
      "systems",
      "users",
      "user experience",
      "user",
      "system",
      "models",
      "reasonable scale",
      "Makes SEO Important",
      "constantly haunts engineering",
      "haunts engineering managers",
      "correct",
      "October"
    ],
    "concepts": [
      "data",
      "users",
      "user",
      "model",
      "models",
      "prediction",
      "predictions",
      "predict",
      "different",
      "differently"
    ]
  },
  {
    "chapter_number": 45,
    "title": "Segment 45 (pages 383-391)",
    "start_page": 383,
    "end_page": 391,
    "summary": "They’re not only users but also developers of ML systems.\nFor example, to help SMEs get more involved in the development of ML systems, many companies are building no-code/low- code platforms that allow people to make changes without writing code.\nTo be able to bring all these areas of expertise into an ML project, companies tend to follow one of the two following approaches: have a separate team to manage all the Ops aspects or include data scientists on the team and have them own the entire process.\nIn this approach, the data science/ML team develops models in the dev environment.\nThen a separate team, usually the Ops/platform/ML engineering team, production i zes the models in prod.\nFor example, the platform team has ideas on how to improve the infrastructure but they can only act on requests from data scientists, but data scientists don’t have to deal with infrastructure so they have less incentives to proactively make changes to it.\nApproach 2: Data scientists own the entire process\nIn this approach, the data science team also has to worry about productionizing models.\nData scientists become grumpy unicorns, expected to know everything about the process, and they might end up writing more boilerplate code than data science.\nAbout a year ago, I tweeted about a set of skills I thought was important to become an ML engineer or data scientist, as shown in Figure 11-2.\nEugene Yan also wrote about how “data scientists should be more end-to-end.” Eric Colson, Stitch Fix’s chief algorithms officer (who previously was also VP data science and engineering at Netflix), wrote a post on “the power of the full-stack data 3 science generalist and the perils of division of labor through function.”\nHowever, as I learned more about low-level infrastructure, I realized how unreasonable it is to expect data scientists to know about it.\nI love Erik Bernhardsson’s analogy that expecting data scientists to know about infrastructure is like expecting app developers to know about how Linux kernels work.\nI joined an ML company because I wanted to spend more time with data, not with spinning up AWS instances, writing Dockerfiles, scheduling/scaling clusters, or debugging YAML configuration files.\nFor data scientists to own the entire process, we need good tools.\nWhat if I can just tell this tool, “Here’s where I store my data (S3), here are the steps to run my code (featurizing, modeling), here’s where my code should run (EC2 instances, serverless stuff like AWS Batch, Function, etc.), here’s what my code needs to run at each step (dependencies),” and then this tool manages all the infrastructure stuff for me?\nThey need tools that “abstract the data scientists from the complexities of containerization, distributed processing, automatic failover, and other advanced computer science concepts.”\nThe question of how to make intelligent systems responsible is relevant not only to ML systems but also general artificial intelligence (AI) systems.\nResponsible AI is the practice of designing, developing, and deploying AI systems with good intention and sufficient awareness to empower users, to engender trust, and to ensure fair and positive impact to society.\nGiven ML is being deployed into almost every aspect of our lives, failing to make our ML systems fair and ethical can lead to catastrophic consequences, as outlined in the book Weapons of Math Destruction (Cathy O’Neil, Crown Books, 2016), and through other case studies mentioned throughout this book.\nAs developers of ML systems, you have the responsibility not only to think about how your systems will impact users and society at large, but also to help all stakeholders better realize their responsibilities toward the users by concretely implementing ethics, safety, and inclusivity into your ML systems.\nWe will then propose a preliminary framework for data scientists and ML\nengineers to select the tools and guidelines that best help with making your ML systems responsible.",
    "keywords": [
      "Data Scientists",
      "Cross-functional Teams Collaboration",
      "data",
      "Teams Collaboration SMEs",
      "systems",
      "Scientists",
      "Teams Collaboration",
      "data science",
      "team",
      "Collaboration SMEs",
      "data science team",
      "code",
      "systems responsible",
      "model",
      "infrastructure"
    ],
    "concepts": [
      "data",
      "teams",
      "team",
      "models",
      "model",
      "modeling",
      "ethics",
      "ethical",
      "make",
      "makes"
    ]
  },
  {
    "chapter_number": 46,
    "title": "Segment 46 (pages 392-399)",
    "start_page": 392,
    "end_page": 399,
    "summary": "However, the objective that Ofqual seemingly chose to optimize was “maintaining standards” across schools—fitting the model’s predicted grades to historical grade distributions from each school.\nOfqual prioritized fairness between schools over fairness between students—they preferred a model that gets school-level results right over another model that gets each individual’s grades right.\nDue to this objective, the model disproportionately downgraded high- performing cohorts from historically low-performing schools.\nFailure 2: Insufficient fine-grained model evaluation to discover biases\nBias against students from historically low-performing schools is only one of the many biases discovered about this model after the results were brought to the public.\nBecause the model took into account each school’s historical performance, Ofqual acknowledged that their model didn’t have enough data for small schools.\nIt might have been possible to discover these biases through the public release of the model’s predicted grades with fine-grained evaluation to understand their model’s performance for different slices of data—e.g., evaluating the model’s accuracy for schools of different sizes and for students from different backgrounds.\nThe public, therefore, couldn’t express their concern over this objective as the model was being developed.\nThis case study shows the importance of transparency when building a model that can make a direct impact on the lives of so many people, and what the consequences can be for failing to disclose important aspects of your model at the right time.\nIt also shows the importance of choosing the right objective to optimize, as the wrong objective (e.g., prioritizing fairness among schools) can not only lead you to choose a model that underperforms for the right objective, but also perpetuate biases.\nSince the development of ML systems relies heavily on the quality of data, it’s important for user data to be collected.\nPractitioners and companies require access to data to discover new use cases and develop new AI-powered products.\nHowever, collecting and sharing datasets might violate the privacy and security of the users whose data is part of these datasets.\nStrava stated that the data used had been anonymized, and “excludes activities that have been marked as private and user-defined privacy zones.”\nSince Strava was used by military personnel, their public data, despite anonymization, allowed people to discover patterns that expose activities of US military bases overseas, including the “forward operating bases in Afghanistan, Turkish military patrols in Syria, and a possible guard patrol in 18 the Russian operating area of Syria.” An example of these discriminating patterns is shown in Figure 11-4.\nFirst, Strava’s default privacy setting was “opt-out,” meaning that it requires users to manually opt out if they don’t want their data to be collected.\nDevelopers of applications that gather user data must understand that their users might not have the technical know-how and privacy awareness to choose the right privacy settings for themselves, and so developers must proactively work to make the right settings the default, even at the cost of gathering less data.\nIs the data used for developing your model representative of the data your model will handle in the real world?\nIf not, your model might be biased against the groups of users with less data represented in the training data.\nDisparate impact occurs “when a selection process has widely different outcomes for different groups, even as it appears to be neutral.” This can happen when a model’s decision relies on information correlated with legally protected classes (e.g., ethnicity, gender, religious practice) even when this information isn’t used in training the model directly.",
    "keywords": [
      "model",
      "data",
      "Ofqual",
      "schools",
      "students",
      "objective",
      "Strava",
      "school",
      "public",
      "system",
      "users",
      "privacy",
      "biases",
      "Failure",
      "n’t"
    ],
    "concepts": [
      "data",
      "model",
      "schools",
      "school",
      "grading",
      "grade",
      "grades",
      "teachers",
      "teacher",
      "user"
    ]
  },
  {
    "chapter_number": 47,
    "title": "Segment 47 (pages 400-407)",
    "start_page": 400,
    "end_page": 407,
    "summary": "Are you optimizing your model using an objective that enables fairness to all users?\nAre you performing adequate, fine-grained evaluation to understand your model’s performance on different groups of users?\nAs an example, to build an equitable automated grading system, it’s essential to work with domain experts to understand the demographic distribution of the student population and how socioeconomic factors get reflected in the historical performance data.\nFor example, you might want your system to have low inference latency, which could be obtained by model compression techniques like pruning.\nYou might also want your model to have high predictive accuracy, which could be achieved by adding more data.\nYou might also want your model to be fair and transparent, which could require the model and the data used to develop this model to be made accessible for public scrutiny.\nOften, ML literature makes the unrealistic assumption that optimizing for one property, like model accuracy, holds all others static.\nPeople might discuss techniques to improve a model’s fairness with the assumption that this model’s accuracy or latency will remain the same.\nDifferential privacy is a popular technique used on training data for ML models.\nThe trade-off here is that the higher the level of privacy that differential privacy can provide, the lower the model’s accuracy.\nAs pointed out by Bagdasaryan and Shmatikov (2019), “the accuracy of differential privacy models drops much more for the underrepresented classes and subgroups.”\nWe learned that it’s possible to reduce a model’s size significantly with minimal cost of accuracy, e.g., reducing a model’s parameter count by 90% with minimal accuracy cost.\nIt’s important to be aware of these trade-offs so that we can make informed design decisions for our ML systems.\nCompanies might decide to bypass ethical issues in ML models to save cost and time, only to\nCreate model cards\nModel cards are short documents accompanying trained ML models that provide information on how these models were trained and evaluated.\ncard paper, “The goal of model cards is to standardize ethical practice and reporting by allowing stakeholders to compare candidate models for deployment across not only traditional evaluation metrics but also along the axes of ethical, inclusive, and fair considerations.”\nThe following list has been adapted from content in the paper “Model Cards for Model Reporting” to show the information you might want to report for your models:\nPerson or organization developing model\nModel date\nEvaluation data: Details on the dataset(s) used for the quantitative analyses in the card.\nModel cards are a step toward increasing transparency into the development of ML models.\nThey are especially important in cases where people who use a model aren’t the same people who developed this model.\nFor models that update frequently, this can create quite an overhead for data scientists if model cards are created manually.\nTherefore, it’s important to have tools to automatically generate model cards, either by leveraging the model card generation feature of tools like TensorFlow, Metaflow, and scikit-learn or by building this feature in-house.\nFor example, Google has published recommended best practices for responsible AI and IBM has open-sourced AI Fairness 360, which contains a set of metrics, explanations, and algorithms to mitigate bias in datasets and models.\nBuilding an ML system often requires multiple skill sets, and an organization might wonder how to distribute these required skill sets: to involve different teams with different skill sets or to expect the same team (e.g., data scientists) to have all the skills.\nThe main cons of the second approach is that it’s difficult to hire data scientists who can own the process of developing an ML system end-to-end.\nIncorporating ethics principles into your modeling and organizational practices will not only help you distinguish yourself as a professional and cutting-edge data scientist and ML engineer but also help your organization gain trust from your customers and users.",
    "keywords": [
      "model",
      "model cards",
      "model cards Model",
      "data",
      "models",
      "cards Model cards",
      "system",
      "data scientists",
      "model Model",
      "developing model Model",
      "model Model date",
      "Model date Model",
      "Model version Model",
      "evaluation data",
      "cards"
    ],
    "concepts": [
      "model",
      "models",
      "modeling",
      "data",
      "evaluation",
      "evaluated",
      "different",
      "ethical",
      "ethics",
      "fairness"
    ]
  },
  {
    "chapter_number": 48,
    "title": "Segment 48 (pages 408-416)",
    "start_page": 408,
    "end_page": 416,
    "summary": "15 “Royal Statistical Society Response to the House of Commons Education Select Committee Call for Evidence: The Impact of COVID-19 on Education and Children’s Services Inquiry,” Royal Statistical Society, June 8, 2020, https://oreil.ly/ernho.\nMilitary Bases,” Wired, January 30, 2018, https://oreil.ly/eJPdj.\n20 Matt Burgess, “Strava’s Heatmap Data Lets Anyone See”; Rosie Spinks, “Using a Fitness App Taught Me the Scary Truth About Why Privacy Settings Are a Feminist Issue,” Quartz, August 1, 2017, https://oreil.ly/DO3WR.\n22 Matt Burgess, “Strava’s Heatmap Data Lets Anyone See.”\nVenkatasubramanian, “Certifying and Removing Disparate Impact,” arXiv, July 16, 2015, https://oreil.ly/FjSve.\n“Differential privacy,” https://oreil.ly/UcxzZ.\nModel Accuracy,” arXiv, May 28, 2019, https://oreil.ly/nrJGK.\nCompressed Deep Neural Networks Forget?” arXiv, November 13, 2019, https://oreil.ly/bgfFX.\n“Characterising Bias in Compressed Models,” arXiv, October 6, 2020, https://oreil.ly/ZTI72.\n28 Hooker et al., “Characterising Bias in Compressed Models.”\nStecklein, Jim Dabney, Brandon Dick, Bill Haskins, Randy Lovell, and Gregory Moroney, “Error Cost Escalation Through the Project Life Cycle,” NASA Technical Reports Server (NTRS), https://oreil.ly/edzaB.\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru, “Model Cards for Model Reporting,” arXiv, October 5, 2018, https://oreil.ly/COpah.\n31 Mitchell et al., “Model Cards for Model Reporting.”\ncontinual learning and, Algorithm challenge-Algorithm challenge\narchitectural search, Hard AutoML: Architecture search and learned optimizer\nbase model, fine tuning, Transfer learning\nbaselines, offline model evaluation, Baselines\nbinary classification, Binary versus multiclass classification\nbinary data, Text Versus Binary Format\ncardinality, classification tasks and, Binary versus multiclass classification\nchampion model, Continual Learning\nbinary, Binary versus multiclass classification\nhierarchical, Binary versus multiclass classification\nhigh cardinality, Binary versus multiclass classification\nmulticlass, Binary versus multiclass classification, Multiclass versus multilabel classification\nmultilabel, Multiclass versus multilabel classification\nsentiment analysis, Learned Features Versus Engineered Features\nclassification models, Classification versus regression\ncloud computing, ML on the Cloud and on the Edge, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\nelasticity, Public Cloud Versus Private Data Centers\nmulticloud strategy, Public Cloud Versus Private Data Centers",
    "keywords": [
      "Versus Private Data",
      "Binary versus multiclass",
      "Awarding GCSE",
      "Versus",
      "Prediction Versus Online",
      "Cloud Versus Private",
      "versus multiclass classification",
      "Batch Prediction Versus",
      "Binary versus",
      "Versus Online Prediction",
      "Public Cloud Versus",
      "Private Data Centers",
      "Versus Binary Format",
      "versus multiclass",
      "continual learning"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "learning",
      "learned",
      "baselines",
      "processing",
      "processes",
      "process",
      "classification"
    ]
  },
  {
    "chapter_number": 49,
    "title": "Segment 49 (pages 417-424)",
    "start_page": 417,
    "end_page": 424,
    "summary": "covariate data distribution shift, Covariate shift-Covariate shift\ndata, When to Use Machine Learning, Data\ntraining (see training data)\nunseen data, When to Use Machine Learning\ndata augmentation, Data Augmentation\ndata distribution shifts\nML system failure, Data Distribution Shifts\nconcept drift, Types of Data Distribution Shifts, Concept drift\nfeature change, General Data Distribution Shifts\nlabel schema change, General Data Distribution Shifts\nlabel shift, Types of Data Distribution Shifts, Label shift\ndata engineering, Iterative Process\ndata formats, Data Formats\nmultimodal data, Data Formats\ndata freshness, model updates and, Value of data freshness-Value of data freshness\ndata iteration, Stateless Retraining Versus Stateful Training\nmodel updates and, Model iteration versus data iteration\ndata leakage, Data Leakage\ndetecting, Detecting Data Leakage\nKaggle competition, Data Leakage\ndata models\ndata normalization, Relational Model\ndata parallelism, distributed training and, Data parallelism-Data parallelism\ndatabases, internal, Data Sources\nlogs, Data Sources\nsmartphones and, Data Sources\nsystem-generated data, Data Sources\ndatabases and dataflow, Data Passing Through Databases\nmessage queue model, Data Passing Through Real-Time Transport",
    "keywords": [
      "Retraining Versus Stateful",
      "Stateless Retraining Versus",
      "data distribution shifts",
      "Versus Stateful Training",
      "Versus Column-Major Format",
      "Versus Unstructured Data",
      "Versus Binary Format",
      "Structured Versus Unstructured",
      "Text Versus Binary",
      "General Data Distribution",
      "Row-Major Versus Column-Major",
      "Distribution Shifts label",
      "Versus Data-Mind Versus",
      "Unstructured Data-Structured Versus",
      "Versus Unstructured Data-Structured"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "services",
      "service",
      "shift",
      "shifts",
      "learning",
      "failure",
      "failures"
    ]
  },
  {
    "chapter_number": 50,
    "title": "Segment 50 (pages 425-432)",
    "start_page": 425,
    "end_page": 432,
    "summary": "perturbation tests, Perturbation tests-Perturbation tests\nslice-based, Slice-based evaluation-Slice-based evaluation\nexisting data, When to Use Machine Learning\nexperiment tracking, Experiment tracking-Experiment tracking\nthird-party tools, Experiment tracking\nF1 metrics, Using the right evaluation metrics\nfactorization, low-rank, Low-Rank Factorization-Low-Rank Factorization\nfeature importance, Feature Importance\nMNAR (missing not at random), Handling Missing Values\nfeature scaling, Scaling-Scaling\nfailures and, Versioning\nfixed positional embeddings, Discrete and Continuous Positional Embeddings\nFourier features, Discrete and Continuous Positional Embeddings\nfraud detection, Machine Learning Use Cases, Challenges of Class Imbalance\nheuristics-based slicing, Slice-based evaluation\nfailures and, Versioning\nIDE (integrated development environment), IDE\ninvariation tests, Invariance tests\nproject scoping, Iterative Process\nk-means clustering models, Evaluating ML Models\nKubernetes (K8s), From Dev to Prod: Containers, Cron, Schedulers, and Orchestrators\nnatural labels, Natural Labels\nfeedback loop length, Feedback loop length\nperturbation, Perturbation-Perturbation\nlanguage modeling, sampling and, Nonprobability Sampling\nloss curve, Experiment tracking\nloss functions, Objective Functions\n(see also objective functions)\nlow-rank factorization, Low-Rank Factorization-Low-Rank Factorization",
    "keywords": [
      "Cloud Versus Private",
      "Private Data Centers",
      "Versus Private Data",
      "Features Versus Engineered",
      "Public Cloud Versus",
      "Versus Engineered Features",
      "Learned Features Versus",
      "Continuous Positional Embeddings",
      "Engineered Features positional",
      "Centers public cloud",
      "Features categorical features",
      "Batch Prediction Versus",
      "Engineered Features categorical",
      "Compute private data",
      "Versus Online Prediction"
    ],
    "concepts": [
      "feature",
      "features",
      "data",
      "model",
      "models",
      "modeling",
      "labels",
      "label",
      "labeling",
      "learned"
    ]
  },
  {
    "chapter_number": 51,
    "title": "Segment 51 (pages 433-440)",
    "start_page": 433,
    "end_page": 440,
    "summary": "ML (machine learning)\nexisting data and, When to Use Machine Learning\nmodel optimization, Using ML to optimize ML models-Using ML to optimize ML models\npredictions and, When to Use Machine Learning\nproduction and, Machine Learning in Research Versus in Production- Discussion\nresearch and, Machine Learning in Research Versus in Production- Discussion\nscale, When to Use Machine Learning\nunseen data, When to Use Machine Learning\nML algorithms, Overview of Machine Learning Systems, Model Development and Offline Evaluation\ndeep learning and, Evaluating ML Models\nlabels, Evaluating ML Models\nversus neural networks, Evaluating ML Models\nML model logic, Model Deployment and Prediction Service\nML models\ndata iteration, Stateless Retraining Versus Stateful Training\nedge computing, optimization, Compiling and Optimizing Models for Edge Devices-Using ML to optimize ML models\nevaluation, Evaluating ML Models\ndata problems, Versioning\npoor model implementation, Versioning\noptimization, Using ML to optimize ML models-Using ML to optimize ML models\nselection criteria, Evaluating ML Models\ndistributed, Distributed Training-Model parallelism\ndata iteration and, Model iteration versus data iteration\nmodel iteration and, Model iteration versus data iteration\nML system failures\nversus traditional software, Machine Learning Systems Versus Traditional Software-Machine Learning Systems Versus Traditional Software\nMLOPs, ML systems design and, Overview of Machine Learning Systems- Overview of Machine Learning Systems\nmodel development, Iterative Process\nmodel implementation, failures and, Versioning\nmodel performance, business analysis, Iterative Process\nmonitoring, Monitoring and Observability, Continual Learning and Test in Production",
    "keywords": [
      "Machine Learning Systems",
      "data distribution shifts",
      "Machine Learning learning",
      "Software System Failures",
      "Learning Systems Versus",
      "Machine Learning model",
      "Machine Learning production",
      "Stateless Retraining Versus",
      "Machine Learning research",
      "Retraining Versus Stateful",
      "Systems Versus Traditional",
      "General Data Distribution",
      "Versus Stateful Training",
      "Learning Systems MNAR",
      "machine learning"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "evaluation",
      "evaluating",
      "evaluate",
      "failures",
      "failure",
      "learning",
      "learned"
    ]
  },
  {
    "chapter_number": 52,
    "title": "Segment 52 (pages 441-448)",
    "start_page": 441,
    "end_page": 448,
    "summary": "Norvig, Peter, Mind Versus Data\non-demand instances, Public Cloud Versus Private Data Centers\non-demand prediction, Batch Prediction Versus Online Prediction\nOne Billion Word Benchmark for Language Modeling, Mind Versus Data\nonline features, Batch Prediction Versus Online Prediction\nonline learning, Stateless Retraining Versus Stateful Training\nonline prediction, Batch Prediction Versus Online Prediction-Batch Prediction Versus Online Prediction, Bandits\noverfitting, Data-level methods: Resampling\nSMOTE, Data-level methods: Resampling\nPearl, Judea, Mind Versus Data\nprediction, When to Use Machine Learning, Multiple ways to frame a problem\nasynchronous, Batch Prediction Versus Online Prediction\nbatch prediction, Batch Prediction Versus Online Prediction-Batch Prediction Versus Online Prediction\non-demand prediction, Batch Prediction Versus Online Prediction\nonline, Batch Prediction Versus Online Prediction-Batch Prediction Versus Online Prediction\nsynchronous, Batch Prediction Versus Online Prediction\nbatch processing, Batch Processing Versus Stream Processing-Batch Processing Versus Stream Processing\nstream processing, Batch Processing Versus Stream Processing-Batch Processing Versus Stream Processing\nproduction, ML and, Machine Learning in Research Versus in Production- Discussion\nstreaming data and, Batch Processing Versus Stream Processing\nregression models, Classification versus regression\ndata normalization, Relational Model\nresampling, Data-level methods: Resampling\ndynamic sampling, Data-level methods: Resampling\noverfitting and, Data-level methods: Resampling\nSMOTE, Data-level methods: Resampling\ntwo-phase learning, Data-level methods: Resampling\nundersampling, Data-level methods: Resampling\nRogati, Monica, Mind Versus Data",
    "keywords": [
      "Prediction Versus Online",
      "Versus Online Prediction",
      "Online Prediction online",
      "Batch Prediction Versus",
      "Online Prediction predictions",
      "Prediction batch prediction",
      "Online Prediction batch",
      "Online Prediction-Batch Prediction",
      "Online Prediction streaming",
      "Continuous Positional Embeddings",
      "Unifying Batch Pipeline",
      "Training online prediction",
      "Prediction streaming pipeline",
      "Cloud Versus Private",
      "Online Prediction"
    ],
    "concepts": [
      "data",
      "models",
      "model",
      "modeling",
      "sampling",
      "learning",
      "processing",
      "process",
      "objectives",
      "objective"
    ]
  },
  {
    "chapter_number": 53,
    "title": "Segment 53 (pages 449-456)",
    "start_page": 449,
    "end_page": 456,
    "summary": "dataflow and, Data Passing Through Services-Data Passing Through Services\ndriver management, Data Passing Through Services\nprice optimization, Data Passing Through Services\nride management, Data Passing Through Services\nSMOTE (synthetic minority oversampling technique), Data-level methods: Resampling\ndata leakage and, Scaling before splitting\nstateful training, Stateless Retraining Versus Stateful Training-Stateless Retraining Versus Stateful Training\nstateless retraining, Stateless Retraining Versus Stateful Training-Stateless Retraining Versus Stateful Training\nprivate data centers, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\npublic cloud, Public Cloud Versus Private Data Centers-Public Cloud Versus Private Data Centers\nstorage engines, Data Storage Engines and Processing\nstreaming data, real-time transport, Batch Processing Versus Stream Processing\nstructured data, Structured Versus Unstructured Data-Structured Versus Unstructured Data\nSutton, Richard, Mind Versus Data\nsynthetic minority oversampling technique (SMOTE), Data-level methods: Resampling\nsystem-generated data, Data Sources\ntext data, Text Versus Binary Format\nstateful, Stateless Retraining Versus Stateful Training-Stateless Retraining Versus Stateful Training\nstateless retraining, Stateless Retraining Versus Stateful Training- Stateless Retraining Versus Stateful Training\ntraining data, Training Data\nresampling, Data-level methods: Resampling-Data-level methods: Resampling\ndata distributions, Production data differing from training data\nhand labels, Hand Labels-Data lineage\ntraining the model, iteration and, Iterative Process-Iterative Process\ndata engineering, Iterative Process\ntwo-phase learning, Data-level methods: Resampling\nundersampling, Data-level methods: Resampling\nunseen data, When to Use Machine Learning\nunstructured data, Structured Versus Unstructured Data-Structured Versus Unstructured Data\nuser input data, Data Sources",
    "keywords": [
      "Retraining Versus Stateful",
      "Versus Private Data",
      "Versus Stateful Training",
      "Stateless Retraining Versus",
      "Prediction Service services",
      "Versus Unstructured Data",
      "Learned Features Versus",
      "Versus Engineered Features",
      "Features Versus Engineered",
      "Cloud Versus Private",
      "Batch Prediction Versus",
      "simple random sampling",
      "Processing Versus Stream",
      "Batch Processing Versus",
      "Prediction Versus Online"
    ],
    "concepts": [
      "data",
      "process",
      "processing",
      "model",
      "models",
      "sampling",
      "samples",
      "learned",
      "learning",
      "evaluation"
    ]
  },
  {
    "chapter_number": 54,
    "title": "Segment 54 (pages 457-461)",
    "start_page": 457,
    "end_page": 461,
    "summary": "Kubeflow, Data Science Workflow Management\nMetaflow, Data Science Workflow Management\nChip Huyen (https://huyenchip.com) is co-founder and CEO of Claypot AI, developing infrastructure for real-time machine learning.\nPreviously, she was at NVIDIA, Snorkel AI, and Netflix, where she helped some of the world’s largest organizations develop and deploy machine learning systems.\nShe is currently teaching CS 329S: Machine Learning Systems Design at Stanford.\nChip’s expertise is in the intersection of software engineering and machine learning.\nLinkedIn included her among the 10 Top Voices in Software Development in 2019, and Top Voices in Data Science & AI in 2020.\nThe animal on the cover of Designing Machine Learning Systems is a red- legged partridge (Alectoris rufa), also known as a French partridge.",
    "keywords": [
      "Science Workflow Management",
      "Workflow Management Airflow",
      "Workflow Management Argo",
      "Workflow Management DAG",
      "Workflow Management Metaflow",
      "Embeddings workflow management",
      "Positional Embeddings workflow",
      "Data Science Workflow",
      "workflow management",
      "Science Workflow Management-Data",
      "Workflow Management-Data Science",
      "Management-Data Science Workflow",
      "Continuous Positional Embeddings",
      "Browsers weak supervision",
      "Science Workflow"
    ],
    "concepts": [
      "learning",
      "partridge",
      "partridges",
      "management",
      "importance",
      "important",
      "data",
      "develop",
      "development",
      "fonts"
    ]
  }
]