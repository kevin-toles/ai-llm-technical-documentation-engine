[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 2-9)",
    "start_page": 2,
    "end_page": 9,
    "summary": "The second edition updates the advice for Python 3, and it’s fantastic!\nbook, and the publisher was aware of a trademark claim, the designations have \nFor government sales inquiries, please contact governmentsales@pearsoned.com.\nFor questions about sales outside the U.S., please contact intlcs@pearson.com.\npermissions, request forms and the appropriate contacts within the Pearson \nEducation Global Rights & Permissions Department, please visit www.pearson.\nManaging Editor\nChapter 1: Pythonic Thinking \nChapter 3: Functions \nChapter 6: Metaclasses and Attributes \nChapter 7: Concurrency and Parallelism \nChapter 10: Collaboration \nChapter 1 Pythonic Thinking \nItem 1: Know Which Version of Python You’re Using \nItem 3: Know the Differences Between bytes and str \nItem 4:  Prefer Interpolated F-Strings Over C-style \nItem 5:  Write Helper Functions Instead of \nItem 6:  Prefer Multiple Assignment Unpacking \nItem 7: Prefer enumerate Over range \nItem 8: Use zip to Process Iterators in Parallel \nItem 10:  Prevent Repetition with Assignment Expressions \nItem 11: Know How to Slice Sequences \nItem 12:  Avoid Striding and Slicing in a Single Expression \nItem 13: Prefer Catch-All Unpacking Over Slicing ",
    "keywords": [
      "Item",
      "Effective Python",
      "Python",
      "experienced programmers",
      "novice and experienced",
      "Effective Python makes",
      "Editor",
      "updated Effective Python",
      "blank Effective Python",
      "Brett Slatkin",
      "Prefer",
      "Author",
      "Effective",
      "Pearson Education",
      "great book"
    ],
    "concepts": [
      "item",
      "editor",
      "sales",
      "python",
      "pythonic",
      "prefer",
      "electronic",
      "expressed",
      "expressions",
      "expression"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 10-18)",
    "start_page": 10,
    "end_page": 18,
    "summary": "Item 16:  Prefer get Over in and KeyError to \nItem 18:  Know How to Construct Key-Dependent \nItem 23:  Provide Optional Behavior with Keyword Arguments \nItem 24:  Use None and Docstrings to Specify \nItem 26:  Define Function Decorators with functools.wraps \nItem 27:  Use Comprehensions Instead of map and filter \nItem 29:  Avoid Repeated Work in Comprehensions by Using \nItem 30:  Consider Generators Instead of Returning Lists \nItem 31: Be Defensive When Iterating Over Arguments \nItem 32:  Consider Generator Expressions for Large List \nItem 36:  Consider itertools for Working with Iterators \nItem 37:  Compose Classes Instead of Nesting \nItem 38:  Accept Functions Instead of Classes for \nItem 39:  Use @classmethod Polymorphism to \nItem 41:  Consider Composing Functionality \nItem 44:  Use Plain Attributes Instead of Setter and \nItem 45:  Consider @property Instead of \nItem 46:  Use Descriptors for Reusable @property Methods \nItem 47:  Use __getattr__, __getattribute__, and \nItem 50: Annotate Class Attributes with __set_name__ \nItem 51:  Prefer Class Decorators Over Metaclasses for \nItem 52: Use subprocess to Manage Child Processes \nItem 53:  Use Threads for Blocking I/O, Avoid for Parallelism \nItem 54: Use Lock to Prevent Data Races in Threads \nItem 55:  Use Queue to Coordinate Work Between Threads \nItem 56:  Know How to Recognize When Concurrency \nItem 58:  Understand How Using Queue for \nItem 59:  Consider ThreadPoolExecutor When Threads \nItem 64:  Consider concurrent.futures for True Parallelism \nItem 66:  Consider contextlib and with Statements \nItem 67: Use datetime Instead of time for Local Clocks \nItem 68: Make pickle Reliable with copyreg \nItem 69: Use decimal When Precision Is Paramount \nItem 73: Know How to Use heapq for Priority Queues \nItem 74:  Consider memoryview and bytearray for \nItem 75: Use repr Strings for Debugging Output \nItem 77:  Isolate Tests from Each Other with setUp, \nItem 78:  Use Mocks to Test Code with \nItem 80: Consider Interactive Debugging with pdb \nItem 81:  Use tracemalloc to Understand Memory \nItem 82: Know Where to Find Community-Built Modules \nItem 83:  Use Virtual Environments for Isolated and \nItem 84:  Write Docstrings for Every Function, \nItem 85:  Use Packages to Organize Modules and \nItem 86:  Consider Module-Scoped Code to \nItem 89:  Consider warnings to Refactor and Migrate Usage \nThis book provides insight into the Pythonic way of writing programs: \nthe best way to use Python.\nEach chapter in this book contains a broad but related set of items.\nEach item \nItems include advice on what to \nItems reference each other to make it easier to fill in the \n(see Item 1: “Know Which Version of Python You’re Using”), up to and \nChapter 1: Pythonic Thinking\nPython makes it easy to write concurrent programs that do many \nPython can also be used \nThis chapter covers how to best utilize Python in these subtly \ncovers how to use Python to optimize your programs to maximize \ntogether on Python programs.\nPython code snippets in this book are in monospace font and have \ntation (see Item 84: “Write Docstrings for Every Function, Class, and \nor terminal output: what you see when running the Python program \nidea is that you could type the code snippets into a Python shell and ",
    "keywords": [
      "Item",
      "Python",
      "chapter covers",
      "Code",
      "book",
      "Handle Missing Dictionary",
      "Generators",
      "Python programs",
      "Avoid",
      "Attributes",
      "Handle Missing Items",
      "Classes",
      "Handle Missing",
      "Prefer",
      "Comprehensions"
    ],
    "concepts": [
      "item",
      "items",
      "pythonic",
      "useful",
      "consider",
      "classes",
      "programs",
      "program",
      "output",
      "functions"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 19-26)",
    "start_page": 19,
    "end_page": 26,
    "summary": "Thanks \nThanks to all of the translators who made the book \nThanks to the wonderful Python programmers I’ve known and \nPython has an excellent community, and I feel lucky to be a part \nusing Python to manage Google’s enormous fleet of servers.\nPythonic Thinking\nOver the years, the Python community has come to use the adjective \nPythonic to describe code that follows a particular style.\nThe Pythonic \nyour interpreter to read The Zen of Python.)\nProgrammers familiar with other languages may try to write Python \nthat can be expressed in Python.\nbest—the Pythonic—way to do the most common things in Python.\nItem 1: Know Which Version of Python You’re Using\nof Python 3.7 (released in June 2018).\nbook does not cover Python 2.\nHowever, the default meaning of python on the \nbut it can sometimes be an alias for even older versions, like python2.6 \nor python2.5.\nTo find out exactly which version of Python you’re using, \n$ python --version\nPython 2.7.10\nChapter 1 Pythonic Thinking\nPython 3 is usually available under the name python3:\n$ python3 --version\nPython 3.8.0\nYou can also figure out the version of Python you’re using at runtime \nPython 3 includes \nible with and focused on Python 3.\nPython 3 for all your Python projects.\nPython 2 is scheduled for end of life after January 1, 2020, at which \nUsing Python 2 after that date is a liability because it \na Python 2 codebase, you should consider using helpful tools like 2to3 \n(preinstalled with Python) and six (available as a community pack-\nhelp you make the transition to Python 3.\n✦ Python 3 is the most up-to-date and well-supported version of \nPython, and you should use it for your projects.\n✦ Be sure that the command-line executable for running Python on \n✦ Avoid Python 2 because it will no longer be maintained after January 1, \nPython Enhancement Proposal #8, otherwise known as PEP 8, is \nthe style guide for how to format Python code.\nwrite Python code any way you want, as long as it has valid syntax.\nSharing a common style with other Python \nPEP 8 provides a wealth of details about how to write clear Python \nIt continues to be updated as the Python language evolves.\nIt’s worth reading the whole guide online (https://www.python.org/\nIn Python, whitespace is syntactically significant.\nPython program-\nChapter 1 Pythonic Thinking",
    "keywords": [
      "Python",
      "Python code",
      "Python programmers",
      "book",
      "style",
      "write Python",
      "Version",
      "Pythonic Thinking",
      "PEP",
      "Style Guide",
      "code",
      "Pythonic",
      "write Python code",
      "format Python code",
      "Version of Python"
    ],
    "concepts": [
      "pythonic",
      "thanks",
      "line",
      "lines",
      "format",
      "style",
      "styles",
      "editor",
      "version",
      "versions"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 27-34)",
    "start_page": 27,
    "end_page": 34,
    "summary": "Item 3: Know the Differences Between bytes and str \nItem 3: Know the Differences Between bytes and str\ndata: bytes and str.\nInstances of str contain Unicode code points that represent textual \nImportantly, str instances do not have an associated binary encod-\ning, and bytes instances do not have an associated text encoding.\nconvert Unicode data to binary data, you must call the encode method \nWhen you’re writing Python programs, it’s important to do encoding \nof your program should use the str type containing Unicode data \nThe first function takes a bytes or str instance and always returns \ndef to_str(bytes_or_str):\nif isinstance(bytes_or_str, bytes):\nvalue = bytes_or_str.decode('utf-8')\nvalue = bytes_or_str\nreturn value  # Instance of str\nItem 3: Know the Differences Between bytes and str \nprint(repr(to_str(b'foo')))\nThe second function takes a bytes or str instance and always returns \na bytes:\ndef to_bytes(bytes_or_str):\nif isinstance(bytes_or_str, str):\nvalue = bytes_or_str.encode('utf-8')\nvalue = bytes_or_str\nreturn value  # Instance of bytes\nprint(repr(to_bytes(b'foo')))\nUnicode strings in Python.\nThe first issue is that bytes and str seem to work the same way, but \nBy using the + operator, you can add bytes to bytes and str to str, \nBut you can’t add str instances to bytes instances:\nTypeError: can't concat str to bytes\nNor can you add bytes instances to str instances:\nTypeError: can only concatenate str (not \"bytes\") to str\nBy using binary operators, you can compare bytes to bytes and str to \nBut you can’t compare a str instance to a bytes instance:\nNor can you compare a bytes instance to a str instance:\nComparing bytes and str instances for equality will always evaluate \nThe % operator works with format strings for each type, respectively:\nBut you can’t pass a str instance to a bytes format string because \nPython doesn’t know what binary text encoding to use:\n¯implements __bytes__, not 'str'\nItem 3: Know the Differences Between bytes and str \nYou can pass a bytes instance to a str format string using the \nprint('red %s' % b'blue')\nrepr Strings for Debugging Output”) on the bytes instance and sub-\nTypeError: write() argument must be str, not bytes\nmode, write operations expect str instances containing Unicode data \ninstead of bytes instances containing binary data.\nit uses the system’s default text encoding to interpret binary data \nusing the bytes.encode (for writing) and str.decode (for reading) \nbinary data in the file was actually meant to be a string encoded as \nwith open('data.bin', 'r', encoding='cp1252') as f:\nbytes.\n✦ bytes contains sequences of 8-bit values, and str contains \nUTF-8-encoded strings, Unicode code points, etc).\n✦ bytes and str instances can’t be used together with operators (like \n✦ If you want to read or write binary data to/from a file, always open \n✦ If you want to read or write Unicode data to/from a file, be care-\nFormat Strings and str.format\nPython has four different ways of formatting strings that are built \nThe most common way to format a string in Python is by using the \nare new to Python start with C-style format strings because they’re \nThere are four problems with C-style format strings in Python.",
    "keywords": [
      "str",
      "bytes",
      "Python",
      "str instances",
      "Unicode data",
      "Unicode",
      "data",
      "Instances",
      "bytes instances",
      "binary",
      "strings",
      "Item",
      "binary data",
      "instance",
      "Unicode strings"
    ],
    "concepts": [
      "encoding",
      "encode",
      "encodings",
      "encoded",
      "bytes",
      "byte",
      "python",
      "pythonic",
      "values",
      "value"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 35-42)",
    "start_page": 35,
    "end_page": 42,
    "summary": "Now, I make a few modifications to the values that I’m formatting \nThe third problem with formatting expressions is that if you want \nto use the same value in a format string multiple times, you have to \nformatted = template % (name, name)\nprint(formatted)\nsmall modifications to the values being formatted.\nprint(formatted)\nkeys from the dictionary are matched with format specifiers with the \nchange the order of values on the right side of the formatting expres-\nUsing dictionaries in formatting expressions also solves problem #3 \nHowever, dictionary format strings introduce and exacerbate other \nwhich is problem #4 with C-style formatting expressions in Python.\nprint(formatted)\noften must span multiple lines, with the format strings being concat-\none line per value to use in formatting:\nformatted = template % menu\nprint(formatted)\nformat string and the lines of the dictionary.\nmake small modifications to any of the values before formatting.\nexpressive than the old C-style format strings that use the % operator.\nformat values:\nformatted = format(a, ',.2f')\nprint(formatted)\nprint('*', formatted, '*')\nYou can use this functionality to format multiple values together \nBy default the placeholders in the format string are replaced by the \nformatted = '{} = {}'.format(key, value)\nprint(formatted)\nformatted = '{:<10} = {:.2f}'.format(key, value)\nprint(formatted)\nwill be passed to the format built-in function along with the value \n(format(value, '.2f') in the example above).\nthe __format__ special method.\nWith C-style format strings, you need to escape the % character (by \nformatted = '{1} = {0}'.format(key, value)\nprint(formatted)\nthe format string without the need to pass the value to the format \nSee {0} cook.'.format(name)\nprint(formatted)\nto make small modifications to values before formatting them.\nnew_style = '#{}: {:<10s} = {}'.format(\nthe str.format method, such as using combinations of dictionary keys \nprint(formatted)\nusing dictionaries in C-style formatting expressions to the new style \nold_formatted = template % {\nthe dictionary and a few characters in the format specifiers, but it’s \nlimiting that it undermines the value of the format method from str \nInterpolated Format Strings\nyou to prefix format strings with an f character, which is similar to \nF-strings take the expressiveness of format strings to the extreme, \ndancy of providing keys and values to be formatted.\nformatted = f'{key} = {value}'\nprint(formatted)\nthe str.format method:\nformatted = f'{key!r:<10} = {value:.2f}'\nprint(formatted)\nFormatting with f-strings is shorter than using C-style format strings \nstr_args = '{:<10} = {:.2f}'.format(key, value)\nstr_kw   = '{key:<10} = {value:.2f}'.format(key=key,\nnew_style = '#{}: {:<10s} = {}'.format(",
    "keywords": [
      "format",
      "format strings",
      "C-style format strings",
      "formatted",
      "key",
      "formatting",
      "format method",
      "C-style format",
      "C-style formatting expressions",
      "Interpolated Format Strings",
      "string",
      "format specifiers",
      "strings",
      "soup",
      "method"
    ],
    "concepts": [
      "formatting",
      "formatted",
      "strings",
      "values",
      "value",
      "keys",
      "key",
      "multiple",
      "python",
      "pythonic"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 43-50)",
    "start_page": 43,
    "end_page": 50,
    "summary": "Item 5: Write Helper Functions Instead of Complex Expressions \nPython expressions may also appear within the format specifier \nby f-strings makes them the best built-in option for Python pro-\nAny time you find yourself needing to format values into \n✦ F-strings are a new syntax for formatting values into strings that \nItem 5:  Write Helper Functions Instead of Complex \nPython’s pithy syntax makes it easy to write single-line expressions \nmy_values = parse_qs('red=5&blue=0&green=',\nprint(repr(my_values))\nSome query string parameters may have multiple values, some may \nprint('Red:     ', my_values.get('red'))\nprint('Green:   ', my_values.get('green'))\nprint('Opacity: ', my_values.get('opacity'))\nIt’d be nice if a default value of 0 were assigned when a parameter isn’t \nred = my_values.get('red', [''])[0] or 0\ngreen = my_values.get('green', [''])[0] or 0\nThe red case works because the key is present in the my_values dictio-\nThe value is a list with one member: the string '5'.\nItem 5: Write Helper Functions Instead of Complex Expressions \nThe green case works because the value in the my_values dictionary is \nThe opacity case works because the value in the my_values dictionary \nThe default value in this case is a list with one member: an empty \nred = int(my_values.get('red', [''])[0] or 0)\nred_str = my_values.get('red', [''])\ngreen_str = my_values.get('green', [''])\ndef get_first_int(values, key, default=0):\nfound = values.get(key, [''])\ngreen = get_first_int(my_values, 'green')\n✦ Python’s syntax makes it easy to write single-line expressions that \n✦ Move complex expressions into helper functions, especially if you \nItem 6:  Prefer Multiple Assignment Unpacking Over \nitems = tuple(snack_calories.items())\nprint(items)\n(('chips', 140), ('popcorn', 80), ('nuts', 190))\nThe values in tuples can be accessed through numerical indexes:\nfirst = item[0]\nItem 6: Prefer Multiple Assignment Unpacking Over Indexing \nOnce a tuple is created, you can’t modify it by assigning a new value \nPython also has syntax for unpacking, which allows for assigning \nmultiple values in a single statement.\nvalues, you can assign it to a tuple of two variable names:\nfirst, second = item  # Unpacking\nUnpacking has less visual noise than accessing the tuple’s indexes, \nof unpacking works when assigning to lists, sequences, and multiple \n(type3, (name3, cals3))) = favorite_snacks.items()\nprint(f'Favorite {type1} is {name1} with {cals1} calories')\nprint(f'Favorite {type2} is {name2} with {cals2} calories')\nprint(f'Favorite {type3} is {name3} with {cals3} calories')\neven be used to swap values in place without the need to create tem-\nvalues between two positions in a list as part of an ascending order \nHowever, with unpacking syntax, it’s possible to swap indexes in a \nItem 6: Prefer Multiple Assignment Unpacking Over Indexing \nof the assignment (a[i-1], a[i]) is used to receive that tuple value \nexpressions (see Item 27: “Use Comprehensions Instead of map and \nlist of snacks without using unpacking:\nitem = snacks[i]\nname = item[0]\ncalories = item[1]\nprint(f'#{i+1}: {name} has {calories} calories')\nprint(f'#{rank}: {name} has {calories} calories')\nPython provides additional unpacking functionality for list con-\n✦ Python has special syntax called unpacking for assigning multiple \nvalues in a single statement.\nprint(f'{flavor} is delicious')",
    "keywords": [
      "Item",
      "Unpacking",
      "green",
      "calories",
      "red",
      "Expressions",
      "Python",
      "Write Helper Functions",
      "C-style format strings",
      "Prefer Multiple Assignment",
      "Multiple Assignment Unpacking",
      "list",
      "string",
      "Helper Functions",
      "multiple"
    ],
    "concepts": [
      "item",
      "items",
      "values",
      "value",
      "python",
      "pythonic",
      "expressions",
      "expressiveness",
      "expression",
      "unpacking"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 51-58)",
    "start_page": 51,
    "end_page": 58,
    "summary": "enumerate wraps any iterator with a lazy generator (see Item 30: \npairs of the loop index and the next value from the given iterator.\nstatement (see Item 6: “Prefer Multiple Assignment Unpacking Over \nenumerate should begin counting (1 in this case) as the second \n✦ enumerate provides concise syntax for looping over an iterator and \ngetting the index of each item from the iterator as you go.\n✦ Prefer enumerate instead of looping over a range and indexing into a \nItem 8: Use zip to Process Iterators in Parallel\nprint(counts)\nindexes into names and counts make the code hard to read.\nItem 8: Use zip to Process Iterators in Parallel \nTo make this code clearer, Python provides the zip built-in function.\nzip wraps two or more iterators with a lazy generator.\ncode is much cleaner than the code for indexing into multiple lists:\nfor name, count in zip(names, counts):\nzip consumes the iterators it wraps one item at a time, which means \nHowever, beware of zip’s behavior when the input iterators are of \nRunning zip on the two input lists \nfor name, count in zip(names, counts):\nfor name, count in itertools.zip_longest(names, counts):\nprint(f'{name}: {count}')\n✦ The zip built-in function can be used to iterate over multiple itera-\n✦ zip truncates its output silently to the shortest iterator if you supply \n✦ Use the zip_longest function from the itertools built-in mod-\nule if you want to use zip on iterators of unequal lengths without \nItem 9: Avoid else Blocks After for and while Loops\na loop’s repeated interior block:\nprint('Loop', i)\nprint('Else block!')\nLoop 0\nLoop 1\nLoop 2\nElse block!\nSurprisingly, the else block runs immediately after the loop finishes.\nItem 9: Avoid else Blocks After for and while Loops \nUsing a break statement in a loop actually skips the else block:\nprint('Loop', i)\nprint('Else block!')\nLoop 0\nLoop 1\nAnother surprise is that the else block runs immediately if you loop \nprint('For Else block!')\nFor Else block!\nThe else block also runs when while loops are initially False:\nprint('While Else block!')\nThe rationale for these behaviors is that else blocks after loops are \ncoprime because the loop doesn’t encounter a break:\nprint('Not coprime')\nprint('Coprime')\nYou should avoid using else blocks after loops entirely.\nfollow for and while loop interior blocks.\n✦ The else block after a loop runs only if the loop body did not encoun-\n✦ Avoid using else blocks after loops because their behavior isn’t \ndef make_lemonade(count):\ncount = fresh_fruit.get('lemon', 0)\nif count:\nmake_lemonade(count)\nThe count variable is used only within the first block \nDefining count above the if statement causes it \nlows, including the else block, will need to access the count variable, \nif count := fresh_fruit.get('lemon', 0):\nmake_lemonade(count)\nit’s now clear that count is only relevant to the first block of the if ",
    "keywords": [
      "count",
      "block",
      "loop",
      "Item",
      "zip",
      "enumerate",
      "list",
      "coprime",
      "Loops",
      "Python",
      "Prefer Multiple Assignment",
      "counts",
      "Prefer enumerate",
      "Assignment",
      "Blocks"
    ],
    "concepts": [
      "counting",
      "counts",
      "count",
      "loop",
      "looping",
      "loops",
      "zip",
      "item",
      "items",
      "assignment"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 59-69)",
    "start_page": 59,
    "end_page": 69,
    "summary": "Item 10: Prevent Repetition with Assignment Expressions \ncount = fresh_fruit.get('apple', 0)\nassignment of count puts distracting emphasis on that variable.\nif (count := fresh_fruit.get('apple', 0)) >= 4:\ndef slice_bananas(count):\ncount = fresh_fruit.get('banana', 0)\npieces = slice_bananas(count)\ncount = fresh_fruit.get('banana', 0)\npieces = slice_bananas(count)\nwhere the pieces = 0 assignment is first.\nif (count := fresh_fruit.get('banana', 0)) >= 2:\npieces = slice_bananas(count)\nItem 10: Prevent Repetition with Assignment Expressions \nif (count := fresh_fruit.get('banana', 0)) >= 2:\npieces = slice_bananas(count)\ncount = fresh_fruit.get('banana', 0)\npieces = slice_bananas(count)\ncount = fresh_fruit.get('apple', 0)\ncount = fresh_fruit.get('lemon', 0)\nto_enjoy = make_lemonade(count)\nif (count := fresh_fruit.get('banana', 0)) >= 2:\npieces = slice_bananas(count)\nelif (count := fresh_fruit.get('apple', 0)) >= 4:\nelif count := fresh_fruit.get('lemon', 0):\nThe version that uses assignment expressions is only five lines shorter \ndef make_juice(fruit, count):\nfor fruit, count in fresh_fruit.items():\nbatch = make_juice(fruit, count)\nanother at the end of the loop to replenish the list of delivered fruit.\nItem 10: Prevent Repetition with Assignment Expressions \nfor fruit, count in fresh_fruit.items():\nbatch = make_juice(fruit, count)\nfor fruit, count in fresh_fruit.items():\nbatch = make_juice(fruit, count)\nThere are many other situations where assignment expressions can \nsider using assignment expressions in order to improve readability.\n✦ Assignment expressions use the walrus operator (:=) to both assign \nclearly by using assignment expressions.\nLists and \nItem 11: Know How to Slice Sequences\nPython includes syntax for slicing sequences into pieces.\nSlicing \nThe simplest uses for slicing are the built-in types list, str, and \nSlicing can be extended to any Python class that implements \nThe basic form of the slicing syntax is somelist[start:end], where \na = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nAll but ends: ['b', 'c', 'd', 'e', 'f', 'g']\nWhen slicing from the start of a list, you should leave out the zero \nWhen slicing to the end of a list, you should leave out the final index \nto the end of a list.\na[:]      # ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\na[:-1]    # ['a', 'b', 'c', 'd', 'e', 'f', 'g']\nSlicing deals properly with start and end indexes that are beyond the \nItem 11: Know How to Slice Sequences \nThe result of slicing a list is a whole new list.\nslicing won’t affect the original list:\nNo change: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nWhen used in assignments, slices replace the specified range \na, b = c[:2]; see Item 6: “Prefer Multiple Assignment Unpacking \nOver Indexing”), the lengths of slice assignments don’t need to be the \nThe values before and after the assigned slice will be preserved.\nBefore  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nIf you leave out both the start and the end indexes when slicing, you \nIf you assign to a slice with no start or end indexes, you replace the \nassert a is b             # Still the same list object\nprint('After b ', b)      # Same list, so same contents as a\n✦ Slicing is forgiving of start or end indexes that are out of bounds, \nwhich means it’s easy to express slices on the front or back bound-\n✦ Assigning to a list slice replaces that range in the original sequence \nItem 12:  Avoid Striding and Slicing in \nSequences”), Python has special syntax for the stride of a slice in \nwhen slicing a sequence.\nItem 12: Avoid Striding and Slicing in a Single Expression \nx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nx[::-2]  # ['h', 'f', 'd', 'b']",
    "keywords": [
      "count",
      "Assignment Expressions",
      "list",
      "fresh",
      "Assignment",
      "pieces",
      "make",
      "fruit",
      "Item",
      "slicing",
      "slice",
      "Python",
      "Expressions",
      "fruit.get",
      "end"
    ],
    "concepts": [
      "item",
      "items",
      "list",
      "lists",
      "slices",
      "slice",
      "slicing",
      "count",
      "pythonic",
      "python"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 70-82)",
    "start_page": 70,
    "end_page": 82,
    "summary": "✦ Prefer using positive stride values in slices without start or end \nItem 13: Prefer Catch-All Unpacking Over Slicing\nWhen I try to take the first two items of the list with basic unpack-\ncar_ages_descending = sorted(car_ages, reverse=True)\nItem 13: Prefer Catch-All Unpacking Over Slicing \nNewcomers to Python often rely on indexing and slicing (see Item 11: \nBut it is possible to use multiple starred expressions in an unpacking \nItem 13: Prefer Catch-All Unpacking Over Slicing \nFor example, here I unpack the values from iterating over a range \nBut with the addition of starred expressions, the value of unpack-\n✦ Unpacking assignments may use a starred expression to catch all \nItem 14:  Sort by Complex Criteria Using the key \nThe list built-in type provides a sort method for ordering the items \norder a list’s contents by the natural ascending order of the items.\nFor example, here I sort a list of integers from smallest to largest:\nnumbers.sort()\nThe sort method works for nearly all built-in types (strings, floats, \nWhat does sort do with \nItem 14: Sort by Complex Criteria Using the key Parameter \ntools = [\nSorting objects of this type doesn’t work because the sort method \ntools.sort()\n'Tool'\nto Use heapq for Priority Queues” for an example) to make sort work \nOften there’s an attribute on the object that you’d like to use for sort-\nTo support this use case, the sort method accepts a key param-\nsingle argument, which is an item from the list that is being sorted.\n(i.e., with a natural ordering) to use in place of an item for sorting \neter that enables me to sort the list of Tool objects alphabetically by \ntools.sort(key=lambda x: x.name)\nSorted:   [Tool('chisel',      0.25),\nI can just as easily define another lambda function to sort by weight \nand pass it as the key parameter to the sort method:\ntools.sort(key=lambda x: x.weight)\nprint('By weight:', tools)\nFor basic types like strings, you may even want to use the key func-\ntion to do transformations on the values before sorting.\nhere I apply the lower method to each item in a list of place names to \nplaces.sort()\nplaces.sort(key=lambda x: x.lower())\nSometimes you may need to use multiple criteria for sorting.\nexample, say that I have a list of power tools and I want to sort them \nItem 14: Sort by Complex Criteria Using the key Parameter \nthe sort method.\nto sort the list of power tools first by weight and then by name.\nbutes that I want to sort on in order of priority:\npower_tools.sort(key=lambda x: (x.weight, x.name))\nprint(power_tools)\nparameter to the sort method, it will affect both criteria in the tuple \npower_tools.sort(key=lambda x: (x.weight, x.name),\nprint(power_tools)\nFor numerical values it’s possible to mix sorting directions by using \nthe values in the returned tuple, effectively reversing its sort order \nHere, I use this approach to sort by \npower_tools.sort(key=lambda x: (-x.weight, x.name))\nprint(power_tools)\npower_tools.sort(key=lambda x: (x.weight, -x.name),\nThe sort method of the list type will preserve the order of the input \nlist when the key function returns values that are equal to each \nsort ordering of weight descending and name ascending as I did above \npower_tools.sort(key=lambda x: x.name)   # Name ascending\npower_tools.sort(key=lambda x: x.weight, # Weight descending\nprint(power_tools)\nItem 14: Sort by Complex Criteria Using the key Parameter \npower_tools.sort(key=lambda x: x.name)\nprint(power_tools)\nWhen the second sort call by weight descending is made, it sees that \nThis causes the sort \nmethod to put both items into the final result list in the same order \npower_tools.sort(key=lambda x: x.weight,\nprint(power_tools)\nneed to make sure that you execute the sorts in the opposite sequence \nthe sort order to be by weight descending and then by name ascend-\ning, so I had to do the name sort first, followed by the weight sort.\nand using unary negation to mix sort orders, is simpler to read and \nI recommend only using multiple calls to sort if \n✦ The sort method of the list type can be used to rearrange a list’s \n✦ The sort method doesn’t work for objects unless they define a natu-\n✦ The key parameter of the sort method can be used to supply a \nhelper function that returns the value to use for sorting in place of \neach item from the list.\nused to reverse individual sort orders for types that allow it.\n✦ For types that can’t be negated, you can combine many sorting cri-\nteria together by calling the sort method multiple times using dif-\nferent key functions and reverse values, in the order of lowest rank \nWhen I created the dictionary the keys were in the order 'cat', 'dog', \nbut when I printed it the keys were in the reverse order 'dog', 'cat'.\nItem 15: Be Cautious When Relying on dict Insertion Ordering \non iteration order, including keys, values, items, and popitem, would \nprint(list(baby_names.keys()))\nprint(list(baby_names.values()))\nprint(list(baby_names.items()))\nprint(list(baby_names.keys()))\nprint(list(baby_names.values()))\nprint(list(baby_names.items()))\nprint(baby_names.popitem())  # Last item inserted\nfor key, value in kwargs.items():\nprint('%s = %s' % (key, value))\nfor key, value in kwargs.items():\nprint(f'{key} = {value}')\nfor key, value in a.__dict__.items():\nprint('%s = %s' % (key, value))",
    "keywords": [
      "Tool",
      "list",
      "Sort",
      "key",
      "Item",
      "sort method",
      "Python",
      "Unpacking",
      "order",
      "tools",
      "items",
      "key function",
      "key Parameter",
      "Prefer Catch-All Unpacking",
      "method"
    ],
    "concepts": [
      "sorted",
      "sort",
      "sorting",
      "sorts",
      "orders",
      "ordering",
      "order",
      "orderings",
      "printed",
      "item"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 83-98)",
    "start_page": 83,
    "end_page": 98,
    "summary": "for key, value in a.__dict__.items():\nprint(f'{key} = {value}')\nnames = list(votes.keys())\nnames.sort(key=votes.get, reverse=True)\nfirst key must be the winner:\ncollections.abc built-in module to define a new dictionary-like class \ndef __getitem__(self, key):\nreturn self.data[key]\ndef __setitem__(self, key, value):\nself.data[key] = value\ndef __delitem__(self, key):\ndel self.data[key]\nkeys = list(self.data.keys())\nkeys.sort()\nvalue passed to get_winner is a dict instance and not a MutableMapping \nwith dictionary-like behavior (see Item 90: “Consider Static Analysis \nnames = list(votes.keys())\nnames.sort(key=votes.get, reverse=True)\nItem 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 65\ninstance’s contents will occur in the same order in which the keys \n✦ Python makes it easy to define objects that act like dictionaries but \nfor the dict type at runtime, or require dict values using type anno-\nMissing Dictionary Keys\nies are accessing, assigning, and deleting keys and their associated \nTo increment the counter for a new vote, I need to see if the key exists, \ninsert the key with a default counter value of zero if it’s missing, and \nThis requires accessing the key \nif key in counters:\ncount = counters[key]\ncounters[key] = count + 1\ndictionaries raise a KeyError exception when you try to get the value \ncount = counters[key]\ncounters[key] = count + 1\nThis flow of fetching a key that exists or returning a default value \nis so common that the dict built-in type provides the get method to \ncount = counters.get(key, 0)\ncounters[key] = count + 1\nif key not in counters:\ncounters[key] = 0\ncounters[key] += 1\nif key in counters:\ncounters[key] += 1\ncounters[key] = 1\nItem 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 67\ncounters[key] += 1\ncounters[key] = 1\nThus, for a dictionary with simple types, using the get method is the \nWhat if the values of the dictionary are a more complex type, like a \ning a list of names with each key:\nkey = 'brioche'\nif key in votes:\nnames = votes[key]\nvotes[key] = names = []\nRelying on the in expression requires two accesses if the key is pres-\nent, or one access and one assignment if the key is missing.\nvalue for each key can be assigned blindly to the default value of an \nempty list if the key doesn’t already exist.\nstatement (votes[key] = names = []) populates the key in one line \nthe dictionary value is a list.\nThis approach requires one key access \nif the key is present, or one key access and one assignment if it’s \nnames = votes[key]\nvotes[key] = names = []\nSimilarly, you can use the get method to fetch a list value when the \nkey is present, or do one fetch and one assignment if the key isn’t \nnames = votes.get(key)\nvotes[key] = names = []\nif (names := votes.get(key)) is None:\nvotes[key] = names = []\nThe dict type also provides the setdefault method to help shorten \nsetdefault tries to fetch the value of a key \nIf the key isn’t present, the method assigns that key \nAnd then the method returns the value \nfor that key: either the originally present value or the newly inserted \nnames = votes.setdefault(key, [])\nItem 16: Prefer get Over in and KeyError to Handle Missing Dictionary Keys 69\nsetdefault is assigned directly into the dictionary when the key is \nkey = 'foo'\ndata.setdefault(key, value)\ning a new default value for each key I access with setdefault.\nvalues instead of lists of who voted: Why not also use the setdefault \ncount = counters.setdefault(key, 0)\ncounters[key] = count + 1\nalways need to assign the key in the dictionary to a new value \nwhereas using setdefault requires one access and two assignments.\nshortest way to handle missing dictionary keys, such as when the \nsetdefault to Handle Missing Items in Internal State”).\n✦ There are four common ways to detect and handle missing keys \n✦ The get method is best for dictionaries that contain basic types \n✦ When the setdefault method of dict seems like the best fit for your \nvariety of ways to handle missing keys (see Item 16: “Prefer get Over \nin and KeyError to Handle Missing Dictionary Keys”).\nI can use the setdefault method to add new cities to the sets, whether \nvisits.setdefault('France', set()).add('Arles')  # Short\nvalue when a key doesn’t exist.\nthat will return the default value to use each time a key is missing \n(an example of Item 38: “Accept Functions Instead of Classes for Sim-\nassume that accessing any key in the data dictionary will always \ntions (see Item 18: “Know How to Construct Key-Dependent Default \nValues with __missing__,” Item 43: “Inherit from collections.abc for \ntial keys, then you should prefer using a defaultdict instance from \n✦ If a dictionary of arbitrary keys is passed to you, and you don’t con-\nItem 18: Know How to Construct Key-Dependent Default Values \nItem 18:  Know How to Construct Key-Dependent \nThe built-in dict type’s setdefault method results in shorter code \nwhen handling missing keys in some specific circumstances (see Item \n16: “Prefer get Over in and KeyError to Handle Missing Dictionary \n(see Item 17: “Prefer defaultdict Over setdefault to Handle Missing \nand checking for the presence of keys using the get method and an \nWhen the file handle already exists in the dictionary, this code makes \nexist, the dictionary is accessed once by get, and then it is assigned \nhandle = pictures.setdefault(path, open(path, 'a+b'))\nfor other  dictionary-like implementations; see Item 43: “Inherit from \nhelper function that defaultdict calls doesn’t know which specific key \nItem 18: Know How to Construct Key-Dependent Default Values \ndling missing keys.\ndef __missing__(self, key):\nvalue = open_picture(key)\nself[key] = value\nWhen the pictures[path] dictionary access finds that the path key \nisn’t present in the dictionary, the __missing__ method is called.\nmethod must create the new default value for the key, insert it into \n✦ The setdefault method of dict is a bad fit when creating the default \non the key being accessed.\nin order to construct default values that must know which key was ",
    "keywords": [
      "Missing Dictionary Keys",
      "key",
      "Handle Missing Dictionary",
      "Handle Missing Items",
      "Item",
      "Handle Missing",
      "dictionary",
      "Dictionary Keys",
      "handle missing keys",
      "dict",
      "ranks",
      "Missing",
      "keys",
      "Missing Dictionary",
      "handle"
    ],
    "concepts": [
      "keys",
      "type",
      "types",
      "typed",
      "typing",
      "item",
      "items",
      "classes",
      "dictionaries",
      "dictionary"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 99-114)",
    "start_page": 99,
    "end_page": 114,
    "summary": "Functions\nfunction.\nItem 19:  Never Unpack More Than Three Variables \nWhen Functions Return Multiple Values\ntions to seemingly return more than one value.\nfunction that appears to return two values:\nminimum, maximum = get_stats(lengths)  # Two return values\nThe way this works is that multiple values are returned together in a \nThe calling code then unpacks the returned tuple by \nhow an unpacking statement and multiple-return function work the \ndef my_function():\nreturn 1, 2\nfirst, second = my_function()\nMultiple return values can also be received by starred expressions for \nItem 19: Never Unpack More Than Three Return Values \nfunction to also calculate these statistics and return them in the \nreturn minimum, maximum, average, median, count\nFirst, all the return values \nUsing a large number of return values is extremely error \nSecond, the line that calls the function and unpacks the values is \nables when unpacking the multiple return values from a function.\nThese could be individual values from a three-tuple, two variables \nneed to unpack more return values than that, you’re better off defin-\nfunction return an instance of that instead.\n✦ You can have functions return multiple values by putting them in a \n✦ Multiple return values from a function can also be unpacked by \nItem 20: Prefer Raising Exceptions to Returning None\nmers to give special meaning to the return value of None.\nFor example, say I want a helper function \nreturn None\nCode using this function can interpret the return value accordingly:\nItem 20: Prefer Raising Exceptions to Returning None \nIf the denominator is not zero, the function returns zero.\nproblem is that a zero return value can cause issues when you evalu-\nonly looking for None (see Item 5: “Write Helper Functions Instead of \nThis misinterpretation of a False-equivalent return value is a common \nreturning None from a function like careful_divide is error prone.\nThe first way is to split the return value into a two-tuple (see Item 19: \n“Never Unpack More Than Three Variables When Functions Return \nCallers of this function have to unpack the tuple.\nThe caller no longer requires a condition on the return value of the \nfunction.\nInstead, it can assume that the return value is always \nYou can specify that a function’s return value will \nItem 21: Know How Closures Interact with Variable Scope \n✦ Functions that return None to indicate special meaning are error \n✦ Raise exceptions to indicate special situations instead of returning \n✦ Type annotations can be used to make it clear that a function will \nnever return the value None, even in special situations.\nA common way to do this is to pass a helper function as the key argu-\nThe helper’s return value will \nbe used as the value for sorting each item in the list.\ndef sort_priority(values, group):\nThis function works for simple inputs:\nnumbers = [8, 3, 1, 2, 5, 4, 7, 6]\nprint(numbers)\n■Python supports closures—that is, functions that refer to variables \nfunction is able to access the group argument for sort_priority.\narguments to other functions, compare them in expressions and \na closure function as the key argument.\nThis is why the return \nvalue from the helper closure causes the sort order to have two \nIt’d be nice if this function returned whether higher-priority items \nfunction for deciding which group each number is in.\nthe function can return the flag value after it’s been modified by the \ndef sort_priority2(numbers, group):\nreturn found\nItem 21: Know How Closures Interact with Variable Scope \nI can run the function on the same inputs as before:\nprint(numbers)\nThe sorted results are correct, which means items from group were \nYet the found result returned by the func-\n1. The current function’s scope.\n2. Any enclosing scopes (such as other containing functions).\n4. The built-in scope (that contains functions like len and str).\nAssigning a value to a variable works differently.\nnewly defined variable is the function that contains the assignment.\nThis assignment behavior explains the wrong return value of the \nsort_priority2 function.\ndef sort_priority2(numbers, group):\nreturn found\nprevents local variables in a function from polluting the containing \nOtherwise, every assignment within a function would put \nHere, I define the same function again, now using nonlocal:\ndef sort_priority3(numbers, group):\nreturn found\ntion against using nonlocal for anything beyond simple functions.\neasier to read (see Item 38: “Accept Functions Instead of Classes for \nItem 22: Reduce Visual Noise with Variable Positional Arguments \n✦ Closure functions can refer to variables from any of the scopes in \nfunctions.\nItem 22:  Reduce Visual Noise with Variable Positional \nAccepting a variable number of positional arguments can make a \nof arguments, I would need a function that takes a message and a \ndef log(message, values):\nMy numbers are: 1, 2\nrequired, whereas any number of subsequent positional arguments \ndef log(message, *values):  # The only difference\nMy numbers are: 1, 2\nfunction like log, I can do this by using the * operator.\nPython to pass items from the sequence as positional arguments to \nthe function:\nturned into a tuple before they are passed to a function.\nthat if the caller of a function uses the * operator on a generator, it \nItem 22: Reduce Visual Noise with Variable Positional Arguments \nthe number of inputs in the argument list will be reasonably small.\n*args is ideal for function calls that pass many literals or variable \narguments to a function in the future without migrating every caller.\ndef log(sequence, message, *values):\nkeyword-only arguments when you want to extend functions that \n✦ Functions can accept a variable number of positional arguments by \n✦ You can use the items from a sequence as the positional arguments \nfor a function with the * operator.\n✦ Adding new positional parameters to functions that accept *args \narguments by position when calling a function:\ndef remainder(number, divisor):\nreturn number % divisor\nAll normal arguments to Python functions can also be passed by \nwithin the parentheses of a function call.\nItem 23: Provide Optional Behavior with Keyword Arguments \nTypeError: remainder() got multiple values for argument \ncall a function like remainder, you can do this by using the ** opera-\nthe corresponding keyword arguments of the function:\n'number': 20,\narguments in the function call, as long as no argument is repeated:\n'number': 20,\nfor key, value in kwargs.items():\nThe first benefit is that keyword arguments make the function call \nwith keyword arguments, number=20 and divisor=7 make it immedi-\ndefault values specified in the function definition.\nprovide this behavior in the same function by adding an argument for \ntime I call the function, even in the common case of flow rate per sec-",
    "keywords": [
      "function",
      "Functions",
      "Item",
      "Functions Return Multiple",
      "arguments",
      "numbers",
      "Python",
      "Positional Arguments",
      "variable",
      "Variable Positional Arguments",
      "number",
      "Functions Return",
      "Keyword Arguments",
      "argument",
      "Scope"
    ],
    "concepts": [
      "functions",
      "function",
      "item",
      "items",
      "returned",
      "returns",
      "returning",
      "numbers",
      "number",
      "argument"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 115-125)",
    "start_page": 115,
    "end_page": 125,
    "summary": "Item 23: Provide Optional Behavior with Keyword Arguments \nTo make this less noisy, I can give the period argument a default \nThe period argument is now optional:\nThe third reason to use keyword arguments is that they provide a \nThe default argument value for units_per_kg is 1, which makes the \nspecify the new keyword argument to see the new behavior:\nProviding backward compatibility using optional keyword arguments \nSupplying optional arguments positionally can be confusing because \npractice is to always specify optional arguments using the keyword \nnames and never pass them as positional arguments.\n✦ Function arguments can be specified by position or by keyword.\n✦ Keywords make it clear what the purpose of each argument is when \nit would be confusing with only positional arguments.\n✦ Keyword arguments with default values make it easy to add new \n✦ Optional keyword arguments should always be passed by keyword \nDefault Arguments\nSometimes you need to use a non-static type as a keyword  argument’s \narguments are reevaluated each time the function is called:\nA default argument value is evaluated only once per module \nItem 24: Specify Dynamic Default Arguments in Docstrings \nUsing None for default argument values is especially important when \nreturn default\ndecode because default argument values are evaluated only once (at \nThe fix is to set the keyword argument default value to None and then \ndefault: Value to return if decoding fails.\nreturn default\nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \nargument is marked as having an Optional value that is a datetime.\n✦ A default argument value is evaluated only once: during function \n✦ Use None as the default value for any keyword argument that has a \n✦ Using None to represent keyword argument default values also \nPositional-Only Arguments\nPassing arguments by keyword is a powerful feature of Python func-\nThe flexibility of keyword arguments enables you to write \nOther times, I want to ignore OverflowError exceptions and return \ndef safe_division(number, divisor,\nean arguments that control the exception-ignoring behavior.\nreadability of this code is to use keyword arguments.\ndef safe_division_b(number, divisor,\nThen, callers can use keyword arguments to specify which of the \nresult = safe_division_b(1.0, 10**500, ignore_overflow=True)\nresult = safe_division_b(1.0, 0, ignore_zero_division=True)\nThe problem is, since these keyword arguments are optional behavior, \nthere’s nothing forcing callers to use keyword arguments for clarity.\nthe old way with positional arguments:\narguments.\nThese arguments can only be supplied by keyword, never \nHere, I redefine the safe_division function to accept keyword-only \narguments.\nof positional arguments and the beginning of keyword-only \narguments:\ndef safe_division_c(number, divisor, *,  # Changed\nNow, calling the function with positional arguments for the keyword \nTypeError: safe_division_c() takes 2 positional arguments but 4 \nBut keyword arguments and their default values will work as expected \nresult = safe_division_c(1.0, 0, ignore_zero_division=True)\nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \nthis function: Callers may specify the first two required arguments \nassert safe_division_c(number=2, divisor=5) == 0.4\ndef safe_division_c(numerator, denominator, *,  # Changed\ning callers that specified the number or divisor arguments using \nTypeError: safe_division_c() got an unexpected keyword argument \narguments.\nThese arguments can be supplied only by position and \nnever by keyword (the opposite of the keyword-only arguments \nHere, I redefine the safe_division function to use positional-only \narguments for the first two required parameters.\ndef safe_division_d(numerator, denominator, /, *,  # Changed\nI can verify that this function works when the required arguments \nTypeError: safe_division_d() got some positional-only arguments \n¯passed as keyword arguments: 'numerator, denominator'\nNow, I can be sure that the first two required positional arguments \nin the definition of the safe_division_d function are decoupled from \nOne notable consequence of keyword- and positional-only arguments \nthe default for all function arguments in Python).\nanother optional parameter to safe_division that allows callers to \nItem 25: Clarity with Keyword-Only and Positional-Only Arguments \n✦ Keyword-only arguments force callers to supply certain arguments \nKeyword-only arguments are defined after a \n✦ Positional-only arguments ensure that callers can’t supply \nPositional-only arguments are defined before a single / in the argu-\nmay be supplied by position or keyword, which is the default for \ncan access and modify input arguments, return values, and raised \nFor example, say that I want to print the arguments and return value \nIt prints the arguments and return value at each ",
    "keywords": [
      "Arguments",
      "Keyword Arguments",
      "default",
      "Keyword",
      "function",
      "argument",
      "Positional-Only Arguments",
      "division",
      "safe",
      "optional keyword arguments",
      "default argument",
      "keyword argument default",
      "Item",
      "Dynamic Default Arguments",
      "Positional Arguments"
    ],
    "concepts": [
      "arguments",
      "argument",
      "functionality",
      "functions",
      "returned",
      "returns",
      "result",
      "item",
      "default",
      "defaults"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 126-133)",
    "start_page": 126,
    "end_page": 133,
    "summary": "<function trace.<locals>.wrapper at 0x108955dc0>\nThe trace function returns the \nThe wrapper function is what’s \nFor example, the help built-in function is useless when called on the \ndecorated fibonacci function.\nHelp on function wrapper in module __main__:\nItem 26: Define Function Decorators with functools.wraps \nthough the function is decorated:\nHelp on function fibonacci in module __main__:\nBeyond these examples, Python functions have many other standard \n✦ Decorators in Python are syntax to allow one function to modify \nComprehensions \nfunction.\nThe result of a call to a generator function can be used any-\nItem 27:  Use Comprehensions Instead of map \nThese expressions are called list comprehensions.\nChapter 4 Comprehensions and Generators\nWith a list comprehension, I can achieve the same outcome by specify-\nsquares = [x**2 for x in a]  # List comprehension\nUnless you’re applying a single-argument function, list comprehen-\nsions are also clearer than the map built-in function for simple cases.\nmap requires the creation of a lambda function for the computation, \nUnlike map, list comprehensions let you easily filter items from the \nthe list comprehension after the loop:\nThe filter built-in function can be used along with map to achieve the \nItem 28: Control Subexpressions in Comprehensions \n✦ List comprehensions are clearer than the map and filter built-in \n✦ List comprehensions allow you to easily skip items from the input \nlist, a behavior that map doesn’t support without help from filter.\n✦ Dictionaries and sets may also be created using comprehensions.\nSubexpressions in Comprehensions\nBeyond basic usage (see Item 27: “Use Comprehensions Instead of map \nand filter”), comprehensions support multiple levels of looping.\nexample, say that I want to simplify a matrix (a list containing other \nloops in a comprehension.\nThis comprehension is noisier because of the \nChapter 4 Comprehensions and Generators\nIf this comprehension included another loop, it would get so long that \nflat = [x for sublist1 in my_lists\nthan the three-level-list comprehension:\nComprehensions support multiple if conditions.\nsay that I want to filter a list of numbers to only even values greater \nThese two list comprehensions are equivalent:\nExpressing this with a list comprehension does not require a lot of \nItem 29: Control Subexpressions in Comprehensions \nsions in a comprehension.\nhelper function (see Item 30: “Consider Generators Instead of Return-\n✦ Comprehensions support multiple levels of loops and multiple con-\nItem 29:  Avoid Repeated Work in Comprehensions by ",
    "keywords": [
      "Comprehensions",
      "list",
      "function",
      "list comprehensions",
      "Item",
      "fibonacci",
      "map",
      "filter",
      "squares",
      "wrapper",
      "Control Subexpressions",
      "List comprehension print",
      "lists",
      "lambda",
      "multiple"
    ],
    "concepts": [
      "comprehensions",
      "comprehension",
      "lists",
      "list",
      "functions",
      "item",
      "items",
      "returned",
      "returns",
      "filter"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 134-147)",
    "start_page": 134,
    "end_page": 147,
    "summary": "comprehension’s value expression:\nprint(f'Last item of {list(stock.values())} is {count}')\nator expressions (see Item 32: “Consider Generator Expressions for \ngenerator expressions to reuse the value from one condition else-\na comprehension or generator expression’s condition, you should \nItem 30:  Consider Generators Instead of Returning \nis to return a list of items.\nItem 30: Consider Generators Instead of Returning Lists \nA better way to write this function is by using a generator.\nGenerators \nare produced by functions that use yield expressions.\ngenerator function that produces the same results as before:\ndef index_words_iter(text):\nWhen called, a generator function does not actually run but instead \nimmediately returns an iterator.\nfunction, the iterator advances the generator to its next yield expres-\nEach value passed to yield by the generator is returned by the \niterator to the caller:\nit = index_words_iter(address)\nThe index_words_iter function is significantly easier to read because \ntor returned by the generator to a list by passing it to the list built-in \nfunction if necessary (see Item 32: “Consider Generator Expressions \nresult = list(index_words_iter(address))\nFor example, here I define a generator that streams input from \nresults (see Item 36: “Consider itertools for Working with Iterators \nprint(list(results))\nItem 31: Be Defensive When Iterating Over Arguments \nmust be aware that the iterators returned are stateful and can’t be \nreused (see Item 31: “Be Defensive When Iterating Over Arguments”).\nfunction return a list of accumulated results.\n✦ The iterator returned by a generator produces the set of values \npassed to yield expressions within the generator function’s body.\nItem 31: Be Defensive When Iterating Over Arguments\nimportant to iterate over that list multiple times.\nThis function works as expected when given a list of visits:\nI define a generator to do this because then I can \nrequirements (see Item 30: “Consider Generators Instead of Returning \nSurprisingly, calling normalize on the read_visits generator’s return \nThis behavior occurs because an iterator produces its results only \nIf you iterate over an iterator or a generator that has \nalready exhausted iterator.\nfunctions can’t tell the difference between an iterator that has no out-\nTo solve this problem, you can explicitly exhaust an input iterator and \nYou can then iterate over \nsame function as before, but it defensively copies the input iterator:\nnumbers_copy = list(numbers)  # Copy the iterator\nItem 31: Be Defensive When Iterating Over Arguments \nNow the function works correctly on the read_visits generator’s \nThe problem with this approach is that the copy of the input iterator’s \nCopying the iterator could cause \ntion that returns a new iterator each time it’s called:\ndef normalize_func(get_iter):\nfor value in get_iter():  # New iterator\nthe generator and produces a new iterator each time:\ncontainer class that implements the iterator protocol.\nThe iterator protocol is how Python for loops and related expressions \nThe iter built-in \nfunction calls the foo.__iter__ special method in turn.\nThe __iter__ \nmethod must return an iterator object (which itself implements the \nnext built-in function on the iterator object until it’s exhausted (indi-\nas a generator.\nHere, I define an iterable container class that reads \ndef __iter__(self):\nnormalize the numbers also calls __iter__ to allocate a second iter-\njust iterators.\nto the iter built-in function, iter returns the iterator itself.\ntrast, when a container type is passed to iter, a new iterator object is \nItem 31: Be Defensive When Iterating Over Arguments \niterated over:\nif isinstance(numbers, Iterator):  # Another way to check\nthe full input iterator, as with the normalize_copy function above, but \nyou also need to iterate over the input data multiple times.\nThe function raises an exception if the input is an iterator rather than \nit = iter(visits)\n✦ Beware of functions and methods that iterate over input argu-\nIf these arguments are iterators, you may see \n✦ Python’s iterator protocol defines how containers and iterators inter-\nact with the iter and next built-in functions, for loops, and related \n✦ You can easily define your own iterable container type by imple-\nmenting the __iter__ method as a generator.\n✦ You can detect that a value is an iterator (instead of a container) \nif calling iter on it produces the same value as what you passed \nItem 32:  Consider Generator Expressions for Large \ninstances containing one item for each value in input sequences.\nItem 32: Consider Generator Expressions for Large List Comprehensions \nTo solve this issue, Python provides generator expressions, which are \na generalization of list comprehensions and generators.\nGenerator \nInstead, generator expressions evaluate to an iterator that yields \none item at a time from the expression.\nYou create a generator expression by putting list-comprehension-like \nHere, I use a generator expression \nThe returned iterator can be advanced one step at a time to produce \nthe next output from the generator expression, as needed (using \nHere, I take the iterator returned by the gen-\nerator expression above and use it as the input for another generator \noperating on a large stream of input, generator expressions are a \nThe only gotcha is that the iterators returned by gener-\niterators more than once (see Item 31: “Be Defensive When Iterating \n✦ List comprehensions can cause problems for large inputs by using \n✦ Generator expressions avoid memory issues by producing outputs \none at a time as iterators.\n✦ Generator expressions can be composed by passing the iterator from \none generator expression into the for subexpression of another.\nItem 33: Compose Multiple Generators with yield from\nlems (see Item 31: “Be Defensive When Iterating Over Arguments”).\ntwo generators that yield the expected onscreen deltas for each part of \nItem 33: Compose Multiple Generators with yield from \nthis by calling a generator for each step of the animation, iterating \nover each generator in turn, and then yielding the deltas from all of \nThis advanced generator feature allows you to yield all values from ",
    "keywords": [
      "Generator",
      "Generator Expressions",
      "Item",
      "List Comprehensions",
      "Large List Comprehensions",
      "iterator",
      "Generators",
      "list",
      "Comprehensions",
      "function",
      "expressions",
      "result",
      "batches",
      "iter",
      "yield"
    ],
    "concepts": [
      "iterating",
      "iterate",
      "iteration",
      "iterated",
      "generators",
      "generator",
      "result",
      "results",
      "expression",
      "expressions"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 148-156)",
    "start_page": 148,
    "end_page": 156,
    "summary": "yield from move(4, 5.0)\nyield from move(2, 3.0)\nyield i\nyield i\nItem 34: Avoid Injecting Data into Generators with send \nnested generators and yielding their outputs.\nyield expressions provide generator functions with a simple way to \ndef wave(amplitude, steps):\nyield output\niterating over the wave generator:\nulate the amplitude on each iteration of the generator.\nPython generators support the send method, which upgrades yield \nprovide streaming inputs to a generator at the same time it’s yielding \nNormally, when iterating a generator, the value of the yield \ndef my_generator():\nit = iter(my_generator())\noutput = next(it)       # Get first generator output\nItem 34: Avoid Injecting Data into Generators with send \nnext(it)            # Run generator until it exits\nWhen I call the send method instead of iterating the generator with a \nthe value of the yield expression when the generator is resumed.\never, when the generator first starts, a yield expression has not been \nit = iter(my_generator())\noutput = it.send(None)  # Get first generator output\nit.send('hello!')   # Send value into the generator\nthe wave generator to save the amplitude returned by the yield expres-\nsion and use it to calculate the next generated output:\namplitude = yield output  # Receive next amplitude\namplitude into the wave_modulating generator on each iteration.\nfirst input to send must be None, since a yield expression would not \namplitude wasn’t received by the generator until after the initial yield \nway to implement this behavior is by composing multiple generators \nMultiple Generators with yield from”).\nyield from wave(7.0, 3)\nItem 34: Avoid Injecting Data into Generators with send \nyield from wave(2.0, 4)\nyield from wave(10.0, 5)\nmay expect it to also work properly along with the generator send \nthe wave_modulating generator together:\nyield from wave_modulating(3)\nyield from wave_modulating(4)\nyield from wave_modulating(5)\ninitial amplitude from a generator send method call.\nparent generator to output a None value when it transitions between \nchild generators.\ndef wave_cascading(amplitude_it, steps):\nyield output\nI can pass the same iterator into each of the generator functions that \ndef complex_wave_cascading(amplitude_it):\nyield from wave_cascading(amplitude_it, 3)\nyield from wave_cascading(amplitude_it, 4)\nyield from wave_cascading(amplitude_it, 5)\nNow, I can run the composed generator by simply passing in an itera-\na generator function).\n✦ The send method can be used to inject data into a generator by giv-\n✦ Using send with yield from expressions may cause surprising \ngenerator output.\n✦ Providing an input iterator to a set of composed generators is a bet-\nGenerators with throw\nple Generators with yield from”) and the send method (see Item 34: \ngenerator feature is the throw method for re-raising Exception \ndef my_generator():\nyield 1\nyield 2\nyield 3\nit = my_generator()\ndef my_generator():\nyield 1\nyield 2\nyield 3\nyield 4\nit = my_generator()",
    "keywords": [
      "output",
      "yield",
      "generator",
      "Generators",
      "Avoid Injecting Data",
      "wave",
      "yield expression",
      "amplitude",
      "Item",
      "send",
      "Avoid Injecting",
      "Injecting Data",
      "expression",
      "run",
      "generator output"
    ],
    "concepts": [
      "generators",
      "generate",
      "generated",
      "outputs",
      "output",
      "yield",
      "yielding",
      "amplitude",
      "amplitudes",
      "send"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 157-164)",
    "start_page": 157,
    "end_page": 164,
    "summary": "Item 35: Avoid Causing State Transitions in Generators with throw \ntimer generator, which injects exceptions with throw to cause resets, \ncurrent = it.throw(Reset())\nSimple Interfaces”) using an iterable container object (see Item 31: “Be \ndef __iter__(self):\nItem 35: Avoid Causing State Transitions in Generators with throw \nfor current in timer:\nan iterable class if you need this type of exceptional behavior.\nItem 36:  Consider itertools for Working with Iterators \nThe itertools built-in module contains a large number of functions \nthat are useful for organizing and interacting with iterators (see Item \nThe itertools built-in module includes a number of functions for \nUse chain to combine multiple iterators into a single sequential \niterator:\nit = itertools.chain([1, 2, 3], [4, 5, 6])\nUse repeat to output a single value forever, or use the second param-\nUse cycle to repeat an iterator’s items forever:\nit = itertools.cycle([1, 2])\nItem 36: Consider itertools for Working with Iterators and Generators \nUse tee to split a single iterator into the number of parallel iterators \nit1, it2, it3 = itertools.tee(['first', 'second'], 3)\nThis variant of the zip built-in function (see Item 8: “Use zip to \nProcess Iterators in Parallel”) returns a placeholder value when an \nit = itertools.zip_longest(keys, values, fillvalue='nope')\nFiltering Items from an Iterator\nThe itertools built-in module includes a number of functions for fil-\ntering items from an iterator.\nfirst_five = itertools.islice(values, 5)\nmiddle_odds = itertools.islice(values, 2, 8, 2)\ntakewhile returns items from an iterator until a predicate function \nreturns False for an item:\nit = itertools.takewhile(less_than_seven, values)\niterator until the predicate function returns True for the first time:\nit = itertools.dropwhile(less_than_seven, values)\nItem 36: Consider itertools for Working with Iterators and Generators \nreturns all items from an iterator where a predicate function returns \nprint('Filter:      ', list(filter_result))\nfilter_false_result = itertools.filterfalse(evens, values)\nprint('Filter false:', list(filter_false_result))\nProducing Combinations of Items from Iterators\nThe itertools built-in module includes a number of functions for \nproducing combinations of items from iterators.\naccumulate folds an item from the iterator into a running value by \nsum_reduce = itertools.accumulate(values)\nmodulo_reduce = itertools.accumulate(values, sum_modulo_20)\nproduct returns the Cartesian product of items from one or more iter-\nsingle = itertools.product([1, 2], repeat=2)\nwith items from an iterator:\nit = itertools.permutations([1, 2, 3, 4], 2)\nunrepeated items from an iterator:\nit = itertools.combinations([1, 2, 3, 4], 2)",
    "keywords": [
      "ticks remaining",
      "Item",
      "ticks",
      "items",
      "remaining",
      "Generators",
      "Iterators",
      "iterator",
      "list",
      "Reset",
      "Avoid Causing State",
      "timer",
      "Avoid",
      "current",
      "itertools"
    ],
    "concepts": [
      "iterating",
      "iteration",
      "item",
      "items",
      "value",
      "values",
      "lists",
      "list",
      "exception",
      "exceptions"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 165-174)",
    "start_page": 165,
    "end_page": 174,
    "summary": "Classes and \nI can define a class to store the names in a dictionary instead of using \nself._grades = {}\nself._grades[name] = []\ndef report_grade(self, name, score):\nself._grades[name].append(score)\ndef average_grade(self, name):\ngrades = self._grades[name]\na list of grades by subject, not just overall.\nthe _grades dictionary to map student names (its keys) to yet another \n(its keys) to a list of grades (its values).\nself._grades[name] = defaultdict(list)  # Inner dict\nreport_grade \ndef report_grade(self, name, subject, grade):\nby_subject = self._grades[name]\ngrade_list = by_subject[subject]\ndef average_grade(self, name):\nby_subject = self._grades[name]\nItem 37: Compose Classes Instead of Nesting Built-in Types \nfor grades in by_subject.values():\nprint(book.average_grade('Albert Einstein'))\ntrack the weight of each score toward the overall grade in the class \ndictionary; instead of mapping subjects (its keys) to a list of grades \n(its values), I can use the tuple of (score, weight) in the values list:\nself._grades = {}\nself._grades[name] = defaultdict(list)\ndef report_grade(self, name, subject, score, weight):\nby_subject = self._grades[name]\ngrade_list = by_subject[subject]\ngrade_list.append((score, weight))\nAlthough the changes to report_grade seem simple—just make the \ndef average_grade(self, name):\nby_subject = self._grades[name]\nprint(book.average_grade('Albert Einstein'))\nbuilt-in types like dictionaries, tuples, sets, and lists to a hierarchy of \nclasses.\nweighted grades, so the complexity of creating classes seemed unwar-\nHere, I use the tuple of (score, weight) to track grades in \ngrades = []\ntotal = sum(score * weight for score, weight in grades)\naverage_grade = total / total_weight\ngrades = []\ntotal = sum(score * weight for score, weight, _ in grades)\naverage_grade = total / total_weight\nclasses:\nGrade = namedtuple('Grade', ('score', 'weight'))\nItem 37: Compose Classes Instead of Nesting Built-in Types \nclasses.\nNext, I can write a class to represent a single subject that contains a \nset of grades:\nclass Subject:\nself._grades = []\ndef report_grade(self, score, weight):\nself._grades.append(Grade(score, weight))\ndef average_grade(self):\nfor grade in self._grades:\ntotal += grade.score * grade.weight\ntotal_weight += grade.weight\nThen, I write a class to represent a set of subjects that are being stud-\nclass Student:\nItem 37: Compose Classes Instead of Nesting Built-in Types \ndef average_grade(self):\ntotal += subject.average_grade()\nItem 38:  Accept Functions Instead of Classes for \nthan classes.\nFunctions work as hooks because Python has first-class ",
    "keywords": [
      "Albert Einstein",
      "grade",
      "grades",
      "Classes",
      "Albert Einstein book.report",
      "Albert",
      "Einstein",
      "weight",
      "Albert Einstein math",
      "total",
      "subject",
      "self.",
      "Isaac Newton",
      "score",
      "Built-in Types"
    ],
    "concepts": [
      "classes",
      "grades",
      "dictionaries",
      "item",
      "items",
      "functions",
      "functionality",
      "function",
      "book",
      "subject"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 175-187)",
    "start_page": 175,
    "end_page": 187,
    "summary": "Item 38: Accept Functions Instead of Classes for Simple Interfaces \nAfter:  {'green': 12, 'blue': 20, 'red': 5, 'orange': 9}\nChapter 5 Classes and Interfaces\nis to define a small class that encapsulates the state you want to \nclass CountMissing:\nBut in Python, thanks to first-class functions, you can reference \nUsing a helper class like this to provide the behavior of a stateful \ndefaultdict, the class is a mystery.\nclass BetterCountMissing:\n__call__ method indicates that a class’s instances will be used some-\nfor the class’s primary behavior.\nof the class is to act as a stateful closure.\n✦ References to functions and methods in Python are first class, \n✦ The __call__ special method enables instances of a class to be \nclass that provides the __call__ method instead of defining a state-\nIn Python, not only do objects support polymorphism, but classes do \nThis means that many classes \nChapter 5 Classes and Interfaces\nI want a common class to represent the input data.\nsuch a class with a read method that must be defined by subclasses:\nclass InputData:\nclass PathInputData(InputData):\nclass Worker:\ndef __init__(self, input_data):\nclass LineCountWorker(Worker):\nChapter 5 Classes and Interfaces\nThe best way to solve this problem is with class method polymor-\nfor InputData.read, except that it’s for whole classes instead of their \nclass GenericInputData:\nclass PathInputData(GenericInputData):\nGenericWorker class.\nHere, I use the input_class parameter, which \nclass GenericWorker:\ndef __init__(self, input_data):\ndef create_workers(cls, input_class, config):\nfor input_data in input_class.generate_inputs(config):\nChapter 5 Classes and Interfaces\nNote that the call to input_class.generate_inputs above is the \nclass LineCountWorker(GenericWorker):\ndef mapreduce(worker_class, input_class, config):\nworkers = worker_class.create_workers(input_class, config)\n✦ Python only supports a single constructor per class: the __init__ \n✦ Use @classmethod to define alternative constructors for your classes.\n✦ Use class method polymorphism to provide generic ways to build \nItem 40: Initialize Parent Classes with super\nis to directly call the parent class’s __init__ method with the child \nclass MyBaseClass:\ndef __init__(self, value):\nItem 40: Initialize Parent Classes with super \nclass MyChildClass(MyBaseClass):\nFor example, here I define two parent classes that operate \nclass TimesTwo:\nclass PlusFive:\nThis class defines its parent classes in one ordering:\nclass OneWay(MyBaseClass, TimesTwo, PlusFive):\ndef __init__(self, value):\nMyBaseClass.__init__(self, value)\nAnd constructing it produces a result that matches the parent class \nclass AnotherWay(MyBaseClass, PlusFive, TimesTwo):\ndef __init__(self, value):\nMyBaseClass.__init__(self, value)\nChapter 5 Classes and Interfaces\nwhich means this class’s behavior doesn’t match the order of the par-\nbase classes and the __init__ calls is hard to spot, which makes this \nI define two child classes that inherit from MyBaseClass:\nclass TimesSeven(MyBaseClass):\ndef __init__(self, value):\nMyBaseClass.__init__(self, value)\nclass PlusNine(MyBaseClass):\ndef __init__(self, value):\nMyBaseClass.__init__(self, value)\nThen, I define a child class that inherits from both of these classes, \ndef __init__(self, value):\nThe call to the second parent class’s constructor, PlusNine.__init__, \nItem 40: Initialize Parent Classes with super \nI use super to initialize the parent class:\nclass TimesSevenCorrect(MyBaseClass):\ndef __init__(self, value):\nclass PlusNineCorrect(MyBaseClass):\ndef __init__(self, value):\nThe other parent classes are run in the order specified in \nthe class statement:\ndef __init__(self, value):\nMRO defines for this class.\nThe MRO ordering is available on a class \nChapter 5 Classes and Interfaces\n<class '__main__.MyBaseClass'>\n<class 'object'>\nclass ExplicitTrisect(MyBaseClass):\ndef __init__(self, value):\nsuper(ExplicitTrisect, self).__init__(value)\nparameters (__class__ and self) for you when super is called with \nclass AutomaticTrisect(MyBaseClass):\ndef __init__(self, value):\nsuper(__class__, self).__init__(value)\nclass ImplicitTrisect(MyBaseClass):\ndef __init__(self, value):\nItem 41: Consider Composing Functionality with Mix-in Classes \nparent classes.\nMix-in Classes\nClasses with super”).\nA mix-in is a class that \ndefines only a small set of additional methods for its child classes to \nMix-in classes don’t define their own instance attributes nor \nit can then be applied to many other classes.\nall my classes?\npublic method that’s added to any class that inherits from it:\nclass ToDictMixin:",
    "keywords": [
      "init",
      "Classes",
      "Parent Classes",
      "Initialize Parent Classes",
      "Item",
      "data",
      "Python",
      "method",
      "workers",
      "function",
      "parent class",
      "input",
      "parent",
      "result",
      "super"
    ],
    "concepts": [
      "classes",
      "functions",
      "function",
      "functionality",
      "method",
      "methods",
      "value",
      "worker",
      "workers",
      "python"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 188-197)",
    "start_page": 188,
    "end_page": 197,
    "summary": "output[key] = self._traverse(key, value)\ndef _traverse(self, key, value):\nreturn value.to_dict()\nreturn self._traverse_dict(value)\nreturn [self._traverse(key, i) for i in value]\nreturn self._traverse_dict(value.__dict__)\nreturn value\ndef __init__(self, value, left=None, right=None):\nself.value = value\n{'value': 10,\ndef __init__(self, value, left=None,\ndef _traverse(self, key, value):\n{'value': 10,\nclass that has an attribute of type BinaryTreeWithParent to automati-\ndef __init__(self, name, tree_with_parent):\nself.name = name\nmix-in that provides generic JSON serialization for any class.\nthis by assuming that a class provides a to_dict method (which may \nclass JsonMixin:\nreturn json.dumps(self.to_dict())\nSerializing these classes to and from JSON is simple.\n✦ Mix-ins can include instance methods or class methods, depending \nIn Python, there are only two types of visibility for a class’s attributes: \nself.public_field = 5\nself.__private_field = 10\ndef get_private_field(self):\nreturn self.__private_field\nPrivate fields are specified by prefixing an attribute’s name with a \ncontaining class:\nHowever, directly accessing private fields from outside the class raises \nfoo.__private_field\nClass methods also have access to private attributes because they are \nclass MyOtherObject:\nself.__private_field = 71\nreturn instance.__private_field\nAs you’d expect with private fields, a subclass can’t access its parent \nclass’s private fields:\nclass MyParentObject:\nself.__private_field = 71\ndef get_private_field(self):\nreturn self.__private_field\nattribute access to use the name _MyChildObject__private_field \nIn the example above, __private_field is only defined in \nMyParentObject.__init__, which means the private attribute’s real \nformed attribute name doesn’t exist (_MyChildObject__private_field \nIf you look in the object’s attribute dictionary, you can see that private \nvalue of Python trying to prevent private attribute access otherwise?\nclass MyStringClass:\ndef __init__(self, value):\nself.__value = value\ndef get_value(self):\nreturn str(self.__value)\nyou—will want to subclass your class to add new behavior or to \ndef get_value(self):\nreturn int(self._MyStringClass__value)\ndef __init__(self, value):\nself.__value = value\ndef get_value(self):\nreturn self.__value\ndef get_value(self):\ndef get_value(self):\nreturn int(self._MyStringClass__value)  # Not updated\nThe __value attribute is now assigned in the MyBaseClass parent class, \nence self._MyStringClass__value to break in MyIntegerSubclass:\nclass MyStringClass:\ndef __init__(self, value):\nself._value = value\noccurs when a child class unwittingly defines an attribute that was \nalready defined by its parent class:\nself._value = 5\nreturn self._value\nself._value = 'hello'  # Conflicts\nissue occurring, you can use a private attribute in the parent class \nclasses:\nself.__value = 5       # Double underscore\nreturn self.__value    # Double underscore\nself._value = 'hello'  # OK!\n✦ Private attributes aren’t rigorously enforced by the Python compiler.\nMuch of programming in Python is defining classes that contain data \nclass is a container of some kind, encapsulating attributes and func-",
    "keywords": [
      "Private",
      "init",
      "private attributes",
      "field",
      "attribute",
      "self.",
      "Classes",
      "attributes",
      "parent",
      "Python",
      "dict",
      "Private fields",
      "left",
      "Item",
      "Prefer Public Attributes"
    ],
    "concepts": [
      "classes",
      "attributes",
      "value",
      "values",
      "item",
      "python",
      "returns",
      "private",
      "uses",
      "subclass"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 198-208)",
    "start_page": 198,
    "end_page": 208,
    "summary": "tree class:\ndef __init__(self, value, left=None, right=None):\nHow do you make this class act like a sequence type?\nTo make the BinaryNode class act like a sequence, you can provide \nif self.left is not None:\nif self.right is not None:\ndef __getitem__(self, index):\nverse the tree with the left and right attributes:\nprint('LRR is', tree.left.right.right.value)\nclass SequenceNode(IndexableNode):\ndef __len__(self):\ncollections.abc module defines a set of abstract base classes that \nsubclass from these abstract base classes and forget to implement \nclass BadType(Sequence):\nclass BetterNode(SequenceNode, Sequence):\ndefined in collections.abc to ensure that your classes match \nmetaclasses let you intercept Python’s class statement and provide \nspecial behavior each time a class is defined.\nItem 44:  Use Plain Attributes Instead of Setter and \ntry to implement explicit getter and setter methods in their classes:\ndef __init__(self, ohms):\nself._ohms = ohms\ndef get_ohms(self):\nreturn self._ohms\ndef set_ohms(self, ohms):\nself._ohms = ohms\nprint('Before:', r0.get_ohms())\nprint('After: ', r0.get_ohms())\nclass Resistor:\ndef __init__(self, ohms):\nself.ohms = ohms\nself.voltage = 0\nand the getter methods must match the intended property name:\ndef __init__(self, ohms):\nItem 44: Use Plain Attributes Instead of Setter and Getter Methods \nself._voltage = 0\ndef voltage(self):\nreturn self._voltage\nself.current = self._voltage / self.ohms\nmethod, which in turn will update the current attribute of the object \nclass that ensures all resistance values are above zero ohms:\nclass BoundedResistance(Resistor):\ndef __init__(self, ohms):\ndef ohms(self):\nreturn self._ohms\ndef ohms(self, ohms):\nself._ohms = ohms\nResistor.__init__, which assigns self.ohms = -5.\ncauses the @ohms.setter method from BoundedResistance to be called, \nI can even use @property to make attributes from parent classes \ndef __init__(self, ohms):\ndef ohms(self):\nreturn self._ohms\ndef ohms(self, ohms):\nif hasattr(self, '_ohms'):\nself._ohms = ohms\nWhen you use @property methods to implement setters and getters, \nple, don’t set other attributes in getter property methods:\ndef ohms(self):\nItem 44: Use Plain Attributes Instead of Setter and Getter Methods \nself.voltage = self._ohms * self.current\nreturn self._ohms\ndef ohms(self, ohms):\nself._ohms = ohms\nSetting other attributes in getter property methods leads to extremely \nUsers of a class will expect its attributes to be like \n✦ Define new class interfaces using simple public attributes and avoid \ndefining setter and getter methods.\n✦ Use @property to define special behavior when attributes are \n@property methods.\n✦ Ensure that @property methods are fast; for slow or complex work—\nHere, the Bucket class represents how much \nclass Bucket:\ndef __init__(self, period):\nself.quota = 0\nreturn f'Bucket(quota={self.quota})'",
    "keywords": [
      "ohms",
      "methods",
      "tree",
      "attributes",
      "Python",
      "property",
      "Getter Methods",
      "self.",
      "item",
      "def ohms",
      "index",
      "property methods",
      "Container Types",
      "sequence",
      "Setter"
    ],
    "concepts": [
      "classes",
      "methods",
      "method",
      "tree",
      "ohms",
      "attributes",
      "attribute",
      "property",
      "uses",
      "object"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 209-217)",
    "start_page": 209,
    "end_page": 217,
    "summary": "To use this class, first I fill the bucket up:\nself.max_quota = 0\nself.quota_consumed = 0\ndef quota(self):\ndef quota(self, amount):\nself.quota_consumed = 0\nself.max_quota = 0\nself.max_quota = amount\nself.quota_consumed += delta\n✦ Use @property to give existing instance attributes new functionality.\nself._grade = 0\ndef grade(self):\nreturn self._grade\ndef grade(self, value):\n'Grade must be between 0 and 100')\nself._grade = value\nclass Exam:\nself._writing_grade = 0\nself._math_grade = 0\n'Grade must be between 0 and 100')\ndef writing_grade(self):\nreturn self._writing_grade\ndef writing_grade(self, value):\nself._check_grade(value)\nself._writing_grade = value\ndef math_grade(self):\nreturn self._math_grade\ndef math_grade(self, value):\nself._check_grade(value)\nself._math_grade = value\nwrite the @property boilerplate and _check_grade method over and \nHere, I define a new class called Exam with class attributes that are \nGrade instances.\nThe Grade class implements the descriptor protocol:\nclass Grade:\ndef __set__(self, instance, value):\nclass Exam:\n# Class attributes\nexam.writing_grade = 40\nExam.__dict__['writing_grade'].__set__(exam, 40)\nexam.writing_grade\nattribute named writing_grade, Python falls back to the Exam class’s \nKnowing this behavior and how I used @property for grade validation \nclass Grade:\ndef __set__(self, instance, value):\n'Grade must be between 0 and 100')\ning multiple attributes on a single Exam instance works as expected:\nclass Exam:\nfirst_exam.writing_grade = 82\nfirst_exam.science_grade = 99\nprint('Writing', first_exam.writing_grade)\nBut accessing these attributes on multiple Exam instances causes \nsecond_exam.writing_grade = 75\nprint(f'Second {second_exam.writing_grade} is right')\nprint(f'First  {first_exam.writing_grade} is wrong; '\nThe problem is that a single Grade instance is shared across all Exam \ninstances for the class attribute writing_grade.\nThe Grade instance for \nExam class is first defined, not each time an Exam instance is created.\nTo solve this, I need the Grade class to keep track of its value for each \nclass Grade:\nreturn self._values.get(instance, 0)\ndef __set__(self, instance, value):\n'Grade must be between 0 and 100')\nclass Grade:\ndef __set__(self, instance, value):\nclass Exam:\nfirst_exam.writing_grade = 82\nsecond_exam.writing_grade = 75\nprint(f'First  {first_exam.writing_grade} is right')\nprint(f'Second {second_exam.writing_grade} is right')\nPlain instance attributes, @property methods, ",
    "keywords": [
      "grade",
      "exam",
      "quota",
      "Bucket",
      "Attributes",
      "instance",
      "property",
      "Exam instance",
      "Item",
      "class Exam",
      "Python",
      "self.",
      "class Grade",
      "writing",
      "consumed"
    ],
    "concepts": [
      "classes",
      "attributes",
      "attribute",
      "property",
      "exam",
      "exams",
      "grade",
      "python",
      "instances",
      "bucket"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 218-231)",
    "start_page": 218,
    "end_page": 231,
    "summary": "class’s implementation of __getattr__ in order to fetch the real \nclass ValidatingRecord:\nclass MissingPropertyRecord:\nclasses that implement __getattribute__ have that method called \nclass SavingRecord:\nclass BrokenDictionaryRecord:\ndef __init__(self, data):\nThe problem is that __getattribute__ accesses self._data, which \ncauses __getattribute__ to run again, which accesses self._data \nclass DictionaryRecord:\ndef __init__(self, data):\nby using methods from super() (i.e., the object class) to access \nclass was defined correctly.\nods, or have strict relationships between class attributes.\nOften a class’s validation code runs in the __init__ method, when an \nobject of the class’s type is constructed at runtime (see Item 44: “Use \nclass Meta(type):\ndef __new__(meta, name, bases, class_dict):\nprint(class_dict)\nreturn type.__new__(meta, name, bases, class_dict)\nclass MyClass(metaclass=Meta):\nclass MySubclass(MyClass):\nThe metaclass has access to the name of the class, the parent classes \nit inherits from (bases), and all the class attributes that were defined \nin the class’s body.\nAll classes inherit from object, so it’s not explicitly \n* Running <class '__main__.Meta'>.__new__ for MyClass\n* Running <class '__main__.Meta'>.__new__ for MySubclass\nBases: (<class '__main__.MyClass'>,)\ndate all of the parameters of an associated class before it’s defined.\nusing it in the base class of my polygon class hierarchy.\nimportant not to apply the same validation to the base class:\nclass ValidatePolygon(type):\ndef __new__(meta, name, bases, class_dict):\n# Only validate subclasses of the Polygon class\nif class_dict['sides'] < 3:\nreturn type.__new__(meta, name, bases, class_dict)\nclass Polygon(metaclass=ValidatePolygon):\nclass Triangle(Polygon):\nclass Rectangle(Polygon):\nclass Nonagon(Polygon):\nrunning when I define such a class (unless it’s defined in a dynam-\nprint('Before class')\nclass Line(Polygon):\nprint('After class')\nBefore class\nclass BetterPolygon:\nhaving to go into the class’s dictionary with class_dict['sides'].\nprint('Before class')\nclass Point(BetterPolygon):\nprint('After class')\nBefore class\nis that you can only specify a single metaclass per class definition.\nclass ValidateFilled(type):\ndef __new__(meta, name, bases, class_dict):\n# Only validate subclasses of the Filled class\nif class_dict['color'] not in ('red', 'green'):\nreturn type.__new__(meta, name, bases, class_dict)\nclass Filled(metaclass=ValidateFilled):\nclass RedPentagon(Filled, Polygon):\nclass ValidatePolygon(type):\ndef __new__(meta, name, bases, class_dict):\n# Only validate non-root classes\nif not class_dict.get('is_root'):\nif class_dict['sides'] < 3:\nreturn type.__new__(meta, name, bases, class_dict)\nclass Polygon(metaclass=ValidatePolygon):\ndef __new__(meta, name, bases, class_dict):\n# Only validate non-root classes\nif not class_dict.get('is_root'):\nif class_dict['color'] not in ('red', 'green'):\nreturn super().__new__(meta, name, bases, class_dict)\nclass FilledPolygon(Polygon, metaclass=ValidateFilledPolygon):\nclass GreenPentagon(FilledPolygon):\nThe __init_subclass__ special class method can also be used to \nIt can be defined by multiple levels of a class \nHere, I define a class to represent \nregion fill color that can be composed with the BetterPolygon class \nclass Filled:\nI can inherit from both classes to define a new class.\nBoth classes call \nclass RedTriangle(Filled, Polygon):\nprint('Before class')\nclass BlueLine(Filled, Polygon):\nprint('After class')\nBefore class\nprint('Before class')\nclass BeigeSquare(Filled, Polygon):\nprint('After class')\nBefore class\nclass Top:\nclass Left(Top):\nclass Right(Top):\nclass Bottom(Left, Right):\nTop for <class '__main__.Left'>\nTop for <class '__main__.Right'>\nTop for <class '__main__.Bottom'>\nRight for <class '__main__.Bottom'>\nLeft for <class '__main__.Bottom'>\n✦ The __new__ method of metaclasses is run after the class state-\n✦ Metaclasses can be used to inspect or modify a class after it’s \n✦ Be sure to call super().__init_subclass__ from within your class’s \nof classes and multiple inheritance.\nItem 49:  Register Class Existence with \nItem 49: Register Class Existence with __init_subclass__ \nclass Serializable:\nThis class makes it easy to serialize simple, immutable data struc-\nclass Point2D(Serializable):\nHere, I define another class that can deserialize \nthe data from its Serializable parent class:",
    "keywords": [
      "foo",
      "called",
      "init",
      "polygon",
      "getattribute",
      "subclass",
      "data",
      "sides",
      "Item",
      "dict",
      "Attributes",
      "getattr",
      "attribute",
      "Metaclasses",
      "super"
    ],
    "concepts": [
      "classes",
      "data",
      "attributes",
      "attribute",
      "sides",
      "valid",
      "validate",
      "validation",
      "validating",
      "access"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 232-240)",
    "start_page": 232,
    "end_page": 240,
    "summary": "To do this, I can include the serialized object’s class name in the \nclass BetterSerializable:\n'class': self.__class__.__name__,\nname = self.__class__.__name__\nclasses passed to register_class:\ndef register_class(target_class):\nname = params['class']\ntarget_class = registry[name]\nreturn target_class(*params['args'])\nregister_class for every class I may want to deserialize in the future:\nclass EvenBetterPoint2D(BetterSerializable):\nregister_class(EvenBetterPoint2D)\nSerialized: {\"class\": \"EvenBetterPoint2D\", \"args\": [5, 3]}\nregister_class:\nclass Point3D(BetterSerializable):\n# Forgot to call register_class!\nalize an instance of a class I forgot to register:\nItem 49: Register Class Existence with __init_subclass__ \norators (see Item 51: “Prefer Class Decorators Over Metaclasses for \nBetterSerializable and ensure that register_class is called in all \nMetaclasses enable this by intercepting the class statement \nclass Meta(type):\ndef __new__(meta, name, bases, class_dict):\nregister_class(cls)\nthat the call to register_class happened and deserialize will always \nSerialized: {\"class\": \"Vector3D\", \"args\": [10, -7, 3]}\nAn even better approach is to use the __init_subclass__ special class \nregister_class(cls)\nSerialized: {\"class\": \"Vector1D\", \"args\": [6]}\nBy using __init_subclass__ (or metaclasses) for class registration, \nbase class is subclassed in a program.\n✦ Using metaclasses for class registration helps you avoid errors by \nItem 49: Register Class Existence with __init_subclass__ \nItem 50: Annotate Class Attributes with __set_name__\ncontaining class.\ndescriptor class to connect attributes to column names:\nclass Field:\nDefining the class representing a row requires supplying the data-\nclass Customer:\n# Class attributes\nUsing the class is simple.\nItem 50: Annotate Class Attributes with __set_name__ \nBut the class definition seems redundant.\nname of the field for the class on the left ('field_name =').\nclass Customer:\ninstance to know upfront which class attribute it will be assigned to.\nclass Meta(type):\ndef __new__(meta, name, bases, class_dict):\nfor key, value in class_dict.items():\nHere, I define a base class that uses the metaclass.\nAll classes repre-\nclass DatabaseRow(metaclass=Meta):\nclass Field:\nBy using the metaclass, the new DatabaseRow base class, and the new \nField descriptor, the class definition for a database row no longer has \nclass BetterCustomer(DatabaseRow):\nThe trouble with this approach is that you can’t use the Field class for \nclass BrokenCustomer:\nevery descriptor instance when its containing class is defined.\nclass Field:\n# Called on class creation for each descriptor\ninherit from a specific parent class or having to use a metaclass:\nclass FixedCustomer:\nItem 50: Annotate Class Attributes with __set_name__ \n✦ Metaclasses enable you to modify a class’s attributes before the \nclass is fully defined.\n✦ Define __set_name__ on your descriptor classes to allow them to \ndescriptors store data they manipulate directly within a class’s \nItem 51:  Prefer Class Decorators Over Metaclasses for \nAlthough metaclasses allow you to customize class creation in multi-\nand Item 49: “Register Class Existence with __init_subclass__”), they ",
    "keywords": [
      "Field",
      "class Field",
      "init",
      "Annotate Class Attributes",
      "instance",
      "Metaclasses",
      "Item",
      "Register Class Existence",
      "Field descriptor",
      "Class Attributes",
      "register",
      "data",
      "Attributes",
      "Serialized",
      "Register Class"
    ],
    "concepts": [
      "classes",
      "field",
      "fields",
      "item",
      "data",
      "prints",
      "descriptors",
      "descriptor",
      "defined",
      "define"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 241-248)",
    "start_page": 241,
    "end_page": 248,
    "summary": "Item 51: Prefer Class Decorators Over Metaclasses \nclass TraceDict(dict):\nan instance of the class:\ntrace_dict = TraceDict([('hi', 1)])\ncally decorate all methods of a class.\ntrace_func decorator:\ntrace_types = (\nclass TraceMeta(type):\ndef __new__(meta, name, bases, class_dict):\nklass = super().__new__(meta, name, bases, class_dict)\nclass TraceDict(dict, metaclass=TraceMeta):\ntrace_dict = TraceDict([('hi', 1)])\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\nclass OtherMeta(type):\nclass SimpleDict(dict, metaclass=OtherMeta):\nclass TraceDict(SimpleDict, metaclass=TraceMeta):\nclass TraceMeta(type):\nclass OtherMeta(TraceMeta):\nclass SimpleDict(dict, metaclass=OtherMeta):\nclass TraceDict(SimpleDict, metaclass=TraceMeta):\ntrace_dict = TraceDict([('hi', 1)])\nItem 51: Prefer Class Decorators Over Metaclasses \n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\nTo solve this problem, Python supports class decorators.\nClass \ndef my_class_decorator(klass):\n@my_class_decorator\nclass MyClass:\nI can implement a class decorator to apply trace_func to all methods \nand functions of a class by moving the core of the TraceMeta.__new__ \nclass TraceDict(dict):\ntrace_dict = TraceDict([('hi', 1)])\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\nClass decorators also work when the class being decorated already \nclass OtherMeta(type):\nclass TraceDict(dict, metaclass=OtherMeta):\ntrace_dict = TraceDict([('hi', 1)])\n__new__((<class '__main__.TraceDict'>, [('hi', 1)]), {}) -> {}\nItem 51: Prefer Class Decorators Over Metaclasses \nto Use heapq for Priority Queues” for a useful class decorator called \n✦ A class decorator is a simple function that receives a class instance \nas a parameter and returns either a new class or a modified version \nof the original class.\n✦ Class decorators are useful when you want to modify every method \n✦ Metaclasses can’t be composed together easily, while many class \ndecorators can be used to extend the same class without conflicts.\nWithin a single program, concurrency is a tool that makes it easier \nconcurrent programs may run thousands of separate paths of execu-\nPython makes it easy to write concurrent programs in a variety of \nPython can also be used to do parallel work through system calls, \nconcurrent Python code truly run in parallel.\nItem 52: Use subprocess to Manage Child Processes\nPython has battle-hardened libraries for running and managing child \nChild processes started by Python are able to run in parallel, enabling \nRunning a child process with subprocess \nIf I create a subprocess using the Popen class ",
    "keywords": [
      "trace",
      "dict",
      "Class Decorators",
      "Prefer Class Decorators",
      "class TraceDict",
      "TraceDict",
      "trace class TraceDict",
      "pass class TraceDict",
      "Python",
      "exist",
      "metaclass",
      "args",
      "kwargs",
      "pass class",
      "getitem"
    ],
    "concepts": [
      "classes",
      "types",
      "type",
      "concurrency",
      "concurrent",
      "python",
      "running",
      "runs",
      "run",
      "program"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 249-264)",
    "start_page": 249,
    "end_page": 264,
    "summary": "Item 52: Use subprocess to Manage Child Processes \nYou can also pipe data from a Python program into a subprocess and \ndef run_encrypt(data):\nproc = run_encrypt(data)\nThe child processes run in parallel and consume their input.\nencrypt_proc = run_encrypt(data)\nItem 52: Use subprocess to Manage Child Processes \n✦ Use the subprocess module to run child processes and manage their \n✦ Child processes run in parallel with the Python interpreter, enabling \nItem 53:  Use Threads for Blocking I/O, Avoid for \nruns a Python program in two steps.\none thread takes control of a program by interrupting another thread.\nin languages like C++ or Java, having multiple threads of execution \nItem 53: Use Threads for Blocking I/O, Avoid for Parallelism \nAlthough Python supports multiple threads of execution, the GIL \nmeans that when you reach for threads to do parallel computation \nUsing multiple threads to do this computation would make sense in \nthread for doing the same computation as before:\nfrom threading import Thread\nclass FactorizeThread(Thread):\ndef run(self):\nThen, I start a thread for each number to factorize in parallel:\nthreads = []\nthread = FactorizeThread(number)\nthread.start()\nthreads.append(thread)\nFinally, I wait for all of the threads to finish:\nfor thread in threads:\nthread.join()\nWith one thread per number, you might expect less than a 4x speedup \nperformance of these threads to be worse when there are multiple \ndon’t work with the standard Thread class (see Item 64: “Consider \nthreads at all?\nFirst, multiple threads make it easy for a program to seem like it’s \nWith threads, you can leave it to Python to run your func-\nfairness between Python threads of execution, even though only one \nThe second reason Python supports threads is to deal with blocking \nItem 53: Use Threads for Blocking I/O, Avoid for Parallelism \nA Python program uses system calls to ask the computer’s  operating \nThreads help handle blocking I/O by insulating a program from the \nMy program’s main thread of \nit’s time to consider moving your system calls to threads.\nseparate threads.\nthread to do whatever computation is required:\nthreads = []\nthread.start()\nthreads.append(thread)\nWith the threads started, here I do some work to calculate the next \nhelicopter move before waiting for the system call threads to finish:\nfor thread in threads:\nthread.join()\nall the system calls will run in parallel from multiple Python threads \nThis works because Python threads release the GIL just before \nUsing threads is the simplest way to do blocking I/O in parallel with \n✦ Python threads can’t run in parallel on multiple CPU cores because \nItem 54: Use Lock to Prevent Data Races in Threads \n✦ Python threads are still useful despite the GIL because they provide \n✦ Use Python threads to make multiple system calls in parallel.\nItem 54: Use Lock to Prevent Data Races in Threads\n“Use Threads for Blocking I/O, Avoid for Parallelism”), many new \nalready  preventing Python threads from running on multiple CPU \nAlthough only one Python thread runs at a time, a thread’s opera-\naccess the same objects from multiple threads simultaneously.\nImagine that each sensor has its own worker thread because reading \nment, the worker thread increments the counter up to a maximum \nHere, I run one worker thread for each sensor in parallel and wait for \nfrom threading import Thread\nthreads = []\nthread = Thread(target=worker,\nthreads.append(thread)\nthread.start()\nfor thread in threads:\nthread.join()\ninterpreter thread can run at a time?\nThe Python interpreter enforces fairness between all of the threads \nTo do this, Python suspends a thread as it’s running and resumes \nanother thread in turn.\nwhen Python will suspend your threads.\nA thread can even be paused \nequivalent to this statement from the perspective of the worker thread:\nItem 54: Use Lock to Prevent Data Races in Threads \nPython threads incrementing the counter can be suspended between \nHere’s an example of bad interaction between two threads, \n# Running in Thread A\n# Context switch to Thread B\nThread B interrupted thread A before it had completely finished.\nThread B ran and finished, but then thread A resumed mid-execution, \noverwriting all of thread B’s progress in incrementing the counter.\ncorruption, Python includes a robust set of tools in the threading \nvalue against simultaneous accesses from multiple threads.\nthread will be able to acquire the lock at a time.\nfrom threading import Lock\nNow, I run the worker threads as before but use a LockingCounter \nthread = Thread(target=worker,\nthreads.append(thread)\nthread.start()\nfor thread in threads:\nthread.join()\nresponsible for protecting against data races between the threads in \n✦ Use the Lock class from the threading built-in module to enforce \nyour program’s invariants between multiple threads.\nItem 55:  Use Queue to Coordinate Work Between \nThreads\nItem 55: Use Queue to Coordinate Work Between Threads \n(see Item 53: “Use Threads for Blocking I/O, Avoid for Parallelism”).\nqueue (see Item 54: “Use Lock to Prevent Data Races in Threads” to \nunderstand the importance of thread safety in Python; see Item 71: \nfrom threading import Lock\nHere, I represent each phase of the pipeline as a Python thread that \ntakes work from one queue like this, runs a function on it, and puts \nfrom threading import Thread\nclass Worker(Thread):\ndef run(self):\nitem = self.in_queue.get()\nself.out_queue.put(result)\nItem 55: Use Queue to Coordinate Work Between Threads \nthreads = [\nI can start the threads and then inject a bunch of work into the first \nfor thread in threads:\nthread.start()\nthe threads polling their input queues for new work.\nprocessed = len(done_queue.items)\nProcessed 1000 items after polling 3035 times\ninput queues for new work in a tight loop.\nthreads waste CPU time doing nothing useful; they’re constantly rais-\nworker thread that it’s time to exit.\nthread that waits for some input data on a queue:\nmy_queue.get()              # Runs after put() below\nthread = Thread(target=consumer)\nthread.start()\nEven though the thread is running first, it won’t finish until an item \nmy_queue.put(object())          # Runs before get() above\nthread.join()",
    "keywords": [
      "Threads",
      "Python threads",
      "thread",
      "Python",
      "Queue",
      "Python thread runs",
      "Item",
      "multiple Python threads",
      "worker thread",
      "multiple threads",
      "lock",
      "Python interpreter thread",
      "Child Processes",
      "Python program",
      "GIL"
    ],
    "concepts": [
      "threads",
      "thread",
      "threading",
      "item",
      "items",
      "queue",
      "queues",
      "times",
      "processes",
      "process"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 265-274)",
    "start_page": 265,
    "end_page": 274,
    "summary": "Item 55: Use Queue to Coordinate Work Between Threads \nFor example, here I define a thread that waits for a while before \nconsuming a queue:\nmy_queue.get()              # Runs second\nThe wait should allow the producer thread to put both objects on the \nqueue before the consumer thread ever calls get.\nBut the Queue size \nThis means the producer adding items to the queue will have \nto wait for the consumer thread to call get at least once before the \nqueue:\nmy_queue.put(object())          # Runs first\nmy_queue.put(object())          # Runs third\nThe Queue class can also track the progress of work using the \nThis lets you wait for a phase’s input queue to \nwith the done_queue above).\nthread that calls task_done when it finishes working on an item:\nwork = in_queue.get()       # Runs second\nin_queue.task_done()        # Runs third\nNow, the producer code doesn’t have to join the consumer thread or \nThe producer can just wait for the in_queue to finish by calling \njoin on the Queue instance.\nin_queue.put(object())         # Runs first\na close method that adds a special sentinel item to the queue that \nclass ClosableQueue(Queue):\nItem 55: Use Queue to Coordinate Work Between Threads \nwork on the queue (see Item 31: “Be Defensive When Iterating Over \ndef __init__(self, func, in_queue, out_queue):\nself.in_queue = in_queue\nself.out_queue = out_queue\nfor item in self.in_queue:\nself.out_queue.put(result)\nresize_queue = ClosableQueue()\nupload_queue = ClosableQueue()\ndone_queue = ClosableQueue()\nthreads = [\nqueue of the first phase:\ndownload_queue.put(object())\ndownload_queue.close()\nFinally, I wait for the work to finish by joining the queues that con-\nto stop by closing its input queue.\ndownload_queue.join()\nresize_queue.close()\nresize_queue.join()\nupload_queue.close()\nupload_queue.join()\nprint(done_queue.qsize(), 'items finished')\nis by calling close on each input queue once per consuming thread, \nreturn threads\ndef stop_threads(closable_queue, threads):\nfor _ in threads:\nItem 55: Use Queue to Coordinate Work Between Threads \ncess into the top of the pipeline, joining queues and threads along the \nresize_queue = ClosableQueue()\nupload_queue = ClosableQueue()\ndone_queue = ClosableQueue()\ndownload_queue.put(object())\nstop_threads(download_queue, download_threads)\nstop_threads(resize_queue, resize_threads)\nstop_threads(upload_queue, upload_threads)\nprint(done_queue.qsize(), 'items finished')\nthreads.\n✦ The Queue class has all the facilities you need to build robust \nexample of a 5 × 5 Game of Life grid after four generations with time \ndef get(self, y, x):\nreturn self.rows[y % self.height][x % self.width]\ndef set(self, y, x, state):\nself.rows[y % self.height][x % self.width] = state\ndef count_neighbors(y, x, get):\ndef game_logic(state, neighbors):\ndef step_cell(y, x, get, set):\nFinally, I can define a function that progresses the whole grid of cells \nstep_cell(y, x, grid.get, next_grid.set)\ndef game_logic(state, neighbors):\nInternet), and there are 45 cells in the grid, then each generation will \nGame of Life example program (Item 57: “Avoid Creating New Thread \nQueue for Concurrency Requires Refactoring,” Item 59: “Consider \nItem 57:  Avoid Creating New Thread Instances for \nI/O in Python (see Item 53: “Use Threads for Blocking I/O, Avoid for ",
    "keywords": [
      "Queue",
      "Threads",
      "grid",
      "Item",
      "thread",
      "consumer",
      "consumer thread",
      "producer",
      "Concurrency",
      "Game",
      "state",
      "alive",
      "Grid instance",
      "Runs",
      "Work"
    ],
    "concepts": [
      "item",
      "items",
      "grid",
      "queue",
      "queues",
      "threads",
      "thread",
      "state",
      "functions",
      "function"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 275-289)",
    "start_page": 275,
    "end_page": 289,
    "summary": "Item 57: Avoid Creating New Thread Instances for On-demand Fan-out \ncaused by doing I/O in the game_logic function.\nTo begin, threads \nfrom threading import Lock\nthread for each call to step_cell.\nThe threads will run in parallel and \ndef simulate_threaded(grid):\nthreads = []\nthread.start()  # Fan out\nfor thread in threads:\nthread.join()       # Fan in\nthe LockingGrid and simulate_threaded implementations:\ngrid = simulate_threaded(grid)  # Changed\nthreads.\n■The Thread instances require special tools to coordinate with \nThis makes the code that uses threads harder to rea-\nthread.\nRunning a thread per concurrent activity just won’t work.\nI can test what this would do by running a Thread instance pointed at \nItem 57: Avoid Creating New Thread Instances for On-demand Fan-out \nthread = Thread(target=game_logic, args=(ALIVE, 3))\nthread.start()\nthread.join()\nException in thread Thread-226:\nItem 59: “Consider ThreadPoolExecutor When Threads Are Necessary \nfor Concurrency”, and Item 60: “Achieve Highly Concurrent I/O with \n✦ Threads do not provide a built-in way to raise exceptions back in \nthe code that started a thread or that is waiting for one to finish, \nItem 58: Using Queue for Concurrency Requires Refactoring \nItem 58:  Understand How Using Queue for Concurrency \nIn the previous item (see Item 57: “Avoid Creating New Thread \nThread to solve the parallel I/O problem in the Game of Life example \nThe next approach to try is to implement a threaded pipeline using \nworker threads upfront and have them do parallelized I/O as needed.\ncating to and from the worker threads that execute the game_logic \nI can start multiple threads that will consume items from the \nThese threads will run concurrently, allowing for parallel \nclass StoppableWorker(Thread):\ndef game_logic_thread(item):\ny, x, state, neighbors = item\nthreads = []\nthread = StoppableWorker(\ngame_logic_thread, in_queue, out_queue)\nthread.start()\ning items from out_queue until it’s empty causes fan-in:\ndef simulate_pipeline(grid, in_queue, out_queue):\nin_queue.put((y, x, state, neighbors))  # Fan out\nfor item in out_queue:                          # Fan in\ny, x, next_state = item\nsimulate_pipeline function, which means I can use the  single-threaded \nThis code is also easier to debug than the Thread approach used \ngame_logic function, it will be caught, propagated to the out_queue, \nItem 58: Using Queue for Concurrency Requires Refactoring \nfor thread in threads:\nfor thread in threads:\nthread.join()\nsimulate_threaded approach from the previous item.\nber of threads running game_logic_thread—upfront based on my \nin worker threads, propagate them on a Queue, and then re-raise \nthem in the main thread.\npipeline that runs count_neighbors in a thread.\nthat exceptions propagate correctly between the worker threads and \nthe main thread.\nto ensure safe synchronization between the worker threads (see Item \nItem 57: “Avoid Creating New Thread Instances for On-demand Fan-\ndef count_neighbors_thread(item):\ny, x, state, get = item\ndef game_logic_thread(item):\ny, x, state, neighbors = item\nthreads = []\nItem 58: Using Queue for Concurrency Requires Refactoring \nthread = StoppableWorker(\ncount_neighbors_thread, in_queue, logic_queue)\nthread.start()\nthread = StoppableWorker(\ngame_logic_thread, logic_queue, out_queue)\nthread.start()\nitem = (y, x, state, grid.get)\nin_queue.put(item)          # Fan out\nfor item in out_queue:              # Fan in\ny, x, next_state = item\nfor thread in threads:\nfor thread in threads:\nfor thread in threads:\nthread.join()\nusing Queue is a better approach than using Thread instances on their \nby Python (see Item 59: “Consider ThreadPoolExecutor When Threads \n✦ Using Queue instances with a fixed number of worker threads \nItem 58: Using Queue for Concurrency Requires Refactoring \nItem 59:  Consider ThreadPoolExecutor When Threads \n(see Item 57: “Avoid Creating New Thread Instances for On-demand \nallel I/O problem from the Game of Life example (see Item 56: “Know \nInstead of starting a new Thread instance for each Grid square, I can \nseparate thread.\nItem 59: Consider ThreadPoolExecutor When Threads Are Necessary \nrefactoring, easily avoiding the cost of thread startup each time fan-\nblow-up issues of using threads directly, it also limits I/O parallel-\nItem 60:  Achieve Highly Concurrent I/O with \nItem 60: Achieve Highly Concurrent I/O with Coroutines \n“Avoid Creating New Thread Instances for On-demand Fan-out,” Item \ning,” and Item 59: “Consider ThreadPoolExecutor When Threads Are \nPython addresses the need for highly concurrent I/O with coroutines.\nthreads, coroutines are independent functions that can consume \nthreads.\nnization code that’s required for threads.\nallow for I/O to occur within the game_logic function while overcom-\ning the problems from the Thread and Queue approaches in the previ-",
    "keywords": [
      "Thread",
      "threads",
      "Concurrency Requires Refactoring",
      "Item",
      "Queue",
      "Thread Instances",
      "Grid",
      "ALIVE",
      "Concurrency Requires",
      "game",
      "Concurrency",
      "logic",
      "state",
      "neighbors",
      "Requires Refactoring"
    ],
    "concepts": [
      "thread",
      "threading",
      "threaded",
      "item",
      "items",
      "queue",
      "queues",
      "state",
      "classes",
      "function"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 290-298)",
    "start_page": 290,
    "end_page": 298,
    "summary": "async def step_cell(y, x, get, set):\nit returns a coroutine instance that can be used with an await \nItem 60: Achieve Highly Concurrent I/O with Coroutines \nrun the step_cell coroutines concurrently and resume execution \nsimulate coroutine in an event loop and carry out its dependent I/O:\ngrid = asyncio.run(simulate(grid))   # Run the event loop\nasync def step_cell(y, x, get, set):\nItem 61: Know How to Port Threaded I/O to asyncio \nItem 61: Know How to Port Threaded I/O to asyncio\nthe server returns guesses for integer values in that range as they are \ndef __init__(self, connection):\nself.connection = connection\ndef send(self, command):\nself.connection.send(data)\ndef receive(self):\nThe server is implemented as a class that handles one connection at a \ndef __init__(self, *args):\nself._clear_state(None, None)\ndef _clear_state(self, lower, upper):\nself.secret = None\nself.guesses = []\ndef loop(self):\nwhile command := self.receive():\nself.set_params(parts)\nself.send_number()\nself.receive_report(parts)\nthat the server is trying to guess:\ndef set_params(self, parts):\nself._clear_state(lower, upper)\nensures that the server will never try to guess the same number more \ndef next_guess(self):\nif self.secret is not None:\nreturn self.secret\nguess = random.randint(self.lower, self.upper)\nif guess not in self.guesses:\ndef send_number(self):\nguess = self.next_guess()\nself.guesses.append(guess)\nself.send(format(guess))\nThe third command receives the decision from the client of whether \ndef receive_report(self, parts):\nlast = self.guesses[-1]\nItem 61: Know How to Port Threaded I/O to asyncio \nself.secret = last\ndef __init__(self, *args):\nself._clear_state()\ndef _clear_state(self):\nself.secret = None\nself.last_distance = None\ndef session(self, lower, upper, secret):\nself.send(f'PARAMS {lower} {upper}')\nself._clear_state()\nself.send('PARAMS 0 -1')\ndef request_numbers(self, count):\nself.send('NUMBER')\ndata = self.receive()\nif self.last_distance == 0:\nWhether each guess from the server was warmer or colder than the \ndef report_outcome(self, number):\nnew_distance = math.fabs(number - self.secret)\nself.send(f'REPORT {decision}')\nI can run the server by having one thread listen on a socket and \ndef run_server(address):\nItem 61: Know How to Port Threaded I/O to asyncio \nThe client runs in the main thread and returns the results of the \ndef run_client(address):\ntarget=run_server, args=(address,), daemon=True)\nserver_thread.start()",
    "keywords": [
      "server",
      "ALIVE",
      "Item",
      "Grid",
      "state",
      "number",
      "Port Threaded",
      "client",
      "Coroutines",
      "Concurrency and Parallelism",
      "neighbors",
      "guess",
      "connection",
      "grid.set",
      "thread"
    ],
    "concepts": [
      "thread",
      "threads",
      "threaded",
      "threading",
      "grid",
      "connection",
      "connections",
      "guessing",
      "guesses",
      "guess"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 299-314)",
    "start_page": 299,
    "end_page": 314,
    "summary": "def __init__(self, reader, writer):             # Changed\nasync def send(self, command):\nself.writer.write(data)                     # Changed\nasync def receive(self):\nline = await self.reader.readline()         # Changed\nasync def loop(self):                           # Changed\nItem 61: Know How to Port Threaded I/O to asyncio \nwhile command := await self.receive():      # Changed\nawait self.send_number()            # Changed\nasync def send_number(self):                    # Changed\nThe first command method for the client requires a few async and await \nasync def session(self, lower, upper, secret):  # Changed\nawait self.send('PARAMS 0 -1')          # Changed\nasync def request_numbers(self, count):         # Changed\nawait self.send('NUMBER')               # Changed\ndata = await self.receive()             # Changed\nasync def report_outcome(self, number):         # Changed\nThe code that runs the server needs to be completely reimplemented \nto use the asyncio built-in module and its start_server function:\nasync def run_async_server(address):\nThe run_client function that initiates the game requires changes on \nItem 61: Know How to Port Threaded I/O to asyncio \nlines in the function that require interaction with coroutines need to \nasync def run_async_client(address):\nI use the asyncio.create_task function to \nenqueue the server for execution on the event loop so that it runs in \nserver = run_async_server(address)\nresults = await run_async_client(address)\nasyncio.run(main_async())\nItem 61: Know How to Port Threaded I/O to asyncio \nasyncio Event Loop to Maximize Responsiveness”).\ning code that uses threads and blocking I/O over to coroutines and \nItem 62:  Mix Threads and Coroutines to Ease the \nover to use asyncio with coroutines.\nneed threads to be able to run coroutines, and you need coroutines to \nItem 62: Mix Threads and Coroutines to Ease the Transition to asyncio \nto write it to the output log (see Item 38: “Accept Functions Instead of \nWhen the input file handle is closed, the worker thread exits:\ndef tail_file(handle, interval, write_func):\ndef run_threads(handles, interval, output_path):\nGiven a set of input paths and an output path, I can call run_threads \nrun_threads(handles, 0.1, output_path)\nI incrementally convert this code to use asyncio and coroutines \n1. Change a top function to use async def instead of def.\nloop—to use asyncio.run_in_executor instead.\nasyncio.run_coroutine_threadsafe function).\n4. Try to eliminate get_event_loop and run_in_executor calls by \nHere, I apply steps 1–3 to the run_threads function:\nasync def run_tasks_mixed(handles, interval, output_path):\nloop = asyncio.get_event_loop()\nasync def write_async(data):\nfuture = asyncio.run_coroutine_threadsafe(\ntask = loop.run_in_executor(\nThe run_in_executor method instructs the event loop to run a given \nout corresponding await expressions, the run_tasks_mixed coroutine \nThen, the asyncio.gather function along with an await expression \nfans in the tail_file threads until they all complete (see Item 56: \nItem 62: Mix Threads and Coroutines to Ease the Transition to asyncio \nby using asyncio.run_coroutine_threadsafe.\nthreads together and ensures that all writes to the output file are only \ndone by the event loop in the main thread.\nawaitable is resolved, I can assume that all writes to the output file \nI use the asyncio.run \nfunction to start the coroutine and run the main event loop:\nasyncio.run(run_tasks_mixed(handles, 0.1, output_path))\nNow, I can apply step 4 to the run_tasks_mixed function by moving \nasync def tail_async(handle, interval, write_func):\nloop = asyncio.get_event_loop()\nline = await loop.run_in_executor(\nget_event_loop and run_in_executor down the stack and out of the \nasync def run_tasks(handles, interval, output_path):\nasync def write_async(data):\ncoro = tail_async(handle, interval, write_async)\nasyncio.run(run_tasks(handles, 0.1, output_path))\ncoroutine versions and run the event loop instead of implement-\nI can rewrite tail_file so it merely wraps the tail_async coroutine \nTo run that coroutine until it finishes, I need to \nItem 62: Mix Threads and Coroutines to Ease the Transition to asyncio \ncreate an event loop for each tail_file worker thread and then call \nthread and drive the event loop until the tail_async coroutine exits, \ndef tail_file(handle, interval, write_func):\nloop = asyncio.new_event_loop()\nasync def write_async(data):\ncoro = tail_async(handle, interval, write_async)\nI can verify that everything works as expected by calling run_threads \nrun_threads(handles, 0.1, output_path)\nthe run_threads function to a coroutine.\n(see Item 63: “Avoid Blocking the asyncio Event Loop to Maximize \n✦ The awaitable run_in_executor method of the asyncio event \nloop enables coroutines to run synchronous functions in \n✦ The run_until_complete method of the asyncio event loop enables \nsynchronous code to run a coroutine until it finishes.\nasyncio.run_coroutine_threadsafe function provides the same \nItem 63: Avoid Blocking the asyncio Event Loop \nItem 63:  Avoid Blocking the asyncio Event Loop to \n(see Item 62: “Mix Threads and Coroutines to Ease the Transition to \nasync def run_tasks(handles, interval, output_path):\nasync def write_async(data):\ncoro = tail_async(handle, interval, write_async)\nfor the output file handle happen in the main event loop.\nparameter to the asyncio.run function.\nasync def slow_coroutine():\nasyncio.run(slow_coroutine(), debug=True)\neverything required to write to the output file using its own event \ndef __init__(self, output_path):\nself.loop = asyncio.new_event_loop()\ndef run(self):\nasyncio.set_event_loop(self.loop)\nself.loop.run_forever()\nself.loop.run_until_complete(asyncio.sleep(0))\nCoroutines in other threads can directly call and await on the write \nasync def real_write(self, data):\nself.output.write(data)\nasync def write(self, data):\nfuture = asyncio.run_coroutine_threadsafe(\nasync def real_stop(self):\nasync def stop(self):\nfuture = asyncio.run_coroutine_threadsafe(\nwithout slowing down the main event loop thread:\nasync def __aenter__(self):\nloop = asyncio.get_event_loop()\nawait loop.run_in_executor(None, self.start)\nasync def __aexit__(self, *_):\nning slow system calls in the main event loop thread:\nasync def tail_async(handle, interval, write_func):\nasync def run_fully_async(handles, interval, output_path):\ncoro = tail_async(handle, interval, output.write)\nItem 63: Avoid Blocking the asyncio Event Loop \nasyncio.run(run_fully_async(handles, 0.1, output_path))\n✦ Pass the debug=True parameter to asyncio.run in order to detect ",
    "keywords": [
      "async def",
      "event loop",
      "async",
      "Changed async def",
      "Item",
      "asyncio Event Loop",
      "threads",
      "changed",
      "async def run",
      "loop",
      "output",
      "run",
      "thread",
      "await",
      "event"
    ],
    "concepts": [
      "asyncio",
      "threaded",
      "threads",
      "thread",
      "threading",
      "function",
      "functionality",
      "functions",
      "await",
      "changed"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 315-323)",
    "start_page": 315,
    "end_page": 323,
    "summary": "tem of C-extension modules in the Python community that speed up \ninvestment in Python to solve difficult computational problems.\nThe multiprocessing built-in module, which is easily accessed via the \nconcurrent.futures built-in module, may be exactly what you need \nlize multiple CPU cores in parallel by running additional interpreters \nimport my_module\nresults = list(map(my_module.gcd, NUMBERS))\nRunning this code on multiple Python threads will yield no speed \nusing the concurrent.futures module with its ThreadPoolExecutor \nimport my_module\nresults = list(pool.map(my_module.gcd, NUMBERS))\nwith the ProcessPoolExecutor from the concurrent.futures module, \nimport my_module\nresults = list(pool.map(my_module.gcd, NUMBERS))\n(via the low-level constructs provided by the multiprocessing module):\n1. It takes each item from the numbers input data to map.\n2. It serializes the item into binary data by using the pickle module \n3. It copies the serialized data from the main interpreter process to \n4. It deserializes the data back into Python objects, using pickle in \n5. It imports the Python module containing the gcd function.\n6. It runs the function on the input data in parallel with other child \n7. It serializes the result back into binary data.\n9. It deserializes the binary data back into Python objects in the \nof a single process shared between Python threads.\nclass to run isolated, high-leverage functions in threads.\nthe multiprocessing module directly.\nPython code.\n✦ The multiprocessing module provides powerful tools that can paral-\nconcurrent.futures built-in module and its simple ProcessPoolExecutor \nPython has \nyou’re implementing Python programs that handle a non-trivial \nLuckily, Python includes many of the algorithms and data structures \nItem 65:  Take Advantage of Each Block in try/except\nduring exception handling in Python.\ntionality of try, except, else, and finally blocks.\nfinally Blocks\nUse try/finally when you want exceptions to propagate up but also \nwant to run cleanup code even when exceptions occur.\nusage of try/finally is for reliably closing file handles (see Item 66: \ndef try_finally_example(filename):\nprint('* Reading data')\nhandle.close()        # Always runs after try block\nthe calling code, but the close method of handle in the finally block \ndata = try_finally_example(filename)\nYou must call open before the try block because exceptions that occur \ntry_finally_example('does_not_exist.txt')\nUse try/except/else to make it clear which exceptions will be han-\nthe try block doesn’t raise an exception, the else block runs.\nelse block helps you minimize the amount of code in the try block, \nItem 65: Take Advantage of Each Block in try/except/else/finally \nFor example, say that I want to load JSON dictionary data \ndef load_json_key(data, key):\nprint('* Loading JSON data')\nresult_dict = json.loads(data)  # May raise ValueError\nIn the successful case, the JSON data is decoded in the try block, \n* Loading JSON data\nIf the input data isn’t valid JSON, then decoding with json.loads \n* Loading JSON data",
    "keywords": [
      "Python",
      "data",
      "Loading JSON data",
      "Block",
      "module",
      "Item",
      "JSON data",
      "JSON",
      "Finally",
      "main",
      "numbers",
      "original Python code",
      "Python threads",
      "CPU cores",
      "multiple CPU cores"
    ],
    "concepts": [
      "python",
      "data",
      "modules",
      "module",
      "exceptions",
      "item",
      "handle",
      "handles",
      "handled",
      "finally"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 324-335)",
    "start_page": 324,
    "end_page": 335,
    "summary": "Here, the try block is used to read the file and process it; the \nexcept block is used to handle exceptions from the try block that are \nexpected; the else block is used to update the file in place and allow \nprint('* Loading JSON data')\nIn the successful case, the try, else, and finally blocks run:\nIf the calculation is invalid, the try, except, and finally blocks run, \nIf the JSON data was invalid, the try block runs and raises an excep-\nItem 65: Take Advantage of Each Block in try/except/else/finally \ndivide_json function at the same time that my hard drive runs out of \nresult data, the finally block still ran and closed the file handle as \n✦ The try/finally compound statement lets you run cleanup code \nin with statements to indicate that the indented code block runs only \nIt’s easy to make your objects and functions work in with statements \nsage will print to screen when I run the function:\nI can elevate the log level of this function temporarily by defining a \nlevel before running the code in the with block and reduces the log-\ndef debug_logging(level):\nNow, I can call the same logging function again but in the \nThis time, all of the debug messages are \nprinted to the screen during the with block.\nthe file handle every time.\nbecause the logging severity level is set low enough in the with block \nwon’t print anything because the default logging severity level for the \nwith log_level(logging.DEBUG, 'my-log') as logger:\nlogging.debug('This will not print')\nAfter the with statement exits, calling debug logging methods on the \nLogger named 'my-log' will not print anything because the default \nwith log_level(logging.DEBUG, 'other-log') as logger:\nlogging.debug('This will not print')\n✦ The with statement allows you to reuse logic from try/finally blocks \nItem 67: Use datetime Instead of time for Local Clocks\nCoordinated Universal Time (UTC) is the standard, time-zone- \nminus 7 hours.” If your program handles time, you’ll probably find \nyourself converting time between UTC and local clocks for the sake of \nItem 67: Use datetime Instead of time for Local Clocks \nPython provides two ways of accomplishing time zone conversions.\nThe old way, using the time built-in module, is terribly error prone.\nYou should be acquainted with both time and datetime to thoroughly \nunderstand why datetime is the best choice and time should be \nThe time Module\nThe localtime function from the time built-in module lets you convert \nTime in my case).\nThis local time can be printed in human-readable \nimport time\nlocal_tuple = time.localtime(now)\ntime_str = time.strftime(time_format, local_tuple)\nprint(time_str)\nin human-readable local time and converting it to UTC time.\ndo this by using the strptime function to parse the time string, and \nthen calling mktime to convert local time to a UNIX timestamp:\nutc_now = time.mktime(time_tuple)\nHow do you convert local time in one time zone to local time in \nanother time zone?\nues from the time, localtime, and strptime functions to do time zone \nTime zones change all the time \nthe time zone changes automatically.\nPython lets you use these time \nzones through the time module if your platform supports it.\nplatforms, such as Windows, some time zone functionality isn’t avail-\nable from time at all.\nFor example, here I parse a departure time from \nprint(time_str)\nassume that other time zones known to my computer will work.\ntime_tuple = time.strptime(arrival_nyc, time_format)\nthe time module unreliable in Python.\nThe time module fails to consis-\ntently work properly for multiple local times.\nusing the time module for this purpose.\nIf you must use time, use it \nonly to convert between UTC and the host computer’s local time.\nThe second option for representing times in Python is the datetime \nLike the time module, \ndatetime can be used to convert from the current time in UTC to local \ntime.\nHere, I convert the present time in UTC to my computer’s local time, \nThe datetime module can also easily convert a local time back to a \ntime_str = '2019-03-16 15:14:35'\nnow = datetime.strptime(time_str, time_format)\nutc_now = time.mktime(time_tuple)\nUnlike the time module, the datetime module has facilities for reli-\nably converting from one local time to another local time.\ndatetime only provides the machinery for time zone operations with \nis missing time zone definitions besides UTC.\npytz contains a full database of every time zone \nTo use pytz effectively, you should always convert local times to UTC \nThen, convert to local times as a final step.\nFor example, here I convert a New York City flight arrival time to a \nnyc_dt_naive = datetime.strptime(arrival_nyc, time_format)\nItem 67: Use datetime Instead of time for Local Clocks \ntime:\nJust as easily, I can convert it to the local time in Nepal:\n✦ Avoid using the time module for translating between different time \nmodule to reliably convert between times in different time zones.\n✦ Always represent time in UTC and do conversions to local time as \nThe pickle built-in module can serialize Python objects into a stream \ngame to a file so it can be resumed at a later time.",
    "keywords": [
      "time",
      "Loading JSON data",
      "local time",
      "JSON data",
      "Loading JSON",
      "UTC",
      "time zone",
      "time Module",
      "block",
      "JSON",
      "local",
      "data",
      "module",
      "Item",
      "Python"
    ],
    "concepts": [
      "time",
      "times",
      "printed",
      "function",
      "functions",
      "functionality",
      "data",
      "utc",
      "item",
      "logging"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 336-344)",
    "start_page": 336,
    "end_page": 344,
    "summary": "the GameState class\nclass GameState:\nSerializing the new version of the GameState class using pickle will \nstate = GameState()\nserialized = pickle.dumps(state)\nstate_after = pickle.loads(serialized)\n{'level': 0, 'lives': 4, 'points': 0}\nwith the new definition of the GameState class:\nthe returned object is an instance of the new GameState class:\nThis behavior is a byproduct of the way the pickle module works.\nuse of pickle moves beyond trivial usage, the module’s functionality \nfor serializing and deserializing Python objects, allowing you to con-\nclass GameState:\ndef __init__(self, level=0, lives=4, points=0):\nTo use this constructor for pickling, I define a helper function that \ndef pickle_game_state(game_state):\nreturn unpickle_game_state, (kwargs,)\nNow, I need to define the unpickle_game_state helper.\ntion takes serialized data and parameters from pickle_game_state \ndef unpickle_game_state(kwargs):\ncopyreg.pickle(GameState, pickle_game_state)\nstate = GameState()\nstate.points += 1000\nserialized = pickle.dumps(state)\nstate_after = pickle.loads(serialized)\n{'level': 0, 'lives': 4, 'points': 1000}\nclass GameState:\ndef __init__(self, level=0, lives=4, points=0, magic=5):\nBut unlike before, deserializing an old GameState object will result in \nunpickle_game_state calls the GameState constructor directly instead \nstate_after = pickle.loads(serialized)\nBefore: {'level': 0, 'lives': 4, 'points': 1000}\nAfter:  {'level': 0, 'lives': 4, 'points': 1000, 'magic': 5}\nfine the GameState class to no longer have a lives field:\nclass GameState:\ndef __init__(self, level=0, points=0, magic=5):\nAll fields from the old data, even ones removed from the class, will \nbe passed to the GameState constructor by the unpickle_game_state \npickling a new GameState object:\ndef pickle_game_state(game_state):\nreturn unpickle_game_state, (kwargs,)\ndef unpickle_game_state(kwargs):\ncopyreg.pickle(GameState, pickle_game_state)\nstate_after = pickle.loads(serialized)\nBefore: {'level': 0, 'lives': 4, 'points': 1000}\nunpickle_game_state function.\nHere, I rename the GameState class to BetterGameState and remove \ndef __init__(self, level=0, points=0, magic=5):\nAttempting to deserialize an old GameState object now fails because \nobject’s class is encoded in the pickled data:\nfor the function to use for unpickling an object.\ncopyreg.pickle(BetterGameState, pickle_game_state)\nunpickle_game_state is encoded in the serialized data instead of \nserialized = pickle.dumps(state)\n¯\\x94\\x8c\\x13unpickle_game_state\\x94\\x93\\x94}\\x94(\\x8c\nwhich the unpickle_game_state function is present.\n✦ The pickle built-in module is useful only for serializing and deseri-\n✦ Deserializing previously pickled objects may break if the classes \n✦ Use the copyreg built-in module with pickle to ensure backward \nprint(round(cost, 2))\nThe solution is to use the Decimal class from the decimal built-in mod-\nThe Decimal class provides fixed point math of 28 decimal places \nThe class also \ncost = rate * seconds / Decimal(60)\nsmall_cost = rate * seconds / Decimal(60)\nLuckily, the Decimal class has a built-in function for rounding to \nexactly the decimal place needed with the desired rounding behavior.\nfrom decimal import ROUND_UP\nrounded = small_cost.quantize(Decimal('0.01'),\nWhile Decimal works great for fixed point numbers, it still has limita-\n✦ Python has built-in types and classes in modules that can repre-\n✦ The Decimal class is ideal for situations that require high precision \n✦ Pass str instances to the Decimal constructor instead of float ",
    "keywords": [
      "decimal",
      "state",
      "GameState",
      "game",
      "Decimal class",
      "GameState class",
      "points",
      "decimal import Decimal",
      "GameState object",
      "pickle",
      "level",
      "class GameState",
      "serialized",
      "lives",
      "data"
    ],
    "concepts": [
      "points",
      "point",
      "decimal",
      "pickle",
      "pickling",
      "pickled",
      "values",
      "value",
      "serializing",
      "serialized"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 345-356)",
    "start_page": 345,
    "end_page": 356,
    "summary": "Item 70: Profile Before Optimizing \nTo profile insertion_sort and insert_value, I create a data set of ran-\ndom numbers and define a test function to pass to the profiler:\nthe performance of your program while it’s being profiled.\nthe test function through it using the runcall method:\n20003 function calls in 1.320 seconds\n8    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n■ncalls: The number of calls to the function during the profiling \nexcluding time spent executing other functions it calls.\nfunction each time it is called, excluding time spent executing \nother functions it calls.\nfunction, including time spent in all other functions it calls.\nfunction each time it is called, including time spent in all other \nfunctions it calls.\nuse of CPU in my test is the cumulative time spent in the insert_value \nfunction.\nThe new function is much faster, with a cumulative time spent \nfunction:\nItem 70: Profile Before Optimizing \n30003 function calls in 0.017 seconds\nfunction is called by many different parts of your program.\nFor example, here the my_utility function is called repeatedly by two \ndifferent functions in the program:\n20242 function calls in 0.118 seconds\nThe my_utility function is clearly the source of most execution time, \nof each function:\nThis profiler statistics table shows functions called on the left and \nFunction                                was called by...\nProfiling.md:172(second_func)           <-      20    0.000    0.001  main.py:176(my_program)\n✦ It’s important to profile Python programs before optimizing because \nto profile a tree of function calls in isolation.\nItem 71: Prefer deque for Producer–Consumer Queues\nqueue is used when one function gathers values to process and \nItem 71: Prefer deque for Producer–Consumer Queues \nconsumer queue.\nThe producing function receives emails and enqueues them to be con-\nThis function uses the append method on the \nlist to add new messages to the end of the queue so they are pro-\ndef produce_emails(queue):\nqueue.append(email)  # Producer\nThe consuming function does something useful with the emails.\nfunction calls pop(0) on the queue, which removes the very first item \nfrom the beginning of the queue, the consumer ensures that the items \ndef consume_one_email(queue):\nif not queue:\nemail = queue.pop(0)  # Consumer\nkeep_running function returns False (see Item 60: “Achieve Highly \ndef loop(queue, keep_running):\nproduce_emails(queue)\nconsume_one_email(queue)\nthroughput at the cost of end-to-end latency (see Item 55: “Use Queue \nUsing a list for a producer–consumer queue like this works fine up \nTo analyze the performance of using list as a FIFO queue, I can \nqueue using the append method of list (matching the producer func-\ndef list_append_benchmark(count):\ndef run(queue):\nqueue.append(i)\nbaseline = list_append_benchmark(500)\ncomparison = list_append_benchmark(count)\nItem 71: Prefer deque for Producer–Consumer Queues \nitems from the beginning of the queue (matching the consumer func-\ndef list_pop_benchmark(count):\nreturn list(range(count))\ndef run(queue):\nwhile queue:\nqueue.pop(0)\nI can similarly run this benchmark for queues of different sizes to see \ncomparison = list_pop_benchmark(count)\na list with pop(0) scales quadratically as the length of the queue \nI need to call pop(0) for every item in the list, and thus I end up \nqueue.\nTo use the deque class, the call to append in produce_emails can \nstay the same as it was when using a list for the queue.\nlist.pop method call in consume_one_email must change to call the \nmethod must be called with a deque instance instead of a list.\ndef consume_one_email(queue):\nif not queue:\nemail = queue.popleft()  # Consumer\nperformance (matching the producer function’s usage) has stayed \ndef deque_append_benchmark(count):\nItem 71: Prefer deque for Producer–Consumer Queues \ndef run(queue):\nqueue.append(i)\nthe consumer function’s usage of deque:\ndef run(queue):\nwhile queue:\n✦ The list type can be used as a FIFO queue by having the producer \ncall append to add items and the consumer call pop(0) to receive \nItem 71: Prefer deque for Producer–Consumer Queues \ning for a specific value in a list takes linear time proportional to the \nearly scan the list and compare each item to your goal value:\nitem in the list to keep it in sorted order:",
    "keywords": [
      "time Count",
      "count",
      "time",
      "function",
      "queue",
      "data size",
      "list",
      "data",
      "Consumer Queues",
      "Item",
      "size",
      "performance",
      "Consumer",
      "Email",
      "items"
    ],
    "concepts": [
      "functions",
      "queues",
      "queue",
      "time",
      "times",
      "profile",
      "profiled",
      "profiling",
      "item",
      "items"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 357-368)",
    "start_page": 357,
    "end_page": 368,
    "summary": "✦ Searching sorted data contained in a list takes linear time using \nItem 73: Know How to Use heapq for Priority Queues\nbooks.\nThere are people returning their borrowed books on time.\nbooks.\nclass Book:\nItem 73: Know How to Use heapq for Priority Queues \nlist and sorting it by due_date each time a new Book is added:\ndef add_book(queue, book):\nqueue.append(book)\nqueue = []\nadd_book(queue, Book('Don Quixote', '2019-06-07'))\nadd_book(queue, Book('Frankenstein', '2019-06-05'))\nadd_book(queue, Book('Les Misérables', '2019-06-08'))\nadd_book(queue, Book('War and Peace', '2019-06-03'))\nIf I can assume that the queue of borrowed books is always in sorted \noverdue book, if any, and remove it from the queue:\ndef next_overdue_book(queue, now):\nif queue:\nbook = queue[-1]\nreturn book\nfound = next_overdue_book(queue, now)\nfound = next_overdue_book(queue, now)\nIf a book is returned before the due date, I can remove the scheduled \nreminder message by removing the Book from the list:\ndef return_book(queue, book):\nqueue.remove(book)\nqueue = []\nadd_book(queue, book)\nreturn_book(queue, book)\nnext_overdue_book(queue, now)\nIf I have len(queue) books to add, and the \nbooks \ndef run(queue, to_add):\nwhile queue:\nqueue scales superlinearly as the number of books being borrowed \nItem 73: Know How to Use heapq for Priority Queues \nWhen a book is returned before the due date, I need to do a linear \nscan in order to find the book in the queue and remove it.\na book causes all subsequent items in the list to be shifted back \ndef list_return_benchmark(count):\nqueue = list(range(count))\nreturn queue, to_return\ndef run(queue, to_return):\nHere, I reimplement the add_book function using the heapq module.\nthe queue:\ndef add_book(queue, book):\nheappush(queue, book)\nqueue = []\nadd_book(queue, Book('Little Women', '2019-06-05'))\nadd_book(queue, Book('The Time Machine', '2019-05-30'))\nThe heapq module requires items in the priority queue to be compa-\nItem 73: Know How to Use heapq for Priority Queues \nclass Book:\nNow, I can add books to the priority queue by using the heapq.heappush \nqueue = []\nadd_book(queue, Book('Pride and Prejudice', '2019-06-01'))\nadd_book(queue, Book('The Time Machine', '2019-05-30'))\nadd_book(queue, Book('Crime and Punishment', '2019-06-06'))\nadd_book(queue, Book('Wuthering Heights', '2019-06-12'))\nAlternatively, I can create a list with all of the books in any order and \nqueue = [\nqueue = [\nTo check for overdue books, I inspect the first element in the list \ndef next_overdue_book(queue, now):\nif queue:\nbook = queue[0]           # Most overdue first\nheappop(queue)        # Remove the overdue book\nreturn book\nNow, I can find and remove overdue books in order until there are \nbook = next_overdue_book(queue, now)\nbook = next_overdue_book(queue, now)\nnext_overdue_book(queue, now)\ndef run(queue, to_add):\nItem 73: Know How to Use heapq for Priority Queues \nwhile queue:\nqueue \nbook from the priority queue until its due date.\nbe the first item in the list, and I can simply ignore the book if it’s \nclass Book:\nany book that’s already been returned:\ndef next_overdue_book(queue, now):\nwhile queue:\nbook = queue[0]\nif book.returned:\nreturn book\nThis approach makes the return_book function extremely fast \ndef return_book(queue, book):\nbook.returned = True\nAlthough the queue \nItem 73: Know How to Use heapq for Priority Queues \n✦ Priority queues allow you to process items in order of importance \n✦ If you try to use list operations to implement a priority queue, your ",
    "keywords": [
      "book",
      "queue",
      "count",
      "priority queue",
      "Item",
      "books",
      "time",
      "add",
      "overdue",
      "time Count",
      "list",
      "Priority",
      "overdue books",
      "book return book",
      "date"
    ],
    "concepts": [
      "queues",
      "queue",
      "books",
      "book",
      "item",
      "items",
      "returning",
      "returned",
      "returns",
      "function"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 369-376)",
    "start_page": 369,
    "end_page": 376,
    "summary": "video_data = request_chunk(video_id, byte_offset, size)\nof video data?\nchunk is extracted from gigabytes of video data that’s cached in mem-\nvideo_data = ...\n# bytes containing data for video_id\nchunk = video_data[byte_offset:byte_offset + size]\ntors: how much time it takes to slice the 20 MB video chunk from \nvideo_data, and how much time the socket takes to transmit that \ndata to the client.\nstand the performance characteristics of slicing bytes instances this \nchunk = video_data[byte_offset:byte_offset + size]\nIt took roughly 5 milliseconds to extract the 20 MB slice of data to \nproblem is that slicing a bytes instance causes the underlying data to \nA better way to write this code is by using Python’s built-in memoryview \nPython runtime and C extensions to access the underlying data \nbuffers that are behind objects like bytes instances.\nmemoryview instance without copying the underlying data.\nate a memoryview wrapping a bytes instance and inspect a slice of it:\nview = memoryview(data)\nprint(chunk)\nprint('Size:           ', chunk.nbytes)\nprint('Data in view:   ', chunk.tobytes())\nprint('Underlying data:', chunk.obj)\nData in view:    b'haircut'\nvideo_view = memoryview(video_data)\nchunk = video_view[byte_offset:byte_offset + size]\nlatest video data from the user in a cache that other clients can read \nHere’s what the implementation of reading 1 MB of new data \nchunk = socket.recv(size)\nvideo_view = memoryview(video_cache)\nbefore = video_view[:byte_offset]\nafter = video_view[byte_offset + size:]\nThe socket.recv method returns a bytes instance.\nnew data with the existing cache at the current byte_offset by using \nchunk = socket.recv(size)\nbefore = video_view[:byte_offset]\nafter = video_view[byte_offset + size:]\nIt takes 33 milliseconds to receive 1 MB and update the video cache.\nclients streaming in video data this way.\nA better way to write this code is to use Python’s built-in bytearray \nThe bytearray type is like a mutable version of bytes that allows for \nsuch a memoryview, the resulting object can be used to assign data to a \nback together after data was received from the client:\nwrite_view[:] = b'-10 bytes-'\nMany library methods in Python, such as socket.recv_into and \nRawIOBase.readinto, use the buffer protocol to receive or read data \nmemory and creating another copy of the data; what’s received goes \nwith a memoryview slice to receive data into an underlying bytearray \nwrite_view = memoryview(video_array)\nchunk = write_view[byte_offset:byte_offset + size]\nsocket.recv_into(chunk)\nchunk = write_view[byte_offset:byte_offset + size]\nsocket.recv_into(chunk)\nreading and writing slices of objects that support Python’s high- \n✦ The bytearray built-in type provides a mutable bytes-like type \nthat can be used for zero-copy data reads with functions like \n✦ A memoryview can wrap a bytearray, allowing for received data to be \nPython doesn’t have compile-time static type checking.\nalso make it extremely easy to write tests for your code and to debug \nbut having good tests actually makes it easier to modify Python code, \nWhen debugging a Python program, the print function and format ",
    "keywords": [
      "data",
      "video",
      "chunk",
      "Python",
      "byte",
      "memoryview",
      "size",
      "view",
      "video data",
      "offset",
      "bytes",
      "Item",
      "chunk size chunk",
      "underlying data",
      "run"
    ],
    "concepts": [
      "printing",
      "prints",
      "chunks",
      "bytes",
      "memoryview",
      "data",
      "python",
      "size",
      "type",
      "typing"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 377-390)",
    "start_page": 377,
    "end_page": 390,
    "summary": "To define tests, I create a second file named test_utils.py or \ncontains tests for each behavior that I expect:\n# utils_test.py\ndef test_to_str_bytes(self):\ndef test_to_str_str(self):\ndef test_failing(self):\ntwo of the test methods pass and one fails, with a helpful error mes-\n$ python3 utils_test.py\nFile \"utils_test.py\", line 15, in test_failing\nRan 3 tests in 0.002s\nTests are organized into TestCase subclasses.\nEach test case is a \nmethod beginning with the word test.\nIf a test method runs without \ntest fails, the TestCase subclass continues running the other test \nmethods so you can get a full picture of how all your tests are doing \n$ python3 utils_test.py UtilsTestCase.test_to_str_bytes\nRan 1 test in 0.000s\nI have the same test case written with and without using a helper \n# assert_test.py\ndef test_assert_helper(self):\ndef test_assert_statement(self):\n$ python3 assert_test.py\nFAIL: test_assert_helper (__main__.AssertTestCase)\nFile \"assert_test.py\", line 16, in test_assert_helper\nFAIL: test_assert_statement (__main__.AssertTestCase)\nFile \"assert_test.py\", line 11, in test_assert_statement\nRan 2 tests in 0.001s\n# utils_error_test.py\ndef test_to_str_bad(self):\ndef test_to_str_bad_encoding(self):\nTestCase subclasses to make your tests more readable.\nbe run as if they’re test cases.\ntion methods, these custom test helpers often use the fail method to \nI define a custom test helper method for verifying the behavior of a \n# helper_test.py\ndef verify_complex_case(self, values, expected):\ntest_it = zip(expect_it, found_it)\nfor i, (expect, found) in enumerate(test_it):\ndef test_wrong_lengths(self):\ndef test_wrong_results(self):\nThe helper method makes the test cases short and readable, and the \n$ python3 helper_test.py\nFAIL: test_wrong_lengths (__main__.HelperTestCase)\nFile \"helper_test.py\", line 43, in test_wrong_lengths\nFile \"helper_test.py\", line 34, in verify_complex_case\nFAIL: test_wrong_results (__main__.HelperTestCase)\nFile \"helper_test.py\", line 52, in test_wrong_results\nFile \"helper_test.py\", line 24, in verify_complex_case\nRan 2 tests in 0.002s\nI usually define one TestCase subclass for each set of related tests.\nI often create one TestCase subclass for test-\nallows the test method to continue testing other cases even after one \ntest methods).\nTo show this, here I define an example data-driven test:\n# data_driven_test.py\ndef test_good(self):\ndef test_bad(self):\nThe 'no error' test case fails, printing a helpful error message, but \nall of the other cases are still tested and confirmed to pass:\n$ python3 data_driven_test.py\nRan 2 tests in 0.001s\n✦ You can create tests by subclassing the TestCase class from the \nTest methods on TestCase classes must start with \nthe word test.\nas assertEqual, to confirm expected behaviors in your tests instead \nItem 77: Isolate Tests from Each Other \n✦ Consider writing data-driven tests using the subTest helper method \nItem 77:  Isolate Tests from Each Other with setUp, \ntest methods can be run; this is sometimes called the test harness.\ntest method, respectively, so you can ensure that each test runs in \n# environment_test.py\nself.test_dir = TemporaryDirectory()\nself.test_dir.cleanup()\ndef test_modify_file(self):\nwith open(self.test_path / 'data.bin', 'w') as f:\nrun your integration tests.\ntest methods run without repeating that initialization.\n# integration_test.py\nprint('* Test setup')\nprint('* Test clean-up')\ndef test_end_to_end1(self):\nprint('* Test 1')\ndef test_end_to_end2(self):\nprint('* Test 2')\n$ python3 integration_test.py \n* Test setup\n* Test 1\nItem 78: Use Mocks to Test Code with Complex Dependencies \n* Test clean-up\n.* Test setup\n* Test 2\n* Test clean-up\nRan 2 tests in 0.000s\n✦ It’s important to write both unit tests (for isolated functionality) and \nintegration tests (for modules that interact with each other).\n✦ Use the setUp and tearDown methods to make sure your tests are \nmodule-level functions to manage any test harnesses you need for \nthe entire lifetime of a test module and all of the TestCase classes \nItem 78:  Use Mocks to Test Code with Complex \nHow do I get a DatabaseConnection instance to use for testing this \ntested:\nusing them in tests.",
    "keywords": [
      "Verify Related Behaviors",
      "TestCase",
      "def test",
      "main",
      "TestCase Subclasses",
      "str",
      "test methods",
      "Related Behaviors",
      "Verify Related",
      "expected",
      "Item",
      "test.py",
      "Debugging def test",
      "repr",
      "Debugging"
    ],
    "concepts": [
      "tested",
      "classes",
      "expect",
      "expected",
      "python",
      "printing",
      "data",
      "method",
      "methods",
      "fails"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 391-402)",
    "start_page": 391,
    "end_page": 402,
    "summary": "Item 78: Use Mocks to Test Code with Complex Dependencies \nOnce it’s created, I can call the mock, get its return value, and ver-\nthe mock to do anything; all I care about is that the database param-\nresult = mock(database, 'Meerkat')\nthe code that called the mock provided the correct arguments?\nthis, the Mock class provides the assert_called_once_with method, \nmock.assert_called_once_with(database, 'Meerkat')\nmock.assert_called_once_with(database, 'Giraffe')\nI can also use the assert_called_with method of Mock to \nfrom unittest.mock import ANY\nmock = Mock(spec=get_animals)\nmock('database 1', 'Rabbit')\nmock('database 2', 'Bison')\nmock('database 3', 'Meerkat')\nmock.assert_called_with(ANY, 'Meerkat')\nmock = Mock(spec=get_animals)\nresult = mock(database, 'Meerkat')\nanimals at the zoo, given a set of database-interacting functions:\ndef feed_animal(database, name, when):\nItem 78: Use Mocks to Test Code with Complex Dependencies \nfeed_animal(database, name, now)\nI need to mock \nAnd I need to mock out feed_animal to \nbeing tested to use the mock dependent functions instead of the \nfeeding_timedelta = food_func(database, species)\nanimals = animals_func(database, species)\nTo test this function, I need to create all of the Mock instances upfront \nnow_func = Mock(spec=datetime.utcnow)\nfood_func = Mock(spec=get_food_period)\nanimals_func = Mock(spec=get_animals)\nfeed_func = Mock(spec=feed_animal)\nThen, I can run the test by passing the mocks into the do_rounds \ndatabase,\nfrom unittest.mock import call\nfood_func.assert_called_once_with(database, 'Meerkat')\nanimals_func.assert_called_once_with(database, 'Meerkat')\ncall(database, 'Spot', now_func.return_value),\ncall(database, 'Fluffy', now_func.return_value),\nI don’t verify the parameters to the datetime.utcnow mock or how many \nItem 78: Use Mocks to Test Code with Complex Dependencies \nFor the feed_animal function, I verify that two calls were made—\nunittest.mock.call helper and the assert_has_calls method.\nThe unittest.mock.patch family of functions makes \nFor example, here I override get_animals to be a mock using \nfrom unittest.mock import patch\nI need to mock out the current time returned by the datetime.utcnow \ndatetime.utcnow mock and use patch for all of the other mocks:\npatch.multiple function to create many mocks and set their \nnow_func = Mock(spec=datetime.utcnow)\nItem 79: Encapsulate Dependencies to Facilitate Mocking and Testing \nresult = do_rounds(database, 'Meerkat', utcnow=now_func)\nfood_func.assert_called_once_with(database, 'Meerkat')\nanimals_func.assert_called_once_with(database, 'Meerkat')\ncall(database, 'Spot', now_func.return_value),\ncall(database, 'Fluffy', now_func.return_value),\nMocks are useful in tests when \n✦ When using mocks, it’s important to verify both the behavior of the \ncode being tested and how dependent functions were called by that \ncode, using the Mock.assert_called_once_with family of methods.\n✦ Keyword-only arguments and the unittest.mock.patch family of \nfunctions can be used to inject mocks into the code being tested.\nMocking and Testing\nIn the previous item (see Item 78: “Use Mocks to Test Code with \nunittest.mock built-in module—including the Mock class and patch \nsuch as a database.\nating mocks and writing tests.\ndatabase.feed_animal(name, now)\nneed to use unittest.mock.patch to inject the mock into the code \nItem 79: Encapsulate Dependencies to Facilitate Mocking and Testing \nThe Mock \nclass returns a mock object for any attribute name that is accessed.\ndatabase = Mock(spec=ZooDatabase)\nprint(database.feed_animal)\ndatabase.feed_animal()\ndatabase.feed_animal.assert_any_call()\n<Mock name='mock.feed_animal' id='4384773408'>\nfrom unittest.mock import call\nnow_func = Mock(spec=datetime.utcnow)\ndatabase = Mock(spec=ZooDatabase)\ndatabase.get_food_period.return_value = timedelta(hours=3)\ndatabase.get_animals.return_value = [\nresult = do_rounds(database, 'Meerkat', utcnow=now_func)\ndatabase.get_animals.assert_called_once_with('Meerkat')\ndatabase.feed_animal.assert_has_calls(\nDATABASE = None\nif DATABASE is None:\nNow, I can inject the mock ZooDatabase using patch, run the test, and \nI’m not using a mock datetime.utcnow \nfrom unittest.mock import patch\nDATABASE.get_animals.return_value = [\n✦ When unit tests require a lot of repeated boilerplate to set up mocks, \nclasses by returning a new mock, which can act as a mock method, \nmock dependencies in tests.",
    "keywords": [
      "database",
      "animals",
      "Mocks",
      "Item",
      "Meerkat",
      "Mock class",
      "Code",
      "function",
      "func",
      "mock object",
      "food",
      "species",
      "Test Code",
      "call",
      "functions"
    ],
    "concepts": [
      "mocks",
      "mocking",
      "mocked",
      "database",
      "functions",
      "functionality",
      "tested",
      "animals",
      "patch",
      "patched"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 403-410)",
    "start_page": 403,
    "end_page": 410,
    "summary": "modules, inspect global state, construct new objects, run the help \nThree very useful commands make inspecting the running program \ndebugger commands to control the program’s execution in different \n■step: Run the program until the next line of execution in the pro-\nnext line of execution includes calling a function, the debugger \n■next: Run the program until the next line of execution in the \n■return: Run the program until the current function returns, and \n■continue: Continue running the program until the next break-\nThe breakpoint function can be called anywhere in a program.\nWhen I run the program and it enters the debugger, I can confirm \n$ python3 conditional_breakpoint.py \nThis enables you to debug a program after it’s \nI use the command line python3 -m pdb -c continue <program path> \nto run the program under control of the pdb module.\ncommand tells pdb to get the program started immediately.\nrunning, the program hits a problem and automatically enters the \ninteractive debugger, at which point I can inspect the program state:\n$ python3 -m pdb -c continue postmortem_breakpoint.py \nFile \".../pdb.py\", line 1697, in main\nFile \".../pdb.py\", line 1566, in _runscript\nFile \".../bdb.py\", line 585, in run\nFile \"postmortem_breakpoint.py\", line 4, in <module>\nFile \"postmortem_breakpoint.py\", line 16, in compute_rmse\nfunction of the pdb module (which is often done in a single line as \nFile \"my_module.py\", line 17, in compute_stddev\nFile \"my_module.py\", line 13, in compute_variance\nest directly in your program by calling the breakpoint built-in \ninspect and modify the state of a running program.\n✦ pdb shell commands let you precisely control program execution \n✦ The pdb module can be used for debug exceptions after they \nhappen in independent Python programs (using python -m pdb -c \nItem 81:  Use tracemalloc to Understand Memory \nworry about allocating or deallocating memory in their programs.\nwhere a Python program is using or leaking memory proves to be a \nThe first way to debug memory usage is to ask the gc built-in module \nItem 81: Use tracemalloc to Understand Memory Usage and Leaks \nwhere your program’s memory is being used.\n# waste_memory.py\nThen, I run a program that uses the gc built-in module to print out \nhold_reference = waste_memory.run()\n<waste_memory.MyObject object at 0x10390aeb8>\n<waste_memory.MyObject object at 0x10390aef0>\n<waste_memory.MyObject object at 0x10390af28>\nHere, I use this approach to print out the top three memory usage \nx = waste_memory.run()                     # Usage to debug\nwhich objects are dominating my program’s memory usage and where \nThe tracemalloc module can also print out the full stack trace of each \nmemory usage in the program:\nx = waste_memory.run()\nx = waste_memory.run()\nFile \"waste_memory.py\", line 17\nFile \"waste_memory.py\", line 10\nFile \"waste_memory.py\", line 5\n✦ It can be difficult to understand how Python programs use and leak \nItem 81: Use tracemalloc to Understand Memory Usage and Leaks ",
    "keywords": [
      "program",
      "Pdb",
      "Memory Usage",
      "Memory",
      "line",
      "Python",
      "objects",
      "debugger",
      "waste",
      "function",
      "Understand Memory Usage",
      "run",
      "File",
      "execution",
      "module"
    ],
    "concepts": [
      "program",
      "programs",
      "python",
      "pdb",
      "running",
      "memory",
      "file",
      "object",
      "commands",
      "command"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 411-419)",
    "start_page": 411,
    "end_page": 419,
    "summary": "The Python community has \nCollaborating with others on Python programs requires being \nPython has a central repository of modules (https://pypi.org) that you \nTo use the Package Index, you need to use the command-line tool pip \npython3 -m pip to ensure that packages are installed for the correct \nversion of Python on your system (see Item 1: “Know Which Version of \nUsing pip to install a new module is simple.\nexample, here I install the pytz module that I use elsewhere in this \n$ python3 -m pip install pytz\nYou can also create your own PyPI packages to share with the Python \n✦ The Python Package Index (PyPI) contains a wealth of common \npackages that are built and maintained by the Python community.\n✦ pip is the command-line tool you can use to install packages \nvarious packages from the Python community (see Item 82: “Know \nning the python3 -m pip command-line tool to install packages like \nThe problem is that, by default, pip installs new packages in a global \nthat the packages you install depend on.\nwhat the Sphinx package depends on after installing it by asking pip:\n$ python3 -m pip show Sphinx\nItem 83: Use Virtual Environments for Isolated Dependencies \nSummary: Python documentation generator\nLocation: /usr/local/lib/python3.8/site-packages\n$ python3 -m pip show flask\nLocation: /usr/local/lib/python3.8/site-packages\nwith python3 -m pip install --upgrade Jinja2, you may find that Sphinx \nglobal version of a module installed at a time.\nassume the worst: that the versions of Python and global packages \nSince Python 3.4, pip and the venv \nmodule have been available by default along with the Python installa-\ntion (accessible with python -m venv).\nvenv allows you to create isolated versions of the Python environment.\nthe tool, it’s important to note the meaning of the python3 command \nOn my computer, python3 is located in the \n$ which python3\n$ python3 --version\nPython 3.8.0\nworks because I already have the pytz package installed as a global \n$ python3 -c 'import pytz'\nNow, I use venv to create a new virtual environment called myproject.\n$ python3 -m venv myproject\nTo start using the virtual environment, I use the source command \nAfter activation, the path to the python3  command-line tool has moved \n(myproject)$ which python3\n/tmp/myproject/bin/python3\npython3 to version 3.9, my virtual environment will still explicitly \nThe virtual environment I created with venv starts with no packages \ninstalled except for pip and setuptools.\n(myproject)$ python3 -c 'import pytz'\nI can use the pip command-line tool to install the pytz module into \n(myproject)$ python3 -m pip install pytz\nItem 83: Use Virtual Environments for Isolated Dependencies \n(myproject)$ python3 -c 'import pytz'\n(myproject)$ which python3\n/tmp/myproject/bin/python3\n$ which python3\nOnce you are in a virtual environment, you can continue installing \nI can use the python3 -m pip freeze \n(myproject)$ python3 -m pip freeze > requirements.txt\n$ python3 -m venv otherproject\nThe new environment will have no extra packages installed:\n(otherproject)$ python3 -m pip list\nI can install all of the packages from the first environment by  running \npython3 -m pip install on the requirements.txt that I generated with \nthe python3 -m pip freeze command:\n(otherproject)$ python3 -m pip install -r /tmp/myproject/\ninstalls all of the packages required to reproduce the first environ-\n(otherproject)$ python3 -m pip list\nthat the specific version of Python you’re using is not included in the \nItem 83: Use Virtual Environments for Isolated Dependencies \neverything because all of the paths, like the python3 command-line \ntool, are hard-coded to the environment’s install directory.\ning a virtual environment directory, just use python3 -m pip freeze \n✦ Virtual environments allow you to use pip to install many differ-\n✦ Virtual environments are created with python -m venv, enabled with \npython3 -m pip freeze.\npython3 -m pip install -r requirements.txt.\nDocumentation in Python is extremely important because of the \nto run a local web server that hosts all of the Python documentation \nmost open source Python libraries to have decent documentation.",
    "keywords": [
      "Python",
      "virtual environment",
      "environment",
      "Python Package Index",
      "Virtual",
      "pip",
      "packages",
      "Python community",
      "Package",
      "version",
      "myproject",
      "module",
      "Python environment",
      "install",
      "Item"
    ],
    "concepts": [
      "python",
      "packages",
      "package",
      "packaging",
      "version",
      "versions",
      "documentation",
      "documenting",
      "pip",
      "modules"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 420-428)",
    "start_page": 420,
    "end_page": 428,
    "summary": "The module docstring is also a jumping-off point where you can high-\nlight important classes and functions found in the module.\nHere’s an example of a module docstring:\nThis module provides easy ways to determine when words you've\nIf the module is a command-line utility, the module docstring is also \nthe same pattern as the module-level docstring.\nimportant details of the class’s operation.\nImportant public attributes and methods of the class should be high-\nHere’s an example of a class docstring:\nItem 84: Write Docstrings for Every Function, Class, and Module \nEach public function and method should have a docstring.\nlows the same pattern as the docstrings for modules and classes.\nHere’s an example of a function docstring:\nThere are also some special cases in writing docstrings for functions \nfrom typing import Container, List\nItem 85: Use Packages to Organize Modules and Provide Stable APIs \n✦ Write documentation for every module, class, method, and function \n✦ For modules: Introduce the contents of a module and any important \n✦ For classes: Document behavior, important attributes, and subclass \nbehavior in the docstring following the class statement.\n✦ For functions and methods: Document every argument, returned \nItem 85:  Use Packages to Organize Modules and \nYou’ll separate functionality into various modules \nPackages are modules that \ncontain other modules.\nPython files in that directory will be available for import, using a path \nTo import the utils module, I use the absolute module name that \nfrom mypackage import utils\nThe functionality provided by packages has two primary purposes in \nThe first use of packages is to help divide your modules into separate \nhere’s a program that imports attributes from two modules with the \nfrom analysis.utils import log_base2_bucket\nfrom frontend.utils import stringify\nwant to use the inspect function from both the analysis.utils and \nthe frontend.utils modules.\nfrom analysis.utils import inspect\nfrom frontend.utils import inspect  # Overwrites!\nItem 85: Use Packages to Organize Modules and Provide Stable APIs \nThe solution is to use the as clause of the import statement to rename \nfrom analysis.utils import inspect as analysis_inspect\nfrom frontend.utils import inspect as frontend_inspect\nimport statement, including entire modules.\nimport from:\nimport analysis.utils\nimport frontend.utils\nThe second use of packages in Python is to provide strict, stable APIs \nway, you can refactor and improve your package’s internal modules \nthe __all__ special attribute of a module or package.\nWhen consuming code executes from foo import *, \ning underscore—are imported (see Item 42: “Prefer Public Attributes \nI also define a utils module in mypackage to perform operations on the \nmodels import Projectile\nattributes that are available on the mypackage module.\ndownstream consumers to always import directly from mypackage \ninstead of importing from mypackage.models or mypackage.utils.\nTo do this with Python packages, you need to modify the __init__.py \ncontents of the mypackage module when it’s imported.\nspecify an explicit API for mypackage by limiting what you import into \nI can expose the public interface of mypackage by simply import-\nmodels import *\nutils import *\nHere’s a consumer of the API that directly imports from mypackage \nfrom mypackage import *\nNotably, internal-only functions like mypackage.utils._dot_product \nThis whole approach works great when it’s important to provide an \nyour own modules, the functionality of __all__ is probably unneces-\nItem 85: Use Packages to Organize Modules and Provide Stable APIs \nsource of y is explicitly the x package or module.\nIf a module has multiple import * statements, \nnames within the containing module.\nThe safest approach is to avoid import * in your code and explicitly \n✦ Packages in Python are modules that contain other modules.\nchild modules of the directory’s package.\n✦ You can provide an explicit API for a module by listing its publicly \n✦ You can hide a package’s internal implementation by only import-\ning public names in the package’s __init__.py file or by naming \nItem 86:  Consider Module-Scoped Code to Configure ",
    "keywords": [
      "Item",
      "module",
      "modules",
      "Function",
      "docstring",
      "Packages",
      "Stable APIs",
      "API",
      "module docstring",
      "mypackage",
      "package",
      "Organize Modules",
      "functions",
      "Python",
      "Docstrings"
    ],
    "concepts": [
      "imports",
      "importing",
      "imported",
      "module",
      "modules",
      "words",
      "word",
      "item",
      "functions",
      "function"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 429-441)",
    "start_page": 429,
    "end_page": 441,
    "summary": "Item 86: Consider Module-Scoped Code to Configure Enviornments \nOther modules in my program can then import the __main__ \nmodule and use the value of TESTING to decide how they define their \nThe key behavior to notice here is that code running in module \nmodule will define names.\nbefore defining top-level constructs in a module:\nmy module definitions.\nItem 87:  Define a Root Exception to Insulate Callers \nWhen you’re defining a module’s API, the exceptions you raise are \n# my_module.py\nby providing a root Exception in my module and having all other \nexceptions raised by that module inherit from the root exception:\n# my_module.py\nclass Error(Exception):\n\"\"\"Base-class for all exceptions raised by this module.\"\"\"\nHaving a root exception in a module makes it easy for consumers of \nan API to catch all of the exceptions that were raised deliberately.\nexcept my_module.Error:\nItem 87: Define a Root Exception to Insulate Callers from APIs \nFile \".../example.py\", line 3, in <module>\nFile \".../my_module.py\", line 10, in determine_weight\nthe insulating except block that catches my module’s root exception.\nexcept my_module.InvalidDensityError:\nexcept my_module.Error:\nlogging.exception('Bug in the calling code')\nFile \".../example.py\", line 3, in <module>\nFile \".../my_module.py\", line 12, in determine_weight\nfind bugs in an API module’s code.\nexceptions that I define within my module’s hierarchy, then all other \ntypes of exceptions raised by my module must be the ones that I didn’t \ners from bugs in my API module’s code.\nmodule’s implementation that needs to be fixed.\nexcept my_module.InvalidDensityError:\nexcept my_module.Error:\nlogging.exception('Bug in the calling code')\nlogging.exception('Bug in the API code!')\nFile \".../example.py\", line 3, in <module>\nFile \".../my_module.py\", line 14, in determine_weight\n# my_module.py\nItem 87: Define a Root Exception to Insulate Callers from APIs \nexcept my_module.NegativeDensityError:\nexcept my_module.InvalidDensityError:\nexcept my_module.Error:\nlogging.exception('Bug in the calling code')\nlogging.exception('Bug in the API code!')\n# my_module.py\nclass Error(Exception):\n\"\"\"Base-class for all exceptions raised by this module.\"\"\"\n✦ Defining root exceptions for modules allows API consumers to \n✦ Catching root exceptions can help you find bugs in code that \n✦ Catching the Python Exception base class can help you find bugs in \nexceptions in the future without breaking your API consumers.\ninterdependence between modules.\nThe problem is that the app module that contains the prefs object \nalso imports the dialog class in order to show the same dialog on pro-\nimport dialog\nIf I try to import the app module from my \nFile \".../main.py\", line 17, in <module>\nFile \".../app.py\", line 17, in <module>\nimport dialog\nFile \".../dialog.py\", line 23, in <module>\nAttributeError: partially initialized module 'app' has no \nWhen a module is imported, here’s what Python \n2. Loads the code from the module and ensures that it compiles\n5. Runs the code in the module object to define its contents\nmodule aren’t defined until the code for those attributes has executed \nBut the module can be loaded with the import state-\nIn the example above, the app module imports dialog before defin-\nThen, the dialog module imports app.\nmodule is empty (from step 4).\nboth app and dialog can import the same utility module and avoid \nimport the dialog module toward the bottom of the app module, after \nthe app module’s other contents have run, the AttributeError goes \nThis works because, when the dialog module is loaded late, its recur-\nThis makes your module’s dependencies clear to new readers of \nIt also ensures that any module you depend on is in scope \nand available to all the code in your module.\nchanges in the ordering of your code to break the module entirely.\nImport, Configure, Run\nI can have my modules only \nThen, I have each module provide a \nconfigure function that I call once all other modules have finished \nimporting.\nall modules have been imported (step 5 is complete), so all attributes \nHere, I redefine the dialog module to only access the prefs object \nI also redefine the app module to not run activities on import:\nimport dialog\nimport dialog\nis called a dynamic import because the module import happens while \nand initializing its modules.\nHere, I redefine the dialog module to use a dynamic import.\ndialog.show function imports the app module at runtime instead of \nthe dialog module importing app at initialization time:\nThe app module can now be the same as it was in the original exam-\nimport dialog\nchanges to the way the modules are defined and imported.\nmodule.\n✦ Circular dependencies happen when two modules must call into \nmutual dependencies into a separate module at the bottom of the \ndependency between modules while minimizing refactoring and \nFor example, say that I want to provide a module for calculating how ",
    "keywords": [
      "module",
      "API",
      "Exception",
      "exceptions",
      "Root Exception",
      "dialog",
      "Code",
      "API module",
      "API code",
      "dialog module",
      "app module",
      "import dialog",
      "density",
      "Item",
      "modules"
    ],
    "concepts": [
      "module",
      "modules",
      "exception",
      "exceptions",
      "imports",
      "imported",
      "importing",
      "classes",
      "configure",
      "configurations"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 442-451)",
    "start_page": 442,
    "end_page": 451,
    "summary": "def convert(value, units):\ndef localize(value, units):\ndistance_units='miles'):\nIt seems like requiring units to be specified for this function is a much \nFor this purpose, Python provides the built-in warnings module.\nUsing warnings is a programmatic way to inform other programmers \nException to Insulate Callers from APIs”), warnings are all about \nI can modify print_distance to issue warnings when the optional \nimport warnings\ntime_units=None,\ndistance_units=None):\nwarnings.warn(\n'speed_units required', DeprecationWarning)\nif time_units is None:\nwarnings.warn(\n'time_units required', DeprecationWarning)\nif distance_units is None:\nwarnings.warn(\n'distance_units required', DeprecationWarning)\nI can verify that this code issues a warning by calling the function \nput from the warnings module:\nItem 89: Consider warnings to Refactor and Migrate Usage \n.../example.py:97: DeprecationWarning: distance_units required\nwarnings.warn(\nAdding warnings to this function required quite a lot of repetitive boil-\nAlso, the warning message \nLuckily, the warnings.warn function supports the stacklevel param-\nas the cause of the warning.\nfunctions that can issue warnings on behalf of other code, reducing \nHere, I define a helper function that warns if an optional \ndef require(name, value, default):\nif value is not None:\nwarnings.warn(\ntime_units=None,\ndistance_units=None):\ndistance_units = require(\n.../example.py:174: DeprecationWarning: distance_units will be \nwhen a warning is encountered.\nOne option is to make all warnings \nbecome errors, which raises the warning as an exception instead of \nwarnings.simplefilter('error')\nwarnings.warn('This usage is deprecated',\n$ python -W error example_test.py \nwarnings.warn('This might raise an exception!')\nItem 89: Consider warnings to Refactor and Migrate Usage \nwarnings module to ignore the error by using the simplefilter and \nwarnings for all the details):\nwarnings.simplefilter('ignore')\nwarnings.warn('This will not be printed to stderr')\nwarnings to cause errors because they might crash the program at a \n'py.warnings' logger:\n'%(asctime)-15s WARNING] %(message)s')\nlogger = logging.getLogger('py.warnings')\nwarnings.resetwarnings()\nwarnings.simplefilter('default')\nwarnings.warn('This will go to the logs output')\n2019-06-11 19:48:19,132 WARNING] .../example.py:227: \nwarnings.warn('This will go to the logs output')\nUsing logging to capture warnings ensures that any error reporting \nof important warnings in production.\nAPI library maintainers should also write unit tests to verify that \nHere, I use the warnings.catch_warnings func-\nOnce I’ve collected the warning messages, I can verify that their num-\nassert len(found_warnings) == 1\nassert str(single_warning.message) == (\nassert single_warning.category == DeprecationWarning\n✦ The warnings module can be used to notify callers of your API about \n✦ Raise warnings as errors by using the -W error command-line argu-\n✦ In production, you can replicate warnings into the logging module \nwarnings at runtime.\n✦ It’s useful to write tests for the warnings that your code generates to \nItem 90: Consider Static Analysis via typing to Obviate Bugs \nThe benefit of adding type information to a Python program is that \ntions of static analysis tools for Python that use typing.\n(such as name: type).\nReturn value types are specified with -> type \nItem 90: Consider Static Analysis via typing to Obviate Bugs \n.../example.py:6: error: Name 'value' is not defined\nItem 90: Consider Static Analysis via typing to Obviate Bugs ",
    "keywords": [
      "units",
      "distance",
      "warnings",
      "speed",
      "Item",
      "Python",
      "norm",
      "time",
      "Static Analysis",
      "type",
      "function",
      "warning",
      "duration",
      "code",
      "warnings module"
    ],
    "concepts": [
      "warnings",
      "warning",
      "warns",
      "typing",
      "type",
      "types",
      "value",
      "values",
      "python",
      "errors"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 452-461)",
    "start_page": 452,
    "end_page": 461,
    "summary": "87–88\nin comprehensions, 112–114\n__call__ method, 154–155\n__init__ method, 160–164\nsubclasses), 73–75\ndescriptors versus, 190–195\n186–189\nattributes, 214–218\ndynamic default values, 93–96\niterating over, 116–121\nin comprehensions, 110–114\n87–88\nannotating, 214–218\n181–185\n6–7\navoiding, 289–292\nthreads, 230–235\n413–418\n379–384\n145–148\nbytearray built-in type, 346–351\nversus, 5–10\n48–52\ninstances, 5–10\n226–230\n413–418\nbuilt-in types versus, 145–148\ndecorators, 218–224\ndocumentation, 398–399\n151–155\n160–164\n169–174\n413–418\n415–416\n389–390\ndocumentation, 396–401\nfrom, 174–178\n110–114\n121–122\n107–109\n109–110\n252–256\noutput), 266–271\nusing Queue class for, 257–263\n264–266\nwith threads, 230–235\nwhen to use, 248–252\n292–297\n390–396\nmodule, 174–178\n282–288\n11–21\n174–178\n379–384\n319–322\nclass decorators, 218–224\ndynamic, 93–96\n315–316\n__missing__ method, 73–75\nmethods, 70–72\nexpressions, 65–70\nversus, 70–72\ninjecting, 378–379\nconfiguring, 406–408\ndecorator, 190–195\nmodule, 312–319\n207–208\n__missing__ method, 73–75\nmethods, 70–72\nexpressions, 65–70\n13–15\n108–109\n93–96\nwriting, 396–401\nfor classes, 398–399\nfor functions, 399–400\nfor modules, 397–398\n400–401\n375–379\nbuilt-in function versus, 28–30\n299–304\nversus, 80–82\nhelper functions versus, 21–24\nwith Queue class, 257–263\n264–265\nwith Queue class, 257–263\n264–265\n326–334\ninstances, 9–10\n107–109\nwith statements versus, 304–308\n326–334\n32–35\n11–21\n15–19\n19–21\n11–15\nstr.format method versus, 15–19\n83–86\ndecorators, 101–104\ndocumentation, 399–400\n93–96\nexceptions versus, 80–82\n96–101\nmultiple return values, 77–80\n86–89\n123–126\n155–160\nkeys, 65–70\n181–185\nversus, 21–24\nimporting modules, 5, 414–415\nunpacking versus, 24–28\n174–178\n207–208\ndependencies, 378–379\n58–65\nloop, 289–292\nusing threads for, 230–235\nbuilt-in module, 271–282\n121–122\nas function arguments, 116–121\n139–140",
    "keywords": [
      "versus",
      "type",
      "built-in module",
      "method",
      "Real",
      "module",
      "built-in",
      "type hints",
      "method versus",
      "built-in function",
      "arguments",
      "built-in types versus",
      "versus str instances",
      "type annotations",
      "bytes versus str"
    ],
    "concepts": [
      "type",
      "types",
      "method",
      "methods",
      "classes",
      "modules",
      "built",
      "function",
      "functions",
      "argument"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 462-467)",
    "start_page": 462,
    "end_page": 467,
    "summary": "140–141\n137–138\nmethods, 70–72\n384–387\n121–122\nversus, 114–116\n48–52\n334–336\n235–238\n107–109\nmethods, 70–72\n375–379\n415–416\n401–406\n77–80\n292–297\n248–252\n235–238\nversus, 145–148\n93–96\n155–160\n322–326\n401–406\n401–406\nmodule, 292–297\n226–230\n13–15\n326–334\n322–326\n238–247\n228–229\n257–263\n86–89\n354–357\nversus, 169–174\n326–334\nversus, 169–174\n389–390\nPython 2, 1–2\nPython 3, 1–2\n389–390\n238–247\n28–30\nto classes, 148–151\n354–357\n114–116\nexceptions versus, 80–82\n190–195\n304–308\n299–304\n304–308\nclass, 319–322\n48–52\nclasses, 168–169\n312–319\n68–70\n181–185\n365–367\n248–252\n48–52\n58–65\n334–336\nversus, 5–10\n11–21\n19–21\n11–15\n109–110\n226–230\n365–367\n357–365\n375–379\n357–365\n282–288\n264–266\n132–136\n384–387\n24–28\n55–56\n6–7\n32–35\nfor classes, 398–399\nfor modules, 397–398",
    "keywords": [
      "built-in module",
      "versus",
      "built-in",
      "built-in function",
      "method",
      "module",
      "method versus",
      "built-in function versus",
      "versus setdefault methods",
      "Python Package Index",
      "Index itertools.dropwhile method",
      "zip built-in function",
      "pickle built-in module",
      "attributes versus",
      "function"
    ],
    "concepts": [
      "classes",
      "method",
      "methods",
      "module",
      "modules",
      "built",
      "functions",
      "function",
      "index",
      "indexing"
    ]
  }
]