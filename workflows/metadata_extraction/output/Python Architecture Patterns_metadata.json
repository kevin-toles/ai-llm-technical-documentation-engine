[
  {
    "chapter_number": 3,
    "title": "[ 77 ]",
    "start_page": 96,
    "end_page": 131,
    "summary": "database complexity have made this role less common, though it's still in use by \nRelational databases: The default standard in databases.\nNon-relational databases: New alternatives to the traditional databases.\nRelational databases\ntalking about databases.\nThe relational model for databases was developed in the \n1970s, and it's based on creating a series of tables that can be related to each other.\nEach defined table has a number of fields or columns that are fixed and data is \ndatabase.\nData Modeling\nThe primary key is used to reference that record, when necessary, in other tables.\nThis creates the relation aspect of the database.\nreference to another table, this is called a foreign key.\nwhich requires an intermediary table to cross over the data.\ntable, what the fields and types of each are, as well as the relations between them.\ndatabase not being available for some time, or, in the worst-case scenario, data can \nA query can also be executed that searches for data fulfilling certain conditions.\nVirtually all relational databases are interacted with using Structured Query \nhow to query the database and how to add or change data contained there.\nRelations in relational databases are really constraints.\nRelational databases come from a strict mathematical background, \nhow can be different in different databases.\nUsing a specific relational database and \nWhile relational databases are very mature and flexible and are used in very \nRelational databases are thought to be a \nscalability of relational databases later in this chapter.\nNon-relational databases\nNon-relational databases are a diverse group of DBMSes that do not fit into the \nthat's different from the final database that will be in place once the \nto use specific characteristics for a particular database, making it \nNon-relational databases are also called NoSQL, emphasizing \nData Modeling\nWhile there have been non-relational databases even before the introduction \nof relational databases and alongside them, since the 2000s, there has been an \nMost of them aim to address the two main weak spots in relational databases, \nKey-value stores\nGraph databases\nKey-value stores\nKey-value stores are arguably the simplest of all databases in terms of functionality.\nThey define a single key that stores a value.\nno way of querying keys in the system; instead, they need to be an input to any \nare normally based on this kind of data store.\nbetween a cache and a database.\ndata already calculated to speed up its retrieval, while a database \nstores raw data.\nfrom a different system, but if it's not in the database, either the \ndata is not stored or there has been a big problem.\nas the multiple copies need to be compared to detect data corruption.\nSome examples of key-value databases are Riak and Redis (if used with durability \n\"record\" in relational databases.\nin subfields, something that relational databases normally don't do, relying instead \non creating a relationship and storing that data in a different table.\nSo, in our case, we could retrieve the key (ID) ABCDEFG, like in a key-value store; \nData Modeling\ntheir ID, creating a reference, but normally these databases don't allow you to create \nWide-column databases are structured with their data separated by columns.\nrelate a record in one table with another.\nThey are a bit more capable of being queried than pure key-value stores but require \nSome examples of wide-column databases are Apache Cassandra \nNormally, columns are related and can only be queried in a \nGraph databases\nWhile the previous non-relational databases are based on giving up the ability to \nThe query capabilities of graph databases are aimed at retrieving information based \nrelational database (obtain the suppliers of the company and their countries), but \nFigure 3.1: Example of data that is typical of graph databases\nSmall databases\nData Modeling\nSQL database, but it's embedded into the system, without requiring external calls.\nThe database is stored in a binary file.\n>>> cur.execute('''CREATE TABLE pens (id INTEGER PRIMARY KEY DESC, \nconnect to databases.\nIt aims to standardize access to different database backends.\nDatabase transactions\nStoring data can be a complex operation internally for a database.\nit can include changing the data in a single place, but there are operations that can \nHow broad and possible these operations are highly depends on the database, \nbut they are very similar to relational databases.\nThis characteristic can become a strong requirement for the database in some \nData Modeling\nIt means that the data is \nMost relational databases have the concept of starting a transaction, performing \nThe need for durability means that data needs to be stored on disk \nway that it can't see new updates, which may require temporary data to be stored \nVirtually all relational databases are fully ACID compliant, and that has become a \nScaling the database with multiple servers or nodes with these properties proves \nMaintaining full ACID transactions in databases with \ninside the transaction, data can still be queried and be validated \nDistributed relational databases\nAs we've discussed before, relational databases weren't designed with scalability \ntransactions, but their preferred way of operating is through a single server.\nrelational databases.\nIt is worth noting that a database server can grow vertically, which \ndatabases.\ndistributed database.\nData Modeling\nBecause the replicas contain the whole database, and the only \nThis system is natively supported by most relational databases, especially the most \nsolved either by keeping the data temporarily, avoiding the need for the query, or by \ndata is consistent.\ndatabase.\nData Modeling\nNote that this way of structuring the database may require adapting the application \nlevel to be aware of all the changes and access to different database servers.\nqueries that don't require up-to-date information, in cases where perhaps a daily \nThis means dividing the data \ninto different databases according to a specific key, so all related data can go to the \nThe partition key is called the shard key, and based on its value, each row will be \nis horizontal, separating a single table into different servers.\nFigure 3.4: Shard keys\nAny query needs to be able to determine what the proper shard is to be applied to.\nAny query that affects two or more shards may be impossible to do or can only be \ndata is naturally partitioned, and very bad when queries affecting multiple shards \npartitions between data, so performing cross-shard queries is not required.\nexample, if the data of a user is independent of the rest, which may happen with a \nphoto-sharing application, the user identifier could be a good shard key.\nAnother important quality is that the shard to address the query needs to be \ndetermined based on the shard key.\nThat means that every query needs to have \nthe shard key available.\nThis means that the shard key should be an input of every \nSome NoSQL databases allow native sharding that will take care of \nwhich is even capable of running queries in multiple shards in a \nData Modeling\nAnother property of the shard key is that the data should be ideally portioned in a \nenough distributing of the queries, and having one shard being the bottleneck.\nOn pure shards, the data is all partitioned in shards and the shard key is an input of \nTo ensure that the shards are balanced, each key is hashed in a way that is equally \nIf we have 8 shards, we determine which shard the data is \nThis strategy is only possible if the shard key is always available as input for every \nChanging the number of shards is not an easy task, as the destination for each key is \nWe can create \"virtual shards\" that point to the same server.\n100 shards, and use two servers, initially the virtual shard distribution will be like \nIf the number of servers needs to be increased, the virtual shard structure will change \nThis change to the specific server that corresponds to each shard may require some \ncode change, but it's easier to handle as the shard key calculation won't change.\nEach of the operations requires changing the location of data based on the shard key.\nThis is a costly operation, especially if a lot of data needs to be exchanged.\nis required to determine the shard key.\nlogging in if the shard key is the user ID.\nData Modeling\nquery to the shard key.\nFigure 3.5: External tables to translate the input of shard keys\nthat's not directly the shard key, and that it requires keeping all the information of \nall shards in a single database.\nThis strategy can be used as well to store, directly, what shard key goes to what \nshard, and perform a query instead of a direct operation, as we saw above.\nFigure 3.6: Storing shard keys to shards\nThis has the inconvenience that determining the shard based on the key requires a \nquery in a database, especially with a big database.\nshard of the data in a consistent way, which can be used to adapt the number of \nIf the specific shard, not only the shard key, is stored in this translation table, the \nassignment of the shard to the key can be changed one by one, and in a continuous \n1.\t Shard key X is assigned to server A in the reference table.\n2.\t Data from server A for shard key X is copied to server B.\ninvolving shard key X is directed to server B yet.\n3.\t Once all the data is copied, the entry for the reference table for shard key X is \n4.\t All queries for shard key X are directed to server B.\n5.\t Data from shard key X in server A can be cleaned.\nthe reference table that can stop or delay the writing of data while the operation \nTable sharding\nAn alternative to sharding by shard key, for smaller clusters, is to separate tables or \nThis means that any query in table X is directed to a specific \nfor unrelated tables, as it's not possible to perform joins between tables in different \na database cluster needs to downscale, as most applications will \nData Modeling\nbetween one or two tables and the rest, for example, if one table stores logs that \nThe data gets stored in multiple servers, so massive amounts of data can be \nstored, without limiting the data that can be stored in a single server\nNative support for sharding is available only in a small number of databases, \nlike MongoDB, but relational databases don't have the feature implemented \nSome queries will be impossible or almost impossible to do once the data is \nThe shard key needs to be selected carefully, as it will have \nproperties, as some operations may need to involve more than one shard.\nsharded database is less flexible.\nAs we've seen, designing, operating, and maintaining a sharded database only \nmakes sense for very big systems, when the number of actions in the database \nFor databases that need to define a schema, the specific design to use is something \nThe best way to start the design of a schema is to draw the different tables, fields, \nand their relationships, if there are foreign keys pointing to other tables.\nThis section will talk specifically about relational databases, as they \nOther databases are \nof a database.\nData Modeling\nEach of the tables can have foreign key relationships with others of different kinds: \nA simple foreign key relationship works in this case, as the Books table will \nFigure 3.8: The key in the first table references multiple rows in the second\nmodeled as adding all the information into a single table.\nUnder a relational data structure, there's a need for \nIn most cases, the types of fields to store for each of the tables are straightforward, \nThis extra table may include more information, for example, how \nOutside of the relational data world, sometimes there's not such \nrelational databases now allow more flexibility in allowing fields \nData Modeling\nThe internal database representation doesn't need to be the same as what's \nFor example, the time stored in the database should \nAt the same time, it's better to represent the data naturally.\nrestrictive, now the performance improvement is negligible, and storing data \nthe database and using the default time zone of the server \ntimes are stored in the database in UTC.\nAs we've seen, in relational databases, a key concept is the foreign key one.\nData \nThis split in data means that a set of \nlimited data can, instead of being stored in a single table, be split in two.\nFor example, let's take a look at this table, initially with the field House as a string:\nEddard Stark\nJaime Lannister\nTo ensure that the data is consistent and there are no errors, the field House can \nThis means that it's stored in a different table, and a FOREIGN KEY \nEddard Stark\nJaime Lannister\nData Modeling\nThis way of operating normalizes the data.\nensures that the data is very consistent and there are no problems, like introducing \nThe data is also more \nIn the first Characters table, we could generate our query in this way:\nJOIN from different tables if we add, for example, a PreferredWeapon field and a \nIt will also take longer to insert and delete data, as more checks need to be \nNormalized data is also difficult to shard.\nAnother problem is that the database is more difficult to read and operate.\nAlso, complex JOIN queries need to be performed for simple \nTo improve the clarity of the database, natural keys can be used to simplify them, \ndescribing the data in this way.\nuse the Name field on the Houses table.\nEddard Stark\nJaime Lannister\nWe recover our original query, even if the data is \nWhere normalizing data \nsplits it into different tables to ensure that all the data is consistent, denormalizing \nData Modeling\nFollowing our example above, we want to replace a JOIN query like this:\nEddard Stark\nJaime Lannister\nFor a query similar to this, querying a single table, use something like this:\nTo do so, the data needs to be structured in a single table.\nEddard Stark\nJaime Lannister\nconcerns for sharding, as now the table can be partitioned on whatever shard key \nunder NoSQL databases, which remove the capability to perform JOIN queries.\nexample, document databases embed data as subfields into a bigger entity.\nData indexing\nproper data from a big table full of information requires performing more internal \nThis process can be greatly speeded up by organizing the data smartly in a way that \nThis leads to creating indexes that allow you to locate data very \nsorted data structure that points to one or more fields of each of the records of the \ndatabase.\nThis index structure is always kept sorted as data in the table changes.\nFor example, a short table may contain this information\nWhile we will describe data indexing in relation to relational \ndatabases.\nData Modeling\nIn the absence of an index, to query what entry has the highest height, the database \nBy creating an index for the Height field, a data structure that is always sorted is kept \nOnce again, if this index doesn't exist, the only way to find these queries is by \nindices sort the data based on the ordered combination of both fields, for example, \nQuerying in composite indices for only the first part of the index is possible.\nexample, an index of (Height, Name) will always work for querying Height.\nthe database; the SQL query doesn't change at all.\ndetermine how to retrieve the data, and what indexes to use, if any.\nThe primary key of a table is always indexed, as it needs to be a \nIndexes greatly speed up the queries that use them, especially for big tables with \nindexes in a single table will use more space, both in the hard drive and in \nEach time the table changes, all indices in the table need to be adjusted to be \na full table scan and an indexed search is small if the number of rows is \nto perform a faster query by combining two indices as the data \nData Modeling\nFor example, the Height index in this table has a cardinality of 4.\nImagine a table with a million rows indexed by a field that's the same in all of them.\nNow imagine that we make a query to find a single row in a different field that's not \nwill return every single row in the database.\nFigure 3.11: Returning every single row from a query using an unhelpful index\nthen we need to query them.\nData Modeling\nWe described the different kinds of databases, both relational and non-relational, and \none of the fundamental characteristics of relational databases, allows compliance \nAs some of the non-relational databases are aimed at dealing \nto scale up relational systems, as that kind of database was not initially designed to \nThe Data Layer\nhow that data is stored in storage.\nIn this chapter, we will describe how to create a software data layer that interacts \nwith storage to abstract the specifics of storing data.\nWe will also talk about how to make changes to the database as the application \nThe Data Layer\nAPI Design, the Model layer is the part that's intimately related with the data and \nfrom the database.\nThis layer needs to understand the way the data is stored \nThe next layer creates business logic and uses the internal data modeling \nIt's very common to deal with the data layer as a pure extension of the database \nwhich makes good business sense, and the database models, which contain the ",
    "keywords": [
      "data",
      "Data Modeling",
      "shard key",
      "Relational databases",
      "database",
      "databases",
      "table",
      "shard",
      "key",
      "Server",
      "tables",
      "data modeling layer",
      "Data Layer",
      "query",
      "Chapter"
    ],
    "concepts": [
      "database",
      "databases",
      "data",
      "like",
      "tables",
      "table",
      "sharding",
      "shard",
      "shards",
      "sharded"
    ]
  },
  {
    "chapter_number": 4,
    "title": "[ 113 ]",
    "start_page": 132,
    "end_page": 167,
    "summary": "into accesses to the database and present a consistent interface that replicates the \nFor a lot of different concepts, the Model works purely as a replication of the \nschema of the database.\nThis way, if there's a table, it gets translated into a Model \nA Model can use multiple tables \nFor example, the example of the user above has the following fields in the database \nWe will use a relational database using SQL as our default \nexample, as it is the most common kind of database.\nthe database.\nThis Model transforms the actions from the raw database access to a fully defined \nobject that abstracts the access to the database.\nan object to a table or collection, is called Object-Relational Mapping (ORM).\ncollections or tables in a database, and generating objects in an OOP environment.\nby the ORM tool and will connect to the database.\nFor example, a low-level access for a query in the \"pens\" table could look like this:\n>>> cur.execute('''CREATE TABLE pens (id INTEGER PRIMARY KEY DESC, \naway the differences between different databases, and allows us to retrieve the \nUsing an ORM, like the one available in the Django framework, instead of creating a \nCREATE TABLE statement, we describe the table in code as a class:\nThe operation that in raw SQL is an INSERT is to create a new object and then use \nthe .save() method to persist the data into the database.\nFor example, this code:\nTo connect Python and an SQL database, the most common \nUsing an ORM detaches the database from the code\nIt removes some problems with composing SQL queries, like security issues\nIndependence from the database\nFirst of all, using an ORM detaches the database usage from the code.\nthat a specific database can be changed, and the code will run unchanged.\nbe useful sometimes to run code in different environments or to quickly change to \nuse a different database.\nThis approach is not problem-free, as some options may be available in one database \nused in the database backend) to work with the data.\nwork with the code, as developers that are not familiar with SQL can understand the \nORM code faster.\nanother database like MySQL or PostgreSQL once the code is \nthe database usage is called the Repository pattern.\nthe database.\ncomplicated queries that require you to JOIN multiple tables.\nYou are still required to understand the details of the database to \nbe able to create efficient code.\n>>> query = 'SELECT * FROM Pens WHERE color IN (' + color_list + ')'\nThis code works for values of colors that contain values but will produce an error if \nTo avoid this problem, any input that may be used as part of a SQL query (or any \nframework than creating a bespoke SQL query.\nORM frameworks will also have an impact in terms of performance, as they require \ntime to compose the proper SQL query, encode and decode data, and do other \nthere's a good chance that, at some point, a specific, tailored SQL query will need to \nIf using SQL is the way to go, a common approach is to use prepared statements, \nFor example, the following code will work in a similar way \ncreated from the ORM are good for straightforward queries but can \nAnother limit of ORM frameworks is that SQL access may allow \noperations that are not possible in the ORM interface.\ndatabase in use.\nThis code will safely replace the color with the proper input, encoded in a safe way.\nNew columns can be added to a table, so retrieving all columns may change the \nAn ORM will handle this case automatically, but using raw SQL requires you to take \nproblems when making changes in the schema later on.\nstatements, in certain cases dynamic queries are still very useful.\nEven if the selected way to access the database is raw SQL statements, it's good to \nlayer should be responsible for storing data, in the proper format in the database, \n(a query stored in the database itself beforehand and called with \nand the database table is direct, for example, a user object, this is fine.\ndatabase and objects.\nThis creates a representation of the data itself, in the way it's \nstored in the database.\nIn most situations, the design of the database will be tightly related to the business \nthe data, as it's stored inside the database.\nmultiple database operations.\nIf the database allows for it, all the operations in a unit of work \ntightly associated with transactions and relational databases and \nnormally is not used in databases that are not capable of creating \nTo do so, each Account should have debit and credit internal values that change \ncorrespond to tables in the database.\nwill generate multiple accesses to the database, as we'll see later.\n''' This is the model related to a DB table '''\n''' This models stores the operations '''\nself.internal, _ = InternalAccount.objects.get_or_create(\ndatabase but keeps a relation to the InternalAccount using the unique reference of \nWhenever there's an operation, it requires another account, and then a new Log is \nclass than the ORM models.\nthat the ORM model classes are the Repositories classes and the \nAccount model is the Unit of Work class.\nother ORMs can work differently.\ndatabase implementation and store any relevant business logic there.\nCQRS, using different models for read and \nSometimes a simple CRUD model for the database is not descriptive of how the data \nA possibility is that sending data and reading it happen at different ends of a \nthis data is processed or aggregated in a different database.\nFinally, a relational database \nexternal process to the relational database, where it is then represented with a \nrelational model in an ORM way, and then back to the Domain Model.\nmeaning that the Command (write operations) and Query (read operations) are \nThe Domain Model may require different methods to deal with the information.\nIn certain cases, the models and data may be quite different for read and write.\nIn our example, that process would be how the data is stored in the \ndatabase, including the amount paid.\n''' This is the usual ORM model '''\nSale_id = models.IntegerField(unique=True)\nThis is the exposed Domain Model that handled the operations \n# Create a new sale\nDatabase migrations\nWhile the pace of changes in the database is typically not as fast as other areas, there \nData changes are roughly categorized into two different kinds:\nFormat or schema changes: New elements, like fields or tables, to be added \nor removed; or changes in the format of some fields.\nData changes: Requiring changing the data itself, without modifying the \nFor example, normalizing an address field including the zip code, \nThe basic principle related to changes in the database is backward compatibility.\nThis means that any single change in the database needs to work without any change \nIf the changes in \nthe database require a change in the code to understand it, the service will have to \nThis is because you can't apply both changes at the same time, and if \nDepending on the database, there are different approaches to data changes.\nFor relational databases, given that they require a fixed structure to be defined, \nany change in the schema needs to be applied to the whole database as a single \nFor other databases that don't force defining a schema, there are ways of updating \nthe database in a more iterative way.\nRelational schema changes\nIn relational databases, each individual schema change is applied as a SQL statement \nThe schema change, called a migration, can happen \nMigrations are SQL commands that perform changes in an atomic way.\ninvolve changing the format of tables in the database, but also more operations like \nsupport to create migrations and perform these operations natively.\nFor example, Django will automatically create a migration file by running the \ndetect any change in the models and make the proper changes.\n''' This is the model related to a DB table '''\nthe migration.\nMigrations for 'example':\nchanges creating the proper migration files.\nRunning migrations:\nFor more details about Django migrations, check the documentation at https://\nChanging the database without interruption\nThe process to migrate the data, then, needs to happen in the following order:\n1.\t The old code and the old database schema are in place.\n2.\t The database applies a migration that's backward compatible with the old \nAs the database can apply this change while in operation, the service is \n3.\t The new code taking advantage of the new schema is deployed.\nMost of the usual changes are relatively simple, like adding a new table or column \nThe old code won't make use of \nBut other migrations can be more \nDjango will store in the database the status of the applied \nKeep in mind that, to properly use migrations through Django \nIf you need to apply changes that \ncan't be replicated automatically with a change in the model, like a \ndata migration, you can create an empty migration and fill it with \nof the automatically created Django migrations.\nBut obviously, a change that migrates the code from an integer to a string is going to \n1.\t The old code and the old database schema are in place.\n2.\t The database applies a migration adding a new column, Field2.\nmigration, the value from Field1 is translated into a string and copied.\n4.\t A new migration removing Field1, now unused, can be applied.\n5.\t The new code that is only aware of Field2 can now be deployed safely.\na further migration is deployed changing the name from Field2 to Field1.\ncase, the new code needs to be prepared in advance to use Field2 or, if not present, \nthe application of the migration and the new code, the \ncode will need to check if the column Field1 exists, and if \nit does and has a different value than Field2, update the \napplied – if the value in Field1 is different from the one \nA new deployment could be done after that to use only Field1 again:\nperform the migration with the format change in Field1, and then start the new \nproblem is testing the migration in a database much smaller than the production one.\nDepending on the size of the data, a complex migration may \nBut another problem is the risk of introducing a step, at the start of the new code, \nWith this process, after the migration is applied, there's no possibility of using the \nIf there's a bug in the new code, it needs to be fixed and a newer version \nrelated to distributed databases.\nFor example, a sharded database will need to apply \nIt's possible that some migrations may need to be \neven possible in some cases that the database will require more \nThis way migrations need to be applied very carefully and by \nData migrations\nData migrations are changes in the database that don't change the format but change \nThese migrations are produced normally either to correct some error in the data, like \nIn cases like the scale change described above, the process may require more steps \n1.\t Create a migration to set a new column, scale, to all rows, with a default \nAny new row introduced by the old code will automatically \n2.\t Deploy a new version of the code able to work with both inches and \n3.\t Set up another migration to change the value of measurement.\n4.\t Now all the values in the database are in centimeters.\n5.\t Optionally, clean up by deploying a new version of the code that doesn't \nAfter that, a new migration removing the column can also be run.\nAs we discussed before, the key element is to deploy code that's able to work with \nboth database values, the old and the new, and understand them.\nThis means that, instead of an all-or-nothing change as for relational databases, a \nhere, the code will have to perform the changes over time.\n1.\t The old code and the old database schema are in place.\n2.\t Each of the documents in the database has a version field.\n3.\t The new code contains a Model layer with the migration instructions from \nenough time, it will migrate, document by document, the whole database.\nconcatenated, migrating an old document from different versions \nversion 0 and be migrated to version 1, now including the field.\nmigrate from version 1 to 2, version 2 to 3, etc, if still present in the code.\nupdating and saving it until the whole database is migrated.\nAlso note that, if this functionality is encapsulated in the internal database access \nWhile there's still data in the database with the old version, the code needs to be \nto migrate all the data in the background, as it can be done document to document, \nmigration is done, the code can be refactored and cleaned to remove the handling of \nThis process is similar to the one described for data migration, \nthough databases enforcing schemas need to perform migrations \nIn a schema-less database, the format can be \nchanged at the same time as the value.\nIn the same way, a pure data change, like the example seen \nthe need for a migration, slowly changing the database as we \nDoing it with a migration ensures a cleaner change, \nDealing with legacy databases\nORM frameworks can generate the proper SQL commands to create the database \nWhen designing and implementing a database from scratch, that means \nthat we can create the ORM Model in code and the ORM framework will make the \nBut sometimes, we need to work with an existing database that was created \nthis case, we need a way to detect the existing schema and use it.\nfields and any new changes.\nIn this scenario, we need to create a Model that \nDetecting a schema from a database\nto automatically detect the schema of the database and work with it.\nThis way of describing the schema in code is called declarative.\naccesses to a relational database.\nqueries and create precise mappings.\nTo automatically detect a database, you can automatically detect the tables and \n# Read the database and detect it\n>>> engine = create_engine(\"sqlite:///database.db\")\n# Create a session to query \n# Create a select query\n>>> query = select(Pens).where(Pens.color=='blue')\nNote how the described names for the table pens and columns id, name, and color \nThis creates a models.py file that contains the interpretation of the database based on \ncontrol over the code, require a different approach.\nIn other situations, there's a legacy database that was created by a method that \ncode may use the database, but we want to migrate the code so we are up-to-date \nand formats are, and on another, allow the ORM to make controlled changes to the \nWe will see the latter as migrations.\nThe challenge in this case is to create a bunch of Models in the ORM framework that \nare up-to-date with the definition of the database.\nThere can be database features that are not exactly translated by the ORM.\nFor example, ORM frameworks don't deal with stored procedures natively.\nIf the database has stored procedures, they need to be either removed or \nStored procedures are code functions inside the database \nlike inserting a new row or changing a column.\nwithout the capacity to change the data that is stored.\nwhich may not be compatible with the already-existing database.\nconstraint can remain only in the database, but \"hidden\" in the ORM, but \nthe Django ORM, which may require you to create a new numeric column to \ncreated schema based on the code Models in the ORM framework can be produced \n1.\t Create a dump of the database schema.\n2.\t Create the proper Model files.\n3.\t Create a single migration with all the required changes for the database.\nThis migration is created normally, with makemigrations.\nthat will be applied by the migration.\nThis generates a database schema that \nNote that the inspectdb creates the Models with their \nmetadata set to not track changes in the database.\nchanges as migrations.\nchanged.\nAfter that, changes can be applied normally by changing the Models and then \nare, to orient the abstraction of storing data and use rich objects that follow business \nWe also described ORM frameworks and how they can be useful to \nWe described different useful techniques for the code to \ninteract with the database, like the Unit of Work pattern, which is related to the \nWe also discussed how to deal with database changes, both with explicit migrations \nthat change the schema and with more soft changes that migrate the data as the \nFinally, we described different methods to deal with legacy databases, and how to \ncreate models to create a proper software abstraction when there's no control over \nChanges to \nthe external database to solve incompatibility problems may be \ndifferences that are not creating any problems.\ntools and techniques to use in both cases, including migrating from one to the other.",
    "keywords": [
      "database",
      "ORM",
      "Data Layer",
      "SQL",
      "Data",
      "code",
      "ORM frameworks",
      "Django ORM",
      "ORM model",
      "create",
      "Model",
      "migration",
      "SQL statements",
      "query",
      "SQL query"
    ],
    "concepts": [
      "migrations",
      "migration",
      "migrate",
      "migrates",
      "migrating",
      "migrated",
      "database",
      "databases",
      "query",
      "queries"
    ]
  },
  {
    "chapter_number": 5,
    "title": "[ 149 ]",
    "start_page": 168,
    "end_page": 193,
    "summary": "Continuous Integration, or CI, is the practice of automating the running of tests when \nback in 1991, it could be understood as running a \"nightly build\", as running the tests \nof tests with each new code submission.\nof running the tests automatically and on every single test greatly helps to ensure \nThis is also dependent on the quality of the tests that are run, so in order to have a \nrepository for code, so tests are launched as soon as new changes are forthcoming \na hook that automatically runs the tests.\nother services that integrate with them and allow operations to be run automatically \nthrough some configuration.\nadding a special file to configure the service, thereby simplifying the setup and run \nmind that the tests will run in the background, automatically, \nsoftware into a testable state and then run the tests.\nA typical pipeline to run tests could do the following:\n1.\t As it starts in a new, empty environment, install the required dependency \ntools to run the tests; for example, a particular version of Python and a \n3.\t Run static analysis tools like flake8 to detect style problems.\n4.\t Run the unit tests.\nrun at the same time as there is no dependency between the cases, whereas step 2 \nexample, elements like databases or other dependencies, if required for tests, need to \nto be done during the setting up of the environment.\nOne possibility is to use Docker to build one or more containers that will standardize \nthe process and make all dependencies explicit in the building process.\npipeline, as they aim to have code that is easy to build, to be deployed either for \ntesting or operating and configuration.\nFor example, instead of having two web servers, \nStoring information intra-request, for example, composing a file \nUsing an external service allows all nodes to access the cache and \nelements, such as databases and caches, require a different way of operating, as they \nConfiguration\nOne of the basic ideas of the Twelve-Factor App is that the code is unique, but it \ndeployed in different environments.\nThe use of different environments allows testing environments to be set up, where \ntests can be run without affecting production data.\nAWS S3 is a web service that allows a file to be stored and retrieved \nThis service is very useful for working with files in a scalable way, \nand it allows configuration in such a way that access for reading \nOperational configuration: These are parameters that connect different parts \nthe application doesn't change; for example, a change to log only WARNING \nFeature configuration: These parameters change external behavior, enabling \nCreating a comprehensive and easy-to-use local environment is \nsingle service or process, such as a web server, it is relatively easy \nWhile the operational configuration parameters are tightly related to a single \nenvironment and require parameters that are correct for the environment, the feature \nconfiguration normally moves between the local development to test it until it is \nTraditionally, the configuration has been stored in one or more files, typically \none called staging.cnf that are attached to the code base, and depending on the \nenvironment, one or the other is used.\nMaking a configuration change is, de facto, a code change.\nWhen the number of environments grows, the number of files grows at the \nStoring all the data in the code base makes it more difficult to create \n\"business release\" at a particular time, deploying new code into a \ncan use these credentials to access all environments, including production.\nfiles inside the code base.\nspecifically in the Configuration factor.\n1.\t Code base.\nconfiguration.\n5.\t Build, release, run.\n6.\t Processes.\nExecute the app as a stateless process.\nSet up the services as processes.\nRun one-off admin processes independently.\nCode base, Build, release, run, and Dev/prod parity work around the idea \nof generating a single application that runs in different environments, \nconfiguration and connectivity of different services\nLogs and Admin processes are practical ideas involved with monitoring and \none-off processes\nBuild once, run multiple times\nOne of the key concepts around the Twelve-Factor App is that it's easy to build and \ncode that's changed from one version to another, just configurable options.\nThe aim of the Code base factor is that all the software for an app is a single repo, with \nThis means that the code to deploy is always the same, and only the configuration \nThis allows easy testing of all the configuration changes and does not \nA single code base allows a strict differentiation of the stages in the Build, release, run \nenvironments.\nfor the Twelve-Factor App is to remove them, or at least make \nthem change based just on the configuration.\nconfiguration for the selected environment, and sets it ready for execution\nThe run stage finally executes the package in the selected environment\nBecause stages are strictly divided, it's not possible to change the configuration or the \nthat the run stage may need to be executed again in case there's a new server or the \nconfiguration in other factors.\nproduction one, as they use the same building stage, but with proper configuration \nThis factor also makes it possible to use the same (or as close as \npossible) backing services, like databases or queues, to ensure that local development \nenvironments that contain all the required dependencies.\nObtaining a fast and easy process to develop, build, and deploy is critical for \ncan then be separated by environment, something that makes \nsense, as some environments, like production, are more critical \nStoring the configuration as part of the code base \nconfigurations.\nPerforming tests after the build stage also ensures that the code \nDependencies and configurations\nThe Twelve-Factor App advocates the explicit definition of dependencies and \nThat is why, in the Config factor, it talks about storing all the configuration for the \nsystem in environment variables.\ncode, which allows retention of the strict differentiation that we talked about in the \nBuild, release, run factor and avoidance of the problems that we described previously \nin storing them in files inside the code base.\nenvironment is also easy.\nThis is preferred to other alternatives, such as setting different files into the code \nbase describing environments like staging or production, because they allow more \nchanging the code for environments that are not affected; for example, having to \nupdate the code base for a demo environment that is short-lived.\nConfiguration can be obtained in configuration files directly from the environment \nPARAMETER = os.environ['PATH']\nThis code will store in the constant PARAMETER the value of the PATH environment \nwork means that there are a limited number of environments and \nis storing it in a different place to the code base, managed only on \nKeep in mind that for local development, these environment \nTo allow for optional environment variables, and protect against them going \nPARAMETER = os.environ.get('MYENVVAR', 'DEFAULT VALUE')\nNUMBER_PARAMETER = int(os.environ['ENVINTEGERPARAMETER'])\nBOOL_PARAMETER = strtobool(os.environ['ENVBOOLPARAMETER'])\nenvironment variables need to be defined in your environment.\nYou can run Python, adding a local environment, by running $ \nThis makes configuration problems \nEnvironment variables also allow the injection of sensitive values such as secrets \nwithout storing them in the code base.\nAs part of this configuration, any backing services should be defined as well as \nenvironment variables.\nBacking services are external services that the app uses over \nconfiguration, can be changed based on the environment.\ndatabase, perform a new release with a configuration change, and the app will point \nThis can be done with no code changes.\nTo allow the concatenation of multiple applications, the Port binding factor ensures \nThis makes it easy to consider each app a backing service.\nSome applications require the combination of several processes working in \nFor example, it is typical for a web server for a Python application, such \nas Django, to use an application server like uWSGI to run it, and then a web server \ntightly as possible, to avoid the problem of different versions of dependencies being \nFor example, in a pip file, a dependency can be described in different ways:\nThe Processes factor talks about making sure that the run stage consists of starting \none or more processes.\nmeaning that all the data needs to be retrieved from an external backing service like \nThe processes \nshould be to take not more than a few seconds to have the process up and running.\nFor example, a file upload may use the local hard drive to store \nThe opposite is to allow the graceful shutdown of the process.\ndown the process.\nBecause the system is created through processes, based on that, we can scale out by \nProcesses are independent and can be run at the same time on \nWorking with Docker containers automatically uses this \nIf the process doesn't \nprocesses may be longer than for web servers.\nKeep in mind that the same application can use multiple processes that coordinate \namong them to handle different tasks and each process may have a different number \nthe optimal number may be to have a single nginx process for many more times the \noperation under a Twelve-Factor App. That allows the size of the entire operation to \nThe Twelve-factor App processes should also be run by some sort of operating \nprocesses remain running, even in the event of a crash, handle graceful manual \nThe traditional deployment process was to set up a physical server \nWith containers, this process is somehow reversed.\noptimization process is still required, with containers, it's more \nAllow time to perform tests to \nRestarting the processes automatically, combined with a quick start up time and \nLogs are text strings that provide visibility of the behavior of a running app.\ngenerated as the code is being executed, giving information on the different actions \nbut typically frameworks will automatically create logs based on common practices.\nFor example, any web-related software will log requests received, something like \nis mostly handling containers more than processes.\noperating system process manager, the work is performed by a \ncontainer orchestrator that ensures that the containers are running \nThe container will stop if the process is stopped.\nstatus code, but it can be configured to return extra information, such as the IP of the \nclient making the request, or the time that it took to process the request.\nApplication logs are also very useful.\nApplication logs are generated inside the code and can be used to communicate \nFor example, in Django, you can create logs this way:\nAccess logs are also generated by web servers including nginx \nThe Logs factor suggests that logs shouldn't be managed by the process itself.\nThe environment surrounding the process, like the operating system process \nproblem of requiring the logs to be rotated and ensure that there's enough space.\nThis also requires the different processes to coordinate in terms of having a similar \nThe Admin processes factor covers some processes that sometimes need to be run for \nenvironment as the regular processes, using the same code base and configuration.\nThese admin operations should be included as part of the code base to avoid \nIn traditional environments, it may be necessary to log in to a server through ssh to \nallow the execution of this process.\nIn container environments, a full container can be \nmay consist of running the build to execute migrations.\nTo run these admin commands in containers, the container image should be the same \none that runs the application, but called with a different command, so the code and \nenvironment are the same as in the running application.\nare oriented toward scalable services in the cloud, and containers help to create \nthat an error in a production environment can create a serious \nimage that then gets run works very well with the Build, release, run factor and with \nIncluding the build process as part of the \nrepository also helps in the implementation of the Code base factor.\nEach container also works as a Process, which allows scaling by creating multiple \nThe concept of containers makes them easy to start and stop, leaning into the \nas Kubernetes makes it easy to also set up the Backing services factor, and it's also \neasy to share services between specific ports in containers following the Port binding \nIn Docker and orchestrator tools like Kubernetes, it is very easy to set up different \nenvironments injecting environment variables, thereby fulfilling the Configuration \nThis environment configuration, as well as a description of the cluster, can \nbe stored in files, which allow multiple environments to be created easily.\nenvironment, with only small changes in its configuration.\nensuring that the different environments are kept up to date, as demanded by the \nSending information to standard output as per the Logs factor is also a great way \nto store logs as container tools will receive and deal with or redirect those logs \nwith a different command that runs the specific admin command.\nrecommendations for the Twelve-Factor App, as the tools work in the same direction.\nservices that need to be run in the cloud.\nCI is the practice of constantly validating any new code by running tests automatically \nquickly, although it requires discipline to properly add automated tests as new \nWe saw the challenges for configuration, something that the Twelve-Factor App \nBuild once, run multiple times, based on the idea of generating a single \npackage that runs in a different environment\nFinally, we spent some time talking about how the Twelve-Factor App ideas are very \nand concepts allow us to easily create Twelve-Factor Apps.\nA client sends a request to a remote server and the server processes it \nThis time difference makes it important to handle it properly.",
    "keywords": [
      "Twelve-Factor App Methodology",
      "Twelve-Factor App",
      "App Methodology",
      "App",
      "code",
      "system",
      "environment",
      "configuration",
      "run",
      "code base",
      "Twelve-Factor",
      "logs",
      "Processes",
      "Chapter",
      "process"
    ],
    "concepts": [
      "environment",
      "environments",
      "environ",
      "process",
      "processes",
      "processed",
      "different",
      "difference",
      "logging",
      "log"
    ]
  },
  {
    "chapter_number": 6,
    "title": "[ 175 ]",
    "start_page": 194,
    "end_page": 239,
    "summary": "Any request-response system should take extra \nAnother characteristic of the request-response pattern is that a server cannot call \nThe client is required to initiate the request, and \nthe server only needs to listen for new requests coming.\nsending of messages, is difficult to achieve with request-response.\nThe client can be improved to perform multiple requests at the \nThis can be done when the requests are independent \nTypically, a flow will be required, with some requests that can \nFor example, a common request to retrieve \na web page will make one request to retrieve the page and later \nWeb Server Structures\nA crude example of this is a message server implemented only in request-response.\nRequest any new message addressed to them\nA user needs to check periodically whether there are new messages available \na significant number of checks that return \"no new messages available.\" Even worse, \nEven with these limitations, request-response architecture is the basis of web services \nnew requests makes the architecture simple to implement and quick to evolve, and \nopen, allowing the server to notify the user of new information.\nThey both deviate from the request-response architecture.\nbase for the web server architecture:\nserver and web worker.\nFrom the point of view of an incoming request, a web request accesses the different \nWeb servers\nThe web server exposes the HTTP port, accepts incoming connections, and redirects \nThe web server \ncan directly serve a request, for example, by directly returning static files, permanent \nIf the request requires more computation, it will \nWeb Server Structures\nThe primary objective of the web server in the presented architecture is to work as a \nreverse proxy, accepting HTTP requests, stabilizing the input of data, and queuing \nthe incoming requests.\nmain/chapter_06_web_server/nginx_example.conf.\nserver {\nThe directive server opens and closes the basic block to define how to serve the data.\nfor example, to define different behaviors based on the DNS \nNext, we define where the static files are, both in terms of the external URL, and \nNote the static location needs to be defined before the reverse proxy: \nWeb Server Structures\nIt's important in production environments to serve static files directly from the web \nserver, instead of doing them further along the line with the Python worker.\nserver is optimized to serve static files.\nproduction through a web server.\nAn alternative is to use an external service to handle files, like AWS S3, that allows \nThe files then will be under a different URL than the service, \nfor example:\nThe service URL is https://example.com/index\nThis way of operating requires you to push the code to the external service as part of \na different path, so static files between deployments are not confused.\nFor example:\ncontent is served from https://mybucket.external-service/static/v1/.\nThe calls to the service, like https://example.com/index, return all their static \nstatic/v2 when they call https://example.com/index.\nlike a single-page application, changing the static files effectively can be a new \nThis structure makes both versions of the static content available at the same \nFor example, adding an optional parameter in any call to overwrite the returned \nCalling https://example.com/index returns the default version, for example, \nCalling https://example.com/index?overwrite_static=v3 returns the \nOther options are returning v3 for specific users, like beta testers or internal staff.\nWeb Server Structures\nFor example, \nThe data can be distributed internally between the different servers from the \nthe need to configure the web server for them.\nLet's continue describing the web server configuration.\nfiles, we need to define a connection to the backend, acting as a reverse proxy.\nA reverse proxy is a proxy server that can redirect a received request towards one or \nIn our example, the backend is the uWSGI process.\nFor example, we have a service where their servers \ncompany has servers in Japan that store a copy of the static content.\nlatency than if the request had to reach a server in Europe, more \nThe web server will be able to communicate with the backend in multiple ways, \nHTTP for pure proxying, or, in our case, connecting directly to the uWSGI protocol.\nThe socket needs to be coordinated with the way uWSGI is configured.\nsee later, the uWSGI process will create it:\nFirst of all, the root of the server is at the / URL.\nrequest for a /static request gets detected before checking for / and it's properly \nproxy is only capable of working with web requests.\ndistributing requests across different servers, it can also add some \nfeatures like caching, security, SSL termination (receiving a request \nin HTTPS and connecting to other servers using HTTP), or, in this \nparticular case, receive a web request and transfer it to through a \ndifferent servers, while UNIX sockets are designed to communicate \ncommunication inside the same host and they work like a file, \nWeb Server Structures\nwhere to redirect the requests.\nThey'll be added to the request, so \nthey are available further down the request.\nIn this case, we are adding the Host header, with information about the requested \nrequest is addressed to.\nIn our configuration, we only use a single backend, as uWSGI will balance between \nserver 192.168.1.117:8080;\nserver 10.0.0.6:8000;\nLater, define the uwsgi_pass to use the cluster.\nuwsgi_params is actually a defined file included by default in \nelements like SERVER_NAME, REMOTE_ADDRESS, etc.\nsingle request can pass through multiple proxies, and each of them \nnginx (and other web servers) produces:\nError log: The error log tracks possible problems from the web server itself, \nAccess log: The access log reports any request accessing the system.\nWeb Server Structures\nHTTPS requests to pass them decrypted through regular HTTP, and encrypt the \ntesting, choosing a backend server based on geolocalization of the requester, etc.\nrequests from nginx and redirects them into independent Python workers, in WSGI \nuWSGI will also start and coordinate the different processes, handling the lifecycle \nworkers receiving the requests.\nblob/main/chapter_06_web_server/uwsgi_example.uni.\nWeb Server Gateway Interface (WSGI) is a Python standard to \ndeal with web requests.\nweb servers like Apache and GUnicorn) and from the receiving \nend (virtually every Python web framework, like Django, Flask, or \nmax-requests=5000\nInside this file is the definition of the application function, which uWSGI can use to \nFor example:\nstart_response('200 OK', [('Content-Type', 'text/plain')])\nThe first parameter is a dictionary with predefined variables that detail the request \n(like METHOD, PATH_INFO, CONTENT_TYPE, and so on) and parameters related to the \nThe second parameter, start_response, is a callable that allows you to set up the \nWeb Server Structures\ntakes some time to process but can be returned without being \nIn any case, the WSGI file is normally created by default by whatever framework is \nFor example, a wsgi.py file created by Django will look like this.\napplication function, and connect it with the rest of the defined code – a great \nInteracting with the web server\nThe socket parameter creates the UNIX socket for the web server to connect to.\nwas discussed before in this chapter, when talking about the web server.\nuWSGI also allows you to use a native HTTP socket, using \nFor example, http-socket = \nthis option if the web server is not on the same server and needs to \nthe web server, use the option http instead of http-socket, which \nWeb Server Structures\nThe master parameter creates a master process that ensures that the number of \nThe processes parameter is very straightforward and describes how many Python \nReceived requests will be load balanced across them.\nThe way uWSGI generates new processes is through pre-forking.\na single process gets started, and after the application is loaded (which may take a \nparameter lazy-apps will make each worker start from scratch, \nworkers, and that allows you to deactivate the Python GIL, speeding up the code.\napplication will need to reload with new code changes regularly.\nparameters are related to how processes are created and destroyed.\nmax-requests=5000\nmax-requests specifies the number of requests to be processed by a single worker \nand create another worker from scratch, following the usual process (fork by default, \nallows a single thread to have control of the Python process.\nWeb Server Structures\nRemember that, based on the Twelve-Factor App, web workers need to be able to be \nuWSGI will also recycle the worker when it's idle, after serving its 5,000th request, so \nrequest, a stampede problem can be created where one after another all the workers \nthat, for example, with 16 workers, at least 15 of them will be available, in practice \nTo avoid this problem, use the max-requests-delta parameter.\nBase max-request\nTotal requests to recycle\nThis makes the recycling happen at different times, increasing the number of \ncreating a backlog of requests.\nrequests.\nworker defined in processes.\nThe master-fifo parameter creates a way to communicate with uWSGI and send \nFor example:\nprocesses and the whole uWSGI.\nFor example, sending Q will produce a direct shutdown of uWSGI, while q will \nrequests in uWSGI, then waiting until any request in the internal uWSGI queue \nis being processed, and when a worker has finished its request, stopping it in an \nFinally, when all workers are done, stop the uWSGI master process.\nWeb Server Structures\nIt will also load any new configuration related to uWSGI itself.\nserver, depending on the number of workers and the startup procedure.\nservers simplifies the process.\ndance, creating copies of the uWSGI configuration in multiple servers and then \nreload the uWSGI configuration but will do with code changes \nrequests from uWSGI after they're routed by the external web server, etc.\nEach framework will interact in a slightly different way with the requests, but in \nWe will use Django as an example.\nabout uWSGI but also about how the whole web stack works.\nWeb Server Structures\nThe View receives the HTTP request and processes it, interacting with the \ndifferent databases, including ones not supported natively by Django (like NoSQL \ninterfaces, the Django REST framework (https://www.django-\nWe will look at the Django REST framework later in the chapter.\nRouting a request towards a View\nDjango provides the tools to perform the proper routing from a particular URL to a \nfrom django.urls import path\npath('example/', first_view)\npath('example/<int:parameter>/<slug:other_parameter>', second_view)\nIf the URL is example/, it will call \ntransform the defined parameters properly and pass them over to the view.\nexample, the URL example/15/example-slug will create these parameters:\nThere are different types of parameters that can be configured.\nyou need to use wildly different tools, a good alternative can be \nPyramid (https://trypyramid.com), a Python web framework \nWeb Server Structures\nfrom django.urls import re_path\nre_path('example/(?P<parameter>\\d+)/', view)\nfor example, creating a type to match only months like Apr or Jun. If the type is \ndefined in this way, an incorrect pattern like Jen will return a 404 automatically.\nFor example:\nfrom django.urls import path\npath('example/<str:parameter>/', first_view)\npath('example/<int:parameter>/', second_view)\nThe type slug should cover more typical use cases for parameters \nexample/<int:parameter>/, the new path-defined URL patterns \nNo URL will ever get passed to second_view, as any parameter that is an integer will \nThe View is the central element of Django.\nIt receives the request information, plus \nany parameters from the URL, and processes it.\ndifferent Models to compose the information, and finally returns a response.\nrequest.\npaths, but other distinctions like HTTP method or parameters will need to be \nrequests to the same URL.\nFor example, in a form \nwith a single parameter, the structure will be similar to the following example: \ndef example_view(request):\nif request.method == 'POST':\nvalue = request.POST['my-value']\nWeb Server Structures\nelif request.method == 'GET':\ndef process_data(parameters, form_content):\ndef example_view(request):\nif request.method == 'POST':\ncontent = process_data(request.POST, form_content)\nelif request.method == 'GET':\nNote that the display_form function gets called both from example_view and also \nThe key element for passing information is the request parameter.\ntype is HttpRequest, and contains all the information that the user is sending in the \nrequest.\ndictionary subclass) containing all the query parameters in the request.\nexample, a request such as:\nThe process will be to create a form with the parameters from the \nrequest, validate it, and print it.\nWeb Server Structures\nWill produce a request.GET value like this:\n>>> request.GET['param1']\n>>> request.GET['param2']\nAll the parameters are defined as strings, needing to be converted to other \nit will be filled first by the body of the request, to allow encoding form posts.\ncontent_type with the MIME type of the request.\nFILES, including data for any uploaded files in the request, for certain POST \nrequests.\nheaders, a dictionary containing all the HTTP headers of the request and \nthat may be introduced and are not necessarily HTTP-based, like SERVER_\nIf you need to access all values, use the method getlist:\nThere are also some useful methods to retrieve information from the request, for \nexample:\nThis method is useful to create full references to return them.\nrequest, allow you to retrieve all the relevant information necessary for processing \nthe request and call the required Models.\nThe HttpResponse class handles the information being returned by the View to the \nweb server.\nThe return from a View function needs to be an HttpResponse object.\nfrom django.http import HttpResponse\ndef my_view(request):\nreturn HttpResponse(content=\"example text\", status_code=200)\nthat way, and can be useful for sending big responses over time.\nWeb Server Structures\nfrom django.http import HttpResponse\ndef my_view(request):\nreturn HttpResponse(content=\"example text\", status_code=HTTPStatus.\nThe content parameter defines the body of the request.\nthe case, a content_type parameter should be added to adequately label the data \nresponse = HttpResponse(content=img_data, headers=header)\ndocuments, for example, 201 CREATED, 404 NOT FOUND, 502 BAD \nIt is very important that the returned Content-Type matches the \nHttpResponse, for JSON encoded requests, it's better to use JsonResponse, which will \nresponse = JsonResponse({'example': 1, 'key': 'body'})\na file-like object and directly filling the headers and content type, including if it needs \ndef my_view(request):\nreturn render(request, 'mytemplate.html')\nA key concept in WSGI requests is that they can be chained.\nrequest can go through different stages, wrapping a new request around the orinal \nthrough the headers parameter or through the content_type \nWeb Server Structures\nbetween systems by simplifying handling several aspects of the request, adding \nA typical example of middleware is logging each received request in a standard \nThe middleware will receive the request, produce a log, and hand the \nrequest to the next level.\nAnother example is managing whether the user is logged or not.\nIt will then fill the request.user object with the \nAnother example, enabled by default in Django, checks the CSRF token on POST \nrequests.\nMiddleware can access the request both when it's received and the response when \nreceived request can generate it before the request is sent to the View.\ninformation of the status code, so it will need to do it once the View is \nLogging middleware that logs the time it took to generate the request will \nneed to first register the time when the request was received, and what time \nit is when the response is ready, to log the difference.\nsimplify the handling of requests.\ndef example_middleware(get_response):\ndef middleware(request):\nresponse = get_response(request)\nThe structure to return a function allows the initialization of chained elements.\nfinal_response = chain(request)\nbefore any middleware that can stop the request, as if done in reverse order, any \nrejected request (for example, not adding a proper CSRF) won't be logged.\nWeb Server Structures\nWe will use it as an example of \nThe basic principle behind Django REST framework is to create different classes that \nModel stores a string of text and the user that created the micropost.\nWeb Server Structures\nWith this information, we create two different views, one for each URL that we need \nfrom django.urls import path\nThe new object is created combining both the user and the rest of the data, added as \nWeb Server Structures\nThe href field requires an extra defined class to create a proper URL reference.\ndef get_url(self, obj, view_name, request, format):\nresult = reverse(view_name, kwargs=url_kwargs, request=request,\nWeb Server Structures\nAnd a micropost page will look like this, which allows you to test different actions \nWeb Server Structures\nOn top of the web server, there is the possibility to continue the link by adding \nit reaches the edge load balancer, it directs the requests inside the system.\nbalancers are not required, and the edge load balancer can handle multiple web \nnumber of requests that it can take.\nHTTPS connection, allowing the rest of the system to use only \nThis is convenient as HTTP requests are easier to cache and \nHTTPS requests are encoded end to end and cannot be \nof requests.\nIn this chapter, we went into the details about how web servers work, and the \nWe started by describing the fundamental details of the request-response and web \nnginx as the front web server and uWSGI to handle multiple Python workers that \nWe started with the web server itself, which allows you to serve HTTP, directly \nreturn the static content stored in files, and route it towards the next layer.\nWe continued by describing how uWSGI works and how it's able to create and set up \ndifferent processes that interact through the WSGI protocol in Python.\nhow to set up the interaction with the previous level (the nginx web server) and the \nWe described how Django works to define a web application, and how the requests \nway to create RESTful APIs and show how our example introduced in Chapter 2 \ncan be implemented through the views and serializers provided by Django REST \nWeb Server Structures\nRequest-response is not the only software architecture that can be used in a system.\nThere can also be requests that don't require an immediate response.\nWe will use Celery as an example of a popular task manager in Python that has \nThis makes it different from the request-response architecture that we saw in the \nA request-response process will wait until an appropriate response \nInstead, an event containing the request will be sent, and the task will just continue.\nEvent-driven systems can be implemented with request-response \nservers.\nThis doesn't make them a pure request-response system.\nFor example, a RESTful API that creates an event and returns an ",
    "keywords": [
      "Web Server Structures",
      "Web Server",
      "Server Structures",
      "server",
      "web",
      "Django REST framework",
      "request",
      "Django",
      "view",
      "Django REST",
      "Chapter",
      "requests",
      "web server architecture",
      "HTTP",
      "content"
    ],
    "concepts": [
      "request",
      "requests",
      "requested",
      "requester",
      "likely",
      "like",
      "server",
      "servers",
      "server_name",
      "server_"
    ]
  },
  {
    "chapter_number": 7,
    "title": "[ 221 ]",
    "start_page": 240,
    "end_page": 275,
    "summary": "Asynchronous tasks\nA simple event-driven system is one that allows you to execute asynchronous tasks.\nThe events produced by an event-driven system describe a particular task to execute.\nNormally, each task will require some time to execute, which makes it impractical to \nThe solution is to send an event to handle this task, generate a task ID, and return the \ntask ID immediately.\nThe back-end system will then execute the task, which can take as \nMeanwhile, the task ID can be used to monitor the progress of the execution.\nback-end task will update the status of the execution in shared storage, like a \nBecause the status of the task is stored in a database that's accessible by the front-end \nweb server, the user can ask for the status of the task at any point by identifying it \nthrough the task ID.\nThese operations that take a long time may involve tasks like \n25% or 50% of the task has been completed.\nwhether a task has been finished or not.\nrequired only if the task is required to return some data.\nIn this case, all the information, task IDs, statuses, and results can remain inside the \nFor example, if a task is to generate a report, the back-end will \nRemember that the queue is likely to store the task ID and the \nstatus of the task.\nproduce other tasks.\nSubdividing tasks\ncreating the right event inside a task and sending it to the right queue.\nif a task generates a report and sends it by email to a group of recipients, the task can \nfirst generate the report and then send the emails in parallel by creating new tasks \nSome tasks may require creating huge amounts of information in the background, \nup with the task taking a longer time.\nThe initial task \nmay return the IDs of the new tasks if they need to be monitored.\nScheduled tasks\nSome examples of scheduled tasks include generating daily reports during night \nMost task queues will allow the generation of scheduled tasks, indicating it clearly in \nSome scheduled tasks can be quite big, such as each night sending emails to \nIt's very useful to divide a scheduled task, so a small \nscheduled task is triggered just to add all the individual tasks to the queue that will \nIn the example of sending emails, a single task triggers every night, reading the \nconfiguration and creating a new task for each email found.\nThen the new tasks \nAn important element of asynchronous tasks is the effect that introducing a queue \nAs we've seen, the background tasks are slow, meaning that any worker \nWe will see later in the chapter how to generate a scheduled task \nMeanwhile, more tasks can be introduced, which may mean that the queue starts \nsufficient to handle the average number of tasks introduced in the queue, the queue \ntimes when there are no tasks to execute, and other times when there's a sudden \nspike in the number of tasks to be executed, filling the queue.\nperiod for those spikes, where a task gets delayed because all the workers are busy, \nAn extra difficulty, as we saw with scheduled tasks, is that at a specific time, a \nconsiderable number of tasks can be triggered at the same time.\nthe queue at a particular time, requiring perhaps an hour to digest all the tasks, for \nThis means that, for example, if 100 tasks to create background reports are added, \nthey will block a task to generate a report sent by a user, which will produce a bad \nminutes after the scheduled tasks were fired.\nThis makes those different tasks go to different workers, making it possible \nreports run only once a day, once the 100 tasks are processed, the workers will be \nbackground report worker will pull tasks from both queues.\nreport tasks when there's available capacity.\nWe reserve capacity for the user report tasks, which are priority, and make the rest of \nthe workers pull from all available tasks, including priority and non-priority tasks.\nTo be able to divide work into these two queues, the tasks need to be divided \nPriority tasks.\nBackground tasks.\ntasks.\nIf too many tasks are labeled as \nThat way, a task with priority 3 will be executed before a task with priority \n2, and that before a task with priority 1, and so on.\nby priority costs more than just assigning tasks to a plain queue.\nto start increasing the priority of tasks over time, especially if multiple \nThe decision on what task should return first could get \nexpectations when developing and creating new tasks.\nsome tasks take too long.\ndifferent codebases, making one with priority tasks and another with background \ntasks.\nto have a worker that handles both priority and background tasks.\navailable in the background workers to help with priority tasks.\nNote that for this to work, it will require strict separation of tasks.\nFor example, if some of the tasks require big \nworker for all tasks.\nThe start-up time can add significant time to the execution of the task, even to \nthe point of being longer than the execution time of the task itself.\nCelery is the most popular task queue created in Python.\ntasks easily and can handle the creation of the events that trigger new tasks.\nable to handle those tasks.\ntasks than that, it will create a server that is not quite filled.\nUsing a backend is optional, as tasks don't need to define a return value, and it's very \nstatus of the task.\nWe will use the example to create a task to retrieve, from an external API, \nThe code is divided into two files: celery_tasks.py, which describes the tasks, and \nstart_task.py, which connects with the queue and enqueues a task.\napp = Celery('tasks', broker='redis://localhost')\nThe celery_tasks.py worker defines a main task, obtain_info, and a secondary \ntask, send_email.\napp = Celery('tasks', broker='redis://localhost')\nLet's take a look at the obtain_info task.\nas a Celery task:\n@app.task\nlogger.info('Stating task')\nuser_id = task['userId']\ntask_data = (info, task)\ntask_reminders[user_id].append(task_data)\nfor user_id, reminders in task_reminders.items():\nlogger.info('End task')\nWe wrap the function with INFO logs to provide context to the task execution.\nuser_id = task['userId']\nThe individual task data is added to a list created to store all the tasks for a user.\ntask_data = (info, task)\ntask_reminders[user_id].append(task_data)\nfor user_id, reminders in task_reminders.items():\ncompose_email takes the information in the task list, which includes a group of user_\ninfo, task_info, extracts the title information for each task_info, then the email \nfrom the matched user_info, and then calls the send_email task:\n# remainders is a list of (user_info, task_info)\n# Retrieve all the titles from each task_info\n# Start the task send_email with the proper info\nAs you can see, the send_email task includes a .delay call, which enqueues this task \nsend_email is another Celery task.\n@app.task\nTriggering tasks\nThe start_task.py script contains all the code to trigger the task.\nfrom celery_tasks import obtain_info\nNote that it inherits all the configuration from celery_tasks.py when doing the \nImportantly, it calls the task with .delay().\nThis sends the task to the queue so the \nNote that if you call the task directly with obtain_info(), you'll \nexecute the code directly, instead of submitting the task to the \n$ celery -A celery_tasks worker --loglevel=INFO -c 3\nThis starts the celery_tasks module (the celery_tasks.py file) with the -A \n$ celery -A celery_tasks worker --loglevel=INFO -c 3\n.> task events: OFF (enable -E to monitor tasks in this worker)\n[tasks]\n. celery_tasks.obtain_info\n. celery_tasks.send_email\nNote that it displays the two available tasks, obtain_info and send_email.\nwindow, we can send tasks calling the start_task.py script:\n$ python3 start_task.py\nThis will trigger the task in the Celery worker, producing logs (edited for clarity and \n[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_\n[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task\n[2021-06-22 20:30:54,128: INFO/MainProcess] Task celery_tasks.send_\n[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_\n[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.\n[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.\n[2021-06-22 20:30:54,141: INFO/ForkPoolWorker-3] Task celery_tasks.\n[2021-06-22 20:30:54,192: INFO/ForkPoolWorker-2] Task celery_tasks.\nthe first task, which corresponds to obtain_info.\nThis task has been executed in the \n[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_\n[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task\n[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.\nWhile this task is being executed, the send_email tasks are also being enqueued and \n[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_\n[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.\nIf only one worker is involved, the tasks will be run consecutively, \nWe can see how the send_email tasks start before the end of the obtain_info task, \nand that there are still send_email tasks running after the end of the obtain_info \nScheduled tasks\nInside Celery, we can also generate tasks with a certain schedule, so they can be \nTo do so, we need to define a task and a schedule.\nscheduled_tasks.py file.\napp = Celery('tasks', broker='redis://localhost')\n@app.task\ndef scheduled_task(timing):\nlogger.info(f'Scheduled task executed {timing}')\n'task': 'celery_scheduled_tasks.scheduled_task',\n'task': 'celery_scheduled_tasks.scheduled_task',\nsmall, simple task that just displays when it is executed.\n@app.task\ndef scheduled_task(timing):\nlogger.info(f'Scheduled task executed {timing}')\n'task': 'celery_scheduled_tasks.scheduled_task',\nThe first one defines an execution of the proper task every 15 seconds.\nThe task needs \nto include the module name (celery_scheduled_tasks).\n'task': 'celery_scheduled_tasks.scheduled_task',\nThis crontab object, which is passed as the schedule parameter, executes the task \n$ celery -A celery_scheduled_tasks beat\nWe start the celery_scheduled_tasks worker in the usual way.\n$ celery -A celery_scheduled_tasks worker --loglevel=INFO -c 3\nBut you can see that there's still no incoming tasks.\nwhich is a specific worker that inserts the tasks in the queue:\n$ celery -A celery_scheduled_tasks beat\nOnce celery beat is started, you'll start seeing the tasks being scheduled and \n[2021-06-28 15:13:06,504: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:06,509: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task \n[2021-06-28 15:13:06,511: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:13:06,512: INFO/ForkPoolWorker-1] Task celery_scheduled_\n[2021-06-28 15:13:21,486: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:21,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:13:36,486: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:13:51,486: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:51,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\n[2021-06-28 15:14:00,004: INFO/MainProcess] Received task: celery_\n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Task celery_scheduled_\nYou can see that both kinds of tasks are scheduled accordingly.\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task \n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task \n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task \nWhen creating periodic tasks, keep in mind the different priorities, as we described \nThis leads to the way of monitoring how the different tasks are being executed, in a \nexecuted tasks and find and fix problems.\n$ celery --broker=redis://localhost flower -A celery_tasks  --port=5555\nThis task can be used to \n'celery_tasks.obtain_info',\n'celery_tasks.send_email']\nIn this case, we have 11 tasks \ncorresponding to a whole run of start_task.py.\nYou can go to the Tasks tab to see \nthe details of each of the tasks executed, which looks like this:\nFigure 7.9: Tasks page\nto trigger the tasks directly with an HTTP request.\nThis can be used to call the tasks \nhttp://localhost:5555/api/task/async-apply/celery_tasks.send_email\nThe task is executed in the worker:\n[2021-06-24 22:35:33,052: INFO/MainProcess] Received task: celery_\ntasks.send_email[79258153-0bdf-4d67-882c-30405d9a36f0]\n[2021-06-24 22:35:33,056: INFO/ForkPoolWorker-2] Task celery_tasks.\nUsing the same API, the status of the task can be retrieved with a GET request:\nGET /api/task/info/{task_id}\n$ curl  http://localhost:5555/api/task/info/79258153-0bdf-4d67-882c-\n{\"uuid\": \"79258153-0bdf-4d67-882c-30405d9a36f0\", \"name\": \"celery_tasks.\nexecute these smaller tasks.\ncertain times to allow the execution of predetermined tasks periodically.\nWe explained how to use Celery, a popular task manager, to create asynchronous \ntasks.\nhow to define a proper worker, and how to generate tasks from a different service.\nWe included a section on how to create scheduled tasks in Celery as well.\nallows us to create tasks by sending HTTP requests, allowing any programming ",
    "keywords": [
      "task",
      "tasks",
      "Task celery",
      "info",
      "Celery",
      "Scheduled task executed",
      "scheduled task",
      "task executed",
      "Scheduled",
      "info task",
      "user",
      "Event-Driven Structures",
      "queue",
      "Received task",
      "email"
    ],
    "concepts": [
      "tasks",
      "task",
      "celery",
      "celery_",
      "info",
      "worker",
      "workers",
      "ebscohost",
      "queue",
      "queues"
    ]
  },
  {
    "chapter_number": 10,
    "title": "[ 329 ]",
    "start_page": 348,
    "end_page": 443,
    "summary": "more careful about the testing.\nthe tests are documented.\nThere are multiple ways that a test can be documented, either by specifying a \nlist of steps to run and expected results or by creating code that runs the test.\nThe main idea is that a test can be analyzed, be run several times by different \nIntegration techniques to run many tests over and over, creating a \"safety \nyou to run tests, which can help.\nFrom a known setup: To be able to run tests in isolation, we need to know \nwhat the status of the system should be before running the test.\nthat the result of a test will not create a certain state that could interfere with \nthe next test.\nBefore and after a test, certain cleanup may be required.\nand in some cases, the order of tests can create problems.\nFor example, test A creates an entry that test B reads.\ntest B is run in isolation, it will fail as it expects the entry \nAlso, being able to run tests \nTesting and TDD\nDifferent elements of the application: Most tests should not address the \ndifferent levels of testing, but tests should be specific about what are they \nrunning tests takes time, and that time needs to be well spent.\nAny test needs \ncommenting on this important aspect of testing.\nabout the different tests defined by how much of the system is under test, during \neach test.\nThere's an important kind of testing that we are not covering \nwith this definition, which is called exploratory testing.\nThese tests \nDifferent levels of testing\nAs we described before, tests should cover different elements of the system.\nWe will define three different levels or kinds of tests, from small to big scopes:\nUnit tests, for tests that check only part of a service\nSystem tests, for tests that check multiple services working together\ntakes to create tests to be sure that they are always worth it.\nUnit tests\nunit test.\nThis kind of test checks the behavior of a small unit of code, not the whole \nThis unit of code could be as small as a single function or test a single API \nintegration and unit tests can be defined side by side, and the \ntest a unit test if it involves a single function or class.\nTesting and TDD\nBecause a unit test checks a small part of the functionality, it can be very easy to set \nThe objective of unit tests is to check in depth the behavior of a defined feature of a \nare defined as part of the test.\nWe will cover unit tests in more detail later in the \nIntegration tests\nThe next level is the integration test.\nThe main goal of integration testing is to be sure that the different services or \nThis makes integration tests slower and more expensive \nthan unit tests.\nIntegration tests are great to check that different services work in unison, but there \nmeaning that the test case should produce no errors or exceptions.\nthe bulk of the integration tests.\nSystem tests\nSystem tests check that all the different services \nA requirement for this kind of test is that there are actually multiple services in \nIf not, they are not different from tests at the lower levels.\nobjective of these tests is to check that the different services can cooperate, and the \nactually performing any system tests is to run them in the live environment.\nthat case, given the constraints, only a minimum amount of tests should be run, as \nThe tests to run should also exercise the maximum \nthese tests check.\nTesting and TDD\nTesting philosophy\nAs we've seen, testing is a way of ensuring that the behavior of the code is the \nThe objective of testing is to detect possible problems (sometimes \ncreate a specific test that simulates exactly the same problem, but we can also create \nDifferent testing levels have different effects on this cost.\nDesigning and running a unit test is easier and faster than \nsystem test.\nthe start of the process (design and unit tests while coding), the cheaper it is to create \nBut having tests is not only a good way of capturing problems once.\nBecause a test \nif there's a problem with some test.\nRegression problems are quite common, so having good test \nSpecific tests \nThese are regression tests, and \nTesting and TDD\nHow to design a great test\nDesigning good tests requires a certain mindset.\nThe objective of the test is to be sure that the functionality sticks to the expected \nNow, to be able to really put the functionality to the test, the mindset should be to \nWhile approaching the test, we need to check what the limits are of this, trying \nexample, the following tests could be created:\nNote how we are testing different possibilities:\nWe can really create a lot of test cases for simple functionality!\nfunctionality already checked by an existing test (for example, creating a big table \ndividing numbers with a lot of divisions) may depend greatly on the code under test \ntesting as a failure there could be more important.\nThe best test is the test that really stresses the code and ensures \ncode under test is the best way of preparing your code for the real \nthe best preparation for that is to create tests that try as hard as \nTesting and TDD\nNote that tests are done independently from the implementation of the code.\ntest definition is done purely from an external view of the function to test, without \nThis is called black-box testing.\nknowledge of the code itself and approach tests independently.\nto complement it with tests that check functionality that is not apparent from an \nFor example, any external API should test any input with care and \nAPIs. For example, testing what happens when strings are input \nless testing, as the internal code is less likely to abuse the API.\nrequired to test that the input format is incorrect, just to check that \nthis is not a possible approach for unit tests, which will likely be \nBlack-box testing tries to avoid a common problem where the same developer writes \nboth the code and the test and then checks that the interpretation of the feature \nwhich tries to ensure tests are created without the implementation in mind by \nwriting the tests before writing the code.\nStructuring tests\nThis pattern means the test is in three different phases:\nArrange: Prepare the environment for the tests.\nis to test the functionality from an external perspective.\nDeveloping the ability to be able to create good black-box tests is \nTesting and TDD\nNote that this structure can be used whether the tests are executed through code or \nrequiring to run tests in a specific sequence, which is not great for unit test suites, but \ndef test_example():\n# Create the instance of the class to test\nA common different pattern is to group act steps in tests, testing \nmultiple functionalities in a single test.\nFor example, test that \nInstead, to follow the AAA pattern, two tests should be created, the \nIn the same way, if the code to test is purely functional (meaning \nin the class that we want to test.\nto_test method for the prepared object with the proper parameter.\ncommon functions for testing in Arrange steps.\n# to test requires a lot of things to set up\nreturn object_to_test\ndef test_exampleA():\nobject_to_test = create_basic_environment()\nTesting and TDD\ndef test_exampleB():\nobject_to_test = create_basic_environment()\nHaving big test suites is \nimportant to create good test coverage, as we saw above.\nTest-Driven Development\n2.\t A new test is written to define the new functionality.\n3.\t The test suite is run to show that it's failing.\npart of the code because there are changes, the tests need to be \n5.\t The test suite is run to show that the new test is working.\nWrite the tests before writing the code: This prevents the problem of \nIt also forces the developer to check that the test actually \ntest suite to check that all the functionality in the system is correct.\ndone over and over, every time that a new test is created, but also while \nRunning the tests is an essential part of \nTesting and TDD\nAnother important advantage of TDD is that putting the focus so heavily on the tests \nmeans that how the code is going to be tested is thought about from the start, which \nThe requirement to create small tests and work in increments also tends \nbe tested independently.\nThe general flow is to be constantly working with new failing tests, making them \nway of working is the generation of very extensive test suites that cover each detail \nAnother important aspect of TDD is the requirement of speedy tests.\nAs tests are \nof the test suite will make it take longer to run.\nA full unit test suite for a complex \napplication can consist of 10,000 tests or more!\nThe whole test suite doesn't need to be run all the time.\nof tests to run on each run while the feature is in development.\nonly the tests that are relevant for the same module, for example.\nrunning a single test, in certain cases, to speed up the result.\nAnyway, as the time taken to run tests is important in TDD, observing the duration \nThis is mainly achieved by creating tests that cover small \n(writing tests after writing the code).\nproblems that may arise through creating tests.\nOf course, at some point, the whole test suite should be run.\nrunning tests, this time automatically once the code is checked out \nThe combination of being able to run a few tests locally \nthe whole test suite running in the background once the code is \nTDD practices work best with unit tests.\nTesting and TDD\nexisting code can be difficult to test in this configuration, especially if the developers \nTDD works great for new projects, though, as a test suite for \nOne is the problem of big tests that take too long to run.\nThese tests \nbeginning, as parts of the code will already be written, and perhaps new tests should \nbe added, violating the rule of creating the tests before the code.\nremember to avoid dependencies between tests.\nThis can happen with any test suite, \nbut given the focus on creating new tests, it's a likely problem if the team is starting \nDependencies can be introduced by requiring tests to run in a \nbut a set of ideas and practices that can help you design code that's well tested and \nNot every single test in the system needs to be designed using TDD, \nTo write the code in full TDD fashion, we start with the smallest possible test.\ncreate the smallest skeleton and the first test.\nWe run the test, and get an error with the test failing.\nPython code, but later in the chapter, we'll see how to run tests more efficiently.\nA typical effect on that will be that some tests fail if run \nTesting and TDD\nto pass the first tests.\nLet's run the tests now and you'll see no errors.\nconsidered the same test, as they're checking that the edge is correct.\nLet's run the tests again.\nWhen running the test, we see that it's running the tests correctly.\nTesting and TDD\nThis runs all the tests correctly.\nWe can run the tests all through the process and be sure that the code is correct.\nIntroduction to unit testing in Python\nThere are multiple ways to run tests in Python.\nconcept of creating a testing class that groups several testing methods.\nnew file with the tests written in the proper format, called test_unittest_example.\ndef test_negative(self):\ndef test_zero(self):\ndef test_five(self):\ndef test_seven(self):\ndef test_ten(self):\ndef test_eleven(self):\nWe import the unittest module and the function to test.\ncomes next, which defines the tests.\ndef test_negative(self):\nTesting and TDD\nThe class TestTDDExample groups the different tests.\nThen, methods that start with test_ will produce the \nindependent tests.\nThis runs the tests automatically if we run the file.\n$ python3 test_unittest_example.py\nFile \".../unittest_example.py\", line 17, in test_seven\nRan 6 tests in 0.001s\nAs you can see, it has run all six tests, and shows any errors.\nthe tests that are being run:\n$ python3 test_unittest_example.py -v\ntest_eleven (__main__.TestTDDExample) ...\ntest_five (__main__.TestTDDExample) ...\ntest_ten (__main__.TestTDDExample) ...\nFile \".../unittest_example.py\", line 17, in test_seven\nRan 6 tests in 0.001s\nsearches for matching tests.\n$ python3 test_unittest_example.py -v -k test_ten\ntest_ten (__main__.TestTDDExample) ...\nRan 1 test in 0.000s\nways of testing.\nexecution of each test in the class.\nWe will take a look at it later when testing mocking \nTesting and TDD\nPytest simplifies writing tests even further.\nneeds to structure the tests, adding a bit of boilerplate code, like the test class.\nproblems are not as obvious, but when creating big test suites, the setup of different \ntests can start to get complicated.\nPytest instead simplifies the running and defining of tests, and captures all the \nLet's see how to run the tests defined in the unittest, in the file test_pytest_\ndef test_negative():\nA common pattern is to create classes that inherit from other test \ndef test_zero():\ndef test_five():\ndef test_seven():\ndef test_ten():\ndef test_eleven():\nIf you compare it with the equivalent code in test_unittest_example.py, the code \n$ pytest test_unittest_example.py\n================= test session starts =================\ntest_unittest_example.py ...F..\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ndef test_seven(self):\nTesting and TDD\ntest_unittest_example.py:17: AssertionError\nFAILED test_unittest_example.py::TestTDDExample::test_seven\nAs with unittest, we can see more information with -v and run a selection of tests \n$ pytest -v test_unittest_example.py\n========================= test session starts =========================\ntest_unittest_example.py::TestTDDExample::test_eleven PASSED      [16%]\ntest_unittest_example.py::TestTDDExample::test_five PASSED        [33%]\ntest_unittest_example.py::TestTDDExample::test_negative PASSED    [50%]\ntest_unittest_example.py::TestTDDExample::test_seven FAILED       [66%]\ntest_unittest_example.py::TestTDDExample::test_ten PASSED         [83%]\ntest_unittest_example.py::TestTDDExample::test_zero PASSED        [100%]\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ndef test_seven(self):\ntest_unittest_example.py:17: AssertionError\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \n$ pytest test_pytest_example.py -v -k test_ten\n========================= test session starts =========================\ntest_pytest_example.py::test_ten PASSED                           [100%]\n$ pytest test_unittest_example.py\n========================= test session starts =========================\ntest_unittest_example.py ...F..\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ndef test_seven(self):\ntest_unittest_example.py:17: AssertionError\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \ntest_ and run inside all the tests.\ncan see it runs both test_unittest_example.py and test_pytest_example.py.\n========================= test session starts =========================\nTesting and TDD\ntest_pytest_example.py ...F..\ntest_unittest_example.py ...F..\n_____________________________ test_seven ______________________________\ndef test_seven():\ntest_pytest_example.py:18: AssertionError\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\ndef test_seven(self):\ntest_unittest_example.py:17: AssertionError\nFAILED test_pytest_example.py::test_seven - assert 49 == 0\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \nwe need to go back to how to define tests when the code has dependencies.\nTesting external dependencies\nisolating a unit in the code to test it independently.\ncreate small, clear tests.\nIn our example above, we tested a purely functional function, parameter_tdd, that \ninevitably, at some point, you'll need to test something that depends on something \nThe question in this case is should the other component be part of the test or not?\nSome developers think that all unit tests \nshould not be part of the test.\npieces of code that form a unit that it's easier to test in conjunction than separately.\nTesting and TDD\nAnd the tests are in test_dependent.py.\ndef test_negative():\ndef test_zero():\ndef test_twenty_five():\ndef test_hundred():\ndef test_hundred_and_one():\nIn this case, we are completely using the external library and testing it at the same \ntime that we are testing our code.\nneed to be captured to prevent making them while running tests and to have control \nover the returned values, or other big pieces of functionality that should be tested in \nmain/chapter_10_testing_and_tdd.\nfake calls, under the control of the test itself.\nTo be able to mock the code, in our test code, we need to prepare the mock as part of \ndef test_twenty_five(mock_sqrt):\nAgain, in this case, the tests work perfectly fine with the \nbe used for other purposes than testing, though it should be used \nTesting and TDD\nFor other tests, for example, we can check that the mock was not called, indicating \ndef test_hundred_and_one(mock_sqrt):\nWith that information, the full file for testing, test_dependent_mocked_test.py, will \ndef test_negative(mock_sqrt):\ndef test_zero(mock_sqrt):\ndef test_twenty_five(mock_sqrt):\ndef test_hundred(mock_sqrt):\ndef test_hundred_and_one(mock_sqrt):\nTesting and TDD\ndef test_multiple_returns_mock(mock_sqrt):\ndef test_exception_raised_mock(mock_sqrt):\nMocking is not the only way to handle dependencies for tests.\nexplicit when calling the function under test, so it can be replaced with a testing \nLet's see how this changes the code under test.\ndef test_good_dependency():\nAnd if we want to perform tests, we can do it by replacing the math.sqrt function \ndef test_twenty_five():\nDependency injection, while useful for testing, is not only aimed \nTesting and TDD\nWe can also provoke an error if calling the dependency to ensure that in some tests \ndef test_negative():\nThe code to test becomes, in \nmodel = Model('test')\nTesting and TDD\nWe can compare how to test both cases, as seen in the file test_dependency_\ninjection_test.py.\nThe first test is mocking, as we saw before, the write method of \ndef test_model(mock_write):\nmock_write.assert_called_with('model.txt', 'test_model')\ndef test_modelinjection():\nEXPECTED_DATA = 'test_modelinjection'\nWhile in testing this interference is not the same as doing it for regular code \nTesting and TDD\nGrouping tests\ntwo classes, as we see in test_group_classes.py.\ndef test_negative(self):\ndef test_zero(self):\ndef test_ten(self):\ndef test_eleven(self):\ndef test_five(self):\ndef test_seven(self):\nThis is an easy way to divide tests and allows you to run them independently:\n$ pytest -v test_group_classes.py\n======================== test session starts =========================\n$ pytest -k TestRegularCases -v test_group_classes.py\n========================= test session starts ========================\n$ pytest -v test_group_classes.py::TestRegularCases\n========================= test session starts ========================\ntesting_and_tdd/advanced_pytest\nthrough a decorator in the tests, for example, in test_markers.py.\nTesting and TDD\ndef test_negative():\ndef test_zero():\ndef test_five():\ndef test_seven():\ndef test_ten():\ndef test_eleven():\nSee that we are defining a decorator, @pytest.mark.edge, on all the tests that checks \nIf we execute the tests, we can use the parameter -m to run only the ones with a \n$ pytest -m edge -v test_markers.py\n========================= test session starts ========================\ntest_markers.py::test_ten PASSED                                 [75%]\ntest_markers.py::test_eleven PASSED                              [100%]\ntest_markers.py:5\ntest_markers.py:5: PytestUnknownMarkWarning: Unknown pytest.mark.edge \ntest_markers.py:10\nNow, running the tests shows no warning.\n$ pytest -m edge -v test_markers.py\n========================= test session starts =========================\nTesting and TDD\ntest_markers.py::test_ten PASSED                                 [75%]\ntest_markers.py::test_eleven PASSED                              [100%]\nallows for making markers to identify common patterns across the tests, for example, \ncreating a quick test suite with the most important tests to run with the marker \ntest, meaning that it expects it to fail).\nThe use of fixtures is the preferred way to set up tests in pytest.\nis a context created to set up a test.\nFixtures are used as input for the test functions, so they can be set up and create \nspecific environments for the test to be created.\nA regular test to cover the functionalities could be as follows.\ndef test_counting():\nTesting and TDD\nDefining the fixture this way will allow us to reuse it easily in different test functions, \ndef test_counting_fixture(prepare_string):\ndef test_counting_fixture2(prepare_string):\nIf we run the tests, we can see the effect.\n$ pytest -v test_fixtures.py -k counting_fixture --setup-show\n======================== test session starts ========================\ntest_fixtures.py::test_counting_fixture\ntest_fixtures.py::test_counting_fixture2\nfilename = f'./test_file_{time.time()}.txt'\nTesting and TDD\ndef test_counting_fixture(prepare_file):\ndef test_counting_fixture2(prepare_file):\n$ pytest -v test_fixtures2.py\n========================= test session starts =========================\ntest_fixtures2.py::test_counting_fixture PASSED                  [50%]\ntest_fixtures2.py::test_counting_fixture2 PASSED                 [100%]\ntests.\nto efficiently run tests and design them in the best possible way.\nTesting is a critical \ntesting strategy is required to produce high-quality software and prevent problems \nthat provide more value than their cost, and the different levels of testing to ensure \nTesting and TDD\nWe continued by describing different strategies to ensure that our tests are great \nthe tests before the code, working in small increments, and running the tests over \nWe continued by presenting ways of creating unit tests in Python, both using the \nWe described how to test external dependencies, something that is critically \nimportant when writing unit tests to isolate functionality.",
    "keywords": [
      "test",
      "tests",
      "TDD",
      "def test",
      "code",
      "assert parameter",
      "testing",
      "Unit tests",
      "Assert",
      "parameter",
      "test suite",
      "def",
      "Integration tests",
      "Chapter",
      "tdd def test"
    ],
    "concepts": [
      "testing",
      "tests",
      "test",
      "tested",
      "to_test",
      "test_five",
      "test_ten",
      "test_eleven",
      "test_",
      "test_twenty_five"
    ]
  },
  {
    "chapter_number": 14,
    "title": "[ 465 ]",
    "start_page": 484,
    "end_page": 521,
    "summary": "Types of profilers\nThere are two main kinds of time profilers:\nDeterministic profilers, through a process of tracing.\nA deterministic profiler \ndeterministic profilers very detailed, as they can follow up the code on each \nProfiling\nStatistical profiles, through sampling.\nWhile they don't give as detailed information as deterministic profiles, \nAn external profiler has the advantage that, even if there's any \nStatistical profilers are good tools for \nThe deterministic profilers are tools for analyzing specific use cases in the petri dish \nprofiler.\nThe deterministic profiler can show, for a specific task, \nAnother kind of profiler is the memory profiler.\nA memory profiler records when \nProfiling \nand deterministic profilers to logs.\nProfiling\nof a memory profiler.\nLet's introduce some code and profile it.\nProfiling code for time\nPrime numbers are numbers that are only \ndef check_if_prime(number):\nif number % i == 0:\nThis code will take every number from 2 to the number under test (without \nthe number is not a prime number.\nMemory profiling is typically more complicated and takes more \neffort than time profiling.\nmistakes, we will include the first prime numbers lower than 100 and compare \nprofiling/primes_1.py.\ndef check_if_prime(number):\nif number % i == 0:\nif check_if_prime(number)]\nThe calculation of prime numbers is performed by creating a list of all numbers \nif check_if_prime(number)]\nProfiling\nThe next line asserts that the first prime numbers are the same as the ones defined in \n$ time python3 primes_1.py\nThe easiest, faster way of profiling a module is to directly use the included cProfile \n$ time python3 -m cProfile primes_1.py\n4999    0.754    0.000    0.754    0.000 primes_1.py:7(check_if_\nncalls: Number of times each element has been called\nProfiling\nIn this case, the time is clearly seen to be spent in the check_if_prime function, which \n$ time python3 -m cProfile -o primes1.prof  primes_1.py\ncProfile increases the time it takes to execute the code.\ngenerally use cProfile as it's faster, but profile can be useful at \nProfiling\nWe will see how to use a profiler that has a higher resolution, analyzing each line \nLine profiler\nTo analyze the check_if_prime function, we need to first install the module line_\nprofiler\n$ pip3 install line_profiler\nWe will add the decorator @profile for the check_if_prime function, to indicate \nto the line profiler to look into it.\nPatterns/blob/main/chapter_14_profiling/primes_2.py.\n@profile\ndef check_if_prime(number):\nif number % i == 0:\nKeep in mind that you should only profile sections of the code \nprofiled in this way, it would take a lot of time to analyze.\nProfiling\nline_profiler.\n$ time kernprof -l primes_2.py\nWrote profile results to primes_2.py.lprof\nexecution without the profiler enabled.\n$ python3 -m line_profiler primes_2.py.lprof\nFile: primes_2.py\nFunction: check_if_prime at line 7\n7                                           @profile\nprime(number):\nnumber):\n12  12487503    3749127.0      0.3     54.2          if number % i \nreduce the number of times they're being called.\n(stored in primes_3.py, available at https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/chapter_14_profiling/primes_3.py):\n@profile\ndef check_if_prime(number):\nif number % i == 0:\nLet's take a look at the profiler result.\n$ time kernprof -l primes_3.py\n$ python3 -m line_profiler primes_3.py.lprof\nFile: primes_3.py\nFunction: check_if_prime at line 7\nProfiling\n7                                           @profile\nprime(number):\nnumber):\n11   1563868     473788.0      0.3     54.9          if number % i \nFor example, for 19, we try these numbers (as 19 is a prime number, it's not \nSo we check only the numbers \nTo apply all of this, we need to tweak the code again and store it in primes_4.py, \nPatterns/blob/main/chapter_14_profiling/primes_4.py:\ndef check_if_prime(number):\nif number % i == 0:\nThe code always checks for divisibility by 2, unless the number is 2.\nrange step on 2 integers at  time so that all the numbers are odd, since we started \nProfiling\nFor example, to test a number like 1,000, this is the equivalent code.\nLet's profile the code again.\n$ time kernprof -l primes_4.py\nWrote profile results to primes_4.py.lprof\nLet's see the line profile.\n$ python3 -m line_profiler primes_4.py.lprof\nFile: primes_4.py\nFunction: check_if_prime at line 8\n8                                           @profile\nprime(number):\n11      4999       1924.0      0.4     10.5      if number % 2 == 0 \nand number != 2:\n15     21558       7476.0      0.3     40.9          if number % i \nprimes_3.py and over 12 million in primes_2.py, when we started the line profiling.\nLet's take a look at how to profile in \nPartial profiling \nIn many scenarios, profilers will be useful in environments where the system is in \nProfiling\nExample web server returning prime numbers\nWe will use the final version of the function check_if_prime and create a web service \nthat returns all the primes up to the number specified in the path of the request.\ncode will be the following, and it's fully available in the server.py file on GitHub at \nchapter_14_profiling/server.py.\ndef check_if_prime(number):\nif number % i == 0:\ndef prime_numbers_up_to(up_to):\nprimes = [number for number in range(1, up_to + 1)\nif check_if_prime(number)]\nreturn primes\nreturn prime_numbers_up_to(param)\nall primes up to that number</p>\nProfiling\ncalculate the threshold number for us to calculate primes up to.\npassed to prime_numbers_up_to.\nreturn prime_numbers_up_to(param)\nThe function prime_numbers_up_to, finally, calculates the prime numbers up to the \ndef prime_numbers_up_to(up_to):\nprimes = [number for number in range(1, up_to + 1)\nif check_if_prime(number)]\nreturn primes\nProfiling\nAnd then tested by going to http://localhost:8000/500 to try to get prime numbers \nLet's move on to profiling the \nProfiling the whole process\nWe can profile the whole process by starting it under cProfile and then \nProfiling\nthat most of the time is spent on the multiple check_if_prime calls, which comprise \nthe bulk of prime_numbers_up_to and the list comprehension included in it, and very \nThese problems can be solved by applying the profiler to only the part that is of \nGenerating a profile file per request\nThis will profile and produce an \nIn the file server_profile_by_request.py, we get the same code as in server.py, but \ndef profile_this(func):\nprof = cProfile.Profile()\nfilename = f'profile-{time()}.prof'\nProfiling\nInside, we start a profiler and run the function under it using the runcall function.\nThis line is the core of it – using the profiler generated, we run the original function \nWe also decorate the get_result function, so we start our profiling there.\n@profile_this\nreturn prime_numbers_up_to(param)\nThe full code is available in the file server_profile_by_request.py, available on \nblob/main/chapter_14_profiling/server_profile_by_request.py.\n$ python3 server_profile_by_request.py\n$ ls profile-*\nprofile-1633882197.634005.prof \nprofile-1633882200.226291.prof\n$ snakeviz profile-1633882197.634005.prof\nFigure 14.6: The profile information of a single request.\nto create a random sample, so only 1 in X calls produces profiled code.\nreduce the overhead of profiling and allow you to completely profile some requests.\nNext, we'll see how to perform memory profiling.\nprofile some requests, instead of detecting what's going on at a \nProfiling\nMemory profiling\nthey use more and more memory as time goes by, normally due to what's called a \nTo profile memory and analyze what the objects are that use the memory, we need \nThe first Leonardo number is one\nWe present the first 35 Leonardo numbers by creating a recursive function and store \nPython-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_1.py.\nif number in (0, 1):\nreturn leonardo(number - 1) + leonardo(number - 2) + 1\nNUMBER = 35\n$ time python3 leonardo_1.py\nWe change the code like this, creating the leonardo_2.py file (available on GitHub at \nchapter_14_profiling/leonardo_2.py).\nif number in (0, 1):\nif number not in CACHE:\nresult = leonardo(number - 1) + leonardo(number - 2) + 1\nCACHE[number] = result\nreturn CACHE[number]\nNUMBER = 35000\nProfiling\nThis uses a global dictionary, CACHE, to store all Leonardo numbers, speeding up the \n$ time python3 leonardo_2.py\nUsing memory_profiler\nNow that we have our application storing information, let's use a profiler to show \nWe need to install the package memory_profiler.\nprofiler.\n$ pip install memory_profiler\nWe can now add a @profile decorator in the leonardo function (stored in \nArchitecture-Patterns/blob/main/chapter_14_profiling/leonardo_2p.py), and run \nit using the memory_profiler module.\n$ time python3 -m memory_profiler leonardo_2p.py\n5  104.277 MiB   97.082 MiB      104999   @profile\n15  104.277 MiB    0.000 MiB      104994       return CACHE[number]\nWe don't really need to keep all the previous Leonardo numbers in memory at all \nProfiling\nWe create the file leonardo_3.py with the following code, available on GitHub at \nchapter_14_profiling/leonardo_3.py:\n@profile\nif number in (0, 1):\nif number not in CACHE:\nresult = leonardo(number - 1) + leonardo(number - 2) + 1\nCACHE[number] = result\nNUMBER = 35000\nNote we keep the @profile decorator to run the memory profiler again.\nThis code will keep the number of elements in the CACHE dictionary within a limit.\nprofiling.\n$ time python3 -m memory_profiler leonardo_3.py\n5   38.441 MiB   38.434 MiB      104999   @profile\nCACHE[number]\nProfiling\nThe memory-profiler module is also able to perform more actions, including \npypi.org/project/memory-profiler/.\nIn this chapter, we described what profiling is and when it's useful to apply it.\ndescribed that profiling is a dynamic tool that allows you to understand how code \nand memory profilers.\nperformance of code and memory profilers analyze the memory used by the code in \nDeterministic profilers instrument the code to detail the flow of the code \nStatistical profilers sample the code at periodic times to provide a \nWe then showed how to profile the code using deterministic profilers, presenting \nThe next step was to see how to profile a process intended to keep running, like a \nWe showed the problems with trying to profile the whole application \nin these cases and described how we can profile each individual request instead \nFinally, we also presented an example to profile memory and see how it's used by \nusing the module memory-profiler.",
    "keywords": [
      "number",
      "primes",
      "prime numbers",
      "prime",
      "Leonardo",
      "time",
      "Profiling",
      "numbers",
      "code",
      "Leonardo numbers",
      "return",
      "Chapter",
      "memory",
      "result",
      "profiler"
    ],
    "concepts": [
      "profiling",
      "profiler",
      "profilers",
      "profiles",
      "profile",
      "profiled",
      "profile_this",
      "times",
      "time",
      "timing"
    ]
  },
  {
    "chapter_number": 15,
    "title": "[ 503 ]",
    "start_page": 522,
    "end_page": 595,
    "summary": "a clear view and be efficient by spending time on important problems and not \nTeams themselves can perform some triage of problems, but \nKeep in mind that usually, you need to both correct bugs and implement new \nFixing bugs is important, not only for the resulting quality of the service, as any user \nthe development team, as working with a low-quality service is also frustrating for \nAny detected problem, except the catastrophic ones, where context is irrelevant, \nThe problem can \nbe ideally replicated into a test, so it can be tested over and over until the problem is \nIn the best situations, this test can be a unit test, if the problem \nthe problem affects more than one system, it may be necessary to create integration \nOnce a problem is categorized and replicable, the investigation can proceed to \nVisually inspecting the code and trying to reason where problems and bugs are is \nBeing able to analyze how, in a particular case, the code is executing \nwith precision is critical for analyzing and fixing problems that are found.\nOnce we are aware that we have a problem in production, we need to understand \nThe most important tools when analyzing why a particular problem is produced are \nto be sure to be able to find problems when required.\nWe talked in previous chapters about logs and metrics.\nan increase in returned errors can be important to detect that there's an error, but \ndetecting what error will require more precise information.\nexample, if there's a single server that's producing errors, or if it has run out of \nA common problem during an investigation is to find out what \nfor example, data that's set up in a particular way in production \nproblem can be complicated in this environment.\nin the chapter about finding a problem in production.\nreplicate a problem.\nIf that's the case, tests can be done to produce \nBut in any case, logs will generally be more useful in determining which part of the \nAs we saw in Chapter 12, Logging, we can describe error logs \nas detecting two kinds of problems:\nIn this case, we did the work of debugging the error \ncan be an external request that returns an error, a database that cannot be \nMost of these errors will be related to external services (from the point of \na network problem, misconfiguration, or problems in other services.\nThe sign of these errors are logs indicating that \nstack trace of some sort in the logs detailing the line of code when the error \nerrors, if the external requests are directed to different servers, \nIn any of the two cases, the main tool to detect what the problem was will be logs.\nEither the logs show a known problem that is captured and properly labeled, or the \nlogs show a stack trace that should indicate what specific part of the code is showing \nFinding the element and part of the code that is the source of the error is important \nfor understanding the problem and for debugging the specific problem.\nif there's an external dependency calling an external API and it has a problem, this \na state of \"service not available.\" But the root of the error may not be possible to \nneeds to be done, it will require some debugging.\nUnderstanding the problem in production\nThe challenge in complex systems is the fact that detecting problems becomes \nThe objective in this step, though, should be to analyze enough of the problem in \nof the environment will make it easier and less invasive to probe and make changes.\nenvironment alone and focus on the specifics of the problem.\nLogging a request ID\nOne of the problems when analyzing a large number of logs is correlating them.\nenough as two or more different tasks can be running at the same time.\nunique identifier per task or request that can trace all logs coming from the same \nproblems in the integration of its different parts.\nthe problem is in a blind spot of the integration tests.\nBut monoliths can also have problems as their parts grow more \nOnce the problem can be categorized as a replicable set of steps \nlocally, a test can be created to produce it over and over and debug \nIn cases where multiple services are involved, like in microservice architectures, it \nis very important to keep a common request ID that can work to trace the different \nrequests between different services.\nlogs in the system from different services that have the same origin.\nFigure 15.1: Request ID across multiple services\nBecause all of them share the same request ID, logs can be filtered by that \nTo achieve this, we can use the module django_log_request_id to create a request ID \nArchitecture-Patterns/tree/main/chapter_15_debug following the example across \ncom/dabapps/django-log-request-id/.\nThe code has been changed to include some extra logs in the microposts/api/views.\nPatterns/blob/main/chapter_15_debug/microposts/api/views.py):\nimport logging\nNote how this is now adding some logs when accessing the list collections page and \nWe will use the example URL /api/users/jaime/\nTo enable the usage of the request ID, we need to properly set up the configuration \nArchitecture-Patterns/blob/main/chapter_15_debug/microposts/microposts/\nLOG_REQUEST_ID_HEADER = \"HTTP_X_REQUEST_ID\"\nLOGGING = {\n'()': 'log_request_id.filters.RequestIDFilter'\nfilters adds extra information, in this case, our request_id, formatter describes the \nspecific format to use (note that we add request_id as a parameter, which will be \nWe set all the logs on the root level to use \nLOG_REQUEST_ID_HEADER = \"HTTP_X_REQUEST_ID\"\nstate that a new Request ID parameter should be created if not found as a header in \nOnce all of this is configured, we can run a test starting the server with: \nLogging \nAt the same time, you'll see the logs on the server screen:\nWhich, as you can see, added a new request ID element, 66e9f8f1b43140338ddc3ef56\nBut the request ID can also be created by calling with the proper header.\n$ curl -H \"X-Request-ID:1A2B3C\" http://localhost:8000/api/users/jaime/\nYou can check the logs in the server again:\nThe request ID can be passed over other services by using the Session included in \nfrom log_request_id.session import Session\nIf the default logs are not enough to understand the problem, the next stage in those \ncases is understanding the data related to the problem.\nIf investigating the data is not enough to be able to understand the problem, it may \nbe necessary to increase the information on the logs.\nBe sure to check the django-log-request-id documentation.\nproduction to be able to understand and replicate the problem \nIn some cases, when investigating a problem in production, it is \npossible that changing the data manually will fix the issue.\nthe data has been possible or how the service should be changed \nThen the code can be \nchanged accordingly to ensure that the problem doesn't happen in \nIf the regular logs and an investigation of the data don't bear fruit, it may be \nnecessary to increase the level of logging with special logs, following the problem.\nThis is a last-resort method, because it has two main problems:\nAny change in the logs needs to be deployed, which makes it costly and \nThe number of logs in the system will be incremented, which will require \nsystem, this can create pressure on the logging system.\nWhile enabling an extra level of logging, like setting logs to DEBUG level, is technically \npossible, this will probably increase the logs too much, and will make it difficult to \nWith some DEBUG logs, \nhigher to make sure that they are properly logged.\nlike PII should not be logged.\nInstead, try to log surrounding information that can \nhelp find out the problem.\na problem with the algorithm to check the password, instead of logging the \nFor example, assuming there's a problem with a password or secret that has an \ndeployments until finding out the problem.\nRemember that the work is just to be able to reproduce the problem locally, so you \ncan more efficiently investigate and fix the problem locally.\nSometimes the problem \nin Chapter 10, Testing and TDD, tests displaying and then fixing the bug.\nOnce we can detect the problem locally, it is time to go to the next step.\nDebugging locally means exposing and fixing a problem once we have a local \nThe basic steps of debugging are reproducing the problem, knowing what the \n1.\t You realize there's a problem\n4.\t You fix the problem\nA great way of creating the reproduction of the problem is with a \nCreate a test that fails and then change \nthe code to make it pass.\nKeeping this process in mind is also useful from a local debugging perspective, \npossible, or creating a specific \"experiment\" (some specific code, like a test) to \n4.\t Use the resulting information to iterate the process until the source of the \nproblem is totally understood\nNote that this process doesn't necessarily need to be applied to the whole problem.\nIt can be focused on the specific parts of the code that can influence the problem.\nexample, is this setting activated in this case?\nit's possible to simplify the code and make it digestible.\nunderstanding when there's a problem in the code is detecting when there \nof a problem is not where an error is raised or obvious, but instead the \nallows you to ignore all code that comes after the problem, and have a clear \nto fall into bad assumptions and think that the problem is in a particular part \nof the code when it really is in another.\nor a type error that needs to be checked.\nBefore we move on to specific techniques, we need to understand the tools in Python \n>>> my_object = {'example': True}\n>>> another_object = {'example'}\nA typical example error is to have a problem because a variable can be either an \nWhile type is useful in debugging environments, avoid using it directly in your code.\nobjects, so every time that we need to verify if an object is None, it's better to make an \nKeeping the code simple helps a lot in later debugging problems.\nSimple code is easy to understand and debug.\nFor example, in the following code we see \nAll these methods can help you navigate code that's new or under analysis without \ncode.\nDebugging with logs\nA simple yet effective way of detecting what's going on and how the code is being \ncode locally in a test or similar.\ninstead, as we introduced in Chapter 12, Logging.\nthe code well commented and adding context for developers \nworking in the code, but also in case of debugging in parts \nAnother important advantage is that tests can be run very quickly, as adding more \nlogs is a simple operation, and logs won't interfere with the execution of code.\nThe fact that the logs won't interfere with the code and code can be running \nWhile debugging through logs can be quite convenient, it requires certain knowledge \nof where and what logs to set to obtain the relevant information.\nAnother problem is that new logs are new code, and they can create problems if \nto fix, but can be an annoyance and require a new run.\nIdeally, these logs will be DEBUG logs, which will only be displayed \nwhen running tests, but won't be produced in a production \nWhile logs can be added and not produced later, it's good practice \nanyway to remove any spurious logs after fixing the bug.\nLogs \nnormally require extensive logs to try to capture the specifics of \nthat problem.\nmakes it possible to understand interactively what the code is doing.\nThe code can be found on \nPerhaps you are able to understand what the code does, but let's take a look at it \nWe investigated similar code and improvements in Chapter 14, \nsetting code to check this, but it has been added as an example and \nquickly run the code without interruptions.\nrunning tests, as we described in Chapter 10, Testing and TDD.\nIn this chapter, we described the general process of detecting and fixing problems.\nimportant to be able to reliably reproduce the problem in order to show all the \nOnce a problem is deemed important, there needs to be an investigation into why \nthis problem is happening.\nThis can be on the running code, and use the available \ntools in production to see if it can be understood why the problem occurs.\nobjective of this investigation is to be able to replicate the problem locally.\nAs the main tool to understand the behavior of the code in production is logs, \nwe talked about creating a request ID that can help us to trace the different calls \nand relate logs from different systems.\nenvironment may have the key to why the problem is occurring there.\nnecessary, the number of logs may need to be increased to extract information from \nWe then moved on to how to debug locally, after replicating the problem, ideally, \ndebugging when required in difficult cases.\nWe introduced some of the tools that help with debugging in Python, which make \nlanguage, there are a lot of possibilities, as it's able to execute any code, including all \nWe then talked about how to create logs to debug, which is an improved version \nbetter logs in the long run.\nIn the next chapter, we will talk about the challenges of working in the architecture \nThere are always changes, adjustments, and tweaks that need \nto be performed in order to improve the system: adding new features; improving \nperformance; fixing security problems.\nmore about making changes and improvements.\nsome of the techniques and ideas around making changes in a real working system, \nsystem can be changed continuously while at the same time maintaining service \nLet's start by taking a look at why to make changes in the architecture of a system.\nexample, adding an event-driven system to run asynchronous tasks, allowing \nBig API changes, like introducing a new version of an API either internally or \nFor example, adding a new endpoint that works better for other \ninternal systems to perform some action, where the calling services should be \nChanges in the storage system, including all the different ideas that we \nsecurity problem.\nto use their favorite language to create a service.\ncause problems by complicating maintenance as expertise in this language \nOther kinds of technical debt – for example, refactors that can clean the code \nand make it more readable, or to allow for changing names of components to \nThe challenge is not only to design these changes to achieve the expected results, but \ninterrupted, setting a high bar for any change.\nTo achieve this, changes need to be taken in small steps, taking extra care to ensure \nchanges made, sometimes it's simply not possible to perform big changes without \nThat changed \nIn other cases, like a small new service with very little traffic, customers will either be \nservice.\nIf possible, is a good practice to define maintenance windows to properly set clear \nexpectations about times when the service will or might have a high risk of some sort \nit will happen, not every maintenance window needs to involve downtime – there is \nIt's important to communicate maintenance windows in advance, for example \nService \nrequires the service to be \ntime, as it's better to set expectations with a large maintenance window that can \nWhile scheduled downtime and maintenance windows will help frame the times \nwhere the service is active and what times are riskier for the user, it's still possible \nproduce an error so important that it needs to be taken care of immediately.\nAn incident is defined as a problem that disrupts the service so much that it requires \nDuring incidents, using all monitoring tools available is critical to find the problem \nHow good the monitoring tools are at detecting and understanding problems\nHow fast a change can be introduced in the system, related to how quick it is \nto change a parameter or to deploy new code\n(though it may be necessary to make changes to get a better understanding of the \nproblem, as we saw in Chapter 14, Profiling).\nThis is why these two elements, the observability and the time required to make \na change, are so important.\nto make a change is normally just a minor annoyance, but in a critical situation, it \nservice, or even a problem in one internal service that reduces the \n12, Logging.\ndifference in how long it takes new code to be ready to deploy.\nproblem has impacted the service.\nand take corrective measures to ensure that the problem doesn't happen again, or at \nto correct the problem?\nDescribe the impact of the problem.\nWhat was the external problem?\nExample: All user requests were returning 500 errors.\nExample: The monitoring system alerted about the problem at 8:35 UTC, after \nActions taken to correct the problem.\n8:30 Start of the problem.\ninto the problem.\n9:30 The logs in the database server had filled up the server disk space, \n9:40 Old logs are removed from the server, freeing disk space.\nA description of the identified root cause of the problem that, if \nfixed, will completely remove this problem.\nto recover the service during the incident, where a quick fix may have been \nof a certain tool or metric that was useful when analyzing the problem.\nThe amount of disk space that logs use should be limited in all cases.\nproblem.\nAction: Tweak the error alert to change it to alert when there's only one \nIn recent years, an equivalent process to try to foresee problems has been put in \nthe problem.\nand to try to make sure that problems are not repeated.\ncan be an endless task, and as time will be limited, it needs to be focused on the \nA key element of preparation in these cases is load testing.\nLoad testing is creating a simulated load that goes to an increased level of traffic.\nLoad testing is typically done not in production environments, but in staging ones, \ncreate a final load test verifying that the configuration in the production environment \nThe basic element of a load test is to simulate a typical user performing actions on \nFor example, a typical user can log in, check a few pages, add some \ninformation, and then log out.\ntimes to simulate the effect of N users, producing enough load to test our system.\nLoad tests, though, are sometimes needed when there is no solid \ndata, as they are done typically when new features are introduced, so estimations \nprocess, including readjusted system tests using any existing software.\nThis will help detect possible problems.\nThe more intensive load tests are, the more problems they'll be able \nWhen making changes to any service, a system needs to be in place to track the \ndifferent changes.\nVersioning means assigning a unique code version to each service or system.\nbeen changed from one version to another.\nThis tool allows us to create a web session, simulating a user \nsession explicitly for the load test and is only capable of working \ndeployment, as new code can create new problems.\nthat an incident is produced due to the release of a new version.\nprecise definition of the code under that unique version number.\nversion number that is applicable to multiple iterations of the code \nVersion numbers are about communicating the differences in code when talking \nWhen the internal version was required, a build number was used, which was a \nIn the same way, different versions can be used \neffectively for the same software, such as for creating an internal version for the \nIn modern software, where the releases are frequent and the version needs to change \nAn increase in the minor version means that this version contains new \nWe talked about semantic versioning in Chapter 2, API Design, but \nconcept can be used both for APIs and code releases.\nIt fixes problems, but doesn't change the \nA good example of this kind of versioning is the Python interpreter itself:\nPython 3 was an increase in the major version, and as such, code from \nPython 2 required changes to be run under Python 3\nPython 3.9 introduced new features compared with Python 3.8, for example, \nexpectation, from just the version number, on what to expect from a new change, and \nallows clarity at the time of adding new features.\nprovide an understanding of how the code changes, but without necessarily having \nto force changes in major or minor versions.\nmark changes that would ordinarily appear in minor version \nA change in the major version number will likely \nsemantic versioning for this reason, deciding that instead new \nmajor versions will be small and not change things, and won't \nWhen communicating through external APIs, though, version numbers do not only \nKeep in mind that it can be possible to create a general version of a whole system, \ncases like online services, though, that can be tricky or pointless.\nThe key aspect of changing architecture in a running system is the necessity of \nalways keeping backward compatibility in its interfaces and APIs. Backward compatibility means that systems keep their old interfaces working as \nexpected, so any calling system won't be affected by the change.\nAs versioning is so important, a good idea is to allow services to \nself-report their version number via a specific endpoint like /api/\nversion or another easily accessed way to be sure that it's clear \nand can be checked by other dependant services.\ndatabases changes in Chapter 3, Data Modeling.\nThis concept is quite simple, but it has implications on how changes need to be \nEven additive changes in externally accessible APIs are difficult.\ncustomers tend to remember the API as it is, so it can be difficult to change \nAdding a new field in a JSON object \nis safer than changing a SOAP definition, which needs to be defined \nAPI changes are normally done in stages, creating a new version \nof the API and trying to encourage customers to change to the new and better \nadjust to any changes unless there's a good reason, and \nWeb interfaces allow greater flexibility for changes as they \nthe tests without a problem, as the old behavior won't change.\nIntroducing changes in external interfaces is more complicated and normally \nrequires the definition of stricter APIs and a slower pace of change.\ninterfaces allow greater flexibility, as their changes can be communicated across the \nthe service at any point.\nIncremental changes\nIncremental changes to the system, slowing mutating and adjusting the APIs, can \nBut the changes need to be \nFor example, let's say that we have two services: service A generates an interface \nA good example of how painful a change in APIs can \n2 needed to be changed.\nthere's still code in legacy systems working with Python \nThere's a new feature that needs to be introduced in service A that requires extra \ninformation from the examinees, and requires us to know the number of times that \nWith the current information, that's impossible, but service B can be \nTo do so, the API needs to be extended, so it returns that information:\nOnly after this change is properly done and deployed can service A use it.\n2.\t Deployment of service B with /examinees (v2).\n3.\t Deployment of service A reading and using the new parameter exam_tries.\nThe service works without a problem throughout each \nThis detachment is important because if there's a problem with a \nThe worst situation is to have two changes in services \nthat need to happen at the same time, as a failure in one will affect \nthe problem could be in the interaction between them, and in that \nThis way of operating allows us to implement greater changes, for example, \nchange it for a more appropriate student_id.\n1.\t Update the returned object to include a new field called student_id, \nreplicating the previous value in service B:\n2.\t Update and deploy service A to use student_id instead of examinee_id.\n4.\t Remove the old field from service B and deploy the service:\n5.\t Remove the old field from service B and deploy the service.\nUse monitoring tools and logs to verify this!\nThis illustrates how we can deploy changes without interrupting the service in \navailable while deploying a new version?\nTo allow continuous releases without service interruption, we need to take the \nbackward-compatible changes and deploy them while the service is still responding.\n1.\t This is the initial stage, where all the instances have version 1 of the service to \n2.\t A new instance with service 2 is created.\n3.\t The new version is added to the load balancer.\nbackward compatibility, though, this should not cause any problems.\nFigure 16.3: New server included in the load balancer\ninstance in the load balancer, so no new requests will be addressed.\nservice finishes all the already-ongoing requests (remember, no new requests \nunder a configuration change.\nquick iteration makes it impossible to create big changes, like a new user interface.\nTo complicate things further, these big changes will likely happen in parallel with \nuntil the new user interface is working correctly.\nThat means that other changes, like bug fixes or performance improvements, are \nAnd the work done on the big new feature is \nof the big new feature are also being released to the production environment, but \nTests need to ensure that both options – the feature active and \nThe process of implementing changes in a system has some human elements \nchanges.\nChanges in \nIn the same way, technology changes require support and training, even if \nchange, be sure to have a point of contact where the team can go to resolve \nA change in one will likely \naffect the other, which means that big enough architectural changes will lead \nAt the same time, changes may have winners and losers in the affected teams.\nThis problem can be particularly poignant in team shuffling when people \npace of development is to have an efficient team and making changes to \nsecurity updates, but also tasks like upgrading OS versions, dependencies, \nwithin three to six months of a new LTS version being released.\nof it, and is manifested with a progressively slower pace of code changes.\nto work with, making the development process more difficult and risking \nAs a general consideration, just keep in mind that changes in architecture need to be \ncomponent, this presents its own challenges and problems, as communicating with \nrunning while developing and changing it, including its architecture.\nWe started by describing different ways that architecture can require adjustments \nand changes.\nWe then moved on to talk about how to manage changes, including \nexpectations of stability and change.\nWe next went over the different incidents that can happen when problems arise, \nmix this process of releasing bigger features that need to be activated as a whole.\nFinally, we described different aspects of how changes in a system and architect",
    "keywords": [
      "problem",
      "Chapter",
      "system",
      "code",
      "logs",
      "service",
      "Ongoing Architecture",
      "EBSCOhost",
      "printed",
      "8:47",
      "subject",
      "Debugging",
      "problems",
      "version",
      "architecture"
    ],
    "concepts": [
      "logs",
      "logging",
      "log",
      "logged",
      "ebscohost",
      "version",
      "versioning",
      "versions",
      "changes",
      "changed"
    ]
  }
]