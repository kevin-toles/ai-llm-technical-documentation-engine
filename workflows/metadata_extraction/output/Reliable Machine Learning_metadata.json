[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 1-8)",
    "start_page": 1,
    "end_page": 8,
    "summary": "Reliable Machine Learning\nReliable Machine Learning Whether you’re part of a small startup or a multinational corporation, this practical book shows data scientists, software and site reliability engineers, product managers, and business owners how to run and establish ML reliably, effectively, and accountably within your organization.\nBy applying an SRE mindset to machine learning, authors and engineering professionals Cathy Chen, Kranti Parisa, Niall Richard Murphy, D.\nSculley, Todd Underwood, and featured guest authors show you how to run an efficient and reliable ML system.\nNiall Richard Murphy is a CEO of a startup in the ML & SRE space, and has worked for Amazon, Google, and Microsoft.\nTodd Underwood is a senior director and founder of machine learning SRE at Google.\nPraise for Reliable Machine Learning\nI don’t care how much data science work you’ve done in the past, or how expert you are on the statistical foundations of machine learning.\nBefore you ever put a real system based on machine learning into deployment, you will benefit from reading this book.\n—Chip Huyen, author of Designing Machine Learning Systems\nReliable Machine Learning is a must-read for people building real-world machine learning systems.\nReliable Machine Learning by Cathy Chen, Niall Richard Murphy, Kranti Parisa, D.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Reliable Machine Learning, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n. 1 The ML Lifecycle 1 Data Collection and Analysis 3 ML Training Pipelines 3 Build and Validate Applications 5 Quality and Performance Evaluation 6 Defining and Measuring SLOs 7 Launch 8 Monitoring and Feedback Loops 11 Lessons from the Loop 12\n47 Training Data 48 Labels 50 Training Methods 51 Infrastructure and Pipelines 54 Platforms 55 Feature Generation 55 Upgrades and Fixes 56 A Set of Useful Questions to Ask About Any Model 57 An Example ML System 59 Yarn Product Click-Prediction Model 59 Features 59 Labels for Features 61 Model Updating 61 Model Serving 62 Common Failures 63 Conclusion 64\nAn Annotation Platform 80 Active Learning and AI-Assisted Labeling 81 Documentation and Training for Labelers 81 Metadata 82 Metadata Systems Overview 82 Dataset Metadata 83 Feature Metadata 84 Label Metadata 85 Pipeline Metadata 85 Data Privacy and Fairness 86 Privacy 86 Fairness 87 Conclusion 87\n107 Fairness (a.k.a. Fighting Bias) 108 Definitions of Fairness 112 Reaching Fairness 117 Fairness as a Process Rather than an Endpoint 120 A Quick Legal Note 121 Privacy 121 Methods to Preserve Privacy 124 A Quick Legal Note 126 Responsible AI 127 Explanation 128 Effectiveness 130 Social and Cultural Appropriateness 131 Responsible AI Along the ML Pipeline 132 Use Case Brainstorming 132 Data Collection and Cleaning 132 Model Creation and Training 133 Model Validation and Quality Assessment 133 vii\n. 137 Requirements 138 Basic Training System Implementation 140 Features 141 Feature Store 141 Model Management System 142 Orchestration 143 Quality Evaluation 144 Monitoring 144 General Reliability Principles 145 Most Failures Will Not Be ML Failures 145 Models Will Be Retrained 146 Models Will Have Multiple Versions (at the Same Time!) 146 Good Models Will Become Bad 147 Data Will Be Unavailable 148 Models Should Be Improvable 149 Features Will Be Added and Changed 149 Models Can Train Too Fast 150 Resource Utilization Matters 151 Utilization != Efficiency 152 Outages Include Recovery 154 Common Training Reliability Problems 154 Data Sensitivity 154 Example Data Problem at YarnIt 155 Reproducibility 155 Example Reproducibility Problem at YarnIt 157 Compute Resource Capacity 159 Example Capacity Problem at YarnIt 159 Structural Reliability 160 Organizational Challenges 160 Ethics and Fairness Considerations 161 Conclusion 162",
    "keywords": [
      "Niall Richard Murphy",
      "Machine Learning",
      "Reliable Machine Learning",
      "Sam Charrington",
      "Machine Learning Systems",
      "Richard Murphy",
      "Niall Richard",
      "Kranti Parisa",
      "machine learning SRE",
      "Reliable Machine",
      "Machine",
      "Learning",
      "model",
      "Todd Underwood",
      "Machine Learning Applying"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "reliable",
      "reliability",
      "reliably",
      "production",
      "product",
      "products",
      "featured"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 9-18)",
    "start_page": 9,
    "end_page": 18,
    "summary": "190 The Concerns That ML Brings to Monitoring 192 Reasons for Continual ML Observability—in Production 193 Problems with ML Production Monitoring 193 Difficulties of Development Versus Serving 194 A Mindset Change Is Required 196 Best Practices for ML Model Monitoring 196 Generic Pre-serving Model Recommendations 197 Training and Retraining 198 Model Validation (Before Rollout) 202 Serving 205 Other Things to Consider 216 High-Level Recommendations for Monitoring Strategy 221 Conclusion 223\nFiltering Out Bad Data 227 Feature Stores and Data Management 228 Updating the Model 228 Pushing Updated Models to Serving 229 Observations About Continuous ML Systems 230 External World Events May Influence Our Systems 230 Models Can Influence Their Own Training Data 232 Temporal Effects Can Arise at Several Timescales 234 Emergency Response Must Be Done in Real Time 235 New Launches Require Staged Ramp-ups and Stable Baselines 239 Models Must Be Managed Rather Than Shipped 241 Continuous Organizations 242 Rethinking Noncontinuous ML Systems 245 Conclusion 246\n247 Incident Management Basics 248 Life of an Incident 249 Incident Response Roles 250 Anatomy of an ML-Centric Outage 251 Terminology Reminder: Model 252 Story Time 252 Story 1: Searching but Not Finding 252 Story 2: Suddenly Useless Partners 257 Story 3: Recommend You Find New Suppliers 264 ML Incident Management Principles 274 Guiding Principles 274 Model Developer or Data Scientist 275 Software Engineer 277 ML SRE or Production Engineer 278 Product Manager or Business Leader 281 Special Topics 282 Production Engineers and ML Engineering Versus Modeling 282 The Ethical On-Call Engineer Manifesto 284 Conclusion 286\nBusiness Goal Setting 292 MVP Construction and Validation 295 Model and Product Development 296 Deployment 296 Support and Maintenance 297 Build Versus Buy 298 Models 298 Data Processing Infrastructure 299 End-to-End Platforms 300 Scoring Approach for Making the Decision 301 Making the Decision 301 Sample YarnIt Store Features Powered by ML 302 Showcasing Popular Yarns by Total Sales 302 Recommendations Based on Browsing History 303 Cross-selling and Upselling 303 Content-Based Filtering 303 Collaborative Filtering 304 Conclusion 305\nChapter Assumptions 308 Leader-Based Viewpoint 308 Detail Matters 308 ML Needs to Know About the Business 309 The Most Important Assumption You Make 310 The Value of ML 311 Significant Organizational Risks 312 ML Is Not Magic 312 Mental (Way of Thinking) Model Inertia 312 Surfacing Risk Correctly in Different Cultures 313 Siloed Teams Don’t Solve All Problems 314 Implementation Models 314 Remembering the Goal 315 Greenfield Versus Brownfield 316 ML Roles and Responsibilities 316 How to Hire ML Folks 317 Organizational Design and Incentives 318 Strategy 319 Structure 320 Processes 321 Rewards 321\nAccommodating Privacy and Data Retention Policies in ML Pipelines 337 Background 337 Problem and Resolution 338 Takeaways 340 2.\nContinuous ML Model Impacting Traffic 341 Background 341 Problem and Resolution 341 Takeaways 342 3.\nTesting and Measuring Dependencies in ML Workflow 356 Background 356 Problem and Resolution 357 Takeaways 361\nMachine learning (ML) is at the heart of a tremendous wave of technological innova‐ tion that has only just begun.\nPicking up where the “data-driven” wave of the 2000s left off, ML enables a new era of model-driven decision making that promises to improve organizational performance and enhance customer experiences by allowing machines to make near-instantaneous, high-fidelity decisions, at the point of interac‐ tion, based on the most current information available.\nTo support the productive use of ML models, the practice of machine learning has had to evolve rapidly from a primarily academic pursuit to a fully fledged engineering discipline.\nPart of what we see in the evolution of machine learning roles is a healthy shift in focus from simply trying to get models to work to ensuring that they work in a way that meets the needs of the organization.\nThe first wave of MLOps focused on the application of technology and process disci‐ pline to the development and deployment of models, resulting in a greater ability for organizations to move models from “the lab” to “the factory,” as well as an explosion of tools and platforms for supporting those stages of the ML lifecycle.\nA significant contributor to maturing the operational side of DevOps was that community’s broader awareness and applica‐ tion of site reliability engineering (SRE), a set of principles and practices developed at Google and many other organizations that sought to apply engineering discipline to the challenges of operating large-scale, mission-critical software systems.\nRather than leaving it to each individual or team to identify how to apply SRE principles to their machine learning workflow, the authors of this book aim to give you a head start by sharing what has worked for them at Google, Apple, Microsoft, and other organizations.\nIn the fall of 2019, I organized the first TWIMLcon: AI Platforms conference to provide a venue for the then-nascent MLOps community to share experiences and advance the practice of building processes, tooling, and platforms for supporting the end-to-end machine learning workflow.\nAt our second conference, in 2021, Todd Underwood joined us to present “When Good Models Go Bad: The Damage Caused by Wayward Models and How to Prevent It.”2 The talk shared the results of a hand analysis of approximately 100 incidents tracked over 10 years in which bad ML models made it, or nearly made it, into production.\n(Though I’ve not previously come across Cathy and Kranti’s work, it is clear that their experience structuring SRE organizations and driving large-scale consumer-facing applications of ML informs many aspects of the book, particularly the chapters on implementing ML organizations and integrating ML into products.)\nThis book provides a valuable lens into the authors’ experiences building, operating, and scaling some of the largest machine learning systems around.\nIf we’ve learned anything as a community over the past several years it’s that the ability to create, deliver, and operate ML models in an efficient, repeatable, and scalable manner is far from easy.\nI’m grateful to Cathy, Niall, Kranti, D., and Todd for allowing us all to benefit from their hard won lessons and for helping to advance the state of machine learning in production in the process.\nThis is not a book about how machine learning works.\nThe way that machine learning (ML) works is fascinating.\nSRE as the Lens on ML A plethora of ML books exist already, many of which promise to make your ML journey better in some way, often focusing on making it easier, faster, or more productive.\nFew, however, talk about how to make ML more reliable, an attribute often overlooked or undervalued.3 That is what we focus on, since looking at how to do ML well through that lens has specific benefits you don’t get in other ways.\n3 Or to put it another way, there is plenty of material on how to build an ML model, but not much on how to",
    "keywords": [
      "Machine learning",
      "Machine Learning Systems",
      "Model Serving Architectures",
      "Model",
      "Models",
      "Machine",
      "learning",
      "Serving",
      "Systems",
      "Serving Model",
      "Model Serving",
      "Data",
      "Learning Systems",
      "Problem and Resolution",
      "machine learning workflow"
    ],
    "concepts": [
      "model",
      "models",
      "modeling",
      "data",
      "systems",
      "conclusion",
      "serving",
      "engineer",
      "engineers",
      "engineering"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 19-26)",
    "start_page": 19,
    "end_page": 26,
    "summary": "Similarly, ML systems, with their surprising behaviors and indirect yet profound interconnections, motivate a more holistic approach toward deciding how to integrate development, deployment, pro‐ duction operations, and long-term care.\nWe believe, in short, that ML systems being reliable captures the essence of what customers, business owners, and staff really want from them.\nIntended Audience We are writing for anyone who wants to take ML into the real world and make a difference in their organization.\nAccordingly, this book is for data scientists and ML engineers, for software engineers and site reliability engineers, and for organiza‐ tional decision makers—even nontechnical ones, although parts of the book are quite technical:\nData scientists and ML engineers\nSoftware engineering building ML infrastructure or integrating ML into existing products\nWe address both how to integrate ML into systems and how to write ML infra‐ structure.\nAn improved understanding of how the ML lifecycle works helps with developing functionality, designing application programming interfaces (APIs), and supporting customers.\nWe’ll also explore the implications of ML model quality not being something a reliability engineer can entirely ignore.\nOrganizational leaders who want to add ML to their existing products or services\nWe will help you understand how best to integrate ML into your existing products and services, and the structures and organizational patterns required.\nFor example, Chapter 2 can certainly be read by data scientists and ML engineers.\nOur Approach Engineers need to employ specific approaches and techniques to make ML systems work well.\nIn some ways, it’s quite a simple business, but the complexity will show up almost immediately as we try to add ML.\nML has the potential to positively or negatively transform the business fundamentally, changing the way products are created and selected, customers are identified and served, and commercial opportunities are uncovered.\nML-adopting businesses that successfully deploy these technologies will outperform their competitors in the long run; a recent survey of over 2,000 executives conducted by McKinsey indicated that 63% of execs had ML/AI projects that improved the bottom line, though organizations are often cagey about precisely how much.\nOur website, yarnit.ai, has many sources of data to implement these initial, concrete improvements and many potential applications for ML.\nConcretely, we’ll work through several examples of areas where ML can improve our operations, although, of course, not limited to these requirements:\nWe might want to use ML to predict how to order replacement products from our suppliers based on a prediction of future sales and the predicted delivery delays.\nWe can use various techniques to try to improve the profit that we make on each sale—ranging from suggesting additional higher-margin products to customers while they shop, to running marketing campaigns to generate more demand for the highest-margin products.\nStand‐ ing up a complex pipeline with exacting data formats, significant engineering, and professional production operations requirements is a costly undertaking that needs to produce noticeable and definite value for the customers and the business.\nOrganizations like our yarn store should approach ML with an open mind but a willingness to experiment, measure, and possibly cancel the applications if they do not work out.\nFor completeness, we need to say that obviously this is only a single example of the kind of organization and application that might find uses for ML.\nHere, relative newcomers to ML (or ML in production) can orient themselves to the problem space as a whole.\nIt is also where we cover critical topics that impact all of the rest of the chapters, such as data management, what an ML model is, how to evaluate its quality, what a feature is (and why you would care), and fairness and privacy.\nWe start with a concrete illustration of the complexities of ML in the incident response domain—something we expect basically every engineer with production responsibilities can relate to.\nWe look at some of the critical questions about how organizations could integrate ML into existing (and emerging) products, often a process best done with some thought put into it in advance.\nThen we look at how ML could be implemented organizationally: centralized, distributed, and every point in between, followed by concrete guidelines and recommendations in the next chapter.\nFinally, we end with real case studies of ML implemented in organizations around the world.\nAfter all of that, well, first of all, you deserve a break—but second of all, you should be well equipped to understand everything you’re likely to come across when doing ML for the first time, or even when refining how it works inside your organization when you already have experience.\nAbout the Authors The authors of this book collectively have decades of experience building and run‐ ning various kinds of ML systems in production.\nad-targeting systems; built large search-and-discovery systems; published ground‐ breaking research on ML in production; and constructed and run the critical data ingestion, processing, and storage systems wrapped around them all.\nThank you to all the coauthors and our amazing group of volunteers who have reviewed, edited, commented, and generally helped us improve this book.",
    "keywords": [
      "book",
      "products",
      "systems",
      "business",
      "customers",
      "data",
      "engineers",
      "Preface",
      "site reliability engineers",
      "existing products",
      "organization",
      "model",
      "production",
      "work",
      "intimately connected"
    ],
    "concepts": [
      "products",
      "production",
      "data",
      "customer",
      "preface",
      "businesses",
      "systems",
      "chapters",
      "concrete",
      "concretely"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 27-35)",
    "start_page": 27,
    "end_page": 35,
    "summary": "We begin with a model, or framework, for adding machine learning (ML) to a website, widely applicable across a number of domains—not just this example.\nThis model we call the ML loop.\nML model developers often hope their lives will be simple, and they’ll have to collect data and train a model only once, but it rarely happens that way.\nSuppose we have an ML model, and we are investigating whether the model works well enough (according to a certain threshold) or doesn’t.\nIf it doesn’t work well enough, data scientists, business analysts, and ML engineers will typically collaborate on how to understand the failures and improve upon them.\nThis involves, as you might expect, a lot of work: perhaps modifying the existing training pipeline to change some features, adding or removing some data, and restructuring the model in order to iterate on what has already been done.\nThis typically involves—you guessed it—modifying the existing training pipeline, changing features, adding or removing data, and possibly even restructuring the model.\nEither way, more or less the same work is done, and the first model we make is simply a starting point for what we do next.\nProd‐ uct and ML engineers will also be involved, thinking about what to do with all of this data, and site reliability engineers (SREs) will make recommendations and decisions about the overall pipeline in order to make it more monitorable, manageable, and reliable.\nManaging data for ML is a sufficiently involved topic that we’ve devoted Chapter 2 to data management principles and later discuss training data in Chapters 4 and 10.\nOnce we have the data in a suitable place and format, we will begin to train a model.\nML Training Pipelines ML training pipelines are specified, designed, built, and used by data engineers, data scientists, ML engineers, and SREs. They are the special-purpose extract, transform, load (ETL) data processing pipelines that read the unprocessed data and apply the\nML algorithm and structure of our model to the data.1 Their job is to consume training data and produce completed models, ready for evaluation and use.\nThese models are either produced complete at once or incrementally in a variety of ways— some models are incomplete in that they cover only some of the available data, and others are incomplete in scope as they are designed to cover only part of the ML learning as a whole.\nTraining pipelines are one of the only parts of our ML system that directly and explicitly use ML-specific algorithms, although even here these are most commonly packaged up in relatively mature platforms and frameworks such as TensorFlow and PyTorch.\nTraining pipelines have all the reliability challenges of any other data transformation pipeline, plus a few ML-specific ones.\nBut ML models can fail silently for reasons related to data distri‐ bution, missing data, undersampling, or a whole host of problems unknown in the\nregular ETL world.3 One concrete example, covered in more detail in Chapter 2, hinges on the idea that missing, misprocessing, or otherwise not being able to use subsets of data is a common cause of failure for ML training pipelines.\nIn case it’s not already clear, ML training pipelines are absolutely and completely a production system, worthy of the same care and attention as serving binaries or data analysis.\nBuild and Validate Applications An ML model is fundamentally a set of software capabilities that need to be unlocked to provide value.\nBy logging such events, this integration will provide new feedback for our model, so that it can train on the quality of its own recommenda‐ tions and begin improving.4 At this stage, though, we will simply validate that it works at all: in other words, that the model loads into our serving system, the queries are issued by our web server application, the results are shown to users, the predictions are logged, and the logs are stored for future model training.\nQuality and Performance Evaluation ML models are useful only if they work, of course.\nFinally, there’s a middle ground: we might build the capability in our application to only sometimes use the model for a fraction of users.\nA concrete example is “99.99% of HTTP requests completing successfully (with a 20x code) within 150 ms.” SLOs are the natural domain of SREs, but they are also critical for product managers who specify what the product needs to do, and how it treats its users, as well as data scientists, ML engineers, and software engineers.\nSpecifying SLOs in general is challenging, but specifying them for ML systems is doubly so because of the way that subtle changes in data, or even in the world around us, can significantly degrade the performance of the system.\nThis is unlikely to help us figure out all aspects of whether the model works and might generate genuinely bad user experiences.\nSo then, for a web application, we might select 1% of all logged-in users to get the model-generated results or perhaps 1% of all cookies.\nFor training, we should probably look at throughput (examples per second trained or perhaps bytes of data trained if our models are all of comparable complexity).\nNotice, however, that none of these examples is about the ML performance of the models.\nWe examine this in more detail in Chapter 9, but for the moment we ask you to accept there are reasonable ways to arrive at SLOs for an ML context, and they involve many of the same techniques that are used in non-ML SLO conversations elsewhere (though the details of how ML works are likely to make such conversations longer).\nOnce we have gathered the data, built the model, integrated it into our application, measured its quality, and specified the SLOs, we’re ready for the exciting phase of launching!\nHere product software engineers, ML engineers, and SREs all work together to ship an updated version of our application to our end users.",
    "keywords": [
      "model",
      "data",
      "Training Pipelines",
      "training",
      "model training",
      "system",
      "models",
      "Kranti Parisa",
      "Pipelines",
      "engineers",
      "application",
      "SLOs",
      "users",
      "training data",
      "ETL data pipeline"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "train",
      "trained",
      "production",
      "product",
      "products",
      "pipelines",
      "useful"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 36-44)",
    "start_page": 36,
    "end_page": 44,
    "summary": "Remember that models are code every bit as much as your training system binaries, serving path, and data processing code are.\nIt is important to treat code and model launches similarly: even though some organizations ship new models over (say) the holiday season, it’s entirely possible for the models to go wrong, and we’ve seen this happen in a way that required code fixes shortly thereafter.\nThis is not just an ML problem, and failure to isolate the data of a new code path from an older code path has provided some exciting outages over the years.\nThis can happen to any system that processes data produced by a different element of the system, although the failures in ML systems tend to be subtler and harder to detect.\n6 For completeness, it is also true that there’s a safe way to roll out a new data format: specifically, by adding support for reading the format in a progressive rollout that completes before the system starts writing the format.\nIs new data arriving?\nUltimately, product and business leaders will have to establish real-world metrics that indicate whether models are performing according to their requirements, and the ML engineers and SREs will need to work together to determine which quality measures are most directly correlated with those outcomes.\nAs a final step in the loop, we need to ensure that the ways that our end users interact with the models make it back into the next round of data collection and are ready to travel the loop again.\nLessons from the Loop It should be clear now that ML begins and ends with data.\nSuccessfully, reliably integrating ML into any business or application is not possible without understanding the data that you have and the information you can extract from it.\n7 Don’t forget data drift either: a model from 2019 would have a very different idea about the importance and\nThe training environment constrains the kind of data we will use and how much of it we can process.\nEven for organizations with a lot of experience with ML and the ability to evaluate the quality and value of models, most new ML ideas should be trialed first, because most new ML ideas don’t work out.\nCHAPTER 2 Data Management Principles\nInstead, we are overwhelmingly interested in two things: the data used to construct the models, and the processing pipeline that takes the data and transforms it into models.\nUltimately, ML systems are data processing pipelines, and their purpose is to extract usable and repeatable insights from data.\n(We cover these topics at length in Chapter 9.) Fundamentally, they consume data, and output a processed representation of that data (though vastly different forms of both).\nAs such, ML systems depend thoroughly and completely on the structure, performance, accuracy, and reliability of their underlying data systems.\nIn this chapter, we will start with a deep dive on data itself:\nWhere data comes from •\nHow to interpret data •\n• Data quality\nUpdating data sources (which we use and how we use them) •\nAssembling data into an appropriate form for use •\nWe’ll cover the production requirements of data and show that, just like models, data in production has a lifecycle:\nAt the end of this chapter, we expect you to have a complete but superficial understanding of the primary issues involved in making the data processing chain reliable and manageable.\nData as Liability Writing about ML almost universally suggests that data is an important asset in ML systems.\nThis perspective is sound: it’s certainly impossible to have an ML system without data.\nAs shown in Figure 2-1, it is often true that a simple (or even simplistic) ML system with more (and higher-quality) training data can outperform a more sophisticated system with less, or less representative, data.2\nNetflix also reportedly used this data, once it got into the content production side of the business, to figure out what shows to make for which audiences, based on a detailed understanding of what people want to watch.\n2 For the data to be useful, it has to be of high quality (accurate, sufficiently detailed, representative of things in the world that our model cares about).\nAnd for supervised learning, the data has to be consistently and accurately labeled—that is, if we have pictures of yarn and pictures of needles, we need to know which ones are which so that we can use that fact to train a model to recognize these kinds of pictures.\nChapter 2: Data Management Principles\nIllustrative trade-offs of data size, model error rates, and risk of problems or issues associated with the data\nData as Liability\nPseudo‐ nymization also hopefully preserves the properties of data that are relevant to our model.\nExample Applications for Different Levels of Anonymization Different controls are required for data, depending on the sensitivity of the data that is being accessed:\nPseudonymized data\nChapter 2: Data Management Principles",
    "keywords": [
      "data",
      "system",
      "model",
      "systems",
      "models",
      "Data Management Principles",
      "Model quality",
      "Data Management",
      "quality",
      "data systems",
      "serving system",
      "data processing",
      "underlying data systems",
      "online system",
      "data collection"
    ],
    "concepts": [
      "models",
      "model",
      "modeling",
      "systems",
      "quality",
      "different",
      "difference",
      "differences",
      "processes",
      "process"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 45-52)",
    "start_page": 45,
    "end_page": 52,
    "summary": "Anonymized data\nAll data processing pipelines are, in some sense, subject to the correctness and volume of their input data, but ML pipelines are furthermore sensitive to subtle changes in distribution of the data.\nA pipeline can easily go from mostly right to significantly wrong simply by omitting a small fraction of the data, provided that small fraction is not random, or is somehow not evenly sampled in the range of characteristics our model is sensitive to.\nIn many of these cases, losing a small amount of data that turns out to be systematically biased results in significant confusion in the understanding and predictions of our models.\nAs a result of this sensitivity, the ability to aggregate, process, and monitor data, rather than only the live systems, is critical to successfully managing ML data pipe‐ lines.\nThe Data Sensitivity of ML Pipelines\nIf we train on this data, our model will probably have terrible results for Spanish-language searches.\nWe are lucky enough to have SREs who will address reliability concerns related to the storage and processing of data.\nThe data management stage is fundamentally about transforming the data we have into a format and storage organization suitable for using it for later stages of the process.\nDuring this process, we will also probably apply a range of data transforma‐ tions that are model specific (or at least model-domain specific) in order to prepare the data for training.\nOur next operations on the data will be to train ML models, anonymize certain sensitive items of data, and delete data when we no longer need it or are asked to do so.\na basic understanding of model training does directly inform what we do with data as we get it ready.\nData management in modern ML environments consists of multiple phases before feeding the data into model-training pipelines, as illustrated in Figure 2-2:\n• Post-processing (which includes data management, storage, and analysis)\nML data management phases\nCreation It may seem either odd or obvious to state, but ML training data comes from some‐ where.\nImplicit in this process is that we will want to design new systems and adapt existing systems to generate more data than we might otherwise, so that our ML systems have something to work with.\nPhases of Data\nSince it is well formatted, structured data can be easily processed by relatively simple code using obvious heuristics.\nOn the other hand, unstructured data is qualitative without a standard data model/ schema, so it cannot be processed and analyzed using conventional data methods and tools.\nThe character of the internal structure of the data will have significant implications for the way we process, store, and use it.\nML training data categories\nAlthough bias in models comes from the structure of the model as well as the data, the circumstances of data creation have profound implications for correctness, fairness, and ethics.\nYou could relatively easily combine the effort to detect bias into a data provenance or data lifecycle meeting or tracking process, for example.\nOne final note on dataset creation, or rather, dataset augmentation: if we have a small amount of training data but not enough to train a high-quality model on, we may need to augment that data.\nPhases of Data\nIngestion The data needs to be received into the system and written to storage for further processing.\nWe may filter data by type at this stage (data fields or elements that we do not believe will be useful for our model).\nWe may also simply sample at this stage if we have so much data we do not believe we can afford to process all of it, as ML training and other data processing is often extremely computationally expensive.\nSampling data can be an effective way to save money on intermediate processing and training costs, but it is important to measure the quality cost of sampling and compare that to the savings.\nIn general, ML training systems perform better with more data.\nA date- or time-oriented bucketing system in storage combined with an off-by-one error in the ingestion process could end up with every day’s data stored in the previous day’s directory.\nProcessing Once we have successfully loaded (or ingested) the data into a reasonable feature storage system, most data scientists or modelers will go through a set of common operations to make the data ready for training.",
    "keywords": [
      "data",
      "Data Management",
      "Data Management Principles",
      "model",
      "data processing pipelines",
      "data model",
      "data processing",
      "training data",
      "data storage system",
      "process",
      "system",
      "data storage",
      "Structured data",
      "private data",
      "Pipelines"
    ],
    "concepts": [
      "data",
      "storage",
      "systems",
      "process",
      "processing",
      "processed",
      "model",
      "models",
      "modelers",
      "management"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 53-61)",
    "start_page": 53,
    "end_page": 61,
    "summary": "No matter how efficient and powerful our ML models are, they can never do what we want them to do with bad data.\nPhases of Data\nNormalization generally refers to a set of techniques used to transform the input data into a similar scale, which is useful for methods like deep learning that rely on gradient descent or similar numerical optimization methods for training.\nPhases of Data\nBut labeling is only one way to extend data.\nWe might use many external data sources to extend our training data.\nStorage Finally, we need to store the data somewhere.\nHow and where we store the data is mostly driven by how we tend to use it, which is really a set of questions about training and serving systems.\n• Are we training models once over this data or many times?\nWill each model read all of the data or only parts of it?\nOn the subject of reuse of data, it turns out that almost all data is read multiple times and the storage system should be built for that, even if model owners assert that they will train only one model on the data once.\nAn ML engineer makes a model (reading the data necessary to do so), measures how well the model performs at its designed task, and then deploys it.\nEvery system we build, from data to training all the way to serving, should be built with the assumption that model developers will semi-continuously retrain the same models in order to improve them.\nIndeed, as they do so, they might read different subsets of the data each time.\nGiven this, a column-oriented storage scheme with one column per feature is a common design architecture, especially for models training on structured data.7 Most readers will be familiar with row-oriented storage, in which every fetch of data from the database retrieves all of the fields of a matching row.\nThis is much more useful for a collection of applications (ML training pipelines in this case) that each use a given subset of the data.\nIn other words, column-oriented data storage allows different models that efficiently read different subsets of features and do so without reading in the whole row of data every time.\nThe more data we collect in one place, the more likely it is that we will have different models using very different subsets of that data.\nWhen multiple people work on building models on the same data (or when the same person works on this over time), metadata about the stored features provides huge value.\n7 Most common, large data storage and analysis services from large cloud providers are column oriented.\nPhases of Data\nAnd if data management is about how and why we write data, ML training pipelines are about how and why we read it.\nManagement Typically, data storage systems implement credential-based access controls to restrict unauthorized users from accessing the data.\nFor example, we might want to allow only model developers to access the features that they directly work on or restrict their access to a subset of the data in some other way (perhaps only recent data).\nSREs can configure data access restrictions on the storage system in production to allow data scientists to read data securely via authorized networks like virtual private networks (VPNs), implement audit logging to track which users and training jobs are accessing what data, generate reports, and monitor usage patterns.\nThis section covers just the basics of making sure that the data is not lost (durability), is the same for all copies (consistency), and is tracked carefully as it changes over time (version control).\nThis might include cases where the data needs to be recovered from another slower storage system (say, a tape drive) or copied from off-site over a slow network connection.\nData Reliability\nFor an ML storage system with many data transformations, we need to be careful about how those transformations are written and monitored.\nConsistency We may want to guarantee that, as we access data from multiple computers, the data is the same with every read; this is the property of consistency.\nWhether the model-training system cares about consistency is actually a property of the model and the data.\nNot all training systems are sensitive to inconsistency in the data.\nThe first is to build models that are resilient to inconsistent data.\nIf we can tolerate inconsistent data, especially when the data is recently written, we might be able to train our models significantly faster and operate our storage system more cheaply.\nThe most common way to do that for a replicated storage system is for the system itself to provide information about what data is completely and consistently replicated.\nSystems that read the data can consume this field and choose to train only on the data that is fully replicated.\nIf we want to use the data quickly after ingestion and transformation, we may need to have lots of resources provisioned for networking (to copy the data) and storage I/O capacity (to write the copies).\nVersion Control ML dataset versioning is, in many ways, similar to traditional data and/or source code versioning used to bookmark the state of the data so that we can apply a specific version of the dataset for future experiments.\nVersioning becomes important when new data is available for retraining and when we’re planning to implement different data preparation or feature engineering techniques.\nData Reliability",
    "keywords": [
      "data",
      "storage system",
      "Data Management",
      "Data Management Principles",
      "system",
      "Storage",
      "data management system",
      "model",
      "systems",
      "data storage",
      "data storage systems",
      "training",
      "Data Reliability",
      "models",
      "read"
    ],
    "concepts": [
      "data",
      "model",
      "consistency",
      "consistent",
      "consistently",
      "different",
      "differently",
      "value",
      "values",
      "systems"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 62-69)",
    "start_page": 62,
    "end_page": 69,
    "summary": "Performance The storage system needs fast-enough write throughput to rapidly ingest the data and not slow transformations.\nAvailability The data we write needs to be there when we read it.\nIf the data is in our storage system and is consistently replicated, and we are able to read it with reasonable performance, then the data will count as available.\nThis means respecting provenance, security, and integrity.10 Our data management system will need to be designed for these properties from the beginning to be able to make appropriate guarantees about the kind of access controls and other data integrity we can offer.\n9 Some readers might read “version control” and think “Git.” A content-indexed software version control system like Git is not really appropriate or necessary to track versions of ML data.\nSince this entire chapter is about data management, durability has been grouped with reliability concepts, and integrity here refers to properties we can assert about the data, beyond its mere existence and accessibility.\nSecurity Valuable ML data often begins its life as private data.\nAs previously mentioned, it is notoriously difficult to identify PII without a thoughtful analysis so unless there is careful, time- consuming, human review of all data that is added to the feature store, it is extremely likely that some of the data in combination with other data does contain PII.\nBeyond concerns about PII, teams will likely develop particular uses for particular kinds of data.\nReasonable use of the datastore will restrict access to certain data to the team most likely to need and use that data.\nThoughtful restriction of access will actually increase productivity if model developers can easily access (and only access) the data they are most likely to use to build models.\nPrivacy When ML data is about individuals, the storage system will need to have privacy- preserving characteristics.\nOne of the fastest ways to transform data from an asset to a liability is by leaking private information about customers or partners.\nIf we prevent PII data from ever being stored in the data storage system, we eliminate most of the risk of holding private data.\nData Integrity\nTo make those kinds of recommendations, however, we need private data.\nIf we decide that our models can achieve their goals only by having access to private data, we will need to have a serious conversation about the architecture of storing, using, and eventually deleting that private data.\nAdditionally, using data like this, combined with general datastores with data from multiple users, requires federated\nlearning—an advanced topic that’s beyond the scope of this book.12 (See Figure 2-5 for a list of the types of data and access control implications.)\nChoices and processing as data moves through an ML system\nNote that doing so durably will require periodic review to ensure that current anonymization still matches the assumptions made about the data and access permissions when it was implemented, as well as a review every time new data sources are added to ensure that connections among data sources do not undermine anonymization.\nData Integrity\nPolicy and compliance requirements for data storage should be taken seriously.\nIf pri‐ vate data requires special treatment, there may be a way to avoid those requirements simply by determining (and documenting) that we are not storing any private data.\nSo we will have to think carefully about what data storage and processing requirements we need to comply with.\nConclusion This has been a rapid and superficial introduction (though it probably doesn’t feel like that) to thinking about data systems for ML.\nIf we have started using ML at all, it is likely that our data science teams are already building bespoke data transformation pipelines in various places around the organization.\nCleaning, normalizing, and transforming the data are normal operations that are required to do ML.\nWe obviously need significant data storage and processing infrastructure to man‐ age ML data well.\nThis involves work that is quite different from the work often performed by many data scientists and ML researchers, who try to spend their days developing new predictive models and methods that can squeeze out another percentage point of accuracy.\nWe will say at the outset that our goal here is not to teach you everything about how to build ML models, which models might be good for what problems, or how to become a data scientist.",
    "keywords": [
      "data",
      "private data",
      "Data Management Principles",
      "data management",
      "Data Integrity",
      "system",
      "data management system",
      "Data Integrity Data",
      "storage system",
      "data storage",
      "private",
      "models",
      "PII",
      "data storage system",
      "systems"
    ],
    "concepts": [
      "data",
      "requirements",
      "require",
      "required",
      "requires",
      "requirement",
      "models",
      "modeling",
      "compliance",
      "reason"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 70-77)",
    "start_page": 70,
    "end_page": 77,
    "summary": "In using data to train a model, we hope the resulting model will both fit our past data well and generalize to predict well on new, previously unseen data in the future.\nFeatures represent key qualities of the data in a way that ML models can easily digest.\nFor supervised ML, we also require a label of some kind, showing the historical outcome that we would like our model to predict, if it were to see a similar situation in the future.\nWe’ll then train a model on this historical data, using a chosen model type and a chosen ML platform or service.\nCurrently, many folks choose to use model types based on deep learning, also known as neural networks, which are especially effec‐ tive when given very large amounts of data (think millions or billions of labeled examples).1 Neural networks build connections among layers of nodes based on the examples that are used in training.\nIn other settings, methods like random forests or gradient boosted decision trees work well when presented with fewer examples.2 And more complex, larger models are not always preferred, either for predictive power or for maintainability and understandability.\nThose who are not sure which model type might work best often use methods like automated machine learning (AutoML), which trains many model versions, and try to automatically pick the best one.\nRegardless, at the end, our training process will produce a model, which will take the feature inputs for new examples and produce a prediction as an output.\nInstead of using this for training, we wait and use it to stress-test our model, reasoning that because the model has never seen this held-out data in training, it can serve well as the kind of previously unseen data for which we hope it will make useful predictions.\nSo we can feed each example in this test set to the model and compare its prediction to the true outcome label to see how well it measures up.\nIt is critical that this validation data is indeed held out from training, because it is all too easy for a model to experience overfitting, which happens when the model memorizes its training data perfectly but cannot predict well on new unseen data.\nThe configuration of the model plus the training environment, and the type and definition of data we will train on.\nTrained model\nA specific snapshot or instantiated representation of the configured model trained on specific data at a point in time, containing a specific set of trained model parameters such as weights and thresholds.\nAs a result, the exact same configured model trained twice on the exact same data may or may not produce a significantly different trained model.\nThe qualities of the training data establish the qualities and behavior of our models and systems, and imperfections in training data can be amplified in surprising ways.\nTraining on this data would likely result in a model that shows very high accuracy on held-out test data, but that is essentially just a white-wall detector.\nDeploying this model on real data could have terrible results, even while showing excellent performance on held-out test data.\nAgain, uncovering such issues requires careful consideration of the training data, and targeted probing of the model with well-chosen examples to stress-test its behavior in a variety of situations.\nMany production modeling systems collect additional data over time in deployment and are retrained or updated to incorporate that data.\nThis can happen for systems that were not set up to collect data initially, like if a weather service wants to use a newly created network of sensors that has never been previously deployed.\nThese systems have little or no training data at the very start of their lifecycle, and can also encounter cold-start problems for individual items when new products are released over time.\nMany models are used within feedback loops, in which they filter data, recommend items, or select actions that then create or influence the model’s training data in the future.\nA model that helps recommend yarn products to users will likely get future feedback only on the few near-the-top rankings that are actually shown to users.\nThis can cause a situation in which data that is not selected never gets positive feedback, and thus never rises to prominence in the model’s estimation.\nSolving this often requires some amount of intentional exploration of lower-ranked data, occasionally choosing to show or try data or actions that the model currently thinks are not as good, in order to ensure a reasonable flow of training data across the full spectrum of possibilities.\nFor example, imagine that our model helps recommend hotels to users, based on its learning on feedback from previous user bookings.\nIt is easy to imagine a scenario in which a COVID-style lockdown creates a sudden sharp drop in hotel bookings, meaning that a model trained on pre-lockdown data is now extremely overly optimis‐ tic.\nLabels In supervised ML, the training labels provide the “correct answer,” showing the model what it should be trying to predict for a given example.\nBecause labels are so important to model training, it is easy to see that problems with labels can be the root cause for many downstream model problems.\nFor example, some email spam systems allow users to label messages as “spam” or “not spam.” It is easy to imagine that a motivated spammer may try to fool such a system by sending many spam emails to accounts under their own control and try to label them as “not spam” in an attempt to poison the overall model.\nIt is also easy to imagine that a model attempting to predict the number of stars a certain product will receive in user reviews may be potentially vulnerable to bad actors who try to overrate their own products—or to underrate those of their competitors.\nIn addition to problems with developing a complete and representative dataset, or labeling examples correctly, we can encounter threats to the model during the model- training process.",
    "keywords": [
      "model",
      "data",
      "training data",
      "models",
      "training",
      "Basic Model Creation",
      "Trained Model",
      "systems",
      "Versus Trained Model",
      "Versus Model Definition",
      "Model Definition Versus",
      "Data Training data",
      "Model Architecture Versus",
      "Architecture Versus Model",
      "Model Creation Workflow"
    ],
    "concepts": [
      "model",
      "models",
      "modeling",
      "data",
      "label",
      "labeled",
      "labels",
      "labeling",
      "train",
      "training"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 78-85)",
    "start_page": 78,
    "end_page": 85,
    "summary": "As we discussed in the brief overview of a typical model lifecycle, a good model will generalize well to new, previously unseen data and not just narrowly memorize its training data.\nEach time a model is retrained or updated, we need to check for overfitting by using held-out validation data.\nFor example, if our validation dataset about purchases on yarnit.ai is never updated, while our customers’ behavior changes over time to favor brighter wools, our models will fail to track this change in purchase preferences because we will score models that learn this behavior as being of “lower quality” than models that do not.\nOne version of a model might perform great recognizing cats and poorly at dogs, while another version might be quite a bit better on dogs and less good on cats—even if both models have similar aggregate accuracy on validation data.\nWhen deep learning models are trained from scratch—with no prior information— they begin from a randomized initial state and are fed a huge stream of training data in randomized order, most often in small batches.\nWe stop training after we decide that the model shows good performance on held-out validation data.\nTherefore, repeating the process of training the same model with the same settings on the same data can lead to substantially different final models.\nThe model performance on held-out validation data generally improves with additional training steps, but sometimes bounces around early on, and later can often get significantly worse if the model starts to overfit the training data by memorizing it too closely.\nDeep learning models sometimes explode in training\nAs has been mentioned, hyperparameters are the various numeric settings that must be tuned for an ML model to achieve best performance on any given task or dataset.\nDeep learning models are notoriously sensitive to such settings, which means that significant experimentation and testing is required.\nDeep learning methods extrapolate from their training data, which means that the more unfamiliar a new previously unseen data point is, the more likely we are to have an extreme prediction that might be completely off base or out of the range of typical behavior.\nInfrastructure and Pipelines Models are just one component in larger ML systems, which are typically supported by significant infrastructure to support model training, validation, serving, and mon‐ itoring in one or more pipelines.\nThese systems thus inherit all of the complexities and vulnerabilities of traditional (non-ML) pipelines and systems in addition to those of ML models.\nAn additional consideration is that because these platforms are typically created as general-purpose tools, we usually need to create a significant number of adapter components, or glue code, that can help us transform our raw data or features into the correct formats to be used by the platform, and to interface with the models at serving time.\nThe first is that errors in feature generation are often not visible by aggregate model performance metrics, such as accuracy on held-out test data.\nFor example, if our model relies on a localized time of day for an on-device model, that may be computed via a global batch-processing job when training data is computed, but it may be queried directly from the device at serving time.\nFor example, if our model for generating yarn-purchase predictions depends on a lookup query to another service that reports user reviews and satisfaction ratings, our model would have serious problems if that service suddenly went offline or stopped returning sensible responses.\nThis is because any change to the distribution of feature values that our model expects to see associated with certain data may cause erroneous behavior.\nIn MLOps, we can find value in a different set of questions that will help us understand where our models and systems can go wrong, how we can fix issues when they occur, and how we can preventatively engineer for long-term system health:\nFeatures, the information we extract from raw data to enable easy digestion for ML models, are often added by model developers with a “more is always better” approach.\nIn such cases, the model’s predictions can go awry, but we may have no ground-truth validation data to use to detect this.\nNearly all ML models are imperfect and will make mistakes in their predictions on at least some kinds of examples.\nIt is important to spend time looking at data that our model makes errors on, understanding any commonalities or trends, so that we can identify whether these failure modes will impact the downstream use cases in important ways.\nHow is the model updated over time?\nSome models are rarely updated, such as models for automated translation that are trained on huge amounts of data in large batches, and pushed to on-device applications once every few months.\nWe need to know the full set of upstream dependencies that provide data to our model, both at training time and serving time, and know how they might change or fail and how we might be alerted if this happens.\nSimilarly, we need to know all of the downstream consumers of our model’s predictions, so that we can appropriately alert them if our model should experience issues.\nWe also need to know how the model’s predictions impact the end use case, if the model is part of any feedback loops (either direct or indirect), and if there are any cyclic dependencies such as time-of-day, day-of-week, or time-of-year effects.\nPerhaps most importantly, we need to know what happens to the larger system if the ML model fails in any way, or if it gives the worst possible prediction for a given input.\nAn Example ML System To help ground our introduction to basic models, we will walk through some of the structure of an example production system.\nYarn Product Click-Prediction Model In our imaginary yarnit.ai site, ML models are applied to many areas.\nFeatures The model used in this setting is a deep learning model that takes as input the following set of features:\nBecause these characteristics are expressed in a wide variety of ways in product descrip‐ tions from different manufacturers, each characteristic is predicted by a separate component model specially trained to identify that characteristic from product",
    "keywords": [
      "model",
      "deep learning models",
      "models",
      "data",
      "deep learning",
      "training data",
      "validation data",
      "learning models",
      "training",
      "system",
      "time",
      "deep",
      "learning",
      "Deep learning methods",
      "systems"
    ],
    "concepts": [
      "model",
      "models",
      "modeling",
      "data",
      "predictions",
      "predictive",
      "prediction",
      "predicting",
      "predicted",
      "training"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 86-93)",
    "start_page": 86,
    "end_page": 93,
    "summary": "Because products that appear higher in the listed results are more likely to be viewed and clicked than products listed farther down, it is important at training time to have features that show where the product was listed at the time the data was collected.\nNote, however, that this introduces a tricky dependency—we cannot know at serving time the value of these features, because the ranking and ordering of the results on the page depends on our model’s output.\nLabels for Features The training labels for our model are a straightforward mapping of 1 if the user clicks the product and 0 if the user does not click the product.\nOther, more nuanced but equally ill-intentioned manufacturers attempt to lower their competitors’ listings by issuing many queries that place their competitors listings near the top without clicking them.7 Both of these are attempts at fraud or spam and need to be filtered out before the model is trained on this data.\n7 This works because every time a product is shown but not clicked, the model learns that the product was not a good result for that customer in that context, or at least learns that the product was not the best result.\nOf course, this happens frequently for all products, but if a competitor is able to swamp the system with millions (or more!) of instances of a product being shown but not clicked, the model will learn that customers, in general, just don’t like that product.\nFinally, the feature-extraction jobs that create the training data require batch-processing resources and introduce several more hours of delay.\nIn practice, then, our model is updated about 12 hours after a given user has viewed or clicked a given product.\nThis is done from time to time by model developers when new model types or additional features are added to the system.\nIf our model were to score 0 for all products, none would be shown to users.\nThis would, of course, indicate an issue with the model, but we need to monitor average predictions and fall back to another system if things go awry.\nImagine that our model is corrupted, or an input feature signal becomes unsta‐ ble.\nA small amount of randomization can be helpful in these situations to ensure that the model gets some amount of exploration data, which will allow the model to learn about products that had not previously been shown.\nThis form of exploration data is also useful for monitoring, as it allows us to reality-check the model’s predictions and ensure that when it says a product is unlikely to be clicked, this holds true in reality as well.\n• Model behavior is determined by data, rather than by a formal (program) specification.\n• If our data changes, our model behavior will change.\n• Features and training labels are critical inputs to our models.\nCHAPTER 4 Feature and Training Data\nThis chapter is about the data: how it is created, processed, annotated, stored, and ultimately used to create the model.\nWe discussed features a little in the previous chapter; another way of thinking about them is that they are characteristics of the input data, especially characteristics that we have determined are predictive of something we care about.\nFeatures are what make it useful for ML.1 A feature is any aspect of the data that we determine is useful in accomplishing the goals of the model.\nHere, “we” includes the humans building the model or many times can now include automatic feature-engineering systems.\nFeatures are used to build models.\nPreviously, we have said that a model is a set of rules that takes data and uses it to generate predictions about the world.\nBut a trained model is very much a formula for combining a collection of features (essentially, feature definitions used to extract feature values from real data—we cover this definition more completely later in this chapter).\nAs modeling techniques continue to develop, it seems likely that more features will start to resemble raw data rather than these extracted elements.\nThis has significant implications for modeling, of course, but people building production ML systems should also be aware of the impact on systems of having the training data grow larger and less structured.\n1 We are aware that one of the promises of deep learning is that it is sometimes possible to use raw data to train a model without specifically identifying features.\nIn those cases, we may not need to extract specific features from the underlying data, although we may still get good value from metadata features.\nSo even deep learning modelers will benefit from an understanding of features.\nChapter 4: Feature and Training Data\nIn this context, we might consider some of the following useful features that we would want available to the model as it is queried for additional products:",
    "keywords": [
      "model",
      "data",
      "features",
      "product",
      "Feature",
      "products",
      "system",
      "models",
      "training data",
      "time",
      "user",
      "systems",
      "predictions",
      "training",
      "customer"
    ],
    "concepts": [
      "features",
      "feature",
      "models",
      "model",
      "modeling",
      "modelers",
      "products",
      "production",
      "prediction",
      "predictions"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 94-106)",
    "start_page": 94,
    "end_page": 106,
    "summary": "This is a specific output of a feature definition applied to incoming data.\nFeature Selection and Engineering Feature selection is the process by which we identify the features in the data that we will use in our model.\nChapter 4: Feature and Training Data\nFor algorithmic feature engineering, sometimes included as part of AutoML, the process is considerably more automatic and data bound.\nStill, algorithms are generally able to identify only features that exist in the data, whereas humans can imagine new data that could be collected that might be relevant.\nWithout data, we have no features.\nWe need to collect or create data in order to create features.\nAlthough even the process of feature creation could be considered some kind of data normalization, here we’re referring to coarser preprocessing: eliminating obviously malformed examples, scaling input samples to a common set of val‐ ues, and possibly even deleting specific data that we should not train on for policy reasons.\nFeatures\nWe need to write code that reads the input data and extracts the features that we need from the data.\nBut if we expect to train on the same data more than a few times, it’s probably sensible to extract the feature from the raw data and store it for later efficient and consistent reading.\nIt is important to remember that if our application involves online serving, we need a version of this code to extract the same features from the values that we have available at serving in order to use them to perform inference in our model.\nA feature store is just a place to write extracted feature values so that they can be quickly and consistently read during training a model.\n7. Model training and serving using feature values\nChapter 4: Feature and Training Data\noptionally, reprocess older data to create a new set of feature values for the new version.\nSometimes we need to be able to delete feature values from our feature store.\nWe will need to remove any serving code that refers to the feature, remove the feature values in the feature store, cancel the code that extracts the feature from the data, and proceed to delete the feature code.\nFeature Systems To successfully manage the flow of data through our systems, and to turn data into features that are usable by our training system and manageable by our modelers, we need to decompose the work into several subsystems.\nAs was mentioned in the introduction to this chapter, one of these systems will be a metadata system that tracks information about the data, datasets, feature generation, and labels.\nFor now, let’s walk through the feature systems starting with raw data and ending up with features stored in a format ready to be read by a training system.\nWe will have to write software that reads raw data, applies our feature extraction code to that data, and stores the resulting feature values in the feature store.\nFeatures\nThis helps them write reliable feature-extraction code so that we can run that code reliably in our data ingestion system.\nAt this point, we may want to allow the feature to be run or may want additional human review for reliability concerns (dependence on external data, for example).\nIn cases like this, the data ingestion system will also generate labels for those features as well as the features themselves, and both can be stored together in a common datastore.\nFeature store\nA feature store is a storage system designed to store extracted feature (and label) values so that they can be quickly and consistently read during training a model and even during inference.\nMost ML training and serving systems have some kind of a feature store even if it is not called that.2 They are most useful, however, in larger, centrally managed services, especially when the features (definitions and val‐ ues both) are shared among multiple models.\na structured, managed feature store, since there’s no need to mediate that data for other on-device users.\nChapter 4: Feature and Training Data\nour feature and label data in a single, well-managed place has significantly improved the production readiness of ML in the industry.\nStore feature definitions\nUsually these are stored as code that extracts the feature in a raw data format and outputs the feature data in the desired format.\nStore feature values themselves\nServe feature data\nProvide access to feature data quickly and efficiently at a performance level suitable to the task.\nTo get the most out of our feature store, we should keep information about the data we store in it in the metadata system.\nMany feature stores also provide basic normalization of data in ingestion as well as more sophisticated transformations on data in the store.\nFeatures\nWill we read the features frequently or only when training a new model?\n• Is the feature data ingested once, never appended to, and seldom updated?\nOnce we are clear on the requirements for an API and have a clearer understanding of our data access needs, in general we will find that our feature store falls into one of two buckets:4\nChapter 4: Feature and Training Data\nLifecycle Access Patterns As we make choices about our feature store, it is critical to keep in mind how, and how often, we will use the data that we are storing.\nMany feature stores will need to be replicated, partially or completely, in order to store data near the training and serving stacks.\nFor those of us who have worked on relational database systems, transforming features are the stored procedures of feature stores—they are fixed transformations of the data, written in code, but stored in the storage system.\nFeatures\nIn all cases, the key is to remember that transforming features offer a consistent and deterministic value that is the programmatic result of the code we have written being applied to the data in the feature store.\nIf a transforming feature is computationally intensive and we intend to read that column frequently, a feature store might choose to materialize that feature, by processing new data as it arrives and writing out the result of the transforming feature into a new column.\nFirst, we may change the definition of the feature, which would require recomputing the column over all of the data.\nChapter 4: Feature and Training Data\nLabels Although features seem like the most important aspects of the data, one thing is more important: labels.\nWhile features serve as the input to the model, labels are examples of the correct model output.\nSince this data is generated from the system’s log data often with the feature data, these labels are most commonly stored in the feature system for model training and evaluation.\nIn fact, all labels can be stored in the feature store, although labels generated by humans need additional systems to generate, validate, and correct these labels before storing them for model training and evaluation.\nHuman-Generated Labels So let’s turn our attention to the large classes of problems requiring human annota‐ tions to provide the model training data.\nOne technique commonly used is data augmentation: the feature data is “fuzzed” in a way that changes the features but doesn’t change the correctness of the label.5 For example, consider our stitch classification problem.\nChapter 4: Feature and Training Data\nBy treating human annotations as their own columns, we can take advantage of all the other functionality provided by the feature store.\nChapter 4: Feature and Training Data",
    "keywords": [
      "feature",
      "feature store",
      "data",
      "features",
      "feature data",
      "transforming features",
      "model",
      "Training Data",
      "Feature definition",
      "store",
      "Training",
      "system",
      "Feature engineering",
      "feature data Provide",
      "Data ingestion system"
    ],
    "concepts": [
      "feature",
      "features",
      "data",
      "labels",
      "labeling",
      "label",
      "labeled",
      "model",
      "models",
      "modelers"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 107-114)",
    "start_page": 107,
    "end_page": 114,
    "summary": "Active Learning and AI-Assisted Labeling Active learning techniques can focus the annotation effort on the cases in which the model and the human annotators disagree or the model is most uncertain, and thereby improve overall label quality.\nSuch active learning techniques can actually increase overall label quality since the model and humans will often have their best performance on different kinds of input data.\nA semi-supervised system allows the modeler to bootstrap the system with weak heuristic functions that imperfectly predict the labels of some data, and then use humans to train a model that takes these imperfect heuristics to high-quality training data.\nWhile labeling instructions often start simply, they inevitably get more complex as data is labeled and various corner cases are discovered.\nLabeling definitions and directions should be updated as new corner cases are discov‐ ered, and the annotation and modeling teams should be notified about the changes.\nIf the changes are significant, previously labeled data might have to be re-annotated to correct data labeled with old instructions.\nMetadata Feature systems and labeling systems both benefit from efficient tracking of metadata.\nNow that you have a relatively complete understanding of the kinds of data that will be provided by a feature or labeling system, you can start to think about what metadata is produced during those operations.\nIn the case of features and labels, it should minimally keep track of the feature definitions we have and the versions used in each model’s definitions and trained models.\nEven within this very chapter, you’re about to see that we’re going to need to store metadata about labels, including their specification and when particular labels were applied to particular feature values.\nLater, we’re going to need a system for mapping model definitions to trained models, along with data about the engineers or teams responsible for those models.\nFrom a systems design perspective, we should ensure that our metadata systems are never in the live path of either model training or model serving.\nBut it’s difficult to imagine how feature engineering or labeling can take place without the metadata system being functional, so it can still cause production problems for our humans working on those tasks.\nChapter 4: Feature and Training Data\nPerhaps we would have one for features and labels, one for training, one for serving, and one for quality monitoring.\nWe will have processes that need to join data across the features, labeling, training, and serving systems.\nMultiple systems should always be designed in a way that allows their data to be joined, which either means creating and sharing unique identifiers or establishing a meta-metadata system that tracks the relationships of data fields across the meta‐ data systems.\nIf the needs are simple and well understood, prefer a single system.7 If the area is rapidly developing and our teams expect to continue extending what they track and how they work, multiple systems will simplify the development over time.\nDataset Metadata For metadata about features and labels, here are a few specific elements that we should ensure are included:\nFeature Metadata Keeping track of metadata about our feature definitions is part of what will enable us to reliably use and maintain those features.\nThe feature definition is a reference to code or another durable description to what data the feature reads and how it processes the data to create the feature.\nChapter 4: Feature and Training Data\nSwitching to metadata specific to labels and analogously to features, we must store the version of any label definitions to understand which labeling instruc‐ tions the labels were made with.\nUsers of these labels might choose different thresholds to decide which labels to use in training their models.\nData Privacy and Fairness Feature and labeling systems give rise to profound privacy and ethical considerations.\nPII data and features\nChapter 4: Feature and Training Data\nPrivate data and labeling\nThe details about how to properly manage this kind of data used in human annotation systems is extremely context specific and is beyond the scope of this book.\nConclusion Production ML systems require mechanisms to efficiently and consistently manage training data.\nTraining data almost always consists of features, so having a structured feature store significantly facilitates writing, storing, reading, and ultimately deleting features.\nMany ML systems also have a component of human annotation of data.\nHumans annotating data require their own systems to facilitate rapid, accurate, and verified annotations, which ultimately need to be integrated into the feature store.",
    "keywords": [
      "data",
      "Feature",
      "Metadata",
      "PII data",
      "Systems",
      "system",
      "metadata system",
      "model",
      "features",
      "labels",
      "training data",
      "Metadata Feature systems",
      "training",
      "label",
      "Labeling"
    ],
    "concepts": [
      "labeling",
      "label",
      "labels",
      "labeled",
      "feature",
      "features",
      "data",
      "annotation",
      "annotators",
      "annotate"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 115-127)",
    "start_page": 115,
    "end_page": 127,
    "summary": "Similarly, a model might be shown to have wonderful predictive performance in offline tests, but rely on a particular feature version that is not currently available in the production stack, or use an incompatible version of some ML package, or rarely give values of NaN that cause downstream consumers to crash.\nChapter 5: Evaluating Model Validity and Quality\nEvaluating Model Validity\nImagine that a model developer creates a new version of a feature in training, but neglects to implement or hook up a pipeline that allows that feature to be used in the serving stack.\nChapter 5: Evaluating Model Validity and Quality\nEvaluating Model Quality Ensuring that a model passes validation tests is important, but by itself does not answer whether the model is good enough to do its job well.\nOffline Evaluations As discussed in the whirlwind tour of the model development lifecycle in Chapter 2, model developers typically rely on offline evaluations, such as looking at accuracy on a held-out test set as a way to judge how good a model is.\nEvaluating Model Quality\nAn evaluation is always composed of both a metric and a distribution together— the evaluation shows the model’s performance on the data in that distribution, as computed by the chosen metric.\nIndeed, many of the issues around fairness and bias that emerged in the late 2010s can likely be tracked down to not giving sufficient consideration to the specifics of the data distribution used at test time during model evaluations.\nThe most common evaluation distribution used is held-out test data, which we cov‐ ered in Chapter 3 when reviewing the typical model lifecycle.\nAs an example, imagine we have a large set of stock-price data, and we want to train a model to predict these prices.\nthis kind of information, so we would need to be careful to create our evaluations in a way that does not allow the model to train on “future” data.\nThe basic idea is to simulate how training and prediction would work in the real world, but playing the data through to the model in the same order that it originally appeared.\nFor each simulated time step, the model is shown the next example and asked to make its best prediction.\nThat prediction is then recorded and incorporated to the aggregate evaluation metric, and only then is the example shown to the model for training.\nA second drawback is that we must be careful to make comparisons between models based on evaluation data from exactly the same time range.\nIn models that continually retrain and evaluate using some form of progressive validation, it can be difficult to know whether the model performance is changing or whether the data is getting easier or harder to predict on.\nOne way to control this is to create a golden set of data that models are not ever allowed to train on, but that is from a specific point in time.\nWhen we set aside the golden set of data, we also keep with it either the results of running that set of data through our model or, in some cases, the result of having humans evaluate the golden set.\nEvaluating Model Quality\nPerformance on golden set data like this can reveal any sudden changes in model quality, which can aid debugging greatly.\nNote that golden set evaluations are not particularly useful for judging absolute model quality, because their relevance to current performance diminishes as their time period recedes into the past.\nThe primary benefit of golden sets is to identify changes or bugs, because, typically, model performance on golden set data changes only gradually as new training data is incorporated into the model.\nWhen deploying models in the real world, one worry is that the data they may encounter in reality may differ substantially from the data they were shown in training.\nFor example, we might have a model trained largely on image data from North America and Western Europe, but that is then later applied in countries across the globe.\nFirst, the model may not perform well on the new kinds of data.\nSecond, and even more important, we may not know that the model would not perform well because the data was not represented in the source that supplied the (supposedly) IID test data.\nIf our training data does not include portrait images with a wide range of skin tones, an IID test set might not have sufficient representation to uncover problems if the model does not do well for images of people with especially dark skin tones.\n(This example harkens back to seminal work by Buolamwini and Gebru.)1 In cases like this, it’s important to create specific stress-test distributions in which carefully constructed test sets each probe for model performance on different skin tones.\nOne way to understand a model’s performance at a deeper level involves learning what the model would have predicted if the data had been different.\nThis is some‐ times called counterfactual testing because the data that we end up feeding to the model runs counter to the actual data in some way.\nA Few Useful Metrics In some corners of the ML world, there is a tendency to look at a single metric as the standard way to view a model’s performance on a given task.\nFor example, accuracy was, for years, the one way that models were evaluated on ImageNet held-out test data.\nEvaluating Model Quality\nJust as there is no one best place to watch the sun rise, there is no one best metric to evaluate a given model, and the most effective approach is often to consider a diverse range of metrics and evaluations, each of which has its own strengths, weaknesses, blind spots, and peculiarities.\nAs we’ve noted, this set of metrics offers a useful way to tell when something is horribly wrong with our model.\nStatistical bias is an easy concept—if we add up all the things we expect to see based on the model’s predictions, and then add up all the things we actually see in the data, do we get the same amount?\nIn an ideal world, we would, and typically a well-functioning model will show very low bias, meaning a very low difference between the total expected and observed values for a given class of predictions.\nWe can use sliced analysis to identify particular parts of the data that the model is performing badly on as a way to begin debugging and improving the overall model performance.\nWhen we have a model that predicts a probability value or a regression estimate, like a probability that a user will click a given product or a numerical pre‐ diction of tomorrow’s temperature, creating a calibration plot can provide significant insight into the overall quality of the model.\nThis is done essentially in two steps, which can be thought of roughly as first bucketing our evaluation data in a set of buckets and then computing the model’s bias in each of these buckets.\nCalibration plots can show systemic effects such as overprediction or underpre‐ diction in different areas, and can be an especially useful visualization to help understand how a model performs near the limits of its output range.\nEvaluating Model Quality\nWhen we think of model evaluation metrics, classification metrics like accuracy are often the first ones that come to mind.\nTo place an accuracy metric into context, we need to have some understanding of how good a naive model that always predicts the most prevalent class would be, and also to understand relative costs of different types of errors.\nEvaluating Model Quality\nConceptually, AUC ROC is computed by creating a plot showing the true-positive rate and the false-positive rate for a given model at every possible classification threshold, and then finding the area under that plotted curved line; see Figure 5-3 for an example.\n(This sounds expensive, but efficient algorithms can be used for this computation that don’t involve actually running a lot of evaluations with different thresholds.) When the area under this curve is scaled to a range from 0 to 1, this value also ends up giving the answer to the following question: “If we randomly choose one positive example and one negative from our data, what is the probability that our model gives a higher prediction score to the positive example rather than the negative?”",
    "keywords": [
      "model",
      "Evaluating Model Validity",
      "Evaluating Model Quality",
      "Evaluating Model",
      "model quality",
      "Model Validity",
      "data",
      "IID test set",
      "test data",
      "IID test",
      "models",
      "IID test data",
      "model performance",
      "quality",
      "test set"
    ],
    "concepts": [
      "model",
      "models",
      "metric",
      "metrics",
      "examples",
      "evaluating",
      "evaluations",
      "evaluation",
      "evaluate",
      "evaluated"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 128-135)",
    "start_page": 128,
    "end_page": 135,
    "summary": "When comparing predictions from a model to a ground-truth value, the first metric we might look at is the difference between our prediction and reality.\nIf we were to aggregate those values without thinking about it, so we could look at averages over many values, we would run into the mild annoyance that in the first example the difference was 0.3 and in the second it was –0.3, so our average error would appear to be 0, which feels misleading for a model that is clearly imperfect.\nBoth metrics have the useful quality that a value of 0.0 shows a perfect model.\nMSE can be less useful if the data contains noise or outlier examples that are better ignored, in which case MAE is likely a better metric.\nIt can be especially useful to compute both metrics and see if they yield qualitatively different results for a comparison between two models, which can provide clues into a deeper level of understanding their differences.\nA more convenient way to think of it might be as the loss we want to use when we think about our model outputs as actual probabilities.\nChapter 5: Evaluating Model Validity and Quality\nFor example, if we are creating a risk-prediction model predicting the chance of an accident, there is an enormous difference between an operation being 99% reliable and 99.99% reliable—and we could end up making very bad pricing decisions if our metrics did not highlight these differences.\nIn other settings, we might just loosely care how likely a picture is to contain a kitten, and 0.01 and 0.001 probabilities might both be best interpreted as “basically unlikely,” in which case log loss would be a poor choice of final metric.\nObviously, it is highly useful for MLOps folks to have a working knowledge of the various distributions and metrics that are most critical for assessing model quality for our system.\nFor example, the main problems in developing the first version of a yarn store product recommen‐ dation model are much more likely to be around creating a data pipeline and a serving stack, and model developers might not have bandwidth to choose carefully between varying classification or ranking metrics.\nmetrics or distributions being developed that help shed light on important areas of model performance.\nFor example, we might notice a cold-start problem in which new products are not represented in the held-out set, or we might decide to look at calibration and bias metrics across a range of slices by country or product listing type to understand more about our model’s performance.\nThis most often involves assessment of the model’s performance on some form of held-out or test data, with the appropriate choice of evaluation metric suited for the application task.\nTogether, these two forms of testing establish a decent level of trust in a model and will be a reasonable starting point for many statically trained ML systems.\nChapter 5: Evaluating Model Validity and Quality\nEditor’s note: When we put together the list of topics for what MLOps folks truly need to know, issues of fairness, privacy, and ethical concerns in AI and ML systems were right at the top of the list.\nIndeed, one reasonable position right now is that a quite viable approach to promoting fairness in computing systems is not to use AI/ML.\nHowever, for those who find themselves compelled to do so, or who believe that this skepticism of algorithmic solutions can be overcome in their specific use cases, this chapter provides a starting point for understanding the challenges of how to do it right.\nTherefore, when we think about fairness and ethics in AI and ML, we also have to recognize that they may very well provide improve‐ ments in some cases relative to human decision makers.\nSo, despite the gloom and doom you will find in this chapter, we also recognize from the outset that some uses of algorithms have been tremendously successful in terms of increasing overall fairness, even if there isn’t yet a clear solution to making AI and ML fair and just in a guaranteed or global sense.\nWithin this chapter, we also provide notes on how you might consider refactoring work at your organization, in practical ways, so as to enhance fairness and ethics in your own AI/ML work.\nFairness (a.k.a. Fighting Bias) Algorithmic fairness, and other variations of this term, are a hot topic in ML and have been for many years.\nIn framing such discussions, many have emphasized that this could happen because data used to train ML systems could be taken from biased systems or collected in a biased way.\nThis is a key point: it’s important to realize that the fundamental source of bias can come from many steps in the ML pipeline, including bad data, but also including bad modeling choices.\nCrime-prediction algorithms, such as those built to influence police patrolling allocations, likely suffer from sampling bias.3 Across many nations and conti‐ nents, it remains a consistent pattern that policing is directed at communities of low socioeconomic status.\nYet, this data, sampled in a biased fashion, then creates new biased inputs for more ML modeling to allocate future police patrols.\nAmazon famously developed but did not deploy an in-house hiring algorithm that applied a strong negative parameter for attending a woman’s college.4 Thus we see a case of an algorithm using irrelevant factors to make a decision in a way that directly discriminates against a group (in this case, women going to women’s colleges).5 If the algorithm had been used, this would have looked like a case of disparate treatment.\nBroadly, we can think of systemic bias as a host of factors that likely cannot be identified in individual cases as explanatory features but that, on an aggregate level, clearly influence differences in outcomes for individuals due to the structural limitations (limitations that are baked into our social, educational, and employment systems, among others).",
    "keywords": [
      "model",
      "bias",
      "Evaluating Model Validity",
      "Model Quality",
      "Fairness",
      "Model Validity",
      "Evaluating Model",
      "systems",
      "decision",
      "metrics",
      "decision makers",
      "ROC curve",
      "Evaluating Model Quality",
      "model quality evaluations",
      "ROC curve maps"
    ],
    "concepts": [
      "metric",
      "metrics",
      "bias",
      "biases",
      "biased",
      "decisions",
      "model",
      "models",
      "modeling",
      "algorithmic"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 136-144)",
    "start_page": 136,
    "end_page": 144,
    "summary": "Therefore, modeling human behavior should require paying particular attention to make sure that the model is giving everyone a fair shake rather than modeling everyone according to a prototype that in fact is reasonably accurate only for someone in the majority class.\nTragically, it remains indisputable that these forms of bias (and many others) con‐ tinue to pop up in a wide range of ML applications, even with increasing media attention about algorithmic fairness.\nBy doing so, we can confine our efforts to ML applications that truly add significant value, and therefore are worth the effort required to ensure privacy and fairness as we implement the system.\nDefinitions of Fairness In the ML community, we have not cohered around a single definition of fairness, but common categories exist that have intuitive and appealing descriptions.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nsome definitions of fairness emphasize individual fairness.\nThese notions of fairness argue that, for two individuals who are “the same” other than irrelevant factors (race, gender, etc.), these individuals should be treated the same.6 Other definitions of fair‐ ness emphasize that ML should look to fairness at the level of groups.\nStill other definitions of fairness might get more complicated and look to establish causal mechanisms to understand what might drive individuals to success or failure, before looking to categorize them algorithmically.\nTwo common categories of fairness that are intuitive and appealing are group parity and calibration-based assessments.\nOn the other hand, calibration emphasizes individual fairness, which is another value most of us find quite intuitive and appealing.\nIn a perfect world, where groups and individuals are treated fairly, we would get the same outcomes.\nUnfortunately, this turns out not to be the case in our imperfect world, for a variety of reasons, including fundamental mathematical limitations.7 For this reason—at least for now—practitioners must choose their definition of fairness.\nAs we will describe, different use cases will have different reasonable definitions of fairness.\nWhile applying different notions of fairness in different situations may seem strange, mature readers will realize that we likewise apply the same principles in real life.\nThus, ordinary people in fact seem to commonly apply different notions of fairness to different domains of life, and we propose that the same could plausibly be true in different contexts or use cases of ML.\nIt might seem that the ML ethics community should be attempting to converge on a single definition of fairness.\nThe state of the art of most models deployed in the real world is still fundamentally unfair by any metric (or at least of unexamined fairness).\nWe have many useful definitions of fairness to select from, and picking one or several to work toward provides us with the opportunity for rapid progress right away without needing to await further theoretical or legal developments.\nGroup parity fairness definitions require that relevant rates of algorithmic perfor‐ mance be the same across various groups.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nIn contrast to group parity, calibration fairness definitions require that a ML model work equally well for all individuals.\nSo a calibration-oriented definition of fairness would require that for any group, the ML score means the same thing.\nBecause this algorithm has been shown to be calibrated correctly across racial categories, the same COMPAS score (say, 0.5) means the same thing for both Black and white data subjects: that is, a white offender and a Black offender with the same risk score have the same probability of reoffending.8 When understood in plain English, this seems intuitive as well—that a model score should mean the same thing for everyone, no matter what group they belong to.\nAfter all, there are many correct and intuitive ways of describing the world in other contexts, so why not in the case of fairness?\nMathematically, not all fairness definitions can be satisfied at the same time, given real world conditions.\nWe have to decide which fairness goals to pursue in an ML context, and likewise decide whether focusing on a specific fairness metric might even tend to reduce the tendency toward a holistic viewpoint that might otherwise be more helpful in enhancing fairness and other ethical values.\nFor now, we consider that for a specific ML tool, it may make sense to choose and emphasize a particular fairness metric, while recognizing that it will not be possible to satisfy all intuitive and normatively desirable notions of fairness at the same time.\nWhat’s more, some researchers have recognized that different situations may call for different fairness metrics, given the relative harms or policy goals of a particular use case of an algorithm, and so interested readers can find useful guidance as to how to choose a fairness metric for a specific task given specific concerns about the likely consequences of various kinds of mistakes.\nFor example, Arrow’s impossibility theorem demonstrates that three intuitive fairness criteria for a specific form of voting cannot all be met at the same time.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nIn contrast, an example normative statement could be that algorithmic bias is wrong because all equally qualified people should have an equal chance at a job as a matter of funda‐ mental fairness.\nAnd so, we can see how different sets of similar but not identical normative values can lead to conflicts that will continuously surface in real-world debates and decisions about the best way to enhance fairness in society.\nReaching Fairness Concretely, a classic ML setup has three modes of working toward fairer (less biased) outcomes.\nAnother method is similar to that described in preprocessing (learned fair repre‐ sentations) in terms of motivation to remove identifying information from the model’s knowledge about the sensitive attribute.\nAnother example of post-processing is to set different thresholds—say, for a credit score or college admissions scores—for different groups such that the predictions or decisions made with the score can be equally accurate for different\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nFor example, people may be uncomfortable with the idea of having explicitly different score cutoffs for different groups, and they also might be uncomfortable with the notion of eliminating, and replacing with randomized numbers, those outputs from an algorithm that are, in fact, likely to be correct.",
    "keywords": [
      "fairness",
      "model",
      "Definitions of Fairness",
      "fairness definitions",
      "groups",
      "fairness definitions require",
      "bias",
      "Definitions",
      "data",
      "group",
      "systems",
      "Fighting Bias",
      "group parity",
      "Group parity fairness",
      "parity fairness definitions"
    ],
    "concepts": [
      "fair",
      "fairness",
      "fairly",
      "different",
      "differences",
      "groups",
      "group",
      "individual",
      "individuals",
      "individualized"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 145-152)",
    "start_page": 145,
    "end_page": 152,
    "summary": "Chapter 6: Fairness, Privacy, and Ethical ML Systems\nIf you are doing ML work that touches on these core areas, you likely need to be deeply concerned about fairness—and, specifically, about fairness as it is implemented in your nation’s antidiscrimination laws.\nWith the advent of big data, many datasets were compiled, often about overlapping groups of people, and it then became possible to use different datasets together to identify people from de-identified information.\nFor example, for most Americans, it turned out to be possible to identify them in a de-identified dataset merely from knowing their birthday, zip code, and gender.13 And, the world is increasingly full of new datasets, sometimes resulting from data breaches but other times resulting from people voluntarily sharing information about themselves—information that becomes very easy to access by any casual creepy stalker.\nTwo key ideas about privacy are described here, both in terms of how they relate to a notion of privacy, but also in how they relate to taking an existing dataset with personal information and turning it into a dataset that can be used or released without compromising individual identity:\nSo, for example, we could apply k-anonymity to a dataset listing individuals in a town by requiring that data be bucketed such that for any zip code / birth information / gender category, there were at least 10 individuals.\nThe appropriate size for k will depend on the particular domain, which could have different implications regarding sensitivity of the data, possibility to build useful datasets with large k, and possibility of reidentification given other known potentially linkable datasets.\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nThis mathematical method adds noise to data such that probabilistic guarantees can be made regarding the possibility (or more importantly, lack of possibility) to make inferences about a specific individual when given access to that noisified data.\nHowever, if differential privacy is applied, it would, in probability, not be possible to infer that individ‐ ual’s age at a pre-specified level of precision and given a pre-specified query budget.14 Differential privacy can apply quite broadly, not just to the computation of aggregate statistics but also to the training of ML models, with methods to ensure that a model’s outputs or training is not contingent on inclusion of a particular data point.\nFor example, does an individual’s right to delete their data mean that they can also force a company to prove that its ML models have also “forgotten” their data?15\nSome have explored how training models with differential privacy might address this concern.\nHowever, technical challenges remain because producing ML models with differential privacy guarantees is quite a technical challenge.\nMethods to Preserve Privacy The definitions and scenarios described previously with respect to k-anonymity and differential privacy refer to very specific notions of privacy and computational measures.\nAny data about people in a database should be treated as a “need to know” resource, with ML engineers requesting access for specific purposes rather than being able to freely access or browse data.\nThis makes it possible to understand data use patterns, see when someone might be inap‐ propriately accessing data, and preserve evidence in case allegations of inappro‐ priate use are made later.\nAn analysis of such logs may also indicate ways in which data storage schema could be refactored to reduce the extent of data that different use patterns can access.\nFor example, if an ML model calls for access to a sensitive table of data merely to access one column, consider splitting off that column of data rather than granting access to a full table of additional but unnecessary pieces of information.\n17 One recommended starting point is TensorFlow Privacy, which includes training algorithms for differentially\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nThe data needed for legitimate business uses should be separated from sensitive data (such as names and addresses) that is unlikely to be relevant to creating an ML model.\nFor example, to predict users’ clicks, there doesn’t seem to be a justification for knowing a user’s name or address.18 Therefore, there is no reason for that information to be stored with information that might be useful for that particular prediction task, such as past browsing history or demographic information.19 As noted previously, studying your data access logs can help you identify ways in which data storage can be refactored to minimize exposure of data to ML applications.\nML engineers should be given—but usually are not —a thorough review not only of general ethics training (such as the possibility of bias in data) but also domain-specific training when building algorithms for a specific use case.\nIn addition to technical measures already described, it makes sense to have explicit rules that are readily available regarding what constitutes appropriate access to data and use of that data and what use cases are expressly prohibited.\nOrganizations should have clear data access and appropriate data use guidelines in place in a location that is readily apparent and accessible.\nHere we highlight a few key categories of laws that are related to privacy and ML:\nChapter 6: Fairness, Privacy, and Ethical ML Systems\nNonetheless, such laws are useful in providing notice to consumers as to when they may be most at risk because of exposure of their personal data.\nData protection and personal privacy laws\nThe most prominent example of data protection law is the EU’s GDPR.\nNevertheless, such laws give consumers active ways that they can take steps to protect their privacy.\nThis category of more general consumer protection law is important in the US context, since the US otherwise lacks a comprehensive national personal data privacy regime.\nThus it is—at the least—essential to ensure that, as your organization builds out data collection and ML modeling capabilities, you ensure that such practices are consistent with public-facing privacy policies and terms of service.",
    "keywords": [
      "data",
      "Privacy",
      "Differential privacy",
      "Fairness",
      "access",
      "data access",
      "Data access guidelines",
      "laws",
      "information",
      "Data breach",
      "n’t",
      "training",
      "personal data privacy",
      "Differential",
      "privacy laws"
    ],
    "concepts": [
      "data",
      "privacy",
      "fairness",
      "fair",
      "access",
      "accessing",
      "accessible",
      "useful",
      "uses",
      "dataset"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 153-161)",
    "start_page": 153,
    "end_page": 161,
    "summary": "Explanation ML model explanation is the process of analyzing and presenting information about an ML system to describe how that system works.\nThis process and the goal of making the model amenable to human understanding is often discussed in shorthand as explainability, and it’s a key area of interest with respect to ML.\nFor both technical and ethical reasons, it is desirable to have methods that can “explain” how an ML model works or why it reached a particular outcome in a particular case.\nThe technical motivation for explainable AI is related to controlling model quality and possibly learning about the data through the model.\nThey find it empowering to know how a model came to a conclusion, so that the conclusion can make sense to those affected by it, or even be challenged where the ML conclusion doesn’t make sense.\nIn this way, model explanation can feel a lot like thinking through the problems that highly trained professions, such as doctors or lawyers, face in trying to give advice or a diagnosis of a real-world situation to a particular audience.\nIf the purpose of an ML explanation is to inspect model quality, such as to make sure the model is making the right decisions for the right reasons, then a global explanation might be preferred.\nA global explanation indicates how the model works generally and why certain general decision rules will be followed.\n• Explanations need to be useful and sometimes actionable.\nContemplate whether you can offer global (explaining the model overall) or • local (explaining a particular ML model decision/classification) explanations and explore a few techniques for doing so.\nBut if a variety of ML algorithms are all using the same logic to not hire a particular candidate, that candidate never gets the chance at a job, and we never actually know whether that candidate could have done a good job (systematically missing data).\nIf someone is labeled by a ML algorithm in a particular way, sometimes that label is trusted to settle the issue, even if humans are supposed to be exercising some level of supervision.23 In the employment scenario, someone could be labeled a poor candidate and not be hired by the human who is using the algorithmic assistance.\nAt some point, their extended period of unemployment will itself become another factor that an ML algorithm is likely to use as a flag against hiring someone.\nModels must work in their deployed use cases.\nWhile some celebrate ML on the basis that such models can “find patterns that humans can’t see,” sometimes the patterns identified don’t make any sense, or make sense for the wrong reason.\nIs this a use case that touches on fundamental concerns about human dignity • or social expectations, which could add additional limitations to the scope of appropriate use of algorithmic approaches?\nIs the decision or classification to be made by ML an important decision, where • fairness should be particularly guarded, and if so, are there indicators that this can be accomplished?\nData Collection and Cleaning At this point in the pipeline, you have decided on a use case and are looking for data and preparing data for modeling.\nWill data be acquired in a way that respects informed consent?25 Have you dis‐ • closed the purposes for which you will use the data to the subjects in a reasonably informative and transparent way?\nHow • will you choose from various forms of fairness interventions based on the potential harms of bias and based on the particular normative values or legal restrictions of your use case?\n• Are you training in a manner that will reduce data leaks from the trained model and enhance robustness against malicious attacks?\nHave you chosen your model architecture (say, opaque neural network versus • interpretable linear function) by understanding the relative importance of accu‐ racy and explainability for your particular application?\n• Have you asked whether the model was trained on a proxy, and if so, what data is available to justify the use of that proxy?\nHave you tested the model robustly, with a fair selection of held-out data in • realistically challenging situations?\n• Are you able to identify and understand the logic driving the model globally and to generate individual explanations in case an individual might ask for them?\nModel Deployment Now it’s time to make the model available for its planned use:\nHave you tried running the model in an online mode so that you can watch how • it performs, counterfactually, in advance of an actual product launch?\nProducts for the Market Whether models are intended for internal or external use, they need to meet the same standards for fairness and privacy protections.\nWill you make available explanations about how the ML system works, possibly • even specific guidance for individuals about how their outcomes were formed?27\nConclusion We have reviewed many fairness, privacy, and other ethical considerations that affect the design, training, and deployment of real-world ML systems.\n• Address fairness, privacy, and ethics concerns routinely at product inception.\nMost fairness, privacy, and other ethical problems in ML originate at the level of • product conception and creation.\nThe more you learn and the longer you keep Responsible AI concerns in mind, • the more your ML pipeline and ML offerings will reflect good fairness practices.\nWe wish you good luck as you begin practical steps to make the world a better place, starting with your ML products (including, sometimes, not using them).",
    "keywords": [
      "model",
      "Explanation",
      "model explanation",
      "data",
      "Fairness",
      "algorithm",
      "Responsible",
      "Privacy",
      "ethical",
      "n’t",
      "make",
      "models",
      "good",
      "Systems",
      "case"
    ],
    "concepts": [
      "models",
      "modeling",
      "algorithm",
      "algorithms",
      "algorithmic",
      "explanation",
      "explanations",
      "data",
      "useful",
      "particular"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 162-169)",
    "start_page": 162,
    "end_page": 169,
    "summary": "ML training is the process by which we transform input data into models.\nA training algorithm describes the specific steps by which software reads data and updates a model to try to represent that data.\nThe simplest imple‐ mentation of an ML training system is on a single computer running in a single process that reads data, performs some cleaning and imposes some consistency on that data, applies an ML algorithm to it, and creates a representation of the data in a model with new values as a result of what it learns from the data.\nTraining on a single computer is by far the simplest way to build a model, and the large cloud providers do rent powerful configurations of individual machines.\nIn part, because of our broad conception of what an ML training system is, ML train‐ ing systems may have less in common with one another across different organizations and model builders than any other part of the end-to-end ML system.\nSome training systems are closest to the input data, performing their function almost completely offline from the serving system.\nAdditional differences appear when we look at the way that training systems maintain and represent the state of the model.\nBecause of this significant variety of differences across legitimate and well-structured ML training systems, it is not reasonable to cover all of the ways that organizations train models.\nWe’ll describe a system that lives in a distinct part of the ML loop, next to the data and producing artifacts bound for the model quality evaluation sys‐ tem and serving system.\nData to train on\nMany training systems have a means of representing the configuration of an individual model separate from the configuration of the training system as a whole.1 These should store model configurations in a versioned system with some metadata about the teams creating the models and the data used by the models.\nModel-training framework\nMost model creators will not be writing model-training frameworks by hand.\nIt seems likely that most ML engineers and modelers will eventually be exclu‐ sively using a training systems framework and customizing it as necessary.\nTraining or model development software\nIn this simplified training system, the data flows in from the left, and models emerge on the right.\nWe use an ML framework that applies a training algorithm to turn the data into a model.\nAll the while, we keep track of our models and data in a\nFeatures Training data is data about events or facts in the world that we think will be relevant to our model.\nSpecifically, features are those aspects of the data that we believe are most likely to be useful in modeling, categorizing, and predicting future events given similar circumstances.\nTo be useful, features need a consistent definition, consistent normalization, and con‐ sistent semantics across the whole ML system, including the feature store, training, quality evaluation, and serving.\nAs we develop the model, we need easy ways to add new features, produce new features on old data when we get a new idea for what to look for in existing logs, and remove features that turned out to not be important.\nThe characteristics of a feature store will exist, even if our model training system reads raw data and extracts features each time.\nThis is obviously not the most sophisticated data storage environment but has a huge advantage, giving us a fast way to start training, and it appears to facilitate experimentation with new features.\nA feature store gives a consistent place to understand the definition and authorship of features and can significantly improve innovation in model development.\nSnapshots of trained models\nJust as with feature stores, everyone has a rudimentary form of a model management system, but if it amounts to “whatever configuration files and scripts are in the lead model developer’s home directory,” it may be appropriate to check whether that level of flexibility is still appropriate for the needs of the organization.\nThis typically includes scheduling and configuring the various jobs associated with training the model as well and tracking when training is complete.\nThe point of ML training is to transform the input data into a representation of that data, called a model.\nThe ML framework we use will provide an API to build the model that we need and will take care of all of the boilerplate code to read the features and convert them into the data structures appropriate for the model.\nQuality Evaluation The ML model development process can be thought of as continuous partial failure followed by modest success.\nIt necessarily follows, therefore, that one of the essential elements of a model-training environment is a systematic way to evaluate the model that we just trained.\nAt the most basic level, a model quality evaluation system offers a means of performing a quality evaluation, usually authored by the model developer, and storing the results in a way that they can be compared to previous versions of the same model.",
    "keywords": [
      "training system",
      "system",
      "Training",
      "model",
      "Training System Implementation",
      "data",
      "Systems",
      "Basic Training System",
      "model training system",
      "Training Systems models",
      "Model Management System",
      "models",
      "features",
      "model quality evaluation",
      "serving system"
    ],
    "concepts": [
      "models",
      "model",
      "modelers",
      "modeling",
      "features",
      "feature",
      "data",
      "training",
      "train",
      "trained"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 170-177)",
    "start_page": 170,
    "end_page": 177,
    "summary": "Amusing examples of these failures include such straightforward things as “the train‐ ing system lost permission to read the data so the model trained on nothing,” and “the version that we copied to serving wasn’t the version we thought it was.” Most of them are of the form of incorrectly monitored and managed data pipelines.\nSome model developers will train a model from a dataset once, check the results, deploy the model into production, and claim to be done.\nThey will note that if the dataset isn’t changing and the purpose of the model is achieved, the model is good enough and there is no good reason to ever train it again.\nEventually, whether in a day or a year, that model developer or their successor will get a new idea and want to train a different version of the same model.\nPerhaps a better dataset covering similar cases will be identified or created, and then the model developers will want to train on that one.\nFor all of these reasons, assume every model will be retrained and plan accordingly— store configs and version them, store snapshots, and keep versioned data and meta‐ data.\nThis approach has tremendous value: most of the debates about so-called “offline” and “online” models are actually debates about retraining models in the presence of new data.\nModels Will Have Multiple Versions (at the Same Time!) Models are almost always developed in cohorts, most obviously because we will want to train different versions with different hyperparameters.\nMultiple very large language models are being trained by large ML-centric organizations in order to provide answers to complex queries across a variety of languages and data types.\nThese models are so expensive to train that the production model for them is explicitly to train them once and use them (either directly or via transfer learning) “forever.” Of course, if the cost of training these models is significantly reduced or other algorithmic advances arise, these organizations may find themselves training new versions of these models anyway.\nUltimately, the second backup plan, combined with the ability to serve multiple models at the same time and quickly develop new variations of existing models, provides a path to understanding and resolving future model quality problems when\nData Will Be Unavailable Some of the data used to train new models will not be available when we try to read it.\nFor very high-volume systems (web-browsing logs, for example), many organizations subsample this data before training automatically as well, simply to reduce the cost of data processing and model training.\nIf we were to drop one out of every 1,000 training records in a completely random way, this is almost certainly safe to ignore for the model.\nModels Should Be Improvable Models will change over time, not just by adding new data.\nSometimes we will add a feature to the application or implementation that provides new data but also requires new features of the model.\nProcedurally, the most challenging change to model training in the training system we’re describing here is adding a completely new feature.\nFeatures Will Be Added and Changed Most production ML training systems will have some form of a feature store to orga‐ nize the ML training data.\n(Feature stores offer many advantages and are discussed in more detail in Chapter 4.) From the perspective of a training system, what we need to note is that a significant part of model development over time is often adding new features to the model.\nThis happens when a model developer has a new idea about some data that might, in combination with our existing data, usefully improve the model.\nAdding new columns to the feature store and changing to schema and content of data in the past are both activities that can significantly impact reliability of all our models in the training system if the changes are not managed and coordinated carefully.\nModels Can Train Too Fast ML production engineers are sometimes surprised to learn that in some learning systems, models can train too fast.8 This can depend a bit on the exact ML algorithm, model structure, and parallelism of the system in which it’s being implemented.\nThe learning processes read new data, consult the state of the model, and then update the state of part of the model to reflect the piece of data they just read.9 As long as there are either locks (slow!) or no updates to the particular key we are updating (unlikely at scale) in the middle of that process, everything is fine.\nThe problem is that multiple race conditions can exist, where two or more learner tasks consult the model, read some data, and queue and update to the model at the same time.\nThe next time a bunch of learner tasks consult the model, they find it skewed in one direction by a lot, compared to the data that they are reading, so they queue up changes to move it substantially in the other direction.\nUnfortunately for the discipline of ML production engineering, there is no simple way to determine when a model is being trained “too fast.” There’s a real-world test that is inconvenient and frustrating: if you train the same model faster and slower (typically, with more and fewer learner tasks), and the slower model is “better” in some set of quality metrics, then you might be training too fast.\nWhile the biggest costs at the beginning are people and opportunity costs, as we collect more data, train more models, and use them more, computer infrastructure costs will grow to an increasingly large share of our expenditure.\nThis is true if we’re training the model on a single computer or on a huge cluster.\nThe cheaper models are to train,\nEarly-stage ML training systems often rebuild models from scratch on all of the data when new data arrives.\n• Use online learning, whereby the model is incrementally updated as each new data point arrives.\nSimple steps such as these can significantly impact an organization’s computational costs for training and retraining models.\nMost commonly in production ML environments, transfer learning involves starting learning with the snapshot of an already trained, earlier version of our model.",
    "keywords": [
      "model",
      "data",
      "Models",
      "training",
      "training systems",
      "systems",
      "General Reliability Principles",
      "system",
      "training data",
      "train",
      "model training",
      "time",
      "learning",
      "Reliability Principles",
      "training system reliability"
    ],
    "concepts": [
      "models",
      "training",
      "train",
      "systems",
      "common",
      "learning",
      "learn",
      "process",
      "processes",
      "updating"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 178-185)",
    "start_page": 178,
    "end_page": 185,
    "summary": "Here we just calcu‐ late the total amount of money spent on training models.\nWe don’t need to measure the actual business impact of every single trained model.\nIn that case, we need a metric that helps us compare the value being created across different implementations training the same model.\nnumber of experimental models trained\nThis will help us easily see efforts to make reading and training more efficient without requiring us to know anything at all about what the model actually does.\nConversely, overall value attempts to measure value across the entire program of ML model training, considering how much it costs us to add value to the organization as a whole.\nThis will include the cost of the staff, the test models, and the production model training and serving.\nFor example, if a system can tolerate a 24-hour outage of your training system, but it takes you 18 hours to detect any problems and 12 hours to train a new model after the problems are detected, we can‐ not reliably stay within the 24 hours.\nMany people modeling production-engineering response to training-system outages utterly neglect to include model recovery time.\nThis section covers three of the most common reliability problems for ML training systems: data sensitivity, reproducibility, and capacity shortfalls.\nData Sensitivity As has been repeatedly mentioned, ML training systems can be extremely sensitive to small changes in the input data and to changes in the distribution of that data.\nLack of representativeness in the input data is one common source of bias in ML models; here, we are using bias in both the technical sense of the difference between the predicted value and the correct value in a model, but also in the social sense of being prejudiced against or damaging for a population in society.\nExample Data Problem at YarnIt YarnIt uses an ML model to rank the results of end-user searches.\nUntil they notice this, the model will train entirely on full-price purchases.\nAs a result, the model will eventually stop recommending the discounted products, since there is no longer any evidence from our logging, data, and training system that anyone is ever purchasing them!\nThis kind of very small error in data handling during training can lead to significant errors in the model.\nReproducibility ML training is often not strictly reproducible; it is almost impossible to use exactly the same binaries on exactly the same training data and produce exactly the same model, given the way most modern ML training frameworks work.\nAs obvious as it may sound, most ML training feature storage systems are frequently updated, and it is difficult to guarantee that there are no changes at all to data between two runs of the same model.\nEven minor version updates to your ML training framework, learning binaries, or orchestration or scheduling system can result in changes to the resulting models.\nAnother way to read this is “my models could change any time I happen to update TensorFlow or PyTorch, even for a new minor version.” This is essentially true but not common, and the differences often aren’t pronounced.\nSome level of inability to reproduce precisely the same model is a neces‐ sary feature of the ML training process.\nExample Reproducibility Problem at YarnIt At YarnIt, we retrain our search and recommendations models nightly to ensure that we regularly adjust them for changes in products and customer behavior.\nTypically, we take a snapshot of the previous day’s model and then train the new events since then on top of that model.\nThis is cheaper to do but ultimately does mean that each model is really dozens or hundreds of incremental training runs on top of a model trained quite some time ago.\nDetecting that a transaction is fraudulent may take up to several days, and by that point we may have already trained a new model with that transaction included as an authorized purchase.\nWhen they develop a new model, they train it from scratch on all the data with the new model structure and then compare it to the existing model to see if it is materially better or worse.\nIt may be obvious where this is going: the problem is that if we retrain the current production model from scratch on the current data, that model may well be significantly different from the current production model that is in production (which was trained iteratively on the same data over time).\nThe situation is actually even less deterministic than that: even if we train the exact same model on the exact same data with no changes, we might have nontrivial differences, with one model trained all at once and another trained incrementally over several updates.15\nThe only real reliability solution to this problem is to treat each model as it is trained as a slightly different variant of the Platonic ideal of that model and fully renounce the idea of equality between trained models, even when they are the same model configuration trained by the same computers on the same data twice in a row.\n13 A large and growing set of work exists on the topic of deleting data from ML models.\n14 Chapter 6 covers some cases where we want to delete private data from an existing model trained on that data.\nThe basic capacity that we need to train a new model includes the following:\nThe state of the model at any given time is stored somewhere, but most com‐ monly in RAM, so when the training system updates the state, the system requires memory bandwidth to do so.\nOne of the tricky and troubling aspects of ML training system capacity problems is that changes in the distribution of input data, and not just its size, can create compute and storage capacity problems.\nExample Capacity Problem at YarnIt YarnIt updates many models each day.\nAt the very least, we will need to read the logs produced by the serving system, since the models that YarnIt trains daily read the searches, purchases, and browsing history from the website the day before.\nAdditionally, some models might be more important than others, and we probably want a system for prioritization of training jobs in those cases where we are resource constrained and need to focus more resources on those important models.",
    "keywords": [
      "model",
      "training system",
      "training",
      "Training Reliability Problems",
      "data",
      "models",
      "training data",
      "Common Training Reliability",
      "system",
      "systems",
      "Training Reliability",
      "Reliability Problems",
      "model training",
      "problems",
      "training system capacity"
    ],
    "concepts": [
      "models",
      "modeling",
      "model",
      "data",
      "training",
      "trained",
      "train",
      "trains",
      "problems",
      "problem"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 186-194)",
    "start_page": 186,
    "end_page": 194,
    "summary": "That’s a common shorthand for “creating a structure to ensure our system can ask the model to make predictions on new examples, and return those predictions to the people or systems that need them” (so you can see why the shorthand was invented).\nWe need to have a way for the model to share its predictions with our overall system.\nAlong the way, we will also discuss critical practicalities like ensuring that the feature pipeline used in serving is compatible with that used in training, and strategies for updating a model in serving.\nKey Questions for Model Serving There are a lot of ways that we can think about creating structures around a model that support serving, each with very different sets of trade-offs.\nA model serving predictions to millions of daily users may be asked to handle tens of thousands of queries per second.\nA model that predicts housing prices for a real estate service might not be served on demand at all, and may instead be run as part of a large batch-processing pipeline.\nModels that must be served on a device are one such example.\nWhat Are the Prediction Latency Needs of Our Model?\nFactoring in network delays and other processing necessary to build and load the page, this might mean that we have only a few milliseconds for the model to make all of its predictions on candidate products.\nIf prediction latency is too high, we can mitigate the issue by using more powerful hardware, or by making our model less expensive to compute.\nHowever, it is important to note that parallelization by creating a larger number of model replicas is usually not a solution to prediction latency, as the end-to-end time it takes to send a single example through the model isn’t affected by simply having more versions of the model available.\nWhere Does the Model Need to Live?\nIn our modern world, defined as it is by flows of information and concepts like virtual machines and cloud computing, it can be easy to forget that computers are physical devices, and that a model needs to be stored on a physical device in a specific location.\nWe need to determine the home (or homes) for our model, and this choice has significant implications on the overall serving system architecture.\nAlthough this is not really a production-level solution, in some cases a model devel‐ oper may have a model running on their local machine, perhaps invoking small batch jobs to process data when needed.\nIt may also be the right option if latency is a hypercritical concern, or if specialty hardware is needed to run our models.\nServing our models by using a cloud-based provider can allow for easily scaling our overall computational footprint up or down, and may also allow us to choose from several hardware options.\nIn a managed inference service, some monitoring needs may be automatically addressed—although we will still likely need to independently verify and monitor overall model quality and predictive performance.\nWhen a model is needed in these settings, it is much more likely that it will need to be stored on the device itself, because the alternative is to access a model in the cloud, which requires constant network connection and may also have complicated privacy concerns.\nThese settings of serving “on the edge” typically have strict constraints on model size, because memory is limited, as well as the amount of power that may be consumed by model predictions.\nWhat Are the Hardware Needs for Our Model?\nIn recent years, a range of computational hardware and chip options has emerged that has enabled dramatic improvements in serving efficiency for various model\nThe main thing to know about deep models in serving is that they rely on dense matrix multiplications, which basically means that we need to do a lot of multiplica‐ tion and addition operations in a way that is compute intensive, but that is also highly predictable in terms of memory access patterns.3 The little multiplication and addition operations that make up one dense matrix multiplication operation parallelize beautifully.\nA much better choice for serving deep learning models are chips called hardware accelerators.\nThis typically means that either we need to invest organizationally in serving deep models using GPUs, or we are using a cloud service that supplies GPUs (and may charge a premium accordingly), or that we are serving in an on-device setting where a GPU is locally available.\nSparse models are most useful when we need to use only a small number\nHow Will the Serving Model Be Stored, Loaded, Versioned, and Updated?\nAs a physical object, our serving model has a specific size that needs to be stored.\nA model that is serving offline in an environment might be stored on disk and loaded in by specific binaries in batch jobs whenever a new set of predictions needs to be made.\nThe main storage requirements are thus the disk space needed to keep the model, as well as the I/O capacity to load the model from disk, and the RAM needed to load the model into memory for use—and of these costs, the RAM is likely more expensive or more limited in capacity.\nA model that is used in live online serving needs to be stored in RAM in dedicated machines, and for high-throughput services in latency-critical settings, copies of this model likely will be stored and served in many replica machines in parallel.\nThis means that we will need to swap the version of a model currently used in serving on a given machine with a new version.",
    "keywords": [
      "model",
      "Serving",
      "Model Serving",
      "models",
      "model serving predictions",
      "Serving Model",
      "system",
      "latency",
      "serving deep models",
      "predictions",
      "model predictions",
      "training system",
      "Prediction Latency",
      "model Key Questions",
      "Sparse models"
    ],
    "concepts": [
      "model",
      "modeling",
      "models",
      "likely",
      "serving",
      "served",
      "latency",
      "latencies",
      "computational",
      "computing"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 195-202)",
    "start_page": 195,
    "end_page": 202,
    "summary": "Any feature processing or other data manipulation that is done to our data at training time will almost certainly need to be repeated for all examples sent to our model at serving time, and the computational requirements for this may be considerable.\nFor example, for our yarnit.ai store, we might need to supply a product recommenda‐ tion model with the following:\nIn many cases, this means that the actual code used to turn these pieces of information into features for our ML model to use may be different at serving time from the code used for similar tasks at training time.\nIt is also worth noting that creating features for our model to use at serving time is a key source of latency, and in many systems will be the dominating factor.\nIt works by loading the model and executing its predictions offline against a predefined set of input data.\nAs a result, the model’s predictions are stored as a simple dataset, perhaps in a database or a .csv file or another resource that stores data.\nIn essence, by computing the model predictions offline, you convert on-demand model predictions into a more standard problem of simple data lookup (Figure 8-1).\nModel Serving Architectures\nOffline model serving via data store\nIf the use case is less demanding, we might even be able to avoid the complexity of storing and serving model predictions via a database and write the predictions to a flat file, or in-memory data structure, and use them directly within the application (Figure 8-2).\nOffline model serving via in-memory data structures\nEffectively, the serving data needs to be available ahead of time; for fully correct operation, the system needs to know in advance every possible query that will be made of it.\nModel Serving Architectures\nStoring multiple model outputs in memory or in application databases will have storage limitations and/or cause performance problems.\nIn our web store example, we could build a more personalized shopping experience by having the model constantly learn the real-time user behavior by using the current context along with historical information to make the predictions.\nApplications powered by inferences generated by offline models plus training the supplemental models for additional parameters in real time (Figure 8-3) provides huge benefits and significant business impacts.\nHybrid online model serving in combination with predictions generated offline\nInstead of training and changing one global model, we can tune more situation- specific models with a small subset of real-time data (for example, user- or location-specific models).\nAs the predictions are made in real time, rolling out the model changes is highly challenging, especially in a container-orchestration environment like Kubernetes.\nThis approach needs more advanced monitoring and adjustment/rollback mech‐ anisms in place, since real-time changes could well include fraudulent behaviors caused by the bad actors in the ecosystem, and they could interact with, or influence, model behavior in some way.\nModel Serving Architectures\nServing models online is more powerful when it’s combined with the model-as- a-service approach we discuss in the following section.\nWith MaaS, models are stored in a dedicated cluster and served results via well-defined APIs. Regardless of the transport or serialization methods (e.g., gRPC or REST),6 because models are served as a microservice, they’re relatively isolated from the main application (Figure 8-4).\nModels served as a separate microservice\nGiven the wide popularity of X-as-a-service approaches throughout the industry, we will focus on this particular method more than others, and will examine in detail various aspects of serving model predictions via APIs later in the chapter.\nBy definition in the MaaS context, we have the ability to serve predictions in real time by using real-time context and new features.\nIn these cases, adding a new model-serving capacity is as simple as adding new instances to the serving architecture, also known as horizontal scaling.\nAmong other benefits, this allows for rolling redeployments because stakeholder systems can rely on a model identifier to track, route, and collate any event data that may be generated as a result of using the ML model, such as tracking which model was used to serve a particular result in an A/B test.\nModel Serving Architectures",
    "keywords": [
      "Model Serving Architectures",
      "Serving",
      "Model Serving",
      "model",
      "serving model predictions",
      "Offline model serving",
      "Serving Architectures",
      "models",
      "data",
      "time",
      "serving time",
      "Offline Serving",
      "predictions",
      "model predictions",
      "Serving models online"
    ],
    "concepts": [
      "model",
      "models",
      "serving",
      "served",
      "serve",
      "data",
      "requires",
      "requirements",
      "require",
      "required"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 203-211)",
    "start_page": 203,
    "end_page": 211,
    "summary": "Serving at the Edge A slightly less commonly understood serving architecture is used when a model is deployed onto edge devices (Figure 8-5).\nModels served at the edge and as a separate microservice on the server\nRunning models on edge devices is essentially compulsory here.\n7 Federated learning is an approach that trains a model across multiple disconnected edge devices.\nModel Serving Architectures\nNon- edge infrastructure should still handle training, building, and serving of large models, while edge devices can perform local inferences with smaller models.\nML models can consume lots of RAM and be computationally expensive; fitting these on memory-constrained edge devices can be difficult or impossible.\nWe might push out a critical improvement to an on-device ML model in an iOS app, but that doesn’t mean that millions of existing users have to update the version on their iPhones.\nBecause any ML model deployed to the edge device might need to robustly keep operating and have its prediction and on-device learning setup continue working for a long time, it’s a huge architectural commitment that might carry a lot of future-looking tech debt and legacy support with it, and should be chosen carefully.\nOne of the important attributes you need to track when serving ML models in production is versioning.\nModel API Design Production-scale ML models are usually built using a wide variety of programming languages, toolkits, frameworks, and custom-built software.\nWhen trying to integrate with other production systems, such differences limit their accessibility, since ML and software engineers may have to learn a new programming language or write a parser for a new data format, and their interoperability, requiring data format converters and multiple language platforms.\nResource-oriented architectures (ROAs) in the REST style appear well suited to this task, given the natural alignment of REST’s design phi‐ losophy with the desire to hide implementation-specific details.8 Partially in support of this view, we’ve seen rapid growth in the area of ML web services in recent years: for example, Google Prediction/Vision APIs, Microsoft Azure Machine Learning, and many more.\nMost service-oriented architecture (SOA) best practices apply to ML model/inference APIs too.9 But you’ll want to take note of the following points for models:\nTo gain all the benefits of DevOps, however, you will want to empower the data science team to take full ownership of releasing its models to production.\nA fun‐ damental tension exists between requiring a data science team to fully own all end-to-end components of a production deployment and fully separating production concerns away from the data science team so it may focus fully on its domain specialization of model training and model optimization.\nIf the data science team owns too little, it may be disconnected from the constraints or realities of the production system its models must fit into, and will be unable to remediate errors, assist in critical bug fixes, or contribute to architectural planning.\nSo, when we are ready to deploy models in production, we actually deploy two different things: the model itself and the APIs that go and query the model to fetch the predictions for a given input.\nThose two things also generate a lot of telemetry and a lot of information that’ll later be used to help us monitor the models in production, try to detect drift or other anomalies, and feed back into the training phase of the ML lifecycle.\nTesting Testing model APIs, before deploying and serving in production, is extremely critical because models can be have a significant memory footprint and require significant computational resources to provide fast answers.\nData scientists and ML engineers need to work closely with the software and QA engineers, and product and business teams, to estimate API usage.\nWhen serving ML models, a performance increase doesn’t always mean business growth.\nModel performance is an assessment of the model’s ability to perform a task accu‐ rately, not only with sample data but also with actual user data in real time in a production setup.\nDetection is followed by mitigation of these errors by debugging, based on its behavior to ensure that the deployed model is making accurate predictions at the user’s end and is resil‐ ient to data fluctuations.\nML model metrics are measured and evaluated based on the type of model that the users are served by (for example, binary classification, linear regression, etc.), to yield a statistical report that enlists all the KPIs and becomes the basis of model performance.\nA resilient model, while not the best model with respect to data science measures like accuracy or AUC, will perform well on a wide range of datasets beyond just the training set.\nThis means that we don’t need to constantly monitor and retrain the model, which can disrupt model use in production and potentially even create losses for the organization.\n• Similar error rates for longer times in production models\nWe discuss more details about model quality and evaluation in Chapter 5, and API/ system-level KPIs like latencies and resource utilization in Chapter 9.\nML models deployed as API endpoints need to respond to such changes in demand.\nThe number of API instances serving the models should increase when requests rise.\nSeparate from these standard service failure considerations, the deep reliance of ML systems on training and data pipelines (whether online or offline) creates additional require‐ ments, including accommodating data schema changes and database upgrades, onboarding new data sources, durable recovery of stateful data resources (like the state of online learning, or the state of on-device retraining in an edge serving use case after an app crashes), and graceful failure in the face of missing data or upstream data ETL job outages—to name but a few.\nData is constantly changing and growing in data warehouses, data lakes, and stream‐ ing data sources: adding new and/or enhancing existing features in the product/ser‐ vice creates new telemetry, a new data source may be added to supplement a new model, an existing database goes through a migration, someone accidentally begins initializing a counter at 1 instead of 0 in the last version of the model, and the list can go on.\nWithout proper care for failure recovery, ML models that experience unexplained data changes or data disruptions may need to be taken out of production and iterated offline, some‐ times for months or longer.\nIt is critical to factor in this data extensibility as an early architectural consideration to avoid failure scenarios where we are blocked from being able to ingest a new feature for the model because of the logistics of accommodating the new data in production.",
    "keywords": [
      "model",
      "data",
      "Models",
      "Serving",
      "Model API Design",
      "edge devices",
      "Edge",
      "data science team",
      "Model Serving Architectures",
      "production",
      "Model API",
      "Management overhead",
      "Data science",
      "API",
      "architecture"
    ],
    "concepts": [
      "data",
      "model",
      "architecture",
      "architectures",
      "architectural",
      "service",
      "services",
      "serving",
      "serve",
      "production"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 212-220)",
    "start_page": 212,
    "end_page": 220,
    "summary": "When it comes to ethics and fairness while serving the ML models in produc‐ tion, we need to establish checks and balances as part of the development and deployment framework and be transparent with both internal stakeholders and customers about the data being collected and how it will be used.\nAlong with data privacy, especially when dealing with PII, product/business owners and ML/software engineers should invest more time and resources to secure the model API endpoints even though they are accessible only within the internal network (i.e., user requests are first processed by the application server before calling model APIs).\nThere are multiple ways to measure the ML model and product impact over the business, including input from key stakeholders, customers, and employees, and actual ROI as measured in revenue, or some other organizationally relevant metric.\nCHAPTER 9 Monitoring and Observability for Models\nDespite that, this chapter outlines what we know about how to monitor, observe, and alert for ML production systems, and makes suggestions for developing the practice within your own organization.\nThis chapter is about how to monitor systems that are doing ML, rather than using ML to monitor systems.\nWith that out of the way, let’s talk about production monitoring generically, without the complexities of ML, so we can make things easier to understand—and where better to begin than with a definition?\nMonitoring, at the most basic level, provides data about how your systems are performing; that data is made storable, accessible, and displayable in some reasonable way.\nYou use this kind of monitoring data to answer questions like these: Is my service cost-effective?\nChapter 9: Monitoring and Observability for Models\n(Of course, it could be any request/response architecture, like an ML model!) The mon‐ itoring system will obtain these metrics, usually via push or pull, which refers to whether the metrics get pulled from the target systems or get pushed from them.\nDifferent monitoring systems will make different choices about how to receive, store, process, and so on, but the data is generally queryable and often (very often) there’s a graphical way to plot the monitoring data so we can take advantage of our visual comparison hardware (eyes, retinas, optic nerves, and so on) to figure out what’s actually happening.\nThe Concerns That ML Brings to Monitoring One important concern is not necessarily the task of monitoring ML itself, but the perception of the act of monitoring by the model development community.\nChapter 9: Monitoring and Observability for Models\nmean inspection activities applying to model development, or it can mean continual observation of systems in production.\nThis is partially because of the nature of ML, partially a function of the way models are developed today, partially the nature of production operation, and partially a reflection of the fact that tools for inspectability are generally aimed just at model development.\nReasons for Continual ML Observability—in Production Observability data from your models is absolutely fundamental to business—both tactical operations and strategic insights.\nProblems with ML Production Monitoring ML model development is still in its infancy.\nProblems with ML Production Monitoring\nThis leads to two important observations we would make about the difference between model development and production serving.\nDifficulties of Development Versus Serving The first problem is that effectively simulating production in development is extremely hard, even in separate environments dedicated to that task (like test, staging, and so on.) This is not just because of the wide variety of possible serving architectures (model pools, shared libraries, edge devices, etc., with the associated infrastructure that you might or might not be running on) but also because in development you often invoke prediction methods directly or with a relatively small amount of code between you and the model for velocity reasons.\nFinally—and crucially—the data you have in testing is not necessarily distributed like the data the model encounters in production, and as always for ML, data distribution really matters.\nChapter 9: Monitoring and Observability for Models\nThe skew most likely to cause production problems is training-serving skew, which describes any difference between the performance of your model in training and in serving.\nSkew of this kind and other kinds is a common cause of avoidable outages in ML systems, and all of the simple causes of it should be targets for monitoring.\nDifferent models and ML systems are subject to different kinds of skew.\nAstute readers will therefore note a problem: the techniques for monitoring and detecting skew are model specific (or at least specific to a particular model architecture, config‐ uration, and purpose).\nIn terms of impact on monitoring best practices, this implies that the monitoring system has to be general-purpose and flexible, but that individual model and model- family monitoring has to be implemented by production engineers and ML engineers working closely together.\nProblems with ML Production Monitoring",
    "keywords": [
      "Monitoring",
      "model",
      "Production Monitoring",
      "model development",
      "data",
      "systems",
      "production",
      "monitoring system",
      "serving",
      "models",
      "monitoring data",
      "system",
      "Observability",
      "development",
      "Production Observability data"
    ],
    "concepts": [
      "monitor",
      "monitoring",
      "monitored",
      "models",
      "model",
      "product",
      "production",
      "serving",
      "data",
      "observe"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 221-228)",
    "start_page": 221,
    "end_page": 228,
    "summary": "In particular, model developers don’t generally think in terms of detection of issues post- deployment, but instead think in terms of modeling KPI performance pre-deploy‐ ment—and modeling KPIs are not necessarily directly connected to business KPIs!6\nSince the special case tools that today are used for model development (TensorBoard, Weights & Biases, and so on) don’t usually naturally translate into production itself, the particular monitoring system in use, and so on, at the moment we will necessarily have to make some of this up ourselves.\nGiven that, the overall goal for this chapter is to recommend a whole-lifecycle approach to monitoring, and in particular, suggest a default set of things to monitor other than the specific business metrics the model is intended to improve, since they are already well understood.\nBest Practices for ML Model Monitoring Let’s start off with a few framing assumptions: for the purposes of this chapter, model development is generally done in a loop.\nWe select data, train on it, build a model, do basic testing/validation, tweak, retrain, eventually release to production, learn how the model behaves, and begin the cycle again with ideas for improvement.\nMonitoring in serving can be divided into model, data, and service (also known as infrastructure).\nChapter 9: Monitoring and Observability for Models\nFrom the more practical perspective of model monitoring only, however, explainabil‐ ity is important to understand in preproduction and, increasingly, production phases.\nGeneric Pre-serving Model Recommendations We talk about this more in Chapter 3, but from a monitoring point of view, it’s most important to keep in mind the business goal attached to the development of the model, and connect its KPIs to exported metrics for monitoring purposes.\nThe most important recommendation is that your business KPIs should correlate with model metrics; you should be able to trace these continuously from development to production.\nWhen monitoring for data integrity, the most important features of the model should therefore be given high priority.\nPeople responsible for ML models want explainability for a few reasons: establishing which features should be prioritized for data integrity, investigating a specific predic‐ tion or specific slices of predictions, and responding to business requirements for\nBest Practices for ML Model Monitoring\nApplication date is typically not one of the top 10 most important features across all of our predictions, but if for some reason our training dataset has only a few applications on February 29 and those end up being poor risks in the intervening years, we can imagine a model that uses the application date to heavily influence the decision to reject.\nThe most important thing for monitoring training is keeping a holistic view, with the most important metric being how long it takes from starting training to producing a (hopefully working) model.\nHaving said that, other factors are important too—in particular, understanding when to retrain—i.e., to build a new model, in the hope or expectation that it might fix a problem for us.\nThough model rollback is the most common tactic used to resolve production problems, from time to time retraining is used—in this context, you can think of it as being like roll-forward for models: i.e., replacing what’s in production with the latest version of everything.\nChapter 9: Monitoring and Observability for Models\nIn the first, retraining would execute over the exact same data you used to train the old model (in which case, you would broadly expect the same behavior, since it’s the same input).\nBest Practices for ML Model Monitoring\nCompare the — training dataset to either the last time we trained this model or use another exogenous metric that can indicate rough size.\n— If the data is copied from somewhere else to go into, say, a feature store, and — the model is built from a feature store, does that copying happen correctly?\nChapter 9: Monitoring and Observability for Models\n— — What is the time to get the model into production and serving queries?\n— For those who do testing in production, does the model pass exposure to users?\n— Are the business or use metrics that you track in production affected in an unexpected way by the new model?\nBest Practices for ML Model Monitoring\nTo do that, we not only have to test versus historical data, which we are probably doing anyway, but also compare two models against each other.13 We can choose from at least two good approaches:\nA hybrid approach is to send some data to the model locally, and the same data to the model running in production (though with this approach, the local model is not enabled to take full production traffic; it is just taking your test traffic).\nIn that case, there are good arguments for doing two opposite things: (1) preferring the model currently running on the basis that it changes the least and is well understood, and (2) preferring a new model on the basis that something built over more recent data is probably going to survive change better and will be less painful to transition from.\nChapter 9: Monitoring and Observability for Models\nFor all kinds of reasons, it’s highly advisable to have a fallback plan, which is a set of steps you take if the new model fails, the rollout fails, or even the old model is found to have some weird behavior in a particular subset of circumstances.\nThis implies it’s a good idea to keep the actual binaries of these around, versioned correctly, as well as monitoring the actual versions in production, since stressing your training infrastructure to build a model from old data at a time of produc‐ tion outage is generally the opposite of a good idea.\nBest Practices for ML Model Monitoring",
    "keywords": [
      "model",
      "Model Monitoring",
      "monitoring",
      "data",
      "models",
      "production",
      "training",
      "business",
      "model development",
      "important",
      "time",
      "Input data",
      "Mindset Change",
      "n’t",
      "feature"
    ],
    "concepts": [
      "modeling",
      "models",
      "monitoring",
      "monitor",
      "business",
      "production",
      "product",
      "products",
      "process",
      "processed"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 229-236)",
    "start_page": 229,
    "end_page": 236,
    "summary": "Commonly used numerical KPIs are measured to assess an ML model’s performance.\nThe fraction of predictions for which the model is correct.\nWhat are good metrics to measure my model in serving?\n• Is my model performing as expected?\nTo choose good metrics for measuring model performance, we need to properly understand how a model can fail.\nModel\nThe model making predictions.\nThe data that is flowing in and out of the model.\nThis includes the features that the model uses to make a prediction and the prediction itself.\nThe service that actually renders the model.\nModel\nGiven those difficulties, we prefer to use the same metrics to evaluate the model in serving as we do in training/validation, since that will give us more confidence that we’re actually measuring in some sense “the same thing.” Of course, doing this requires being able to match the prediction with the corresponding observed reality.\nFor example, let’s assume a ride-hailing company has a model predicting the ETA of a car for a customer.\nWhile evaluating this model in validation, you care a lot about minimizing the error (usually referred to as root mean squared error, or RMSE16) so that the model’s prediction is close to the actual ETA: in this context, it turns out that customers tend to get more upset if you overpromise and underdeliver than the other way around.\nA similar example is food delivery; as soon as the pizza has arrived at the hungry customer’s house, you have real measurements you can compare your model predictions with, and pizza delivery as a business has a strong time limit baked into it (as well as other ingredients)—\nThe key thing this quick feedback loop enables is the ability to measure the effective‐ ness of your models essentially instantaneously (or at least very quickly); of course, once you have this latent ground truth linked back to your prediction event, no matter how long it took to get it, model performance metrics can easily be calculated and tracked.19 Tracking such metrics on a regular cadence allows you to make certain that performance has not degraded drastically from when a model was trained, or when it was initially promoted to production.\nHowever, many real-world environments change the way you get access to ground- truth data, as well as the tools you have at your disposal to monitor your models.\n19 The best model metric to use primarily depends on the type of model and the distribution of the data it’s\nAs a result, we will never know whether someone the model predicted will default could have actually paid the loan back in full.\n• Manual intervention is required to verify the model’s predictions.\n• The time window of getting actuals is so delayed it cannot meaningfully inform the modeler that the model’s performance should be looked at.\nThis might require sampling so that only a valuable segment of the predictions is manually verified to improve the model’s future training dataset.\nIn these scenarios, it’s not uncommon for ML teams to once again use proxy metrics to give signals of model performance.\nQuite apart from how a model might handle the relationship with actuals, it is possible to use generic measurements of the behavior of a model that can be useful for figuring out whether things have gone truly awry.\nTroubleshooting model performance metrics.\nThe most common causes are typically undersampling in training data, drift, and data integrity issues impacting the quality of data the model uses to make predictions.\nModels do not perform equally well on every possible input: they are highly dependent on the data they were trained on, perform well when they see data that resembles that, and perform less well when they don’t.\nEspecially in hyper-growth businesses where data is constantly evolving, accounting for drift is important to ensure that your models stay relevant.\nSome models are resilient to minor changes in input distributions, but accepting infinite resilience does not exist; at some point data distributions will stray far from\nConversely, when model outputs deviate from the established baseline, this is known as model drift, or prediction drift.\nEven if your model is making the same predictions as yesterday, it can make mistakes today!\nThis drift in actuals can cause a regression in your model’s performance and is commonly referred to as concept drift.\nEspecially when actuals are not available, drift is used in the real world to identify changes in model predictions, features, and actuals.\nIn many ML use cases where performance metrics cannot be calculated directly, drift often becomes the primary way to monitor changes in model\nWhile drift is focused on slow failures, data quality monitoring for models is focused on the hard failures.\nModels rely on the input features coming in to make a predic‐ tion, and these input features can come from a variety of data sources.\nLet’s say, for example, your hypothetical model predicting which pet food to buy for your pet supply store sees data suggesting that people own only cats now.\nThis is, quite simply, a bug in your data stream, and a violation of the contract (semantic expectations) you have set up between the data and the model.\nAt this point, whatever comes out of your model is undefined behavior, and you need to make sure to protect yourself against type mismatches like this in categorical data streams.",
    "keywords": [
      "model",
      "data",
      "model performance",
      "Models",
      "Model Monitoring",
      "model performance metrics",
      "Monitoring",
      "drift",
      "predictions",
      "Models model",
      "metrics",
      "distribution",
      "model predictions",
      "data quality",
      "performance"
    ],
    "concepts": [
      "model",
      "models",
      "modeler",
      "data",
      "cases",
      "case",
      "performance",
      "performing",
      "perform",
      "drift"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 237-245)",
    "start_page": 237,
    "end_page": 245,
    "summary": "While these techniques help you compensate for this problem, it’s not really a sustainable solution: if you have hundreds, thousands, or tens of thousands of data streams used to compute one feature vector for your model, the chance that one of these streams is missing can be very high!\nThough this is more to do with robustness than monitoring, it is possible to compensate for missing values in categorical data in a number of ways, a process commonly referred to as imputation.\nChapter 9: Monitoring and Observability for Models\nIt’s not surprising to ML practitioners today that many models rely on very large numbers of features to perform their task.\nThe reality is that this data schema will inevitably change often as the team experiments to improve the model.\n— Missing values: Is this feature missing data?\nBest Practices for ML Model Monitoring\n— — Missing values: Is this feature missing data?\nFor an ML system to be successful, you need to understand not just the data going in and out of the ML system, and the performance of the model itself, but the overall service performance in rendering or serving the model—making its predictions or classifications available.\nEven if the model performance improves business outcomes and data integrity is maintained, but it takes several minutes for a single prediction, it might not be performant enough to be deployed in a real-time serving system.\nSimilar to other software services deployed to production, the service deploying the model into production and serving the model’s inferences needs to be monitored.\nRegardless of what is used for model serving, it is important (especially in real-time services) to monitor the prediction latency of the service because the expected prediction should happen immediately after the request is sent.\nReduce the time it takes for the model to make a prediction.\nThis is not just about the model, but also gathering the input features (sometimes precomputing or caching them), and quickly catching the predictions to serve.\nTo optimize the model for lower prediction latencies, the best approach is to reduce the complexity of the model.\nChapter 9: Monitoring and Observability for Models\nFor example, if there are more levels in a decision tree, more-complex relationships can be captured from the data and therefore increase the overall effectiveness of the model.\nBefore the model can even make a prediction, all of the input features must be gathered, and this is often accomplished by the service layer of the ML system.\nThe customer wouldn’t provide this when they view the page themselves, but the model service would query a feature store or a real-time database to fetch this information.\nMonitoring the lookup and transforma‐ tions needed for these features is important to trace where the latency is coming from in the ML system.\nBest Practices for ML Model Monitoring\nChapter 9: Monitoring and Observability for Models\nFor environments with lots of models, or lots of model churn, a promising • approach is a self-service infrastructure so ML engineers can define and enforce per model or per class of model SLOs (generally by comparison to a golden dataset); SREs could develop, offer, and support such a service, thus helping the overall SLO approach scale for everyone.\nBest Practices for ML Model Monitoring\nTraining Pretraining: Source data size (i.e., hasn’t grown or shrunk a lot since last build) Training: Training time as a function of model type and input data size Training configurations (e.g., hyperparameters) Post-training: Model quality metrics (accuracy, precision, recall, other model-specific metrics) Built model passes validation tests (not only that aggregate filters work, but results on golden set and held-out set tests are of equal or better quality).\nServing Model serving latency (as a fraction of overall serving latency or compared over time) Model serving throughput Model serving error rate (time-out/ empty value) Serving resource use (RAM in particular) Age/version of model in serving\nApplication (YarnIt) Model-specific metrics (visible on individual page load) Number of recommendations per page Estimated quality (predicted click-through) of each recommendation Similar metrics for search model\n(O’Reilly, 2018) has a reasonable example of a fully specified, exhaustively detailed SLO document; it is also possible to be have something as simple as “99% of this data is younger than two hours” and for that still to provide value.\nChapter 9: Monitoring and Observability for Models\nIn this chapter, we have covered important metrics for moni‐ toring the service health, model efficacy, and data integrity of the model.\nFrom a monitoring point of view, our major concern is the requirement to facilitate such visibility into model decisions, while not facilitating inappropriate visibility—see the next section for more detail.\nBest Practices for ML Model Monitoring\nYou almost certainly want (need!) to PII-proof the data at the first point of egress from the monitoring system, or even do it in a staged way, where different kinds of data can be processed or stripped at different ingress points to different systems—a multistage filtering approach.\nAnother important category we don’t cover is relating the model performance metrics to the business impact of the model.\nAnother important cat‐ egory we won’t be covering is monitoring for dense data types, also sometimes called unstructured data—though since these formats are, in fact, highly structured, this is a bit of a misnomer!\nAs ML increasingly uses images, video, and so on as inputs into its models, it’s necessary to monitor data integrity for these nontabular data types too.\nMonitoring these lower-dimensional vectors provides a proxy to monitoring dense data types.\nDetermining the correlation between available monitoring metrics and business value is almost certainly the most important thing for the organization to do in order to extract the most value out of its ML efforts in production.\nChapter 9: Monitoring and Observability for Models",
    "keywords": [
      "model",
      "data",
      "monitoring",
      "Model Monitoring",
      "model serving",
      "feature missing data",
      "Models",
      "serving",
      "Numerical data",
      "system",
      "Serving Model serving",
      "features",
      "missing data",
      "feature",
      "service"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "monitoring",
      "monitored",
      "monitor",
      "monitors",
      "serving",
      "serve",
      "served"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 246-254)",
    "start_page": 246,
    "end_page": 254,
    "summary": "If you don’t have reasonable actuals, you’ll have to build a picture of what’s happening from a set of partial sources, including infrastructural elements as well as model performance.\nBest Practices for ML Model Monitoring\nThe major challenge here is that since the model building duration can be a large number of minutes, hours, or even days, it doesn’t make much sense to alert on every fluctuation in training performance; otherwise, you’ll be alerting too much.\nConclusion We hope this chapter has provided a useful overview to monitoring your ML systems from birth to happy life in production.\nUp until now, our discussions of ML systems have sometimes centered on the idea that a model is something we train and then deploy, almost as though this is some‐ thing that happens once and only once.\nA slightly deeper view is to draw a distinction between models that are trained once and deployed versus those that are trained in a more continuous fashion, which we will refer to as continuous ML systems.\nTypical continuous ML systems receive new data in a streaming or periodic batch fashion and use this to trigger training an updated model version that is pushed to serving.\nClearly, there are major differences from an MLOps perspective between a model that is trained once versus a model that is updated in a continuous manner.\nManaging continuous data streams, responding to model failures and corruptions, and even seemingly trivial tasks like introducing new features for the model to train on all increase system complexity.\nIf we remember that in ML, data is code, the idea of continuous ML is to accept the equivalent of a steady stream of new code that can change the behavior of our production system.\n• Models may influence their own future training data through feedback loops.\nAnatomy of a Continuous ML System Before we look in detail at the implications of continuous ML systems, let’s take some time to take a pass through the typical ML production stack and see how things change in the continuous setting compared to the noncontinuous setting.\nAt a high level, a continuous ML system regularly takes data in from the world in a steady stream, uses it to update the model, and then after appropriate validation, pushes out an updated version of the model to serve new data.\nTraining Examples Rather than existing as a fixed set of immutable data, training data in a continu‐ ous ML system comes in a steady stream.\nFiltering Out Bad Data Whenever we allow our models to learn directly from behavior in the world, we run the risk that the world will send along behaviors that we wish our model did not have to learn from.\nFor example, spammers or scammers may try to interfere with our yarn product prediction model by issuing many fake queries without purchasing in an attempt to make it appear that some products are desired less by users than they actually are.\nIn all cases, it is important to remove such forms of bad data from the pipeline before training, so that the model training is not impacted.\nEffective organizations often have fully dedicated teams devoted just to the problem of filtering out bad data from a continuous ML pipeline.\nFeature Stores and Data Management In typical production ML systems, raw data is converted into features, which in addition to being useful for learning are also more compact for storage.\nMany pro‐ duction systems use a feature store for storing data in this way, which is essentially an augmented database that manages input streams, knows how to convert raw data into features, stores them efficiently, allows for sharing among projects, and supports both model training and serving.2 For high-volume applications, it is often necessary to do some amount of sampling from the overall stream of data to reduce storage and processing costs.\nUpdating the Model In continuous ML systems, it is often preferable to use a training methodology that allows for incremental updates.\n(Recall that SGD forms the basis of most deep learning training platforms.) To use SGD in a continuous setting, we essentially just close our eyes and pretend that the stream of data shown to the model comes in a stochastic (random) order.\nHowever, more data- starved applications may need to visit individual examples many times to converge to a good model, so this strategy of visiting each example exactly once in order cannot always be followed.\nPushing Updated Models to Serving In most continuous ML settings, we refer to major changes to a model as a launch.\nMajor changes might include changes to a model architecture, the addition or removal of certain features, a change in hyperparameter settings like learning rates, or other changes that would motivate us to fully reevaluate the performance of a model before launching it as our production model.\nMinor changes such as small modifications to internal model weights based on new incoming data are referred to as updates.\nOne way to think about pushing a new model checkpoint out to serving is that it is actually a small, automated model launch, and if we push a new checkpoint four times an hour, then we are doing almost a hundred small, automated model launches a day.\nAll of the things that can go wrong with a major model launch can also go wrong when we push a new checkpoint to serving.\nThe model may be corrupted somehow— perhaps by bugs, perhaps by having been trained on bad data.\nOf course, lots of things can go wrong with our system if we do not regularly push model updates based on new data, so the point is not to avoid updating models, but rather to point out that validation of model checkpoints is a critical step before they are pushed to serving.\nAny change in input distributions to the model in production may cause erratic or unpredictable system behavior, because the theoretical guarantees for most ML models really hold for only the IID setting.3\nSporting events, election nights, natural disasters, daylight savings, bad weather, good weather, traffic accidents, network outages, pandemics, new product releases—all of these are potential sources of changes to our data streams, and thus to our system behavior.",
    "keywords": [
      "model",
      "Data",
      "Continuous",
      "system",
      "training",
      "Models",
      "systems",
      "model performance",
      "Bad Data",
      "continuous ML systems",
      "continuous data streams",
      "Serving",
      "Monitoring",
      "stream",
      "KPI performance"
    ],
    "concepts": [
      "model",
      "models",
      "modeling",
      "data",
      "training",
      "train",
      "trained",
      "setting",
      "sets",
      "settings"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 255-262)",
    "start_page": 255,
    "end_page": 262,
    "summary": "After a short delay, the model is updated on this new influx of search and purchase data, and learns to predict much higher values for brown wool products.\nThe next influx of data shows that no users are purchasing any products, and the model overcompensates, but because the brown_wool products have been shown on such a broad range of queries to such a broad range of users, the model now learns to give lower scores for nearly all products, resulting in no results or junk results for all user queries.\nThis reinforces the trend that no users are purchasing anything, and the system spirals down, until our MLOps team identifies the issue, rolls the model back to a previous well-behaved version, and filters the abnormal data from our store of training data before re-enabling training.\nOne way to do this would be to create a model to do propensity scoring, which shows how likely a given example is to occur in our training data at a given time.\nModels Can Influence Their Own Training Data One of the most important questions to answer for our continuous ML system is whether a feedback loop exists between a model and its training data.\nTo help understand the issues, consider that some model systems have no influence over the stream of data that is collected for retraining.\nOther models do influence the collection of their training data, especially when those models make recommendations to users or decide on actions that impact what they can learn about next.\nAs a more concrete example, consider a model that helps recommend wool products to show to users; the model may then be trained on the user response to those selected products, such as clicks, page views, or purchases.\nproducts that were not selected.4 It is easy to imagine that a new wool product, such as a new color of organic alpaca yarn, might be something that users would love to purchase, but for which the model has no previous data.\nIf the product recommendation relies on purchase behavior as a signal in training, and the presence of discounts influences purchase behavior, then there is a feedback loop that links these two models, and changes or updates to one model can influence the other.\nThe effect of feedback loops can be lessened to some degree by logging model version information along with other training data, and using this information as a feature in the model.\nTemporal Effects Can Arise at Several Timescales We create continuous ML systems when we care about the ways that data changes over time.\nThe most effective way to deal with seasonality effects is to make sure that our model has trained on data from more than one full year in the past—if we are fortunate enough to have it—and that time-of-year information is included as a feature signal in the training data.\nThe subtlety for daily cycles comes in when we consider that most continuous ML systems actually run continuously behind reality, because of the inherent need for delays in pipelines and data streams waiting for training labels—such as click or pur‐ chase behavior that may take some time for a user to decide on—as well as filtering out bad data, updating models, validating checkpoints, and pushing checkpoints to serving and ramping them up fully.\nFortunately, fixing such issues is relatively easy, by logging time-of-day information along with other training signals and using these as inputs to our models.\nSecond, our model may have a feedback loop with itself, meaning that if we do not address issues quickly, the stream of input data may also be corrupted and require care to fix as well.\nReal-time crisis response requires first detecting issues quickly, which means that from an organizational standpoint, a good litmus test for determining whether we are ready for continuous ML is to examine the thoroughness and timeliness of our moni‐ toring and alerting strategies.\nSimilarly, when we find that our data stream is corrupted in some way, perhaps by a bad model, or an outage, or a code-level bug somewhere in the system, a useful response can be to stop model training and halt pushing any new model versions out to serving.\nIt makes sense to ensure that there is an easy way for MLOps folks to stop training on any model that is their responsibility.\nAutomated systems are helpful here, but of course need to alert sufficiently so we do not discover that a model has silently stopped training three weeks ago.\nIn continuous ML systems, it is important to have a fallback strategy that can be used in place of our production model that provides acceptable (even if nonoptimal) results.\nIf we believe that the root cause of our issue is bad data that has caused the model to train itself into a bad state, it makes sense to roll back the model version to a previously known-good version.\nAgain, it is important to keep checkpoints of our trained production model on hand so that we have a set of previous versions to choose from.\nFor example, imagine that Black Friday sales in the US cause such a large increase in purchase requests from users to our yarnit.ai store that the fraud detection portion of the system starts to label all purchases as invalid, making it look to our model as though all products are extremely unlikely to be purchased.\nWhen we have bad data in our system, we need to have an easy way to remove it so that our model will not be corrupted by it.\nOtherwise, when we re-enable training to proceed, this data will be encountered by our rolled-back model as it moves forward in time through the training data, and it will be corrupted by the bad data again.\nRemoving bad data is a useful strategy whenever we believe that the data itself is highly unrepresentative of typical data and is unlikely to give the model useful new information, and that the root cause of the\nHowever, if a crisis is detected due to an external world event, sometimes the best response is to just cross our fingers and roll through it, allowing the model to train on the atypical data and then to recover itself as the world event ends.\nIndeed, it is unfortunately true that this world has few days with no political event, major sporting event, or other major newsworthy disaster happening somewhere, and making sure that our model has enough exposure to atypical data like this from different global regions can be an important way to ensure that our model is generally robust.\nTo answer that question, we need to observe our model’s response to similar historical events, which is most easily done when we have trained the model on historical data in sequential temporal order.",
    "keywords": [
      "model",
      "data",
      "continuous",
      "training data",
      "Continuous ML Systems",
      "models",
      "system",
      "Systems",
      "bad data",
      "training",
      "bad",
      "products",
      "response",
      "feedback",
      "time"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "issue",
      "issues",
      "issuing",
      "products",
      "product",
      "production",
      "response"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 263-271)",
    "start_page": 263,
    "end_page": 271,
    "summary": "New Launches Require Staged Ramp-ups and Stable Baselines When we have had a model running as part of our continuous ML system for a period of time, we will eventually want to launch a new version of that model that creates improvements in various ways.\nAs we describe in Chapter 11, offline testing and validation can give useful guidance on whether a new version of a model is likely to perform well in production, but often cannot give a complete picture.\nThis is especially true when our continuous ML model is part of a feedback loop, because the data that we have previously trained on was most likely chosen by a previous model version, and evaluation on offline data is limited to data that has been collected based on actions or recommendations made by that previous model.\nIn this way, a new model launch requires some amount of testing in production as the final form of validation.\nInstead, we will most often use a staged ramp-up, first allowing the model to serve only a fraction of the overall data, and increasing that amount only as we observe good performance over time.\nThis strategy is commonly known as an A/B test: we test out our new model A against the performance of our old model B, in a format that resembles a controlled scientific experiment and helps verify that the new model will show the appropriate perfor‐ mance on our final business metrics (which may be distinct from offline evaluation metrics like accuracy).\nThe difference between a model launch and an ideal A/B test is that in a scientific experiment, A and B are independent and do not influence each other.\nFor A/B experiments comparing continuous ML models, it turns out that A and B may well influence each other when our models are part of a feedback loop.\nFor example, imagine that our new model A does a great job of recommending organic wool products to yarnit.ai users, whereas our previous model B had never done so.\nAn A/B experiment might initially show that the A model is much better in this regard, but then as training data is produced by A that includes many more organic wool recommendations and purchases, the B model (which is also continuously updating) may then also learn that these products are liked by users and begin to recommend them as well, making the two models appear identical over time.\nThis strategy can work well when each model serves the same amount of data, such as 50% of the overall traffic each, but can make for flawed comparisons in other cases.\nIf A is looking bad early on, is that because the model is bad, or because it has only 1% of training data while B has 99%?\nA stable baseline is a model C that is not influenced by either A or B, and is allowed to serve a certain amount of traffic so that we can use those results as a comparison.\nIf we expect our overall launch process to take, say, two weeks, then a reasonable alternative can be to run a copy of our production model that is set to update continuously, but at a two-week delay.\nAn approach that maintains strict independence from A and B but does not have a limited shelf life is a parallel universe model that is allowed to serve a small fraction of overall data and learns only on the data that it serves itself.\nParallel universe models often take time to stabilize after being set up because of the restricted amount of training data and the overall distribution shift.\nModels Must Be Managed Rather Than Shipped Overall, model launches require particular care because at these times our systems are most vulnerable to crisis.\nIf we are in a model launch process with A and B both serving roughly half the traffic, we have just doubled the potential sources of error and doubled the amount of work needed to address any emergency that may arise.\ncontinuous ML model may be difficult to solve in a complete or permanent way.\nDecision making in continuous ML settings often requires some amount of counterfactual reasoning, thinking through the impact of feedback loops, or wrestling with noise and uncertainty that makes effective decision making challenging.\nThe first is to ensure that we have organizational leadership with enough scope and context to effectively weigh the differing needs of, for example, improving model monitoring and crisis response handling with that of improving model accuracy.\nOrganizationally, understanding that a continuous ML system relies on a steady stream of incoming data to determine system behavior makes clear that the data pipeline itself requires serious, dedicated oversight and management.\nAs we’ve noted, the launch process for ML systems and further improvements neces‐ sarily requires a staged ramp-up procedure in the continuous ML setting.\nThe process for assessing the wider impact of various launch stages and ensuring stability before proceeding must be well established and rigorously followed for a continuous ML organization to be effective in the long run.\nML production engineers shouldn’t be determining the urgency of resolving a particular incident while in the middle of it, and model developers shouldn’t be guessing at the costs and consequences of a given outage while it is ongoing.\nWe should think about every model that is a key part of a production system as one that is trained continuously, even if it is not actually updated on new data every minute, every hour, or even every day or week.\nOne reason is that if we apply the standards and best practices from continuous ML systems to all production-grade ML systems, we will definitely be ensuring that our technical infrastructure, model development, and MLOps or crisis response teams are set up to meet challenges as they arise.\nIf a model is trained only once, applying the standards and best practices from continuous ML may be seen as a waste of resources.\nIn our experience, we have seen that every production-level ML model will eventually be retrained or have a new version launched—maybe in a few months, or next year, as new data becomes available or models are developed.\nConclusion In this chapter, we have laid out sets of procedures and practices that can form the foundation of an organizational playbook for the ongoing care and oversight of continuous ML systems.\nThese systems offer a remarkable range of benefits, enabling models that adapt to new data over time and allow for responsive learned systems that interact with users, marketplaces, environments, and the world.\nAny organization managing a continuous ML system needs to think of this as an ongoing high-priority mission, with special care at times of launches or major updates, but also with continual monitoring and contingencies in place to allow fast response to emergent crises.\nAnd finally, we ended the chapter with the idea that all ML systems should likely best be thought of as continuous ML systems, as all models are eventually retrained, and having strong standards in place will benefit any organization in the long run.\nThis means that we can see unintuitive effects where there is a disconnect among the ML system, the world, or the user behavior we are trying to model.",
    "keywords": [
      "model",
      "continuous",
      "Systems",
      "Continuous ML Systems",
      "models",
      "system",
      "data",
      "time",
      "production model",
      "organization",
      "model launch",
      "production",
      "requires",
      "Continuous Organizations",
      "version"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "systems",
      "organizational",
      "organizationally",
      "continuous",
      "continuously",
      "continual",
      "make"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 272-281)",
    "start_page": 272,
    "end_page": 281,
    "summary": "to understand now is that troubleshooting ML incidents can involve very much more of the organization than standard production incidents do, including finance, supplier and vendor management, PR, legal, and so on.\nML incident resolution is not necessarily something that only engineering does.\nA final serious point we would like to make here at the beginning is that, as with other aspects of ML systems, incident management has serious implications for ethics in general and very commonly for privacy.\nPrivacy and ethics will make an appearance in several parts of the chapter and are addressed directly toward the end because by then we will be in a better place to draw some clear conclusions about how ML ethics principles interact with incident management.\nIndeed, if you’ve worked with incidents for long enough, you’ve probably seen one already, and it probably starts something like this: an engineer becomes aware of a problem; they troubleshoot the problem alone, hoping to figure out the cause; they fail to assess the impact of the problem on end users; and they don’t communicate the state of the problem, either to other members of their team or the rest of the organization.\nOnce the initial troubleshooters realize the scope of the incident, even more delays may arise while they try to figure out which other teams need to be involved and send pages or alerts to track them down.\nIt is enormously helpful to have clearly defined guidelines ahead of time about when to declare an incident, how to manage it, and how to follow up after it.\nWe’ll briefly cover the roles in a typical incident and then will try to understand what differs in handling an ML incident.\nIncident Response Roles Some companies have thousands of engineers working on systems infrastructure, and others might be lucky to have a single person.\nIf they are a poorly staffed afterthought or you assume anyone can jump in when incidents occur with no structure, training, or spare time, the results can be quite bad.\nThis includes recording work items to be fixed, storing logs to be analyzed, and scheduling time to review the incident in the future.\nThese roles are invariant, whether or not you are dealing with an ML incident.\nAs a result, it is harder to write monitoring rules to catch all incidents before a human user detects them.\nML incidents usually involve a broader range of staff during troubleshooting and resolution, including business/product and management/leadership.\nML outages often impact multiple systems because of their role in integrating with and modifying other parts of your infrastructure.\nMany ML incidents involve impact to quality metrics that themselves already vary over time.\nWhat decisions did we make about the system before this point that could have played a role in the incident?\nA specific instance of one configured model trained on one set of data at a point in time\nStory 1: Searching but Not Finding One of the main ML models that YarnIt uses is a search ranking model.\nan ML model that tries to predict how to order those results, given everything we know about the search at the time it’s performed.\nAriel, a production engineer who works on search system reliability, is working on the backlog of monitoring ideas.\nOne of the things the search team has been wishing it monitored and trended over time is the rate that a user clicks one of the first five links in a search result.\nAriel declares an incident and notifies the search model team since it might be a problem with the model.\nAriel also notifies the retail team, just to check that we’re not suddenly making less money from customers who are searching for products (as opposed to browsing for them) and also asks the team to check for recent changes to the website that would change the way results are rendered.\nAriel finds—and the search model team confirms—that no changes have been made to the model configuration in the past two months.\nInstead, one of the search model team members notes something interesting: they use a golden set of queries to test new models daily, and they’ve noticed that in the past three weeks the golden set is producing incredibly consistent results—consistent enough to be suspicious.\nAriel goes to look at the trained model deployed in production.\nSo Ariel looks at the search model training system, which schedules the search model training every night.\nIt has not completed a training run in over three weeks, which would definitely explain why there isn’t a new trained model in production.\nWe have a proximal cause for the outage, but at this point we don’t know the underlying cause, and there’s no obvious simple mitigation: without a new trained model in production, we cannot improve the situation.\nA scheduler loads a set of processes to store the state of the model, and another set of processes to read the last day’s search logs and update the model with the new expressed preferences of the users.\nAriel notes that all of the processes trying to read logs from the search system are spending most of their time waiting for those logs to be returned from the logging system.\nThe team agrees, Ariel makes the change, the log-feeder jobs stop crashing, and the search training run completes a few hours later.\nThe outage is mitigated as soon as the training run completes and the new trained model is put into production.\nAriel works with the team to double-check that the new trained model loads auto‐ matically into the serving system.\nThen they all wait a few hours to accumulate enough logs to generate the data they need to make sure that the updated trained model is really performing well for customers.\nAriel and the team work on a review of the incident, accumulating some post-outage work they’d like to perform, including the following:\nDetermine our requirements for having a fresh model and then distribute the • available time to the subcomponents of the training process.\nFor example, if we need to get a model updated in production every 48 hours at the most, we might give ourselves 12 hours or so to troubleshoot problems and train a new model, so then we can allocate the remainder of the 36 hours to the log processing, log-feeding, training, evaluation, and copying to serving portions of the pipeline.\nStages of ML incident response for story 1\nThis outage, although quite simple in cause, can help us start to see the way that ML incidents manifest somewhat differently for some phases of the incident response lifecycle:\nML systems that are not well instrumented often manifest systems problems as only quality problems—they simply start performing less well and get gradually worse over time.\nML troubleshooting often involves a broader set of teams than some other kinds of outages, precisely because they often manifest as publicly visible quality problems.\nassumptions about the kind of outage we’re experiencing—it could be a systems problem, a model problem, or just a drift in our ability to correctly predict the world.3 Sometimes it’s the world changing faster than we can keep up with— more on this in story 3.\nThe fastest and least risky steps to mitigate the problem, in this case, involved training a new model and successfully deploying it to our production search serving system.\nFor ML systems, especially those that train on lots of data or that produce large models, no such quick resolution might be available.\nWe can see that outages can present as quality problems of models not quite doing what we expect or need them to do.",
    "keywords": [
      "incident",
      "incident management",
      "Model",
      "Incident Response",
      "system",
      "search model team",
      "Incident Management Basics",
      "incidents",
      "Incident Management System",
      "search model",
      "Ariel",
      "Trained model",
      "outage",
      "time",
      "systems"
    ],
    "concepts": [
      "model",
      "models",
      "outage",
      "outages",
      "logs",
      "logging",
      "log",
      "production",
      "products",
      "product"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 282-291)",
    "start_page": 282,
    "end_page": 291,
    "summary": "train a separate model per partner, and extract partner-specific data into isolated repositories, though we can still have a common feature store for shared data.\nAs a result, we built a system that extracts historical data from each partner and puts it into a separate directory or small feature store.\nSam, a production engineer at YarnIt, works on the partner training system.\nWhile preparing the report, Sam notices that the partner in question has zero recent conversions (sales) recorded in the ML training data but that the account‐ ing system reports that it’s selling products every day.\nIn the meantime, Sam leaves this fact off the report on data to the partner team and simply includes the sales data.\nData discrepancies in counts like this happen all the time, so the data extraction team does not treat Sam’s report as a high priority.\nSam is reporting a single discrepancy for a single partner, and the team files a bug and plans to get to it in the coming week or so.\nThe logs of the partner model training system clearly report that the partner models are successfully training every day, and there are no recent changes to either the binaries that carry out the training or the structure and features of the models themselves.\nLooking at the metrics coming from the models, Sam can see that the predicted value of every product in the CrochetStuff catalog has declined significantly every day for the past two weeks.\nOne of the things they notice is what Sam noticed originally: there are no sales for any partners in the last two weeks in the ML training data.\nThe data extraction team resurrects Sam’s bug from a few days before and starts looking at it.\nSam, who needs to find a fast mitigation for the problem, notes that the team stores older copies of trained models for as long as several months.\nThe team confirms that while the old trained model versions won’t have any information about new products or big changes in consumer behavior, they will have the expected recommendation behavior for all existing products.\nSince the scope of the outage is so significant, the partner team decides it is worth the risk to roll back the models.\nIn consultation with the partner team, Sam rolls back all of the partner trained models to versions that were created two weeks earlier, since that seems to be before the impact of the outage began.\nThe ML engineers do a quick check of aggregate metrics on the old models and confirm that recommendations should be back to where they were two weeks ago.5\nWhile Sam has been mitigating, the data extraction team has been investigating.\nIt finds that while the extractions are working well, the process that merges extracted data into the existing data is consistently finding no merges possible for any partners.\nFurther investigation reveals that two weeks ago, in order to facilitate other data analysis projects, the data management team changed the unique partner key, used to identify each partner in its log entries.\nSam requests that a single partner’s data be re-extracted and that a model be trained on the new data in order to quickly verify that the system will work correctly end to end.\nOnce this is done, Sam and the team are able to verify that the newly extracted data contains the expected number of conversions and that the models are now, again, predicting that these products are good recommendations for many customers.\nSam and the data extraction engineers do some quick estimations on how long it will take to re-extract all of the data, and Sam then consults with the ML engineers on how long it will take to retrain all of the models.\nThey arrive at a collective estimate of 72 hours, during which they will continue to serve recommendations from the stale model versions that they restored from two weeks prior.\nSam requests that all partner data be re-extracted and that all partner models be retrained, preferably from scratch, starting with the beginning of data we have.\nThey monitor the process for three days, and once it is done, verify that the new models are recommending not only the older products but also newer products that didn’t exist two weeks prior.\nAfter careful checking, the new models are deemed to be good by the ML engineers and are put into production.\nSam brings the team together to go over the outage and file some follow-up bugs so that they can avoid this kind of outage in the future and detect it more quickly than they did this time.\nThe data extraction team settles on a strategy of storing the count of merged log lines by partner by day and comparing today’s successes to the trailing average of the last n days.\nIt has prominent differences though, and the best way to see those with some context and nuance is to walk through the partner training outage and look at the ML-salient features that occur during each section:\nWe will dig into the tactics for observing and diagnosing outages across systems with coupled data and ML in “ML Incident Management Princi‐ ples” on page 274.\nWe have commercial and product staff (the partner team), ML engineers who build the model, data extraction engineers who get the data out of the feature store and logs store and ship it to our partner model training environment, and production engineers like Sam coordinating the whole effort.\nTroubleshooting ML outages really has to start not with the data but with the outside world: what is our model saying, and why is that wrong?\nML outages can only sometimes be mitigated by restoring an older version of the model, because their job is to help computer systems adapt to the world, and there’s no way to restore a snapshot of the world as it used to be.\nAs was the case with our partner model outage, no cost-free quick mitigation exists.\nSam makes sure that the data in the partner training system is correct (at least in aggregate, and spot checks seem to confirm that it looks good).\nGabi is a production engineer who works on the discovery modeling system.\nGabi also starts doing the normal production investigation, focusing particular atten‐ tion on what changed in the recommendations stack recently.\nNo obvious software, modeling, or data updates correlate with the outage, so Gabi decides that it’s time to dig into the recommendations model itself.\nAs Gabi is explaining to Imani what they know so far (fewer products purchased, fewer recommendations purchased per checkout, no system changes to speak of), the note from customer support comes to mind.\nImani thinks this may be worth investigating and asks Gabi to grab enough data to trend some basic metrics on the recommendations system: number of recommendations per browse page, average hourly delta between expected “value” of all recommendations (probability that a customer will purchase a recommended product times the purchase price), and the observed value (total value of recommended products ultimately purchased).\nThe recommendation system uses the query that a user made, the page that they are on, and their purchase history (if we know it) to make recommendations, so this is the information that Imani will need to query the recommendation model directly.\nImani extracts out about 100,000 queries and page views and sets up a test environ‐ ment where they can be played against the recommendation model.\nAfter a test run through the system, Imani has recommendations for all of the results and has stored a copy of the whole run so that it can be compared to future runs if they need to modify or fix the model itself.",
    "keywords": [
      "data",
      "partner",
      "Sam",
      "model",
      "recommendations",
      "models",
      "system",
      "data extraction team",
      "team",
      "partner team",
      "Gabi",
      "outage",
      "data extraction",
      "partner model training",
      "Incident Response"
    ],
    "concepts": [
      "sam",
      "data",
      "model",
      "models",
      "modeling",
      "recommendations",
      "recommending",
      "recommendation",
      "recommend",
      "recommended"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 292-300)",
    "start_page": 292,
    "end_page": 300,
    "summary": "On a hunch, Gabi and Imani grab another 100,000 queries and page views from a month ago (before there was any evidence of a problem) as well as a snapshot of a model from every week in the last six weeks.\nImani plans to run the old and new queries against each of the models and see what can be learned.\nGabi pushes for a quick test first: today’s queries versus a month-old model.\nHere’s the thinking: if that works, there’s a quick mitigation (restore the old model to serving) while troubleshooting continues.\nThe old model makes different recommendations than the new model and does seem to make slightly more of them.\nBut the old model still makes many fewer recommendations against today’s queries than it did against the queries a month ago.\nImani and Gabi finish running the full sweep of old and new queries against older and newer models.\nImani wants to figure out how they have changed in the last month, thinking maybe the problem is with the model’s ability to handle a shift in user behavior rather than something being wrong with the model itself.\nThere is no obvious change to the recommendations model that can improve the situation, given our supply shortfalls and the way that the weather has impacted our customers’ preferences.\nAt this point, the outage is probably over, since we’ve decided not to change the system or model.\nThe overall stability of the model has been perceived to be of value, but in this case it ended up showing bad recommendations to users for many days and making it harder for the production team to troubleshoot problems with it.\nImani wants to find a way to improve the responsiveness to new situations without making the model overly unstable.\nWe should treat this as an opportunity to think about what the model should • do when it doesn’t have any good recommendations.\nThis is fundamentally a product and business problem rather than an ML engineering problem: we need to figure out the behavior we want the model to exhibit and the kinds of recom‐ mendations we think we should surface to users under these circumstances.\nFiguring out whether there’s a way to identify a product recommendation strategy to do that is a hard problem.\nIn particular, the production engineers should have revenue results in aggregate and broken down by product category in the product catalog, by geography, and by the original source of the user viewing the product (search result or recommendation or home page).\nWe definitely could have made choices that would have made the outage progress differently, and more smoothly for our users, but in the end we cannot recommend products we don’t have, and sales were going to go down.\nThere may be a model that could produce better recommendations under these circumstances (rapid change in demand combined with an inventory problem), but that falls more under the heading of continuous model improvement rather than incident avoidance.\nrecommendations model nor was it preventable by it.\nThe process of investigating this outage includes some of the hallmarks of many ML-centric outage investigations: detailed probing at a particular model (or set of models or modeling infrastructure) coupled with broad investigation of changes in the world around us.\nMoreover, the only way to actually resolve the core outage, and get our revenue back on track, is to change what our users want or to fix the products that we have available to sell.\nOne thing the team didn’t think about, probably in part because it was focused on troubleshooting the model and resolving the ML portion of the outage, was that there may have been other, non-ML ways of mitigating the outage: what if our system showed out-of-stock recommendations and invited customers to be notified when we had those (or similar) products available?\nIn many cases, follow-up from an ML-centric incident evolves into a phase that doesn’t resemble “fix the problem” so much as “improve the performance on the model.” Post-incident follow-up often devolves into longer-term projects, even for non-ML-related systems and outages.\nBut the boundary between a “fix” and “ongoing model improvement” is particularly fuzzy for ML systems.\nrecommendation: first define your model improvement process clearly.\nThese three stories, however different in detail, demonstrate common patterns for ML incidents in their detection, troubleshooting, mitigation, resolution, and ulti‐ mately post-incident follow-up actions.\nKeeping these in mind, it is useful to take a broader view of what is happening to make these incidents somewhat different from other outages in distributed computing systems.\nML outages are often detected first by end users, or at least at the very end of the pipeline, all the way out in serving or integrated into an application.\nThis is partly true because ML model performance (quality) monitoring is difficult.\nML outages are also unclear in impact: it can be hard to see whether a particular condition of an ML system is a significant outage or just a model that is not yet as sophisticated or effective as we would like it to be.\nwork is effective, the model gets better over time as we refine our understanding of how to model the world and improve the data the model uses to do so.\nModel Developer or Data Scientist People working at the beginning of the ML system pipeline sometimes don’t like to think about incidents.\nIf ML ends up mattering in an application or organization, however, the data and modeling staff will absolutely be involved in incident manage‐ ment in the end.\nThis is the most important step that data and modeling staff can take to get ready for forthcoming incidents.\nThe final bit of preparation that is most useful is to think carefully about model quality and performance metrics.\nUltimately, we want a set of metrics that detect when the model stops working well that are independent of how it is implemented.\nModel developers and data scientists play an important role during incidents: they explain the models as they currently are built.\nFinally, during incident handling and triage, model and data staff may be called upon to do custom data analysis and even to generate variants of the current model to test hypotheses.\n8 Of course, these different recommendations might mean the model is picking up on proxies for unfair bias,",
    "keywords": [
      "model",
      "recommendations",
      "incident",
      "Gabi",
      "outage",
      "system",
      "Incident Response",
      "Imani",
      "Response recommendations model",
      "n’t",
      "recommendations model",
      "Incident Response recommendations",
      "recommendations system",
      "users",
      "models"
    ],
    "concepts": [
      "model",
      "models",
      "modeling",
      "recommendations",
      "recommendation",
      "recommends",
      "recommend",
      "product",
      "products",
      "production"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 301-308)",
    "start_page": 301,
    "end_page": 308,
    "summary": "Software Engineer Some, but not all, organizations have software engineers who implement the systems software to make ML work, glue the parts together, and move the data around.\nWe should have tools to show the versions of the data (reading from the metadata) in every environment, and tools for customer support staff or production engineers (SREs) to read data directly for troubleshooting purposes (with appropriate logging and audit trails to respect privacy and data integrity guarantees).\nBut as our system gets more mature, we will be able to treat this as a few large systems that can be well managed: a data system (feature store), a data pipeline (training), an analytics system (model quality), and a serving system (serving).\nEach of these is only slightly harder for ML than for non-ML problems, and so software engineers who do this well may have very low production responsibilities.\nSoftware engineers should work regularly with model developers, with SREs/produc‐ tion engineers, and with customer support in order to understand what is missing and how the software should be improved.\nML SRE or Production Engineer ML systems are run by someone.\nLarger organizations may have dedicated teams of production engineers or SREs who take responsibility for managing these systems in production.\nProduction engineering teams should educate themselves about the business that they are in.\nML systems that work make a difference for the organizations that deploy them.\nTo successfully navigate incidents, SREs or production engineers should understand what matters to the business and how ML interacts with that.\nAnswering those questions ahead of time prepares a production engineer for the necessary work of prioritizing, troubleshooting, and mitigating ML outages.\nThis is a fairly different practice than production engineers normally employ, but it is required for ML systems outages.\nExtensive experience interacting with customers or business leaders is not a typical requirement for production engineers.\nML production engineers tend to get over that preference quickly.\nThe rest of incident handling is normal SRE/production incident handling, and most production engineers are good at it.\nML production engineers will collect many ideas about how the incident could have gone better.\nProduct Manager or Business Leader Business and product leaders often think that following and tracking incidents is not their problem, but rather one for the technical teams.\nBusiness and product leaders can report on the real-world impact of ML problems, and can also suggest which causes are most likely and which mitigations are least costly.\nIf ML systems matter, business and product leaders should and will care about them.\nTo the extent possible, business and product leaders should educate themselves about the ML technologies that are being deployed in their organization and products, including, and especially, the need to responsibly use these technologies.\nBusiness and product leaders who take a basic interest in the way ML works will be astoundingly more useful during a serious incident than those who do not.\nProduction Engineers and ML Engineering Versus Modeling Given that many ML systems problems present as model quality problems, a mini‐ mum level of ML modeling skill and experience seems required by ML production engineers.\nThe converse problem also appears: if there is no robust production engineering group, we might well end up with modelers responsible for the production serving system indefinitely.\nSpecifi‐ cally, in smaller organizations, it will be common to have the model developer, system developer, and production engineer be a single person or the same small team.\nThis is somewhat analogous to the model in which the developer of a service is also responsible for the production deployment, reliability, and incident response for that service.\nAs the organization and services get larger, though, the requirement that production engineers be model developers vanishes entirely.\nIn fact, most SREs doing production engineering on ML systems at large employers never or rarely train models on their own.\nML SREs or ML production engineers do need certain ML-related skills and knowl‐ edge to be effective.\nThey need basic familiarity with what ML models are, how they are constructed, and above all, the flavor and structure of the interconnected systems that build them.\nIn this case, the ML production engineer needs to know something about what TensorFlow is and how it works, how the data is updated in the feature store, how the model training processes are scheduled, how they read the data, what a saved model file looks like, how big it is, and how to validate it.\nOn the other side of the same coin, suppose we have settled on a delivery pipeline in which an ML modeling engineer packages their model into a Docker container, annotates a few configuration details in an appropriate config system, and submits the model for deployment as a microservice running in Kubernetes.\nThe ML model‐ ing engineer may need to understand the implications of how the Docker container is built and how large the container is, how the configuration choices will affect the container (particularly if there are config errors), and how to follow the container to its deployment location and do some cursory log checking or system inspection to verify basic health checks.\nThe ML modeling engineer probably does not, however, need to know about low-level Kubernetes choices like pod-disruption budget settings, DNS resolution of the container’s pod, or the network connectivity details between the Docker container registry and Kubernetes.\nespecially in the case where infrastructure components are part of a failure, the ML modeling engineer won’t be well suited to address them and may need to rely on handing off those types of errors to an SRE specialist familiar with that part of the infrastructure.\nThe Ethical On-Call Engineer Manifesto We’ve written a lot in this chapter about how performing incident response is differ‐ ent and more difficult when ML is involved.\nAnother way in which ML incident response is hard is how to handle customer data when you’re on call and actively resolving a problem, a constraint we call privacy-preserving incident management.\nThis is a difficult change for some to make, since today (and decades previous), on- call engineers are accustomed to having prompt and unmediated access to systems, configuration, and data in order to resolve problems.",
    "keywords": [
      "production engineers",
      "Incident",
      "production",
      "data",
      "Model",
      "engineers",
      "Incident Management",
      "Incident Response",
      "system",
      "business",
      "systems",
      "business leaders",
      "Incident Management Principles",
      "incidents",
      "Engineer"
    ],
    "concepts": [
      "incidents",
      "incident",
      "model",
      "models",
      "modeling",
      "modelers",
      "data",
      "production",
      "product",
      "products"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 309-316)",
    "start_page": 309,
    "end_page": 316,
    "summary": "Such a model in a user-facing production system could be bad for both our customers and our organization.\nIn ideal circumstances, no organization would employ ML without undergoing at least a cursory Responsible AI evaluation as part of the design of the system and the model.12 This evaluation would provide clear guidelines for metrics and tools to be used in identifying and mitigating bias that might appear in the model.\nConclusion An ML model is an interface between the world as it is and as it changes on the one hand, and a computer system on the other.\nThe stories here share several common themes that will help ML production engineers prepare to identify, troubleshoot, mitigate, and resolve issues in ML systems as they arise.\nOf all of the observations about ML systems made in this chapter, the most significant is that ML models, when they work, matter for the whole organization.\nML production engineers who hope to get their organizations ready to manage these kinds of outages would do well to make sure that the engineers understand the business, and the business and product leaders understand the technology.\nCHAPTER 12 How Product and ML Interact\nMany of the product teams and business managers, still anchored in traditional software product development methodologies, find themselves in a new and unfamiliar terri‐ tory: building ML products.\nBuilding your first ML product can be overwhelming.\nIt’s not just a question of getting ML right, difficult enough in itself; rather, the integration of the ML into the rest of the product (and the rest of the business) requires many things that need to work together.\nAmong these, data collection practices and governance, the quality of data, definition of product behavior, UI/UX, and business goals all contribute to the success of an ML-based product or feature.\nDifferent Types of Products One of the important and useful features of ML is that it can be applied to many types of products.\nAs a result of this huge diversity of use cases, organizations focused on integrating ML into their existing or new products face an extremely steep learning curve and numerous choices about their implementation.\nSpecifically, it is not feasible for this chapter to seriously consider each of the many common types of ML-product integrations that exist.\nIntegration of ML into the product violates most of these assumptions.\nML models are never “done,” so ML product integration has limited determinism.\nML Product Development Phases To manage uncertainty during the product development lifecycle, ML projects need to be highly iterative from the beginning.\nChapter 12: How Product and ML Interact\nML product development phases\nFurther, the roadmap is useful for defining the processes that need to be modified for the ML solution to work in the first place.\nML Product Development Phases\nIf the problem is expected to scale to thousands of users or more, it could be a good use case for ML.\nFor example, if users of a web store are writing product reviews constantly, the algorithm for recommending relevant products needs to adapt in real time and is amenable to an ML solution.\nUsers can still have a great experience, and the ML model can learn from the lack of sales to deliver improved recommendations in the future.\nChapter 12: How Product and ML Interact\nWe need to acknowledge that getting it wrong could carry huge costs and that understanding the cost of getting it wrong is a significant part of building an ML product.\nWe must then think about how to improve the model, but the consequences of getting it wrong are not catastrophic in our use case (although they clearly are in many other ML products).\nWe need to use the business purpose of ML in order to think about the desired precision (and recall) of the model.\nAs described in the following list, defining the safety nets and business performance metrics that are relevant to the product we are building is a critical step before introducing ML into the product:\nML Product Development Phases",
    "keywords": [
      "product",
      "Product Development Phases",
      "model",
      "product development",
      "business",
      "system",
      "products",
      "user",
      "systems",
      "data",
      "Interact product development",
      "development",
      "Models",
      "Development Phases",
      "product teams"
    ],
    "concepts": [
      "product",
      "products",
      "business",
      "team",
      "teams",
      "model",
      "modeling",
      "models",
      "customers",
      "customer"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 317-325)",
    "start_page": 317,
    "end_page": 325,
    "summary": "Along with the general ML model performance metrics that we’ve discussed in Chapter 8, it is extremely important for PMs to clearly define the business performance metrics to measure the success of the ML systems in production.\nFor example, the following are a few important business metrics to track for ML-based recommendations on an ecommerce store like yarnit.ai:\nFor example, on the shopping cart page, this metric would help determine the success of the ML model powering the “Frequently bought together” list.\nFor instance, on the product details page, this metric would help determine the success of the ML model powering the “Compare with similar items” list.\nChapter 12: How Product and ML Interact\nMVP Construction and Validation Investing in ML models in order to integrate with our product is likely to be expen‐ sive.\nTo figure out whether the ML integration into our product will work, we need to answer two questions: (1) can we make a model that works (or works well enough), and (2) can we integrate that model into our product in a compelling and useful way?\nSometimes this is referred to as launching a minimal viable product (MVP) with a fixed set of rules or heuristics (without real ML models in place) to prove the point that the feature will really solve the customer needs.\nSimple rule-based engines are often the first steps to evolution into a more complex ML model.\nModel and Product Development With a clear set of goals and targets developed through previous stages, the next step is to build the models and integrate them with customer-facing features.\nDeployment In the production deployment stage, the ML system is introduced to the infrastruc‐ ture, where it will serve the live customer traffic, and gather feedback data that is fed into the ML training pipelines to improve the models.\nIn the context of an ML system, feedback is also important for a model to learn and become better.\nChapter 12: How Product and ML Interact\nIn particular, one challenge is that the specific initiatives or projects within a large company that ultimately contain the day-to-day work of building an ML model may have many layers of separation from the higher-level business goals.\nSo it’s extremely important for product teams to have a plan for how an ML system will be deployed incrementally.\nSupport and Maintenance ML systems designed to integrate into products are not complete on their first ship‐ ping version.\nArguably, they are never complete because they are attempting to model the state of the world and deliver useful value about the state of the world in the product.\nOrganizations that are serious about integrating ML into their products need to be serious about continuing the maintenance of those ML models and the infrastructure that produces them.\nAs our product changes, as the needs of our customers change, as our understanding of the business changes, and as the world changes, we will need to keep developing and shipping models.\nWhile the work of a functional ML product\nIn general, models must come from your learning on your own data.3 But the tools and infrastructure we use to create our models are a different story.\nHowever, in the age of ML, we have more dimensions to this traditional question to consider, includ‐ ing make-or-buy models, data processing infrastructure/tools, and the end-to-end platform that holds it all together.\nThat said, many industry-specific ML models and applications are available.\nVendor-provided solutions may present a significant time- and effort- saving potential, but the key aspects to assess build versus buy for models are dis‐ cussed next.\nIf a use case’s details, data, or processes are fairly specific to the organization, the effort spent adjusting internal systems and processes to match the vendor-provided solution’s specifications may wipe away the benefit from buying something ready-made.\nChapter 12: How Product and ML Interact\nIn all of these examples, pretrained models can work well across a large number of use cases, including ours at YarnIt. In these cases, we might be able to skip training a model entirely.\nAs a result, ML models and/or application build-or-buy decisions should also take into account the company’s data strategy: sometimes the acquisition of expertise and technology are actual ends in and of themselves.\nOpen source ML solutions are generally great point solutions within a broader context but still require that we do the work necessary to integrate them into our environment and assemble a complete solution.\nEnd-to-End Platforms At the present state of maturity, essentially no ML platforms are available that provide relatively well-integrated solutions starting with data and ending with a model that is available in a serving system.\nModels, data processing infrastructure, human resources, and business value all evolve at dif‐ ferent paces in various ways.\nWhile this has sometimes been a significant concern with distributed computing or data storage platforms, it is particularly difficult in the ML space, where tools and technolo‐ gies may become obsolete in the time it takes to properly implement them.4 If the initiative is meant to be long-term, acquiring a full platform able to accom‐ modate the fast-evolving data technologies may be relevant, while for short-term initiatives, it may be possible to assemble based on today’s components.\nChapter 12: How Product and ML Interact\nFor example, commercial ML platforms not only allow teams to complete one data project from start to finish one time, but also introduce efficiencies everywhere to scale.\n• Spending less time cleaning data and doing other parts of the data processing flow that don’t provide direct business value\nScoring Approach for Making the Decision Once we are clear on the problem we’re trying to solve and that we need ML specifically to solve the problem, at a minimum we need to consider each of the following factors to evaluate whether building or buying ML is the right choice for the business:\nMaking the Decision Whether building or buying, incorporating ML technologies as a key business tool is a strategic decision that should not be made quickly or without all the components in\nML can increase revenue by improving conversion rates and average order value, can increase profit margins, and can even improve customer loyalty.\nProduct recommendations is an area in which a lot of features can be powered by ML models.\nJust as humans get to know someone and are then able to choose what the best birthday gift for them would be, ML models can leverage data including the product catalog, search queries, viewing history, past purchases, items placed in the shopping cart, products recommended on social media, location, customer segments/buyer personas, and so on.\nChapter 12: How Product and ML Interact",
    "keywords": [
      "product",
      "model",
      "data",
      "models",
      "Build Versus Buy",
      "Business",
      "user",
      "data processing",
      "Product Development Phases",
      "metrics",
      "Product Development",
      "time",
      "data processing infrastructure",
      "list",
      "recommendations list"
    ],
    "concepts": [
      "production",
      "product",
      "products",
      "business",
      "businesses",
      "model",
      "models",
      "customers",
      "data",
      "solutions"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 326-333)",
    "start_page": 326,
    "end_page": 333,
    "summary": "For example, when the customer is looking for “baby yarns” on the product page, showing cross-selling recommendations like “Patterns featuring this yarn” and/or “Popular baby clothes featuring this yarn” can not only help increase the average order value but also save a lot of time for customers.\nChapter 12: How Product and ML Interact\nConclusion For ML projects to have a massive business impact, PMs and business owners need to be relentlessly focused on asking the right questions at every stage.\nRather than dive directly into technical details and ML implementations, teams must ensure they understand the business problems and goals as specifically as possible.\nOnce the business goals are understood, we must first assess whether ML is really needed to achieve those goals.\nIn either case, a lot of planning and coordination is needed to integrate the ML solutions into customer-facing products.\nClearly defining business goals and measurable product metrics, choosing appropri‐ ate measures of success, and deploying solutions iteratively are each important steps toward building great ML products.\nSuccess here looks like having a high degree of awareness of the new situation that ML presents you with, while keeping a clear eye on what is actually of business benefit, and driving toward that—potentially over bumpy roads.\nCHAPTER 13 Integrating ML into Your Organization\nEven trying to enumerate all the areas of the business that have or process data in some way helps to make this point—data is everywhere, and ML follows too.\nFor ML to be successful, leaders need a holistic view of what’s going on, and a way to influence what’s being done with it—at every level.\nThough there are points of relevance to data scientists, ML engineers, SREs, and so on, this chapter is most urgently addressed to those responsible for the health, structure, and outcomes of their organization, on a scale from a team (two or more people) to a business unit or company (hundreds or thousands of people).\nML needs to be a little different.\nAs per the preceding assumption, since doing ML well involves understanding the principles of how it works, what use it makes of data, what counts as data, and so on, leaders need to know this before the decisions they make are going to be sensible.\nOur main observation here is that by default, leaders are not going to pick up ML-relevant knowledge as part of their regular management activity, and so there needs to be an explicit mechanism for doing so.\nWe currently believe that ML is sufficiently complex, new, and potentially impactful that being aware of the details matters—though we expect this will change as time goes on.1 For the moment, though, leaders need to understand ML basics and\nAt the very least, organizational leaders need to know the business metric being optimized and need to have a means of measuring whether the ML system is optimizing that metric effectively.\nChapter 13: Integrating ML into Your Organization\nML Needs to Know About the Business Our third assumption is that the complexity of the business is a direct input to how ML is conceived and implemented.\nML practitioners need to be more aware of broad business-level concerns and state than the average product developer.\nAn ML developer at YarnIt wants to make a business impact, specifically on the web sales part of the business.\nAs a result, leaders need to be aware of how ML functions in their organization, so they can provide the vital coordination and broad oversight that would otherwise be missing.\nHere is one structural way to think about it: you need to be able to centralize the portions of the ML work where oversight and control are most important, to liberate those portions of the work where domain-specific concerns are most important, and to provide an integration point where these workstreams can meet.\nChapter 13: Integrating ML into Your Organization\nThe Value of ML ML can do more for your business than just make you more money.2 Implementing ML could mean that you could improve civic engagement, raise more funds for disaster relief, or figure out which bridges most urgently need maintenance.3 But for business leaders, it usually means making more money and making customers happier (and happy customers generally lead to more money for the business), or sometimes reducing costs via automation.\nML can model the supply chain constraints and inventory • levels and propose optimal reordering for products to ensure that YarnIt has stock of the appropriate mix of products, given financial, sales, storage, and supply constraints.\nLeaders looking to implement ML often hope for it to improve the thing they already do: make more money, give out more food, pay for more housing.",
    "keywords": [
      "products",
      "product",
      "business",
      "leaders",
      "customers",
      "Organization",
      "make",
      "Chapter Assumptions",
      "model",
      "YarnIt",
      "yarn",
      "assumptions",
      "Similar products",
      "web",
      "user"
    ],
    "concepts": [
      "products",
      "recommendations",
      "recommending",
      "recommended",
      "recommends",
      "recommendation",
      "customer",
      "customers",
      "business",
      "businesses"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 334-342)",
    "start_page": 334,
    "end_page": 342,
    "summary": "Indeed, ML has the potential to realize additional value based on data already collected by the organization.\nIt is the kind of once-in- a-generation technological change that really can transform the way organizations function—hence the necessity to examine carefully what you think ML can do for you, figuring out what subset of that you want to achieve, and writing all that down before you start.\nBy misunderstanding the scope and mechanisms of ML, leadership also overlooks the scope of the impact of those projects across the organization.\nMental (Way of Thinking) Model Inertia Transforming the way an organization works is never a simple proposition, and thousands of pages have been written on that topic.\nChapter 13: Integrating ML into Your Organization\nUltimately, for most practical concerns, implementing ML requires serious stake‐ holder management and a large concerted effort to shift mental (way-of-thinking) models.\nto implement ML on their own in any serious way.\nUnfortunately, such behavior makes it very hard to adequately model and respond to the cross-cutting risks that go with ML; significant losses may well result.\nSiloed Teams Don’t Solve All Problems Another common risk is for ML teams and projects to be treated equivalently to the way other new kinds of work are treated, and a common instinct is to start a new siloed team to do that work, leading to its separation in the organization.\nBut more importantly, because of the broad scope of impact that ML projects can have, success‐ fully deploying ML requires organizational change to support structure, processes, and the people needed to keep it reliable.\nImplementation Models Having discussed some risks involved in introducing ML to an organization, let’s focus on the nuts and bolts—how to actually get it done.\nA small implementation project probably starts with applying ML to something that is integral to your organization’s success.\nChapter 13: Integrating ML into Your Organization\nRemembering the Goal Though it is important to preserve flexibility, particularly in the conduct of the imple‐ mentation, we also have to remember the goal—to experiment with ML in order to build capacity in the organization.\nWe find one useful way to structure this knowledge is by thinking, as always, of the flow of data within your organization.\nThere may also be ML-specific product managers who guide what we implement as well.\nChapter 13: Integrating ML into Your Organization\nThis affects all employers, but the most prestigious of ML companies (generally large tech organizations) continue to hire most of the new graduates and experienced staff.\nBut the staff needed to manage the data, integrate ML into the product, and maintain the models\nNow that we’ve considered some of the concrete challenges that organizations face adapting ML specifically, let’s take a step back and consider the problem from the perspective of traditional organizational design.\nOrganizational Design and Incentives Making an organization function well, given what it is supposed to do—often called organizational design—is a difficult art that involves a mixture of strategy, structure, and process.\nUltimately, though, the main lesson is that thinking about the way your organization currently works, and how that will change, hugely improves your chances of doing ML successfully.\nGalbraith, and apply it specifically to the challenge of implementing ML in an organization (Figure 13-1).\nChapter 13: Integrating ML into Your Organization\nIn this model, strategy, structure, processes, rewards, and people are all design poli‐ cies or choices that can be set by management and that influence the behavior of the employees in the organization.\nThis model is useful because it goes beyond the reporting structure or organization chart, where most leaders tend to start and end their change efforts.\nGalbraith points out that “most design efforts invest far too much time drawing the organization chart and far too little on processes and rewards.” This model allows you to take that observation and then think about whether all of the interconnected aspects are affected or can be changed to support the requirements better.\nLet’s review each of these in the context of an organization trying to implement ML.\nlearning in all aspects of the product” might mean the organization funding new and innovative ways of using ML everywhere, with more tolerance for lower-quality results to start with.\nOne way to think about choices for organizational structure, and the one that Gal‐ braith identifies, is that it includes functional, product, market, geographic, and process structures:\nThis structure organizes the company around a specific function or specialty (for example, centralizing ML implementation in a single team).\nThis is probably not how we would structure an ML implementation.\nThis may be a good model for ML teams that work across various product lines but need to create standards and processes for the organization.\nLeaders will generally have a mental model for the way the organization works and the approach they should use to effectuate change.\nChapter 13: Integrating ML into Your Organization",
    "keywords": [
      "organization",
      "models",
      "Model",
      "structure",
      "models Implementation Models",
      "product",
      "organizational",
      "mental models",
      "team",
      "change",
      "Implementation Models",
      "organizational design",
      "Risks",
      "teams",
      "Significant Organizational Risks"
    ],
    "concepts": [
      "organization",
      "organizes",
      "organized",
      "model",
      "models",
      "product",
      "production",
      "products",
      "teams",
      "team"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 343-351)",
    "start_page": 343,
    "end_page": 351,
    "summary": "Ultimately, leaders will probably need to shift their mental model of the way things work, depending on the chosen ML strategy.\nProcesses Processes constrain the flow of information and decisions through an organization, and hence are critical to the way ML will work.\nOne potential way to begin adding ML to your organization is to treat the introduc‐ tion as a vertical process, with decisions made centrally but implemented throughout the organization.\nFor example, if we fund an ML training and serving team to add a new ML feature to our application, do we also fund teams to curate all of the data, or to handle model quality measurement over time or fairness?\nOnce the organization has several ML projects implemented, centralizing the infra‐ structure from those projects to fulfill specific workflows may add robustness and reliability.\nAt that point, we could centralize serving for some of those models, think about building a central feature store, and so start establishing common aspects of the ML organizational infrastructure regardless of the model team.\nOne thing that should be considered is rewarding staff throughout the organization for learning more about ML.\nFinally, the organization will need people who can work through the ambiguity of problems caused by ML without stopping at a root cause of “the ML model said so.” That’s a fine place to start, but people will need to be able to think creatively about the way ML models are built, how changes in the world and in the data impact them, as well as how those models impact the rest of their organization.\nOrganizational leaders will face new challenges and changes in their organization as a result of adopting ML.\nFor each of these scenarios, we will describe how the organizational leader has chosen to integrate ML into the organization and the impact of that choice.\nScenario 1: A New Centralized ML Team Let’s say that YarnIt decides to incorporate ML into its stack by hiring a single ML expert who develops a model to produce shopping recommendations.\nThe YarnIt CEO decides to hire a new VP to build and run the ML Center of Excellence team as a new, centralized capability for the organization.\nThe centralization also creates a significant nexus of influence: the leaders of the ML organization have more standing to advocate for their priorities across all of YarnIt.\nAs the group grows and the projects diversify, more of YarnIt will need to interact with the ML organization.\nThe ML team cannot be too distant from the rest of the business, as it will take the team longer to see opportunity, to deeply understand the raw data, and to build good models.\nEven worse, placing these two functions (ML and product development) completely separately in the organizational chart might encourage the teams to be competitive instead of cooperative.\nFinally, a centralized organization may not be usefully responsive to the needs of the business units requesting help to add ML to their products.\nWhen it comes to productionizing ML, the business units likely will not understand the reliability needs of the ML teams and not understand why reliability processes are being followed (thus slowing delivery).\nWhile these pitfalls exist for a solely centralized ML team, the organization can always evolve.\nAnother possible evolution is that the centralized team educates and enables others to increase ML literacy within the rest of the organization.\nThese reviews, presented by the ML team on a regular basis, should ensure approval of the current modeling results as well as an understanding by business leaders of the way the system is adapting to the business.\nOne issue that can crop up in a centralized ML team is that all the changes become dependent on one another and may be held up by other changes.\nWhile this is easier in a centralized ML model team, introduce processes so that peo‐ ple from the various business units can also evaluate the changes.\nFor the centralized ML team, this may be the product/business team that requested a change or feature, or the support team that may be affected by the changes.\nRewards To ensure a successful implementation of the ML program, we need to reward interaction between business and model builders.\nScenario 1: A New Centralized ML Team\nProduct leaders need to understand that tuning the ML model for the organizational objectives may take some experimentation and that negative impacts will almost certainly occur along the way.\nBigger Isn’t Always Better The YarnIt ML team begins work on a project whose success definition is to create bigger carts (shopping carts containing more products worth more money).\nThis cart abandonment rate is escalated to a senior leader in sales who starts troubleshooting the problem immediately, first with the web and payments teams (might be a UI problem or payments processing problem) and then ultimately with the ML team.\nSales can generate an acceptable target for cart abandonment for the ML team to include in its model optimization, the web UI team can think of ways to make it easy to check out parts of a cart rather than a whole cart, and the product team can think about remarketing to users who abandoned carts, asking whether they might want to purchase just some of the products.\nYarnIt, and all organizations implementing ML, need to hire for a mindset of nuance in order to be successful.\n• Hire a new leader with ML modeling and production skills.\nScenario 2: Decentralized ML Infrastructure and Expertise YarnIt might decide to invest in several experts across the organization, rather than a single senior leader.",
    "keywords": [
      "organization",
      "team",
      "model",
      "teams",
      "centralized team",
      "organizational",
      "centralized",
      "People",
      "model team",
      "product teams",
      "business",
      "staff",
      "implementation",
      "leaders",
      "work"
    ],
    "concepts": [
      "teams",
      "team",
      "model",
      "modeling",
      "models",
      "organizational",
      "organization",
      "organizations",
      "organ",
      "changes"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 352-359)",
    "start_page": 352,
    "end_page": 359,
    "summary": "Without a central place for ML expertise, especially in management, developing a deeper understanding of what YarnIt needs to do to be successful at ML will be harder.\nIt will be hard to understand when the ML team is advocating for something it really needs (model-specific quality-tracking tools like TensorBoard), as opposed to something that might be nice to have but may not be required (GPUs for some model types and sizes or cloud training services that offer huge scale but also large costs).\nYarnIt should create a template for these reports, possibly generated by collaboration among some of the first groups to start using ML, and a standard schedule for reviews by a small group with representatives beyond just the organization implementing ML.\nML model developers should meet weekly with production engineering staff and stakeholders from the product development group to review any changes or unexpected effects of the ML deployments.\nScenario 2: Decentralized ML Infrastructure and Expertise\n— — Hire ML engineering staff to build models directly with the product teams.\n— — Hire ML staff or shift production engineering staff to run the infrastructure.\n• Run weekly triage or ML production meetings to review changes.\nScenario 3: Hybrid with Centralized Infrastructure/ Decentralized Modeling YarnIt started its implementation via the centralized models, but as the organization matures and ML adoption spreads throughout the company, the company decides to revisit that model and consider a hybrid structure.\nIn this case, the organization will maintain some centralized infrastructure teams and some ML model consulting teams in the central organization, but individual business units are free to hire and develop their own ML modeling experts as well.\nBut decentralizing at least some of the ML expertise will increase the speed of adoption and improve alignment between the ML models and the business needs.\nMeanwhile, the centralized infrastructure might create friction for the decentralized modeling teams.\n— — ML team(s) findings documentation and review\nRewards In the hybrid scenario, YarnIt senior management should reward business units for utilizing the centralized infrastructure, to prevent them from developing their own, duplicative infrastructure.\nCentralized infrastructure teams should be rewarded for meeting the needs of the other business units.\nCentral infrastructure teams should have a plan to identify key technology developed in the business units and extend its use to the rest of the company.\nAnd from a career development perspective, ML modelers from the business units should be able to rotate onto the central infrastructure team for a period of time to understand the services available and their constraints, as well as to provide an end-user perspective to those teams.\nThe ML teams embedded with the business need to have a mindset that cooperation is best, so they should be looking for opportunities to collaborate across divisions.\nHire a centralized team (leader) with ML infrastructure and production skills: •\n— — Hire ML production engineering staff to run the infrastructure.\n— — Hire ML engineering staff to build models directly with the product teams.\nSelect processes that will aid in cross-organizational collaboration such as cross- • team ML findings reviews.\nSome teams, such as the production engineering or software engineering teams, will not require significant ML skills to start being effective.\nWe also need to build ML skills among business leaders.\nCentralized ML infrastructure and expertise\nTeams need to be the champions of ML quality, fairness, ethics, and privacy across the company.\nTeams across all product areas need to gain expertise on ML quality, fairness, ethics, and privacy.\nNeeds a lot of documentation around best practices, knowledge, evaluation, and launch criteria to maintain consistency, or a deliberate decision not to maintain any outside of local team scope (which is problematic for ML).\nmeeting business goals, individual/ team performance needs to be measured based on the effectiveness of cross-functional collaboration.\nOn top of overall quality and meeting business goals, individual/ team performance needs to be measured based on consistency, published quality standards, and operating internal ML communities.\nEstablish mechanisms to compensate both ML and product teams together for successful AI feature launches.\nHybrid with centralized infrastructure and decentralized modeling Centralize ML infrastructure and modeling for common/core business use cases but encourage individual model development for specific needs.\nNeeds cross-functional collaboration and decent documentation between infrastructure and individual product teams on a project/ program basis.\nOn top of overall quality and meeting business goals, individual/team performance needs to be measured based on reusability, evolution of common infrastructure, and speed of execution.",
    "keywords": [
      "infrastructure",
      "Centralized Infrastructure",
      "teams",
      "business",
      "team",
      "model",
      "centralized infrastructure teams",
      "Implementation",
      "business units",
      "infrastructure teams",
      "Centralized",
      "organization",
      "review",
      "quality",
      "Org Implementation"
    ],
    "concepts": [
      "team",
      "teams",
      "model",
      "models",
      "modeling",
      "modelers",
      "developing",
      "developers",
      "development",
      "develop"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 360-367)",
    "start_page": 360,
    "end_page": 367,
    "summary": "Therefore, we continually aspire to improve the ASR models in our ML pipelines.\nIn this case study, we discuss a couple of challenges we’ve come across and the solutions we’ve implemented as we worked toward deploying a model for multiple dialects while respecting user privacy.\nAt Dialpad, we value user privacy, and to power various AI features, we need massive amounts of data for model training.\nThe ASR model did well with North American dialects because we had been feeding it North Ameri‐ can speech, so we could also improve this existing model by adding undersampled data, building a model agnostic of dialects.\nWe manually transcribed this underrepresented data and trained a new model with this dataset plus the original training dataset.\nWithin a few rounds of model tuning and evaluations, the ASR models started performing better on the underrepresented dialects test set that we manually curated, without any changes to the training techniques or model architecture.\nThese substantial privacy wins, however, require equally substantial cleverness across the entire ASR system in terms of model testing and experimentation, and especially so for the Dialects ML pipeline that consists of multiple steps: collection of audio, transcription, data preparation, experimentation, and final productionization.\nThat means training data and test sets are not constant, making it difficult to reproduce experimental results, sometimes leading to delays in training the models and launching the desired improvements for customers.\nLate in the process of rolling out the new Dialects model, we saw that it performed well on multiple test sets, but performed significantly worse with one single test set across multiple internal trials (compared to the model in production, released six months earlier).\nWe used multiple methods, including training the new Dialects model from scratch and checking data partitioning (after a previous misadventure inadvertently mispartition‐ ing between training data and test data).\nWe also wanted to reproduce the results from the production model by using the same process to train a model, but 11 months later, the data subject to retention policies had begun expiring, and we didn’t have the exact training dataset anymore.\n1. Accommodating Privacy and Data Retention Policies in ML Pipelines\nUltimately, the key insight to resolving the discrepancy was that the previous model that had performed well on the test set was actually in use during the time the data from production was taken to make the test set.\nWhile difficult, this challenge suggests a way in which privacy-promoting, data-minimizing techniques could secure much more robust access to ML training data.\nTakeaways Integrating with privacy and data retention policies undoubtedly introduces chal‐ lenges in ML pipelines, especially those powering the primary use cases of a customer-facing product/service.\nIn our use case, working toward a more inclusive ASR model for Dialects, we first learned that even a little diversity in our training data makes the model more robust.\nThe system in question included a continuous ML model that helped predict the likelihood of clicks on certain kinds of results in a search engine setting, continually updating on new data as it came in.\nThis data with many more queries but no additional clicks was staged for retraining of the continuous ML model.\nAt this point, the continuous ML model was now happily training on all the corrup‐ ted data.\nBecause the corrupted data included a lot of traffic with no associated clicks, the model was getting a signal that the overall click-through rate in the world was now about half of what it had been just a few hours before.\nBy this time, the ops folks had pushed the stop button on the continuous ML model training, and all model training was stopped.\nThe most up-to-date version of the model was therefore one that has been impacted by the corrupted data.\nThe behavior that an app update caused a duplicate query was not widely known, and those who knew about it on the app side did not make the connection to the way that it could impact training data in a continuous ML model.\nThe ML model eventually saw clean data with the appropriate click-through rates.\nI am an ML engineer at Landing AI and wrote this case study to show some data-centric techniques we used to develop deep learning models for visual inspection tasks.\nThe customers had been working on this problem for almost 10 years, and the best performance their model was able to achieve was only 80% accuracy, which was not sufficient for the client’s needs.",
    "keywords": [
      "North American dialects",
      "data",
      "model",
      "training data",
      "dialects",
      "Dialects model",
      "North American",
      "American dialects",
      "training",
      "models",
      "model training",
      "ASR models",
      "ASR model",
      "Data Retention Policies",
      "ASR"
    ],
    "concepts": [
      "recognition",
      "kingdom",
      "years",
      "ago",
      "rely"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 368-375)",
    "start_page": 368,
    "end_page": 375,
    "summary": "Timeline for data labeling, label review, and model training\nIf we allowed this label to be added to our training set, it would mislead the model when calculating the losses, and we should avoid that.\nAfter each time we trained a model, I spent most of my time reviewing falsely predicted examples and identified root causes of errors.\nIn 2019, the large language model BERT achieved state-of-the-art NLP performance.1 We planned to leverage it to provide more accurate NLP capabilities, including punctuation restoration and date, time, and currency detection.\nTo reduce cloud cost, however, our real-time production environment has very limited resources assigned to it (GPU is not an option, and we have one CPU at most for many models).\nProblem and Resolution Our team needed to perform local profiling in order to benchmark various NLP models, which ran model inference over a large number of randomly sampled utter‐ ances and calculated average inference time per utterance.\nOnce the average inference speed met a fixed threshold, the packaged model would be handed over to our data engineering (DE) team, which would then do canary deployment in our Google Kubernetes Engine (GKE) cluster, and monitor a dash‐ board of various real-time metrics (Figure 15-7) with an ability to drill down into specific metrics (Figure 15-8).\nFor a large language model based on BERT, often DE teams discovered that the latency or queue time bumped up significantly and they had to roll back the deployment.\nHowever, the reality was that scientists didn’t have the right tools to properly benchmark model inference.\nApart from the production system, the DE team also maintained a staging environ‐ ment where NLP models were deployed and integrated with the product interface (reference) prior to production deployment.\nOur QA team made test calls to test various call features, and applied scientists leveraged this environment to ensure that the model ran properly with the product UI.\nThe DE team proposed a comprehensive and self-serve load-test tool to help applied scientists benchmark model inference in the staging environment.\n• Load-test data should contain trigger phrases for the model.\nLoad-test data should trigger model inference and not get short-circuited by • optimizations/caches that would lead to misleadingly low runtime latencies.\nAfter the tool was developed, applied scientists had two options for performing a load test on staging:\n• Text-based (model specific) load test\nThe Datadog dashboard also provided a com‐ prehensive breakdown of the runtime performance of each model so that applied scientists could monitor the metric numbers more closely.\nWhen our testing methods started failing on the more resource-intensive BERT model, we reached into our staging environment to give our scientists a more representative environment to test against and made it self-serve so they could iterate rapidly.\nBoth teams are now relieved as applied scientists are able to obtain close-to-production estimates on model inference with great confidence.\nThey do so in part by using models to predict the probability that a given ad will be clicked.\nThe row in the database corresponds to an ad being shown to the user, with columns associated with the features used for model training.\nThe model was trained by looking at the rows in this database as examples, and using the click bit as the label.\nThe rows in the data were the raw inputs to the model and the label for each event recording either “resulted in a click” or “was not clicked.” If we were to never update the models, they would work well for some",
    "keywords": [
      "model",
      "Load Test",
      "Staging Load Test",
      "model inference",
      "NLP models",
      "NLP",
      "defect",
      "Staging",
      "models",
      "Case Studies",
      "scientists",
      "applied scientists",
      "NLP models systems",
      "Load",
      "NLP MLOps"
    ],
    "concepts": [
      "model",
      "models",
      "nlp",
      "time",
      "added",
      "adding",
      "ads",
      "defect",
      "defects",
      "defective"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 376-385)",
    "start_page": 376,
    "end_page": 385,
    "summary": "Newly trained models would be sent to this validation system, which generated inferences on recent events with the test_set bit.\nModels would be pushed to production only if the new models performed better than the old model over the recent test set events.\nThe model was acting as if clicks were far more rare, but the data in our database didn’t reflect that.\nThe data science team started by looking at the validation system: this was supposed to keep us from pushing out models that performed worse than the versions we were replacing.\nThe logs from Sunday’s validation run indicated we processed the test events as expected, and that the loss statistic was lower for the new model than our old model.\nWith a hunch, someone decided to rerun the validation system with the same pair of models across the test set.\nWhen the production engineer and data scientist teams conferred with each other and shared their findings, they realized the model was underpredicting because of a failure of the infrastructure responsible for processing the raw click logs and distrib‐ uting the clean click feed to consumers.\nThe clean clicks arrived to the ML training system late—not until early Monday morning, after the model had last trained.\nThe model that we trained concluded that the ads were awful and no one would click on any of them, which was accurate given the data that we had when we trained the model.\nOur models built using supervised learning techniques predict labels in our training set, and it’s of critical importance that our labels reflect reality.\nOur follow-ups included establishing an availability target for the click log processing systems, expanding our validation system to check that the test set’s positive ratio wasn’t suspiciously low or high, and establishing a process for the click log team to communicate outages and pause training if serious problems with model health occurred.\nMultiple speech-recognition models may be used, depending on the user’s location and/or the product line they are using within Dialpad.\nFor instance, a user from the UK using the call-center product line will be provided with a speech- recognition model fine-tuned on the UK dialect of English and trained on call-center domain-specific knowledge.\nSimilarly, a user from the US using the sales product line will have their call transcribed using a speech-recognition model trained on the US English dialect and domain-specific knowledge for sales calls.\nAdditionally, for the NLP, several task-specific models run in parallel to perform tasks such as sentiment analysis, question detection, and action-item identification.\nthis in mind, the simplified flowchart can be extended to highlight the diversity of models in Dialpad’s production ML pipeline.\nProblem and Resolution Figure 15-11 highlights some of the ML dependencies that exist for NLP task-specific models with regards to the upstream speech-recognition models.\nWhile most NLP models are not overly sensitive to minor changes to ASR model outputs, over time the data changes significantly enough, and NLP models experience a degradation in performance due to regression and data drift.\nA few common updates to ASR that result in a change in input data distribution for NLP models are as follows:\nSome of the ML dependencies that exist for NLP task-specific models\nTo combat this phenomenon, the DE team at Dialpad, in collaboration with the data science team, built an offline testing pipeline that could measure NLP model performance for a given ASR model.\n• Ensure that monitoring of NLP model performance would happen automatically whenever newer ASR models are released.\n• Allow for ad hoc evaluation by data science teams when they wish to evaluate a model prerelease.\nThe pipelines in KFP will build the correct infrastructure for evaluation by • selecting the correct model deployment artifacts, given the evaluation criteria.\nDatasets for every task-specific NLP model would be versioned so as to track • changes in evaluation data.\n• Metrics will be collected for every combination of ASR model version, NLP model version, and dataset version.\nThe input to the testing pipeline is raw audio recordings of conversations, since • the idea was to capture whether an ASR model has changed in such a way that it alters the output enough that the downstream NLP model has varied performance.\nAnd within KFP, a pipeline would simulate evaluation for a single combination of ASR model version, NLP model version, and dataset version.\nOnce the pipelines were built on KFP, the next part of the project was to automat‐ ically perform regression tests whenever dependencies changed for NLP models.\nLuckily, at Dialpad we have mature CI/CD workflows managed by engineering, and they were updated to trigger KFP pipelines whenever ASR models were updated in the transcription service.\nThe CI/CD workflow would send a signal to KFP with information about the ASR models, NLP models, etc., and the evaluation would then commence on KFP.\nOnce operational, this process captures performance evaluation data for all NLP task-specific models that have testing data available on the platform.\nFor example, the F1-score of the NLP sentiment analysis model degraded by ~25% over the course of a year, as shown in Figure 15-14; the graph highlights the absolute difference from a baseline.\nAnother tangential benefit of this process is that it allows for ad hoc evaluation of NLP models against different ASR models prior to production release.\nFor instance, it is possible to measure the accuracy of a sentiment analysis model, prior to release, against an ASR model trained on new English dialects, such as Australian or New Zealand English.\nTakeaways This ML regression-testing platform developed at Dialpad has provided data scien‐ tists and engineers with much improved visibility on the impact of new model relea‐ ses on all dependent components in our production stack.\nB backups, 147 baselines, new model launch, 239-241 batch inference, 171-174 batch processing, training systems, 152 bias\nad click prediction, 353-356 continuous ML model and traffic,\nmodels\nexamples, 227 labels, 227 correctness of data, 26 counterfactual testing, 97 CPUs, 168 creation phase of data management, 23-25 crime-prediction algorithms, 109 cultural appropriateness, Responsible AI, 131",
    "keywords": [
      "model",
      "NLP models",
      "NLP model",
      "ASR model",
      "models",
      "NLP",
      "data",
      "ASR models",
      "NLP task-specific models",
      "NLP model version",
      "NLP model performance",
      "ASR",
      "ASR model version",
      "ASR model output",
      "click"
    ],
    "concepts": [
      "model",
      "data",
      "tested",
      "training",
      "train",
      "clicked",
      "clicks",
      "production",
      "product",
      "process"
    ]
  },
  {
    "chapter_number": 45,
    "title": "Segment 45 (pages 386-394)",
    "start_page": 386,
    "end_page": 394,
    "summary": "pseudonymized data, 18 raw data, 18 as liability, 16-20 deletion, 20 distribution, 21 encryption, 20 filtering, continuous ML systems, 227 golden sets, 95-96 input, monitoring, 200-200 numerical, 212 pseudonymization, 18 pseudonymized data, 18 raw data, 18 rewriting, 20 semi-structured, 24 sparse, 34 structured, 24 unstructured, 24\ndata access guidelines, 126 data access, privacy preservation, 125 data analysis, 3-3 data availability data availability, training systems, 148-149 data breach notification laws, 126 Data Cards, 25 data cleaning, responsible AI, 132 data collection, 3\nbusiness purpose and, 31 continuous ML systems, 228 data integrity, 36\npolicy and compliance, 40-41 privacy, 37-39 security, 37-37 data reliability, 33 availability, 36 consistency, 34-35 durability, 33 performance, 36 version control, 35 model structure and, 32 phases of data, 22\ndata separation, privacy preservation, 125 data types, monitoring, 220 databases, case study, 353-356 datasets\ndecoration, monitoring systems, 190 deep learning models, 45 vulnerabilities, 52-54\ndeployment, Responsible AI, 134 development versus model serving, 194-195 DevOps, data science team, 182 Dialpad AI, 337 differential privacy, 123 disaster recovery, model serving, 186-187 discovery and definition phase, product devel‐\necommerce, metrics, 294 edge devices, model serving, 178-181 efficiency, training systems, 152-154 emergency response, continuous ML systems,\nmodel serving, 187\nethics training, privacy preservation, 125 evaluation strategies, continuous ML systems,\nAPI, 73-74 continuous ML systems, 228 data, 73 data normalization, 73 definitions, 73 lifecycle, 75 metadata, 73 training systems, 141-142, 149 values, 73\nmodel building, 66 models, 44 quality evaluation, 76-77 training systems, 141 transforming, 75-76 value, 68\nfeatures overview, 66 feedback loops, model training data, 232-233 feedback, deployment phase, 296 filtering, continuous ML systems, 227 fixes, vulnerabilities, 56 functional organizational structure, 320\nmodels, 314\nimplementation models, 315 in-processing methods, fairness and, 118 incentives, 318 incident management, 247\nML engineering versus modeling, 282-284 ML-centric outages, 251-274 model serving, 282-284 organization, 275 outage start phase, 249, 256 YarnIt case, 262, 272 pre-incident phase, 249\nbusiness leaders, 281-282 communications lead, 250 incident commander, 250 model developers, 275-277 on-call engineers, ethics, 284 operations lead, 250 planning lead, 250 product managers, 281-282 production engineers, 278-280, 282-284\nincomplete coverage, 48 ingestion phase of data management, 26 initializations, reproducibility, 156 input data, monitoring, 200-200 input feature lookup, 215 integration\nassumptions, 310 brownfield, 316 business complexity, 309-310 detail, 308-309 goals, YarnIt example, 315 greenfield, 316 hiring tasks, 317-318 implementation models, 314, 315 integration point, 310 leader-based viewpoint, 308 organizational risks, 312-314 roles and responsibilities, 316-317 value of ML, 311-312\nmonitoring systems, 191 training, continuous ML systems, 227 vulnerabilities, 50 fraud, 51 label noise, 50 malicious feedback, 51 objectives, wrong, 51 latency, prediction latency, 165-166 launching new models, continuous ML sys‐\nM MaaS (model-as-a-service), 176-178 management phase, data management, 32-32 market organizational structure, 320 market products, Responsible AI, 134 mean absolute error, 104 mean squared error, 104 metadata\nMicrosoft Azure SQL Data Warehouse, 31 ML echo chambers, training data, 49 ML engineers, integration and, 317 ML framework, training systems, 144 ML lifecycle, 1-3 applications\nbuilding, 5-6 validating, 5-6 basic model health, 11 data analysis, 3-3 data collection, 3-3 launch, 8-9\nad click prediction, 353-356 continuous ML model and traffic, 341-342 data retention policies and, 337-341 databases, 353-356 dependencies, workflow and, 356-361 NLP applications, 348-353 privacy policies, 337-341 steel inspection, 343-348 visual inspection, 343-348\nmodel and product development phase, 296 model architecture, 47, 252 model definition, 47 model developers, incident management and,\nmodel management system, 142-143 model performance monitoring, 221 model quality, 89\nmodel validity, 89\nbackups, 147 bias in, 25 build versus buy, 298-299 configuration, 156 configured models, 47 corrupt, 92 deep learning and, 45 fail-safe implementation, 147 feature store, training systems and, 149 features, 44 hardware, 167-169 infrastructure crashes, 91 loading, 169 location\npost-deployment, 196 predictions, 44 production environment, loading, 90 provability, training and, 149 resource utilitization, 151-152 Responsible AI, 133 deployment, 134 market products, 134 quality assessment, 133 validation, 133\ndefined, 189 delayed actuals, 207 dense data types, 220 fairness in, 219 high-level point, 221 holistic view, 201 infrastructure performance, 221 input data and, 200-200 metrics, proxy metrics, 207 model performance monitoring, 221 model serving, 205 models, 206-209 performance, 205\nno/few actuals, 208 perception of, 192 privacy in, 219-220 processing and, 200-201 production monitoring, 193 model serving, 194-195\nmodel level, 214 model performance optimization, 214 service performance optimization, 215 serving level, 214 service performance, 221 SLOs and, 216-218 training and, 198-201 training systems, 144 monitoring systems, 190 aggregation, 190 decoration, 190 labeled data and, 191 pull notifications, 191\n(see also data normalization) training systems, 141 normative values, 116-117 numerical data\nO observability, 189 observability data, 193 on-call engineers, ethics, 284 online inference (see online serving) online serving, 174-176 orchestration, training systems, 143\nP parallel universe models, 241 parallelism, reproducibility and, 156 performance metrics, 93 performance monitoring, serving and, 205 performance-oriented cultures, 314 personal privacy laws, 127 personalization, discovery and definition phase,\nanonymization, 18 data collection and, 18 excluding, 37, 37 features and, 86 labeling and, 87 monitoring and, 220-220 privacy and, 86 pseudonymized data and, 18 serving on edge and, 180\ndata sensitivity, 21-22 launching, 9 metadata, 85 model serving and, 170 Responsible AI\ndata collection and cleaning, 132 deployment, 134 market products, 134 modeling, 133 quality assessment, 133 use case brainstorming, 132 validation, 133\nplatforms, build versus buy, 300-301 policy and compliance, 40-41 post-deployment models, 196 post-processing methods, fairness and, 118-119 PR (precision and recall) curve, validation and,\npreprocessing, fairness and, 117-118 privacy, 37-39, 107, 121 big data and, 122 differential privacy, 123 in monitoring, 219-220 k-anonymity, 122 preserving, 124\naccess controls, 124 access logging, 124 data access guidelines, 125 data minimization, 124 data separation, 125 ethics training, 125 institutional measures, 125-126 privacy by design, 126 technical measures, 124-125\nprivacy-preserving incident management, 284 process organizational structure, 320 process, fairness as, 120-121 processes, organizational design and, 321 processing phase of data management, 26-30 processing, monitoring, 200-201 product development, 290\nbusiness goal setting phase, 292-295 deployment phase, 296-297 discovery and definition phase, 291-292 model and product development phase, 296 MVP construction and validation phase,\nproduction monitoring, 189-193 model serving and, 194-195\nproducts, 289 progressive validation, 95 propensity scores, small, 231 proxy metrics, 207 pseudonymization, 18 pseudonymized data, 18\nR race conditions, training speed and, 150 raw data, 18 real-time actuals, 206 real-world events, training data, 49 recall, validation and, 204 regression metrics\nbinary changes and, 156 data differences, 156 initializations, random, 156 model configuration and, 156 parallelism, data, 157 parallelism, system, 156 training systems, 155-159\ndata collection and cleaning, 132 deployment, 134 market products, 134 model validation, 133 modeling, 133 quality assessment, 133 use case brainstorming, 132",
    "keywords": [
      "data ingestion system",
      "model performance monitoring",
      "model training data",
      "product development phase",
      "model management system",
      "data protection laws",
      "Index incident management",
      "data quality monitoring",
      "data availability data",
      "build versus buy",
      "data sensitivity Index",
      "SQL Data Warehouse",
      "Azure SQL Data",
      "training data availability",
      "training data feedback"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "modeling",
      "phases",
      "phase",
      "training",
      "trained",
      "features",
      "feature"
    ]
  },
  {
    "chapter_number": 46,
    "title": "Segment 46 (pages 395-403)",
    "start_page": 395,
    "end_page": 403,
    "summary": "scikit-learn, 55 self-fulfilling algorithms, 131 self-fulfilling prophecies, training data, 49 semi-structured data, 24 sequencing, organizational design and, 322-323 service\nmodel level, 214 model performance optimization, 214 service performance optimization, 215 serving level, 214\nservice-level indicators (SLIs), 7 services, monitoring across, 218-219 serving the model (see model serving) serving, offline, 171-174 shifts in distribution, 5 signals\nsolutionism, 112 sparse data, 34 sprints, 290 spurious correlations, training data, 48 SREs (site reliability engineers), 3\nAmazon Redshift, 31 feature store, 72-76 Google Cloud, 31 Microsoft Azure SQL Data Warehouse, 31 model serving and, 169\nstress-test distributions, 96 structured data, 24 supervised learning, 44\ncontinuous ML systems, case study, 341-342 model serving, 164-165 queries per second, 164\ntrained models, 252 training\nethics training, privacy and, 125 human labelers, 81 model retraining, 146 monitoring and, 198-201 Responsible AI, 133 roll forward and, 199 vulnerabilities, 51\ndeep learning, 52-54 overfitting, 52 stability, 52 training algorithms, 137 training data source, 57 training systems, 141 vulnerabilities, 48 cold start, 49 incomplete coverage, 48 labels, 50-51 ML echo chambers, 49 real-world events, 49 self-fulfilling prophecies, 49 spurious correlations, 48 training methods, 51-54\nbatch processing, 152 compute resource capacity, 159-160 data sensitivity and, 154 feature development, 141 feature store and, 141-142 features, 141 ML training pipelines, 5, 143 model management system, 142-143 money-indexed cost, 153 monitoring, 144 normalization, 141\ncommon problems, 154-160 data availability, 148-149 efficiency, 152-154 failures, 145 feature changes, 149 feature store, 149 model backups, 147 model provability, 149 model retraining, 146 model versions, 146 resource utilization, 151-152 training too fast, 150-151\ndata to train on, 138 framework, 139 model configuration system, 138 quality evaluation system, 139 synching models to serving, 139\nresource-indexed cost, 153 training data, 141 training pipelines, 3-5 training-serving skew, 91, 195\nfeature generation, 56 fixes and, 56 training data and, 48 cold start, 49 incomplete coverage, 48 labels, 50-51 ML echo chambers, 49 real-world events, 49 self-fulfilling prophecies, 49 spurious correlations, 48 training methods, 51-54\nfreshness, 62 labels, 61 latency, 63 prediction failures, 63 prediction stability, 62 previous user behavior, 60 product placement, 61 raw product image, 60 serving, 62-63 updates, 61-62 cross-selling, 303 data problem example, 155 features, options, 67 filtering\njurisdictional rules, 40 KPIs, 184 labels, human-generated, 78 latency, prediction latency, 165-166 lost data, 21 pipeline, 170 recommendation system, privacy and, 38 recommendations, 303 recommendations model, 67 reproducibility problem, 157-159 searches, training on, 21 SLOs, 8 temporal effects, 234 upselling, 303 yarn by total sales, 302-303\nShe has led teams in large tech companies and startups launching product features, internal tools, and operating large systems.\nHe has worked with all of the major cloud providers from their Dublin, Ireland offices, and most recently at Microsoft, where he was global head of Azure site reliability engineering (SRE).\nHis first exposure to machine learning came with managing the Ads ML teams in Google’s Dublin office and working with Todd Underwood in Pittsburgh, though it has continued to fascinate him since.\nHis teams build large-scale, cloud native, real-time business communications and collab‐ oration software with industry-leading in-house AI/ML and telephony technology.\nBefore Dialpad, he led teams responsible for search and personalization platforms, products, and services at Apple.\nD. Sculley is the CEO of Kaggle and GM of Third-Party ML Ecosystems at Google, and previously was a director of the Google Brain Team and the lead of some of Google’s most critical production machine learning pipelines.\nHe has focused on issues of technical debt in machine learning, along with robustness and reliability of models and pipelines, and has led teams applying machine learning to problems as diverse as ad click-through prediction and abuse prevention to protein design and scientific discovery.\nML SRE teams build and scale internal and external ML services, and are critical to almost every significant product at Google.\nThe insect on the cover of Reliable Machine Learning is the honeypot ant (Myrmeco‐ cystus mimicus).",
    "keywords": [
      "SQL Data Warehouse",
      "IID test data",
      "training data",
      "training data source",
      "Azure SQL Data",
      "storage Amazon Redshift",
      "test data held-out",
      "data held-out test",
      "management detection phase",
      "outage start phase",
      "51-54 training systems",
      "incident management",
      "troubleshooting data drift",
      "incident management detection",
      "service level objectives"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "product",
      "production",
      "products",
      "training",
      "trained",
      "train",
      "phase"
    ]
  },
  {
    "chapter_number": 47,
    "title": "Segment 47 (pages 404-409)",
    "start_page": 404,
    "end_page": 409,
    "summary": "Chapter 47: Segment 47 (pages 404-409)",
    "keywords": [],
    "concepts": []
  }
]