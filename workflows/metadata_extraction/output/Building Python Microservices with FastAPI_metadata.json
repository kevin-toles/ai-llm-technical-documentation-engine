[
  {
    "chapter_number": 3,
    "title": "Using the factory method pattern",
    "start_page": 123,
    "end_page": 380,
    "summary": "The factory method design pattern is always a good approach for managing injectable classes and functions \nStoring settings as class attributes\nThe following are classes \nTo fetch the values, first, a component in a module must import the BaseSettings class from the \nTo fetch these details, the application needs another BaseSettings class implementation that declares \nThe following class shows how production_server, \nclass Config: \nThe focus is to establish database connectivity and utilize our data models to implement \nbe mapped to their corresponding entity or model classes to avoid tightly coupled connections to \nAnd these model classes are the ones that are used to connect to the database.\n•\t Creating synchronous CRUD transactions using SQLAlchemy\n•\t Implementing asynchronous CRUD transactions using SQLAlchemy\nclass management, and attendance modules that utilize a PostgreSQL database as their data storage.\nis just a simple FastAPI application that’s been created to help you focus on the data modeling features, \nwith six tables, namely signup, login, profile_members, profile_trainers, \nattendance_member, and gym_class.\nCreating CRUD transactions using SQLAlchemy\nThis ORM is a boilerplated interface that aims to create a database-agnostic data layer that can connect \nTo connect to any database, SQLAlchemy requires an engine that manages the connection pooling \nfrom sqlalchemy import create_engine\nCreating CRUD transactions using SQLAlchemy\ncoordinates with the SQLAlchemy core to pursue the changes to the database if all transactions have \nNext, we need to set up the Base class, which is crucial in mapping model classes to database tables.\nNow, this Base class must be subclassed by the model classes so that the mapping \nInvoking the declarative_base() function is the easiest way of creating the Base class rather than \ncreating registry() to call generate_base(), which can also provide us with the Base class.\nNote that all these configurations are part of the /db_config/sqlalchemy_connect.py \nBut before we implement the CRUD transactions, we need to create the \nmodel layer using the Base class.\nThe model classes of SQLAlchemy have all been placed in the /models/data/sqlalchemy_\nthe Base class is essential in building the data layer.\ncan create model class definitions in SQLAlchemy ORM:\nfrom sqlalchemy.orm import relationship\nfrom db_config.sqlalchemy_connect import Base\nclass Signup(Base):\nCreating CRUD transactions using SQLAlchemy\nThe Signup class is a sample of a SQLAlchemy model because it inherits the Base class’s properties.\nIt is a mapped class because all its attributes are reflections of the column metadata of its physical table \nThe model has a primary_key property set to True because SQLAlchemy \nEach model class \nMost importantly, we need to ensure that the data type of the class attribute matches the column type \nthe Column class, as shown in the username and password columns of Signup.\nSQLAlchemy strongly supports different types of parent-child or associative table relationships.\nclasses involved in the relationship require the relationship() directive from the sqlalchemy.\norm module to be utilized to establish one-to-many or one-to-one relationships among model classes.\nThis directive creates a reference from the parent to the child class using some foreign key indicated \nA child model class uses the ForeignKey construct in its foreign key column object to link the \nmodel class to its parent’s reference key column object.\nThe following model class \nclass Login(Base): \nThis Login model is linked to two children, Profile_Trainers and Profile_Members, \nBoth child models have the ForeignKey directive in their id column \nclass Profile_Trainers(Base):\ngclass = relationship('Gym_Class', \nclass Profile_Members(Base): \nForeignKey('profile_trainers.id'), unique=False, \ngclass = relationship('Gym_Class', \nCreating CRUD transactions using SQLAlchemy\nto specify some of its parameters, such as the name of the child model class and the backreference \nthe related model classes.\nIf it is a one-to-one type, the parent class should set the useList parameter to False \nThe previous Login class definition shows that Profile_Trainers and Profile_Members \nother hand, the model relationship between Profile_Members and Attendance_Member is a \nclass Attendance_Member(Base):\nmember_id = Column(Integer, \nForeignKey('profile_members.id'), unique=False, \nrelated model classes will be using during the join query transactions.\nIn the SQLAlchemy ORM, creating the repository layer requires the model classes and a Session \nthe communication to the database and manages all the model objects before the commit() or \nthe model objects for insert, update, and delete transactions when the database issues a commit() \nWe import the Session class from the sqlalchemy.orm module.\nclass is the blueprint that will show us how to insert, update, delete, and retrieve record(s) to/from \nfrom sqlalchemy.orm import Session\nfrom models.data.sqlalchemy_models import Signup\nCreating CRUD transactions using SQLAlchemy\nto the table, and a commit() transaction to finally flush all the new records into the database.\ndef update_signup(self, id:int, \ndef delete_signup(self, id:int) -> bool: \nNow, the following script shows how to implement the query transactions:\nreturn self.sess.query(Signup).all() \ndef get_signup(self, id:int): \nreturn self.sess.query(Signup).\nThe Session object has a query() method, which requires model class(es) or model \nCreating CRUD transactions using SQLAlchemy\nFor SQLAlchemy’s query transactions, all these functions can close the Session object.\nThe repository classes for SQLAlchemy are in the ch05a folder’s /repository/sqlalchemy/\nFor all the ORMs supported by FastAPI, only SQLAlchemy implements join queries pragmatically \nin SQLAlchemy with model classes in one-to-one relationships:\ndef join_login_members(self):\nquery(Login, Profile_Members).\nfilter(Login.id == Profile_Members.id).all()\njoin_login_members() shows the conventional way of creating JOIN queries.\nrequires passing the parent and child classes as query parameters and overriding the ON condition \nThe parent model class must come first in the column projection \nbefore the child class in the query() builder to extract the preferred result.\nclass from the child.\nbetween the Profile_Members and Attendance_Member model classes:\ndef join_member_attendance(self):\nquery(Profile_Members, Attendance_Member).\ndef outer_join_member(self):\nquery(Profile_Members, Attendance_Member).\njoin_member_attendance() shows the use of the join() method in building the INNER \nJOIN queries between Profile_Members and Attendance_Member.\nThe outer_join_member() repository method implements an OUTER JOIN query from the \nThe outerjoin() method will extract all Profile_Members records \nthe repository, such as SessionFactory, the repository class, and the Signup model class.\nfrom sqlalchemy.orm import Session\nfrom db_config.sqlalchemy_connect import SessionFactory\nfrom repository.sqlalchemy.signup import SignupRepository,\nCreating CRUD transactions using SQLAlchemy\ndef list_signup(sess:Session = Depends(sess_db)):\n@router.get(\"/signup/list/{id}\", response_model=SignupReq)\ndef get_signup(id:int, sess:Session = Depends(create_db)):\nIf request_model is used to capture the query result of the SQLAlchemy \nquery transactions, the BaseModel class or request model must include a nested Config class \nBaseModel for the SQLAlchemy model types used by the repository, before all the record objects are \nclass Config:\nsymbol (:), which means orm_mode is a configuration detail and not part of the class attribute.\nand synchronize the model classes with the schema definitions.\nthe model classes is handy and predictable.\nenough to guide developers regarding the different API classes and methods.\nCreating CRUD transactions using SQLAlchemy\nUsually, SQLAlchemy works with the table schemas that have already been generated by the database \nIn this project, the ORM setup started with designing the domain model classes before \nThe sqlalchemy module has a Table() directive that can create a table object with the essential \nsample script that shows how the ORM creates the signup table at the application level:\nfrom sqlalchemy import Table, Column, Integer, String, \nfrom db_config.sqlalchemy_connect import engine\nNow, let us explore how SQLAlchemy can be used to create asynchronous CRUD transactions for \nfor asynchronous connections, sessions, transactions, and database drivers.\nBefore we begin setting up the database configuration, we need to install the following asyncio-\ncreate_async_engine() method, which creates an asynchronous version of SQLAlchemy’s \nfrom sqlalchemy.ext.asyncio import create_async_engine\nImplementing async CRUD transactions using SQLAlchemy\nparameter is set to False to make that model instances and its attribute values accessible for the \nall entity classes and their column objects are still accessible by other processes, even after transaction \nThe full configuration for the asynchronous SQLAlchemy database connection can be found in the \n/db_config/sqlalchemy_async_connect.py module script file.\nCreating the Base class and the model layer\nCreating the Base class using declarative_base() and creating the model classes using Base \ncomes to query transactions, the new select() directive from the sqlalchemy.future module \nfrom sqlalchemy.orm import Session\nfrom models.data.sqlalchemy_async_models import \nImplementing async CRUD transactions using SQLAlchemy\nof the insert() directive in creating an attendance log for a gym member.\nthe model class name to insert() to let the session know what table to access for the transaction.\nthe model attribute values and the updated values from the database using the fetch method.\nasync def update_attendance(self, id:int, \nAttendance_Member.id == id).values(**details)\nasync def delete_attendance(self, id:int) -> bool: \nWhen it comes to queries, the repository class contains get_all_attendance(), which retrieves \nThe resulting Query object has a scalars() method, which we can call to retrieve the list \ncheck_attendance(), on the other hand, uses the scalar() method of the Query object to \nasync def get_attendance(self, id:int): \nasync def check_attendance(self, id:int): \nThe repository classes for the asynchronous SQLAlchemy can be found in the /repository/\nImplementing async CRUD transactions using SQLAlchemy\nfrom db_config.sqlalchemy_async_connect import \nfrom repository.sqlalchemy.attendance import \nfrom models.data.sqlalchemy_async_models import \nasync def update_attendance(id:int, \nasync def delete_attendance(id:int): \nThe preceding script shows no direct parameter passing between the repository class and the \nOther ORM platforms that have been created for asynchronous transactions are easier to use.\nbuild contextual database connections and transactions.\nNo other APIs are needed to open a connection to the database except for the Gino directive.\nneed to instantiate the class to start building the domain layer.\nThe Gino class can be imported from \nGINO database connectivity script can be found in the /db_config/gino_connect.py script \nThe model class definition in GINO has similarities with SQLAlchemy when it comes to structuring, \nthe superclass type because GINO uses the Model class from the database reference instance’s db.\nThe following script shows how the Signup domain model is mapped to the signup table:\nfrom db_config.gino_connect import db\nclass Signup(db.Model):\nid = db.Column(db.Integer, primary_key=True, \nLike in SQLAlchemy, the __tablename__ property is mandatory for all model classes to indicate \ndirective that can set properties such as the column type, primary key, unique, default, nullable, and \ndirective has its first parameter register the name of the actual column and maps it to the model \nThe username and password columns are example cases of mapping the class attributes \nSetting the ForeignKey property in the Column object of the child model class is \nfetch the child records of the parent model class.\njoin query is the relationship configuration between the Profile_Trainers and Gym_Class \nmodel classes, as shown in the following script:\nclass Profile_Trainers(db.Model):\nid = db.Column(db.Integer, db.ForeignKey('login.id'), \nclass Gym_Class(db.Model): \nid = db.Column(db.Integer, primary_key=True,\nmember_id = db.Column(db.Integer, \ndb.ForeignKey('profile_members.id'), unique=False, \ntrainer_id = db.Column(db.Integer, \ndb.ForeignKey('profile_trainers.id'), unique=False,\nFor the LEFT OUTER JOIN query to work, the parent model class must have \nFor a one-to-one relationship, the parent only needs to instantiate the child model:\nclass Login(db.Model): \nid = db.Column(db.Integer, primary_key=True, \nclass Profile_Members(db.Model): \nid = db.Column(db.Integer, db.ForeignKey('login.id'),\ntrainer_id = db.Column(db.Integer, \ndb.ForeignKey('profile_trainers.id'), unique=False, \nProfile_Members and Gym_Class, and between Login and Profile_Members/Profile_\nPython property in Profile_Members, as well as the custom child property in Login.\ntrainer() method shows the conventional way of implementing insert transactions.\nits model class to call create(), an inherited method from the db reference object.\ncalled create() that inserts the record object without requiring any parameters:\nfrom models.data.gino_models import Profile_Members, \nProfile_Trainers, Gym_Class\nawait Profile_Trainers.create(**details)\n•\t First, it requires the get() class method of the model class to retrieve the record object with \nasync def update_trainer(self, id:int, \nwhere(Profile_Trainers.id == id).gino.status()\nIts delete_trainer() also follows the same approach as the GINO update transaction.\nasync def delete_trainer(self, id:int) -> bool: \n•\t The former retrieves a specific record object using its primary key through the get() class \nmethod of the model class \nasync def get_all_member(self):\nreturn await Profile_Trainers.query.gino.all()\nasync def get_member(self, id:int): \nreturn await Profile_Trainers.get(id)\nBut what translates database rows into model objects in a query’s execution is the built-in loader of \nquery = db.select([Profile_Trainers])\nIn the GINO ORM, all queries utilize ModelLoader to load each database record into a model object:\nasync def join_classes_trainer(self):\nquery = Gym_Class.join(Profile_Trainers).select()\nresult = await query.gino.load(Gym_Class.\ndistinct(Gym_Class.id).\nasync def join_member_classes(self):\nquery = Gym_Class.join(Profile_Members).select()\nresult = await query.gino.load(Profile_Members.\ndistinct(Profile_Members.id).\nIf the normal query requires ModelLoader, what is needed for the JOIN query transactions?\nGINO has no automated support for table relationships, and creating JOIN queries is impossible \nThe join_classes_trainer() method implements a one-to-\nmany query for Profile_Trainers and Gym_Class.\nThe distinct(Gym_Class.id).\nload(parent=Profile_Trainers) clause in the query creates a ModelLoader for GymClass, \nwhich will merge and load the Profile_Trainers parent record into its child Gym_Class.\njoin_member_classes() creates one-to-many joins, while distinct(Profile_Members.\nid).load(add_child=Gym_Class) creates a ModelLoader to build the set of Gym_Class \nrecords, as per the Profile_Members parent.\nOn the other hand, the many-to-one relationship of Gym_Class and Profile_Members uses the \nload() function of Profile_Member, which is a different approach to matching the Gym_Class \nchild records to Profile_Members.\nsetup because the Gym_Class records here are on the left-hand side while the profiles are on the right:\nasync def join_classes_member(self):\nProfile_Members.load(add_child=Gym_Class)\nAll the repository classes for GINO can be found in the /repository/gino/trainers.py script.\nFor our repositories to run in the APIRouter module, we need to open the database connection by \nThe following script shows how to set up this database binding:\nfrom db_config.gino_connect import db\nasync def update_trainer(id:int, req: ProfileTrainersReq): \nGINO and SQLAlchemy have the same approach to creating a table schema at the framework level.\nBoth require the MetaData and Column directives for building the Table definitions.\nfrom sqlalchemy import Table, Column, Integer, String, \nPony ORM relies on Python syntax for building the model classes and repository transactions.\nORM only uses Python data types such as int, str, and float, as well as class types to implement \nSince Pony is an ORM designed to build synchronous transactions, we will need the psycopg2 \nthe Database directive from the pony.orm module to be instantiated to connect to the database \nfrom pony.orm import  Database\nNow, let us create Pony's model classes.\nDefining the model classes\nThe created database object, db, is the only component needed to define a Pony entity, a term that refers \nto a model class.\nIt has an Entity attribute, which is used to subclass each model class to provide \nThe following script shows how the Signup class becomes \nfrom pony.orm import  Database, PrimaryKey, Required, \nclass Signup(db.Entity):\nIf the class has no primary key, Pony ORM will implicitly \nexample, int, str, float, date, or time) or any class type.\nrelationship between model classes:\nclass Login(db.Entity): \nmembers = Optional(\"Profile_Members\", reverse=\"id\")\nThe given Login class has two additional attributes, trainers and members, which serve as \nreference keys to the Profile_Trainers and Profile_Members models, respectively.\nturn, these child entities have their respective class attributes pointing back at the Login model, \nThe following code shows examples of Pony’s child model classes:\nclass Profile_Trainers(db.Entity):\ngclass = Set(\"Gym_Class\", reverse=\"trainer_id\")\nclass Profile_Members(db.Entity): \n_table_ = \"profile_members\"\ntrainer_id = Required(\"Profile_Trainers\", \nLogin has a one-to-one relationship with Profile_Members, which explains the use of the \nOptional attribute to point to the id key of Profile_Members.\nOn the other hand, the Profile_Trainers model has a one-to-many setup with Profile_\nMembers, which explains why the trainer_id attribute of the former uses the Required \nscenario between the Profile_Members and Gym_Class models, where the gclass attribute \nof Profile_Members is declared as a Set collection that contains all the enrolled gym classes of \nThe reference key can be a primary key or just a typical class attribute in this relationship.\nThe following snippet shows the blueprint of the Gym_Class model:\nclass Gym_Class(db.Entity): \n_table_ = \"gym_class\"\nmember_id = Required(\"Profile_Members\", \ntrainer_id = Required(\"Profile_Trainers\", \nin the last part of the module script, as shown in the previous snippet, where Gym_Class was the last \nPony model class to be defined.\nAll the Pony model classes can be found in the /models/data/\nBut unlike SQLAlchemy, its repository classes do not \nThe following code shows a repository class that implements all the \ntransactions needed to manage a list of gym members:\nfrom pony.orm import db_session, left_join\nfrom models.data.pony_models import Profile_Members, \nGym_Class, Profile_Trainers\nfrom models.requests.members import ProfileMembersReq \ndef insert_member(self, \nIn Pony, inserting a record means instantiating the model class with the injected record values.\ndef update_member(self, id:int, \nprofile = Profile_Members[id]\nUpdating a record in Pony, which is implemented in the update_member() script, means retrieving \nOn the other hand, delete_member() of the repository class shows the same approach with \nUPDATE, except that a delete() class method is invoked right after retrieving the object record.\ndef delete_member(self, id:int) -> bool: \nProfile_Members[id].delete()\nThe following code shows Pony’s implementation for query transactions:\ndef get_member(self, id:int): \nget_member() retrieves a single record using the get() class method, which requires a lambda \nSince Login has a one-to-one relationship with Profile_Members, \nfirst, we must extract the Login record of the member and use the login object to fetch the record \nPony model classes have the get() and select() methods, which both return Query objects that \nLike in SQLAlchemy, a ModelBase class with a nested Config \nclass is required to retrieve the records from the Query object.\nfollowing code shows the request model that’s used to extract the records from select() through \nlist comprehension and the Profile_Member dict object from get():\nclass Config:\nTo build LEFT JOIN query transactions, the ORM has a built-in directive called left_join(), which \nThe following code shows another repository class that showcases the use of left_join():\ndef join_member_class(self): \nAll the repository classes can be found in the /repository/pony/members.py script file.\nThe repository classes are directly \nIf the tables are non-existent yet, Pony can generate those tables through its entity classes.\ntransaction is enabled when the create_tables parameter of the generate_mapping() \nwhich can be created using the ContextVar class, bridges Peewee to the FastAPI platform.\nTo apply the new db_state and _ConnectionState classes, cited in the preceding code as \nclass.\nPeewee has several variations of the Database class, depending on the type of database the \nthe correct class to initialize with all the necessary database details.\nThe following snippet shows how to connect to our fitness gym database’s fcms using the \nPeewee prefers auto-generating tables based on its model classes, unlike other ORMs. Peewee recommends \nThe following script shows how Peewee model classes are defined:\nclass Signup(Model):\nWe can’t see any primary keys in the model classes presented because the Peewee engine will create them \nWe must let Peewee create the physical tables from the model classes to avoid this mishap.\nAll model classes inherit properties from the Model directive of the ORM.\nattributes of the model classes.\nthe references to database and db_table, which is mapped to the model class.\nforeign key attributes of the child classes to the non-existent primary keys of the parent classes is \nFor instance, the following Profile_Trainers model implies a \nmany-to-one relationship with the Login class, which is only defined by the ForeignKeyField \ndirective with the trainer backreference attribute and not by the login_id foreign key:\nclass Profile_Trainers(Model):\ndb_table = 'profile_trainers'\nAfter defining all the models, including their relationships, we need to call the following methods \n•\t create_tables() to pursue the schema generation based on its list of model classes\nThe following script shows a snapshot of the class definitions, including the call to the two db methods:\nclass Login(Model): \nclass Gym_Class(Model): \ndb_table = 'gym_class'\ndb.create_tables([Signup, Login, Profile_Members, \nProfile_Trainers, Attendance_Member, Gym_Class],\nAs we can see, we need to set the safe parameter of create_tables() to True so that Peewee \nthe model classes for the Peewee ORM can be found in the /models/data/peewee_models.\nCreating the asynchronous connection and building the model layer for the application in the Peewee \nare entirely derived from its model classes.\nthe following snippet, shows how the create() static method of Login takes the login details for \nfrom models.data.peewee_models import Login, \nProfile_Trainers, Gym_Class, Profile_Members\ndef insert_login(self, id:int, user:str, passwd:str, \npursue multiple insertions through its insert_many() class method.\nSimilarly, the update() class method requires the execute() method after filtering the record \ndef update_login(self, id:int, \nget() class method – for example, Login.get(Login.id == id) – and eventually delete \ndelete_login() transaction shows how to utilize the delete_by_id() class method:\ndef delete_login(self, id:int) -> bool: \nPeewee uses its get() class method to retrieve a single record \nusing the primary key; the same method was applied to its UPDATE transaction in the previous code \nSimilarly, Peewee uses a class method to extract multiple records, but this time, it uses the \ndef get_login(self, id:int): \nOn the other hand, the following repository classes show how to create JOIN queries using its \ndef join_login_trainers(self): \ndef outer_join_member_gym(self): \nreturn list(Profile_Members.\nselect(Profile_Members,Gym_Class).join(Gym_Class, \nof the Profile_Trainers and Login objects.\nof the Profile_Trainers object’s select() directive is the parent model type, followed by its \nchild model class in a one-to-one relationship.\nwith the model class type, which indicates the type of records that belong to the right-hand side of the \na LEFT OUTER JOIN of Profile_Members and Gym_Class using the LEFT_OUTER option \nclasses for Peewee can be found in the /repository/peewee/login.py script.\nSince Peewee’s database connection is set at the model layer, no additional requirements are required ",
    "keywords": [
      "False return True",
      "return False return",
      "False",
      "class",
      "profile",
      "relational database",
      "CRUD transactions",
      "database",
      "model class",
      "members",
      "member",
      "return False",
      "False return",
      "model",
      "True"
    ],
    "concepts": [
      "class",
      "classes",
      "class_",
      "def",
      "import",
      "importing",
      "important",
      "imported",
      "importantly",
      "database"
    ]
  },
  {
    "chapter_number": 358,
    "title": "DETACH DELETE",
    "start_page": 381,
    "end_page": 420,
    "summary": "When retrieving nodes, the following service retrieves all the nodes from the database:\nThe following service only retrieves a single node based on the node’s id:\nOur implementation will not be complete if we have no API endpoint that will link nodes based on \nThe following API endpoint creates a node relationship between the Location \nproven that FastAPI can deal with relational database transactions with ORM and document-based \nThis chapter introduced the scientific side of FastAPI by showing that API services can provide \nintegrate FastAPI with new technology and design strategies to provide new ideas for the microservice \nnode-based data management.\nflexibility of the framework in building microservice applications.\nsome deployment strategies, Django and Flask integrations, and other microservice design patterns \nOur long journey of exploring FastAPI’s extensibility in building microservice applications will \nThe service registry and client-side discovery design \npatterns are included likewise in the detailed discussions on how to manage access to the API endpoints \nA microservice component that checks for the health of the API endpoints will \nFastAPI application’s deployment, which might lead to other design strategies and network setups.\nThe main goal of this chapter is to complete the design architecture of a FastAPI application before its \nHere are the topics that will complete our FastAPI application development venture:\n•\t Setting up the virtual environment\n•\t Setting up service registry and client-side service discovery\n•\t Deploying and running applications using Docker\n•\t Utilizing NGINX as an API gateway \n•\t Integrating Django and Flask sub-applications\nThe application \ncom/PacktPublishing/Building-Python-Microservices-with-FastAPI under \nLet us start with the proper way of setting up the development environment of our FastAPI \napplication.\nIn Python development, it is common to manage the libraries and extension modules \nA virtual environment is a way of creating multiple \napplication(s) to be compiled and run.\nrequirements of its application(s).\n•\t To avoid broken installed module files due to namespace collisions\napplications are very dependent\n•\t To create a template or baseline copy of the set of modules to be replicated on some \nAfter the installation, we need to run the python -m virtualenv command to create an instance.\nFigure 11.1 shows how the ch01-env virtual environment for the ch01 project is created:\nFigure 11.1 – Creating a Python virtual environment\nTo use the virtual environment, we need to configure our VS Code editor to utilize the Python \ninterpreter of the virtual environment instead of the global interpreter to install modules, compile, \nand run the application.\nvirtual environment with the Python interpreter, as shown in Figure 11.3:\ndifferent set of installed module dependencies, as shown in Figure 11.5:\nFigure 11.5 – Creating multiple virtual environments\nPython microservice application.\ndeployment of the application in terms of identifying what modules to install in the cloud servers.\nHowever, before we discuss FastAPI deployment approaches, first, let us discuss what microservice \nPrometheus is a popular monitoring total that can monitor and check API services in any microservice \napplication.\nTo apply Prometheus to FastAPI \napplications, first, we need to install the following module:\nThen, we add PrometheusMiddleware to the application and enable its endpoint to observe the \nThe following script shows the application setup with the Prometheus \napp = FastAPI()\nproviding response to clients, and emitting the status code of each API transaction.\nAnother way of monitoring a FastAPI microservice application is by adding an open tracing tool.\nis preferred when managing API logs and traces.\nprototype, we will be using the Jaeger tool to manage the application’s API traces and logs.\nThe current way to integrate an OpenTracing tool into FastAPI microservices is through the OpenTelemetry \nthe tracing service, OpenTelemetry has an OpenTelemetry Jaeger Thrift Exporter utility, which allows \nyou to export traces to the Jaeger client applications.\napp = FastAPI()\nThe first step in the preceding setup is to create a tracing service with a name using OpenTelemetry’s \ndetails to manage all of the traces and logs using a Jaeger client.\nabout the exchange of requests and responses among all API services and other components across \nwithin an application.\nAfter the completed Jaeger tracer setup, we integrate the tracer client with FastAPI through \nTo utilize this class, first, we need to install the following extension:\npip install opentelemetry-instrumentation-fastapi\nBefore we can run our application, first, we need to download a Jaeger client from \nFigure 11.7 – Monitoring microservices through a Jaeger client\nsports-tracer, after running our microservice application.\nand monitored, thus creating traces and visual analyses regarding all requests and response transactions \nFigure 11.8 shows the traces and graphical plots generated by Jaeger:\nFigure 11.8 – Searching the traces of every API transaction\nSetting up service registry and client-side service discovery\ntrace for the /ch11/login/list/all endpoint, as shown in Figure 11.8, can provide us with \nAside from the traces shown in Figure 11.9, the Jaeger client can also collect the uvicorn logs through \ninstalling the module, we can enable the integration by instantiating LoggingInstrumentor in \nNow, let us add the service registry and client-side service discovery mechanisms to our application.\nSetting up service registry and client-side service \nA service registry tool such as Netflix Eureka enables the registration of microservice applications \nThis service registration is helpful to microservice applications deployed to servers with \nFor FastAPI, we need to utilize the py_eureka_client \nmodule to implement the service discovery design pattern.\nImplementing client-side service discovery\nCreating a FastAPI microservice application to discover and register to a service registry server such \nFirst, we need to install py_eureka_client through pip:\npip install py_eureka_client\nthe client instance must have the appropriate app_name parameter for the FastAPI microservice \napplication (or client app), with the instance_port parameter set to 8000 and the instance_\napp = FastAPI()\ncreate_async_db() \nSetting up service registry and client-side service discovery\nThe client discovery happens in the startup event of the application.\nNow, let us build our Netflix Eureka server registry before running \nand performing the client-side service discovery.\nSetting up the Netflix Eureka service registry\nLet us utilize the Spring Boot platform to create our Eureka server.\nWe can create an application through \napplication.\nOurs is a Maven application with pom.xml that has the following dependency for the \nIn this case, application.properties must have server.port set to 8761, server.\nNow, run the Eureka Server application before the FastAPI client application.\nFigure 11.10 – Discovering the FastAPI microservice application\nThe result of the client-side service discovery is also evident on the Eureka server’s dashboard at \nFigure 11.11 – Creating the service registry\nOur SPORTS_SERVICE being part of the Eureka server registry, as depicted in Figure 11.11, means \nwe successfully implemented the client-side service discovery design pattern, and it is time to deploy \nour application to a Docker container.\nDeploying and running applications using Docker\nDockerization is a process of packaging, deploying, and running applications using Docker \nContainerizing FastAPI microservices saves installation and setup time, space, and \nTo pursue Dockerization, we need to install Docker Hub and/or Docker Engine for the CLI commands.\nThis chapter mainly focuses on how to run CLI commands rather than the Docker Hub GUI \nNow, let us generate the list of modules to be installed in the docker image.\nSince we are using a virtual environment instance for module management, it is easy to identify what \nextension modules to install in the Docker image.\nWe can run the following command to generate a \nThen, we can create a command to copy this file to the image through the Dockerfile.\nDeploying and running applications using Docker\nCreating the Docker image \nThe next step is to build a container image from any available Linux-based container images in Docker \nPython image from Docker Hub, creating a working directory, and copying project files from the \nThe command after that creates an arbitrary folder, /code, which \ntxt file to the /code folder, and then the RUN instruction installs the updated modules from the \nAfterward, the second COPY command copies our ch11 application to the working directory.\napplication at port 8000 using host 0.0.0.0 and not localhost to automatically map and utilize \napplication.\nFigure 11.12 shows the organization of the files and folders that needed to be Dockerized \nFigure 11.12 – Setting up the Docker folder structure\nOnce all the files and folders are complete, we run the following CLI command within the folder \ndocker build -t ch11-app .\nTo check the image, run the docker image ls CLI command.\nThe backend of our application is MongoDB, so we need to pull the latest mongo image from Docker \nAnd before we run both the ch11-app application and the mongo:latest images, first, we need \nto create a ch11-network by running the following command:\ndocker network create ch11-network\nThis network becomes a bridge between mongo and ch11-app once they are deployed as containers.\nCreating the containers\nA container is a running instance of a container image.\nWe use the docker run command to start \nand run a pulled or created image.\nSo, running the Mongo image using the ch11-network routes \nInspect the mongo:latest container using the docker inspect command to derive and use its \nwhich is found in the config/db.py module of ch11-app with the “inspected” IP address.\nsure to re-build the ch11-app Docker image after the update.\nNow, run the ch11-app image with ch11-network using the following command:\ndocker run --name=ch11-app --rm -p 8000:8000-\nAccess the application through http://localhost:8000/docs to check all the API endpoints \nHowever, you need to install the Docker Compose utility in your operating system, which requires \nAfter the installation, the next step is to create the \ndocker-decompose.yaml file containing all the services needed to build the images, process the \nDockerfile, build the Docker network, and create and run the containers.\nthe content of our configuration file that sets up the mongo and ch11-app containers: \nInstead of running separate Docker CLI commands, Docker Compose creates services, such as ch11-\nmongo and ch11-app, to manage the containerization and only uses one CLI command to execute \nThe command not only creates the network of images but \nthe IP address of the mongo container varies for every instance created.\nNow, let us implement an API Gateway design pattern for the containerized applications using the \nIn Chapter 4, Building the Microservice Application, we implemented the API Gateway design pattern \nNGINX that will assign a proxy IP address to each containerized microservice application.\nIPs will redirect client requests to the actual microservices running on their respective containers.\nIP address to the actual container address of each microservice application.\nIntegrating Flask and Django sub-applications\nThe application’s OpenAPI documentation can now be accessed through \nThe Dockerization of NGINX must come after deploying applications to the containers.\nOr we can create another service in the docker-decompose.yaml file to \nbuild and run the NGINX image.\nAnd for the last time, let us explore the power of FastAPI in its integration with other popular Python \nIntegrating Flask and Django sub-applications\napplies the scaffolding of files and folders to build projects and applications.\nDjango applications can \nWe can create, deploy, and run Flask and Django projects inside a FastAPI microservice application.\nThe framework has WSGIMiddleware to wrap both Flask and Django applications and integrate \nRunning the FastAPI application through uvicorn will also run both \napplications.\nOf the two, it is easier to integrate the Flask application with a project than Django.\nimport the Flask app object into the main.py file, wrap it with WSGIMiddleware, and mount \nit into the FastAPI app object.\nAll API endpoints implemented in ch11_flask will be accessed using the URL prefix, /ch11/\nFigure 11.13 shows the location of ch11_flask \nFigure 11.13 – Creating a Flask application inside the FastAPI project\nOn the other hand, the following main.py script integrates our ch11_django application into \nos.environ.setdefault(‘DJANGO_SETTINGS_MODULE’, \ndjango_app = get_wsgi_application()\napp = FastAPI()\nIntegrating Flask and Django sub-applications\nThe Django framework has a get_wsgi_application() method that is uses to retrieve its \nFastAPI app object.\nMoreover, we need to load the settings.py module of the ch11_django \nproject into the FastAPI platform for global access.\nAll views and endpoints created by the sports application of the ch11_django project must be \ndjango project within the ch11 app:\nFigure 11.14 – Creating a Django project and application inside a FastAPI object\nThe last chapter has given us the avenue on how to start, deploy, and run a FastAPI microservice application \ncontrol and manage the installation of modules from the start of the development until the deployment \nof our applications to Docker containers.\nto package, deploy, and run containerized applications.\nNGINX reverse proxy server for the application to build the API Gateway for our specimen.\nAPI services\napplication deployment, with Docker\ncontainers, creating  374\nDocker image, creating  373\napplication settings\ncreating  334, 335\nasync CRUD transactions, creating with Motor\ncreating  183-185\nmodel layer, creating  183\nBase class, creating  125\nmodel layer, creating  125\ncreating  261\nasync repository, for FastAPI\ndatabase connectivity, creating  195, 196\nrepository transactions, running  200\ncreating  41, 42\ncreating  264-269\ncreating  266\nclient-side service discovery\ncreating  82\nused, for creating Observable  278-280\ncreating  156, 157\ncreating  157, 158\nCRUD transactions, creating \ndatabase connection, creating  206\nmodel layer, creating  207, 208\nrepository layer, creating  208-210\nCRUD transactions, creating \nJOIN queries, creating  119, 120\ntransactions, running  120-122\ncreating  334, 335\nDjango applications\nFastAPI\nFlask applications\naccess_token, creating  228\nlogin transaction, creating  229, 230\ncreating  94\nCRUD transactions, creating with  206\nasync CRUD transactions, creating with  182\nCRUD transactions, creating  356-360\ncreating, with coroutines  278-280\ndatabase connection, creating  201\nmodel layer, creating  201, 202\nusers, creating  242\npython-multipart module, installing  222\ndatabase connection, creating  149, 150\ndomain layer, creating  150-153\ndata models, creating  329, 330\nrepository transactions, running  148\ntransactions, running  179-182\nCelery instance, creating  269\nObservable, creating with coroutines  278\ncreating  231\nCRUD transactions, creating  110, 116-118\ncreating  80, 81\ncreating  331, 332\nBuilding Python Web APIs with FastAPI\n•\t Set up a FastAPI application that is fully functional and secure\n•\t Manage concurrency in FastAPI applications\n•\t Implement authentication in a FastAPI application\n•\t Deploy a FastAPI application to any platform\n•\t Implement best practices for building reliable, performant, and secure web apps\n•\t Create effective solutions for the modern web, including task management, bot integration, \nNow you’ve finished Building Python Microservices with FastAPI, we’d love to hear your thoughts!",
    "keywords": [
      "FastAPI microservice application",
      "CRUD transactions",
      "application",
      "Docker",
      "microservice application",
      "creating",
      "API",
      "FastAPI",
      "service",
      "Microservice Features",
      "microservice",
      "Docker Compose",
      "FastAPI application",
      "API services",
      "virtual environment"
    ],
    "concepts": [
      "creates",
      "create",
      "creating",
      "created",
      "implementation",
      "implementing",
      "implement",
      "implemented",
      "app",
      "app_name"
    ]
  }
]