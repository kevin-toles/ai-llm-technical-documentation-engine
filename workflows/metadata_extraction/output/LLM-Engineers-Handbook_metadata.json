[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 1-8)",
    "start_page": 1,
    "end_page": 8,
    "summary": "language models from concept to production\nJulien Chaumond Hamza Tahir\nPaul lusztin | Maxime Labonne <packt\nLLM Engineer’s Handbook\nMaster the art of engineering large language models from \nLLM Engineer’s Handbook\nNeither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any \nPackt Publishing has endeavored to provide trademark information about all of the companies and products \nHowever, Packt Publishing cannot guarantee \nPublished by Packt Publishing Ltd.\nAs my co-founder at Hugging Face, Clement Delangue, and I often say, AI is becoming the default \nLLMs by writing this book and making sure that as many people as possible can not only use \ndorse The LLM Engineer’s Handbook.\nFrom data engineering and model fine-tuning to advanced topics like RAG pipelines \nWhether you’re a seasoned ML practitioner looking to specialize in LLMs or a software engineer \nbest practices make it an invaluable resource for anyone serious about mastering LLM engineering.\nIn an era where AI is reshaping industries at breakneck speed, The LLM Engineer’s Handbook stands \na book; it’s a roadmap to becoming a proficient LLM engineer in today’s AI-driven landscape.\nPaul Iusztin is a senior ML and MLOps engineer with over seven years of experience building \nserved as one of their core engineers in taking large neural networks to production.\nchannel on production-grade ML that provides posts, articles, and open-source courses to help \nothers build real-world ML systems.\nPolytechnic Institute of Paris and is recognized as a Google Developer Expert in AI/ML.\nPython, published by Packt.\nRany ElHousieny is an AI solutions architect and AI engineering manager with over two decades \nof experience in AI, NLP, and ML.\ndeployment of AI models, authoring multiple articles on AI systems architecture and ethical AI de-\nTheir commitment to AI advancements made my experience of reviewing ",
    "keywords": [
      "Packt Publishing",
      "LLM Engineer",
      "LLM",
      "Labonne LLM Engineer",
      "Hugging Face",
      "book",
      "packt",
      "Maxime Labonne",
      "Maxime Labonne LLM",
      "Publishing",
      "Engineer",
      "large language models",
      "LLMs",
      "Paul Iusztin",
      "Co-founder and CTO"
    ],
    "concepts": [
      "ai",
      "editor",
      "production",
      "products",
      "product",
      "llm",
      "engineering",
      "engineer",
      "engineers",
      "packt"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 9-16)",
    "start_page": 9,
    "end_page": 16,
    "summary": "• 2\n• 5\n• 6\nThe feature pipeline • 14\nThe training pipeline • 14\nThe inference pipeline • 14\nData collection pipeline • 19\nFeature pipeline • 19\nTraining pipeline • 21\nInference pipeline • 22\nOrchestrator • 33\n• 51\nImplementing the LLM Twin’s data collection pipeline • 61\n• 66\nThe crawlers • 69\nTroubleshooting • 94\n• 100\nHallucinations • 101\nRetrieval pipeline • 105\nGeneration pipeline • 105\n• 107\n• 111\n• 115\nRetrieval • 122\n• 128\nBatch pipelines • 130\n• 138\nOrchestration • 138\nSettings • 139\nOVM • 154\nThe handlers • 162\nData quantity • 180\nData exploration • 189\nData generation • 191\nLoRA • 213\nQLoRA • 215\nOptimizers • 218\nPreference data • 230\nData generation and evaluation • 233\nEvaluating preferences • 235\nRagas • 272\nARES • 274\nData parallelism • 299\nPipeline parallelism • 300",
    "keywords": [
      "LLM Twin",
      "LLM Twin architecture",
      "LLM Twin Concept",
      "LLM Twin MVP",
      "LLM",
      "RAG Feature Pipeline",
      "Twin",
      "Data",
      "pipeline",
      "Table of Contents",
      "RAG Inference Pipeline",
      "feature pipeline",
      "LLM Twin matters",
      "Twin architecture",
      "LLM Twin product"
    ],
    "concepts": [
      "data",
      "pipelines",
      "pipeline",
      "evaluation",
      "evaluating",
      "evaluations",
      "rag",
      "preference",
      "preferences",
      "classes"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 17-24)",
    "start_page": 17,
    "end_page": 24,
    "summary": "Implementing the LLM Twin’s RAG inference pipeline                                                      338\nBringing everything together into the RAG inference pipeline • 346\nData • 357\nOnline real-time inference • 360\nAsynchronous inference • 361\nExploring the LLM Twin’s inference pipeline deployment strategy                                   368\nThe training versus the inference pipeline • 371\nImplementing the LLM microservice using AWS SageMaker • 373\n• 373\nDeploying the LLM Twin model to AWS SageMaker • 375\nDevOps • 403\nMLOps • 405\nMLOps core components • 407\nMLOps principles • 407\nMLOps engineering • 409\nLLMOps • 410\nGuardrails • 411\nPrompt monitoring • 413\nDeploying the LLM Twin’s pipelines to the cloud                                                               415\nRun the pipelines on AWS • 428\non SageMaker • 432\nLLM Twin’s CI/CD pipeline flow • 434\nThe CI pipeline • 438\nThe CD pipeline • 442\nTest out the CI/CD pipeline • 445\nThe CT pipeline • 446\nTrigger downstream pipelines • 449\nPrompt monitoring • 451\nAlerting • 457\n• 465\nLogs • 468\nMetrics • 468\nSystem metrics • 469\nModel metrics • 469\nDrifts • 469\nobservability • 472\nAlerts • 473\nfine-tune models for specific tasks, optimize inference performance, and implement RAG pipelines.\nRAG theory by architecting and implementing LLM Twin’s RAG feature pipeline using software \nimplementing the LLM Twin’s RAG inference pipeline and a custom retrieval module similar to \nChapter 10, Inference Pipeline Deployment, introduces ML deployment strategies, such as online, \nMLOps. This chapter explains how to deploy the LLM Twin project to the cloud, such as the ML \nIt also adds a prompt monitoring layer on top of LLM Twin’s inference pipeline.",
    "keywords": [
      "LLM Twin",
      "LLM",
      "RAG inference pipeline",
      "inference pipeline",
      "Twin",
      "LLM Twin project",
      "RAG",
      "RAG inference",
      "LLM Twin model",
      "inference",
      "pipeline",
      "Inference Pipeline Deployment",
      "LLM Twin Concept",
      "implementing LLM Twin",
      "LLM engineering"
    ],
    "concepts": [
      "chapter",
      "chapters",
      "pipelines",
      "mlops",
      "inference pipeline",
      "llm",
      "rag",
      "model",
      "models",
      "deployment"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 25-33)",
    "start_page": 25,
    "end_page": 33,
    "summary": "To get the most out of this book\ngramming is particularly beneficial, as the book’s examples and code snippets are predominantly \nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/\nare interested in either writing or contributing to a book, please visit http://authors.packtpub.\nDownload a free PDF copy of this book\nUnderstanding the LLM Twin \nThis book \nwill show you how to build an LLM Twin, an AI character that learns to write like a particular \nperson by incorporating its style, voice, and personality into an LLM.\nMost of the concepts learned while implementing your LLM Twin can be applied in other LLM-\nIn our case, what exactly is an LLM Twin, \nused to build the LLM system.\nUnderstanding the LLM Twin Concept and Architecture\nUnderstanding the LLM Twin concept\nPlanning the MVP of the LLM Twin product\nDesigning the system architecture of the LLM Twin\nthe book.\nUnderstanding the LLM Twin concept\nThe concept of an LLM Twin is new.\nWhat is an LLM Twin?\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and \npersonality into an LLM, which is a complex AI model.\ninto an LLM.\nInstead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned \nNaturally, as an ML model reflects the data it is trained on, this LLM will incorporate \nThus, this LLM will not be you; \nIt is essential to understand that an LLM reflects the data it was trained on.\nTo adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various \nHere are some scenarios of what you can fine-tune an LLM on to become your twin:\nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\nCode: Specialize the LLM in implementing code as you would.\nDo we have enough digital data to project ourselves into an LLM?\n“Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and \nWhy building an LLM Twin matters\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Sub-\nthe skeleton of our main idea to the LLM Twin and let it do the grunt work.\nUnderstanding the LLM Twin Concept and Architecture\non the concrete features in the Planning the MVP of the LLM Twin product section).\nect ourselves into a content-writing LLM Twin that will help us automate our writing process.\nwill likely fail if we try to use this particular LLM in a different scenario, as this is where we will \nspecialize the LLM through fine-tuning, prompt engineering, and RAG.\nSo, why does building an LLM Twin matter?\nAlso, it is critical to understand that building an LLM Twin is entirely moral.\nThe LLM will be \nEveryone will have their own LLM Twin with restricted access.\nbook in itself.\nWhat’s the difference between a co-pilot and an LLM Twin?\nstance, an LLM Twin is an LLM that learns to mimic your voice, personality, \nwrites like you is your LLM Twin co-pilot.",
    "keywords": [
      "LLM Twin",
      "LLM Twin Concept",
      "LLM",
      "LLM Twin product",
      "Twin",
      "LLM Twin co-pilot",
      "Twin Concept",
      "book",
      "LLM Twin matters",
      "content-writing LLM Twin",
      "writing",
      "Understanding the LLM",
      "code",
      "LLM reflects",
      "data"
    ],
    "concepts": [
      "code",
      "writing",
      "write",
      "writes",
      "llm",
      "content",
      "product",
      "production",
      "ml",
      "book"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 34-41)",
    "start_page": 34,
    "end_page": 41,
    "summary": "The key of the LLM Twin stands in the following:\nHow we feed the data into the LLM\nThe solution is to build an LLM system that encapsulates and automates all the following steps \nUnderstanding the LLM Twin Concept and Architecture\ninterface, it can be integrated into the LLM Twin system we will learn to build.\nsuccessful ML products is to be data-centric and make your architecture model-agnostic.\nyou can quickly experiment with multiple models on your specific data.\nPlanning the MVP of the LLM Twin product\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define \nthe product’s features.\nAn MVP is a version of a product that includes just enough features to draw in early users and test \nRisk minimization: Reduces the time and resources needed for a product that might not \nprovide an end-to-end user journey without half-implemented features, even if the product is \nIt must be a working product with a good user experience that people will love and \nDefining the LLM Twin MVP\nin defining our LLM Twin MVP and what features we want to pick.\nTo keep it simple, we will build the features that can do the following for the LLM Twin:\nFine-tune an open-source LLM using the collected data\nHave a simple web interface to interact with the LLM Twin and be able to do the following:\nThat will be the LLM Twin MVP.\nEven if we focus only on the core features of the LLM Twin defined in this section, we \nwill build the product with the latest LLM research and best software engineering \nand scalable LLM application.\nUnderstanding the LLM Twin Concept and Architecture\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives.\nof the LLM Twin.\nBuilding ML systems with feature/training/inference \nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system \npattern at the core of the architecture, known as the feature/training/inference (FTI) architecture.\nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.\nThe problem with building ML systems\nBuilding production-ready ML systems is much more than just training a model.\ngineering point of view, training the model is the most straightforward step in most use cases.\nHowever, training a model becomes complex when deciding on the correct architecture and \nAt this point, we want to focus on how to design a production-ready architecture.\nThese are the types of problems an ML or MLOps engineer must consider, while the research or \ndata science team is often responsible for training the model.\nwhen productionizing an ML model.\nUnderstanding the LLM Twin Concept and Architecture\nIn Figure 1.2, you can observe the typical architecture present in most ML applications.\non a monolithic batch architecture that couples the feature creation, model training, and infer-\nthe ML world: the training-serving skew.\nThe training-serving skew happens when the features \npassed to the model are computed differently at training and inference time.\nIt’s hard to share the work between multiple teams between the features, training, and \nwhole state through the client request so the features can be computed and passed to the model.\nUnderstanding the LLM Twin Concept and Architecture\nIn conclusion, our problem is accessing the features to make predictions without passing them at \nand keeping ML models in production.\nFigure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/",
    "keywords": [
      "LLM Twin",
      "LLM Twin MVP",
      "LLM Twin Concept",
      "LLM Twin architecture",
      "LLM",
      "LLM Twin product",
      "LLM Twin system",
      "Twin",
      "Twin MVP",
      "Twin Concept",
      "Architecture",
      "data",
      "LLM Twin stands",
      "product",
      "LLM Twin defined"
    ],
    "concepts": [
      "data",
      "architecture",
      "users",
      "user",
      "model",
      "models",
      "ml products",
      "figure",
      "content",
      "product"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 42-49)",
    "start_page": 42,
    "end_page": 49,
    "summary": "But here is where the FTI pipeline architectures kick in.\ncan follow to compute the features, train the model, and make predictions.\ncritical steps that any ML system requires, the pattern is known as the FTI pipeline.\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, \nAs shown in Figure 1.5, we have the feature, training, and inference pipelines.\nFigure 1.5: FTI pipelines architecture\nBefore going into the details, it is essential to understand that each pipeline is a different com-\nThe feature pipeline\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels \nrequired by the model for training or inference.\nThus, we can easily send the features to the training and inference pipelines.\nAs the data is versioned, we can always ensure that the training and inference time features match.\nThe training pipeline\nThe training pipeline takes the features and labels from the features stored as input and outputs \nfeature stores, but this time, the model is the first-class citizen.\nversion, track, and share the model with the inference pipeline.\nThus, we will always know what data the model was trained on.\nThe inference pipeline\nThe inference pipeline takes as input the features and labels from the feature store and the trained \nThe feature pipeline takes in data and outputs the features and labels saved to the feature \nThe training pipeline queries the features store for features and labels and outputs a \nThe inference pipeline uses the features from the feature store and the model from the \ncontain only three pipelines.\nFor example, the feature pipeline \ncan be composed of a service that computes the features and one that validates the data.\ntraining pipeline can be composed of the training and evaluation components.\nThe FTI pipelines act as logical layers.\nlines interact with each other through the feature store and model registries.\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how \nStore the vectorized data into a vector DB \nTo learn more about the FTI pipeline pattern, consider reading From MLOps to ML \nSystems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder \nfti-pipelines.\nHow can we apply the FTI pipeline design to implement the preceding list of requirements?\nHow to design the LLM Twin architecture using the FTI \npipeline design\nthree, as the FTI pipeline design clearly states?” That is a great question.\nWe must also implement the data pipeline along the three feature/training/inference \npipelines.\nThe data engineering team owns the data pipeline\nThe ML engineering team owns the FTI pipelines.\nThis includes defining the data collection and FTI pipelines.\nData collection pipeline\nThe data collection pipeline involves crawling your personal data from Medium, Substack, Linke-\ndIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to \nThe output of this component will be a NoSQL DB, which will act as our data warehouse.\nattach an additional ETL in the data collection pipeline, and everything else will work without \nFeature pipeline\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data ware-\nIt is critical to highlight that the data collection pipeline is designed to crawl data \nHere are some custom properties of the LLM Twin’s feature pipeline:\nIt processes three types of data differently: articles, posts, and code\ncretely, a specialized feature store, we used the vector DB, plus some additional logic to check all \nquery the vector DB for new data points without any vector search logic.\nThe training pipeline will use the \ninstruct datasets as artifacts, and the inference pipeline will query the vector DB for additional \nTo conclude, we take in raw article, post, or code data points, process them, and store them in \na feature store to make them accessible to the training and inference pipelines.",
    "keywords": [
      "LLM Twin",
      "LLM Twin Concept",
      "feature store",
      "FTI pipeline",
      "data",
      "pipeline",
      "feature pipeline",
      "LLM Twin architecture",
      "feature",
      "FTI",
      "LLM",
      "features",
      "FTI pipeline design",
      "model",
      "Data collection pipeline"
    ],
    "concepts": [
      "data",
      "pipelines",
      "pipeline architectures",
      "features",
      "feature",
      "differ",
      "different",
      "differently",
      "chapter",
      "chapters"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 50-58)",
    "start_page": 50,
    "end_page": 58,
    "summary": "The training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with \nit, and stores the tuned LLM weights in a model registry.\ndataset is available in the logical feature store, we will trigger the training pipeline, consume the \nthe best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate.\nThe proposed LLM is then stored in the model registry.\nmodel is ultimately tagged as accepted and deployed to the production inference pipeline.\nHow do you implement an LLM agnostic pipeline?\nUnderstanding the LLM Twin Concept and Architecture\nIt loads a fine-tuned LLM from the model registry, and from the logical feature \nIt uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\ncollection and feature pipeline are mostly CPU-based and do not require powerful machines.\ntraining pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it.\narchitecture of the LLM Twin to fit all our technical requirements.\nFrom MLOps to ML Systems with Feature/Training/Inference \nhttps://www.hopsworks.ai/post/mlops-to-ml-systems-with-\nUnderstanding the LLM Twin Concept and Architecture\nMLOps: Continuous delivery and automation pipelines in machine learning.\n5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024.\nhttps://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-\nTooling and Installation\nimplementing and deploying the LLM Twin project.\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage \nmultiple Python versions, create a virtual environment, and install the pinned dependencies re-\nNext, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, \nsuch as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and \nTooling and Installation\nPython ecosystem and project installation\nBy the end of this chapter, you will be aware of all the tools we will use across the book.\nwill have learned how to install the LLM-Engineers-Handbook repository, set up the rest of the \ntools, and use them if you run the code while reading the book.\nPython ecosystem and project installation\nAny Python project needs three fundamental tools: the Python interpreter, dependency manage-\nAll the code within the book is tested with Python 3.11.8.\nversion (Python 3.11.8) to run the LLM Twin project using pyenv, making the installation process \nInstead of installing multiple global Python versions, we recommend managing them using pyenv, \na Python version management tool that lets you manage multiple Python versions between \nAfter you have installed pyenv, you can install the latest version of Python 3.11, using pyenv, as \nNow list all installed Python versions to see that it was installed correctly:\nBecause we defined a .python-version file within the repository, pyenv will know to pick up \npython --version\nTo create the .python-version file, you must run pyenv local 3.11.8 once.\nalways know to use that Python version while working within a specific directory.\nNow that we have installed the correct Python version using pyenv, let’s move on to Poetry, which \nPoetry: dependency and virtual environment management\nPoetry is one of the most popular dependency and virtual environment managers within the \nFor example, this is a simple Poetry requirements file that \nuses Python 3.11 and the requests and numpy Python packages.\n[tool.poetry.dependencies]\nTooling and Installation\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version \nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in \nwhich it installs the specified Python version and requirements.\nto isolate your project’s dependencies from your global Python dependencies and other projects.\nprojects in the global Python environment, that will not work, as Project B will override Project A’s \nisolate each project in its own Python environment with its own Python dependencies, avoiding \nYou can install Poetry from here: https://python-poetry.org/docs/.\nWe use Poetry 1.8.3 \nOnce Poetry is installed, navigate to your cloned LLM-Engineers-Hand-\nbook repository and run the following command to install all the necessary Python dependencies:\npoetry install --without aws\nthe pyproject.toml and poetry.lock files.\nAfter the installation, you can activate your Poetry \nas follows: poetry run <your command>.\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the \npoetry.lock file based on the definitions added to the project.toml file.\ntoml file may specify version ranges (e.g., requests = \"^2.25.1\"), the poetry.lock file records \nversions, the poetry.lock file ensures that all project installations use the same versions of each \nOther tools similar to Poetry are Venv and Conda for creating virtual environments.\nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands \nfile that Poetry already uses for dependencies.\npoetry poe test\nYou can install Poe the Poet as a Poetry plugin, as follows:",
    "keywords": [
      "LLM Twin",
      "Python",
      "LLM",
      "LLM Twin project",
      "LLM Twin architecture",
      "Poetry",
      "LLM Twin Concept",
      "Python version",
      "pipeline",
      "project",
      "LLM Twin MVP",
      "Python project",
      "Training pipeline",
      "inference pipeline",
      "Twin"
    ],
    "concepts": [
      "python",
      "poetry",
      "tools",
      "tool",
      "tooling",
      "llm",
      "pipeline",
      "pipelines",
      "file",
      "files"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 59-66)",
    "start_page": 59,
    "end_page": 66,
    "summary": "Now that we have installed our Python project, let’s present the MLOps tools we will use in the \nuse case, such as model registries and orchestrators, but only provide a quick idea of what they \nRun poetry poe local-infrastructure-up to locally spin up ZenML (http://127.0.0.1:8237/) \nYou can read more details on how to run everything locally in the LLM-Engineers-Handbook re-\nFigure 2.1: Hugging Face model registry example\nMost ML tools provide model registry features.\nZenML: orchestrator, artifacts, and metadata\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that \nThus, ZenML’s main features are orchestrating ML \npipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts \nyou to run ZenML on multiple infrastructure options.\nFor example, in our LLM Twin use case, we used the AWS stack:\ndetails on ZenML stacks, you can start here: https://docs.zenml.io/user-guide/production-\nThe local version of the ZenML server comes installed as a Python package.\npoetry install, it installs a ZenML debugging server that you can use locally.\nwill show you how to use their cloud serverless option to deploy the ML pipelines to AWS.\nHow does ZenML work as an orchestrator?\nIt works with pipelines and steps.\nA function becomes a ZenML pipeline by being \nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented \nIn the code snippet below, we defined a ZenML pipeline that queries \nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\nWe will focus only on the ZenML features used throughout the book, such as orches-\nguide: https://docs.zenml.io/user-guide/starter-guide.\nYou can run the pipeline with the following CLI command: poetry poe run-digital-data-etl.\nTo visualize the pipeline run, you can go to your ZenML dashboard (at http://127.0.0.1:8237/) \nFigure 2.2: ZenML Pipelines dashboard\npipeline runs, as seen in Figure 2.3.\nAlso, you can see the stack used to run the pipeline, where the default stack is the one used to \nrun your ML pipelines locally.\nFigure 2.3: ZenML digital_data_etl pipeline dashboard.\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or \nis still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure \nFigure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, \nlet’s quickly look at how to define a ZenML step.\nor_create_user() step, which works just like a normal Python function but is decorated with \nfrom zenml import get_step_context, step\nWithin a ZenML step, you can define any Python logic your use case needs.\nthen glue multiple steps together within a main function decorated with @pipeline.\nWe also defined the pipelines and steps folders, where \nmodule, we only aggregated ZenML steps to glue them into the final pipeline.\nsign, we can easily swap ZenML with another orchestrator or use our application logic in other ",
    "keywords": [
      "ZenML",
      "Hugging Face",
      "pipeline",
      "LLM Twin",
      "Hugging Face model",
      "run",
      "pipeline run",
      "pipelines",
      "user",
      "etl pipeline run",
      "model registry",
      "ZenML pipeline",
      "LLM",
      "etl pipeline",
      "step"
    ],
    "concepts": [
      "zenml",
      "pipelines",
      "pipeline",
      "ml",
      "steps",
      "step",
      "import",
      "https",
      "user",
      "user_full_name"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 67-75)",
    "start_page": 67,
    "end_page": 75,
    "summary": "As mentioned in the previous section, ZenML transforms any step output into an artifact.\nthe machine learning lifecycle, such as datasets, trained models, checkpoints, or logs.\nFor example, when wrapping your dataset with an artifact, \nyou can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, \nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an ar-\nFigure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\nFigure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\nA more interesting example of an artifact and its metadata is the generated dataset artifact.\nFigure 2.9, we can visualize the metadata of the instruct_datasets artifact, which was auto-\ninstruction datasets are in Chapter 5.\nFigure 2.9: ZenML metadata example for the instruct_datasets artifact\ncan precompute and attach to the artifact’s metadata anything you consider helpful for dataset \nname=\"instruct_datasets\",\nstep_context.add_output_metadata(output_name=\"instruct_datasets\", \nmetadata=_get_metadata_instruct_dataset(datasets))\ndef _get_metadata_instruct_dataset(datasets: InstructTrainTestSplit) -> \ninstruct_dataset_categories = list(datasets.train.keys())\ncategory: instruct_dataset.num_samples for category, instruct_\ntest_num_samples = {category: instruct_dataset.num_samples for \n\"data_categories\": instruct_dataset_categories,\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.\nHow to run and configure a ZenML pipeline\nAll the ZenML pipelines can be called from the run.py file, accessed at tools/run.py in our GitHub \nFor example, to call the digital_data_etl pipeline to crawl Maxime’s content, \npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\npoetry poe run-digital-data-etl-maxime\npoetry poe run-digital-data-etl-paul\n\"run_name\": f\"digital_data_etl_run_{dt.now().\ndigital_data_etl.with_options()(**run_args_etl)\nample, the configs/digital_data_etl_maxime_labonne.yaml configuration file looks as follows:\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nFigure 2.10: ZenML pipeline configs\nAs illustrated in Figure 2.11, we used Comet to track metrics such as training and evaluation loss \nFigure 2.11: Comet ML training metrics example\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your \nFigure 2.12: Comet ML system metrics example\nexperiment tracker, we made the training experiments tracked with Comet ML public while ",
    "keywords": [
      "Figure",
      "artifact",
      "ZenML",
      "Tooling and Installation",
      "metadata",
      "dataset",
      "instruct",
      "data",
      "etl",
      "datasets",
      "pipeline",
      "digital",
      "Comet",
      "Chapter",
      "config"
    ],
    "concepts": [
      "zenml",
      "dataset",
      "datasets trained",
      "train",
      "training",
      "tooling",
      "tools",
      "tool",
      "models",
      "model"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 76-83)",
    "start_page": 76,
    "end_page": 83,
    "summary": "You can read more on Opik at https://github.com/comet-ml/opik.\nDatabases for storing unstructured and vector data\nWe also want to present the NoSQL and vector databases we will use within our examples.\nMongoDB: NoSQL database\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before \nQdrant: vector database\nhttps://superlinked.com/vector-db-comparison, which compares all the top vector databases \nPreparing for AWS\nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have \none), an AWS access key, and the CLI.\nAlso, we will look into what SageMaker is and why we use it.\nWe picked AWS as our cloud provider because it’s the most popular out there and the cloud in \nBut for our MVP, AWS, it’s the perfect option as it provides robust features for every-\nSetting up an AWS account, an access key, and the CLI\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is \nby redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/\nAfter successfully creating an AWS account, you can access the AWS console at http://console.\naws.amazon.com.\nNext, we must generate access keys to access AWS programmatically.\nfirst to create an IAM user with administrative access as described in this AWS official tutorial: \nhttps://docs.aws.amazon.com/streams/latest/dev/setting-up.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html.\naws_access_key_id = <your_access_key_id>\naws_secret_access_key = <your_secret_access_key>\nAlso, be cautious with who you share them, as they could be used to access your AWS \naccount and manipulate various AWS resources.\nThe last step is to install the AWS CLI and configure it with your newly created access keys.\ncan install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/\nAfter installing the AWS CLI, you can configure it by running aws configure.\nof our AWS configuration:\naws_access_key_id = *************\naws_secret_access_key = ************\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://\ndocs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html.\nAWS_ACCESS_KEY=\"<your_aws_access_key>\"\nis an ML platform used to train and deploy ML models.\nSageMaker is a fully managed machine learning service by AWS that enables developers and data \nAll the cloud services used across the book stick to their freemium option, except AWS.\nThus, if you use a personal AWS account, you will be responsible for AWS costs as you \non our tests, the AWS costs can vary between $50 and $100 using the specifications \nSee the AWS documentation on setting up billing alarms to monitor your costs \nat https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\nand to deploy our custom LLM Twin model as a REST API that can be accessed in real time from \nWhy AWS SageMaker?\nWe must also discuss why we chose AWS SageMaker over simpler and more cost-effective options, \nsuch as AWS Bedrock.\nIt provides pre-trained models, which you can access directly \npre-trained models and APIs provided by Amazon Bedrock.\nMeanwhile, SageMaker provides a comprehensive platform for building, training, and deploying \ncomfortable working with cloud platforms such as AWS.\nregarding costs, following a pay-as-you-go pricing model similar to most AWS services.\ndeployed resources on AWS, such as online EC2 instances.\nployment, use EKS, AWS’s Kubernetes self-managed service.\nFinally, we explored the process of setting up an AWS account, generating an access \nkey, and configuring the AWS CLI for programmatic access to the AWS cloud.\ndeep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM \nhttps://github.com/comet-ml/opik\nhttps://neptune.ai/blog/ml-experiment-tracking\ncom/resources/basics/databases/nosql-explained\nneptune.ai/blog/ml-model-registry\nhttps://www.pinecone.io/learn/vector-database/\nhttps://superlinked.com/vector-db-comparison",
    "keywords": [
      "AWS",
      "AWS CLI",
      "AWS account",
      "AWS SageMaker",
      "access",
      "AWS access key",
      "vector database",
      "SageMaker",
      "LLM Twin",
      "vector",
      "key",
      "database",
      "NoSQL database",
      "Databases",
      "Bedrock"
    ],
    "concepts": [
      "aws",
      "com",
      "https",
      "ml",
      "sagemaker",
      "tool",
      "tooling",
      "tools",
      "managed",
      "manage"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 84-97)",
    "start_page": 84,
    "end_page": 97,
    "summary": "design and implement the data collection pipeline to gather the raw data we will use in all our \nThus, implementing our data pipeline will connect the dots \nimplement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, \nWe will show you how to implement various crawling methods, standardize the data, \nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture \nFinally, we will explore how to run the data collection pipeline using ZenML and query the col-\nDesigning the LLM Twin’s data collection pipeline\nImplementing the LLM Twin’s data collection pipeline\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, \nDesigning the LLM Twin’s data collection pipeline\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL \nunderstanding how our data collection pipeline maps to an ETL process.\nWe will crawl data from platforms like Medium, \nFor our project, we use MongoDB as our NoSQL data warehouse.\nFigure 3.1: LLM Twin’s data collection ETL pipeline architecture\nWe want to design an ETL pipeline that inputs a user and a list of links as input.\ncrawls each link individually, standardizes the collected content, and saves it under that specific \nauthor in a MongoDB data warehouse.\nHence, the signature of the data collection pipeline will look as follows:\nOutput: A list of raw documents stored in the NoSQL data warehouse\nWe will use user and author interchangeably, as in most scenarios across the ETL pipeline, a \nHowever, within the data warehouse, we have only \nThe ETL pipeline will detect the domain of each link, based on which it will call a specialized \ncrawler.\nWe implemented four different crawlers for three different data categories, as seen in \nFigure 3.2: The relationship between the crawlers and the data categories\nMedium crawler: Used to collect data from Medium.\nlogs in to Medium and crawls the HTML of the article’s link.\nsafety net when the link’s domain isn’t associated with the other supported crawlers.\nexample, when providing a Substack link, it will default to the custom article crawler, but \nGitHub crawler: This collects data from GitHub. It outputs a repository document.\nLinkedIn crawler: This is used to collect data from LinkedIn. It outputs multiple post \ndata from the MongoDB data warehouse, cleans it further, processes it into features, and stores it \ndata warehouse.\nThus, the data collection pipeline can write data for MongoDB, and the feature \nWhy did we use MongoDB as a data warehouse?\nMongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of docu-\ntured data: text crawled from the internet.\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s \nImplementing the LLM Twin’s data collection pipeline\nThus, let’s start by looking into the ZenML digital_data_etl pipeline.\nthis time, we will dig deeper into the implementation, explaining how the data collection works \ntation of each crawler used to collect data from various sites and the MongoDB documents used \nIn the code snippet below, we can see the implementation of the ZenML digital_data_etl\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user \nin our repository at pipelines/digital_data_etl.py.\nfrom steps.etl import crawl_links, get_or_create_user\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nlast_step = crawl_links(user=user, links=links)\nFigure 3.3 shows a run of the digital_data_etl pipeline on the ZenML dashboard.\nphase is to explore the get_or_create_user and crawl_links ZenML steps individually.\nFigure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\nWe will move on to the crawl_links ZenML step, which collects the data from the provided links.\nfrom llm_engineering.application.crawlers.dispatcher import \nthis function, a crawler dispatcher is initialized and configured to handle specific domains such \ndef crawl_links(user: UserDocument, links: list[str]) -> \nIt attempts to crawl and extract data for each link, updating the count of \nsuccessfull_crawl, crawled_domain = _crawl_link(dispatcher, link, \nstep_context.add_output_metadata(output_name=\"crawled_links\", \nappropriate crawler based on the link’s domain.\nextraction and returns a tuple indicating the crawl’s success and the link’s domain:\ndef _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: \ncrawler = dispatcher.get_crawler(link)\ncrawler_domain = urlparse(link).netloc\ncrawler.extract(link=link, user=user)\nreturn (True, crawler_domain)\nreturn (False, crawler_domain)\ndef _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) \nAs seen in the abovementioned _crawl_link() function, the CrawlerDispatcher class knows \nwhat crawler to initialize based on each link’s domain.\n3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers.\nThe CrawlerDispatcher class knows how to extract the domain of each link and initialize the \nproper crawler that collects the data from that site.\ncrawlers\nimporting our crawler classes:\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, Linke-\nwe will use the key of the dictionary as the domain pattern to match future links with a crawler:\ndef register(self, domain: str, crawler: type[BaseCrawler]) -> None:\n= crawler\nreturn crawler()\nThe next step in understanding how the data collection pipeline works is analyzing each crawler ",
    "keywords": [
      "Data",
      "data collection pipeline",
      "LLM Twin",
      "data collection",
      "data warehouse",
      "Data Engineering",
      "user",
      "ETL pipeline",
      "pipeline",
      "crawler",
      "LLM",
      "data collection ETL",
      "ETL",
      "link",
      "links"
    ],
    "concepts": [
      "data",
      "crawler",
      "crawlers",
      "get_crawler",
      "import",
      "importing",
      "imports",
      "pipeline",
      "pipelines",
      "user"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 98-105)",
    "start_page": 98,
    "end_page": 105,
    "summary": "Before exploring each crawler’s implementation, we must present their base class, which defines \nEach class implements the extract()\ncrawler.extract(link=link, user=user)\nBase classes\nclass BaseCrawler(ABC):\ndef extract(self, link: str, **kwargs) -> None: ...\ndefines a model attribute at the class level that represents the data category document type used \nWe also extend the BaseCrawler class with a BaseSeleniumCrawler class, which implements \nfrom selenium.webdriver.chrome.options import Options\nNext, we define the BaseSeleniumCrawler class for use cases where we need Selenium to collect \nFor the Selenium-based crawlers to work, you must install Chrome on your machine \nclass BaseSeleniumCrawler(BaseCrawler, ABC):\ndef __init__(self, scroll_limit: int = 5) -> None:\noptions.add_argument(\"--headless=new\")\noptions.add_argument(\"--log-level=3\")\noptions.add_argument(\"--disable-notifications\")\noptions.add_argument(\"--disable-extensions\")\noptions.add_argument(\"--ignore-certificate-errors\")\noptions.add_argument(f\"--user-data-dir={mkdtemp()}\")\noptions.add_argument(f\"--data-path={mkdtemp()}\")\noptions.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\nAfter configuring the Chrome options, the code allows subclasses to set any additional driver \nand creates a new instance of the Chrome driver with the specified options:\nself.set_extra_driver_options(options)\nThe BaseSeleniumCrawler class includes placeholder methods for set_extra_driver_options()\ndef set_extra_driver_options(self, options: Options) -> None:\ncontent to load, and repeats the process until it reaches the end of the page or the scroll limit is \ndef scroll_page(self) -> None:\nlast_height = self.driver.execute_script(\"return document.body.\nnew_height = self.driver.execute_script(\"return document.body.\nif new_height == last_height or (self.scroll_limit and \nWe’ve understood what the base classes of our crawlers look like.\nYou can find the implementation of the above crawlers in the GitHub repository at \nThe GithubCrawler class is designed to scrape GitHub repositories, extending the functionality \nclass GithubCrawler(BaseCrawler):\nNext, we implement the extract() method, where the crawler first checks if the repository has \ndef extract(self, link: str, **kwargs) -> None:\nold_model = self.model.find(link=link)\nlogger.info(f\"Repository already exists in the database: {link}\")\nIf the repository is new, the crawler extracts the repository name from the link.\nlogger.info(f\"Starting scrapping GitHub repository: {link}\")\nlogger.info(f\"Finished scrapping GitHub repository: {link}\")\nIt leverages the AsyncHtmlLoader class to read the entire HTML from a link and the \nHtml2TextTransformer class to extract the text from that HTML.\nfrom langchain_community.document_transformers.html2text import \nfrom .base import BaseCrawler\nwe don’t need to log in or use the scrolling functionality provided by Selenium.\nclass CustomArticleCrawler(BaseCrawler):\ndef extract(self, link: str, **kwargs) -> None:\nold_model = self.model.find(link=link)\nlogger.info(f\"Article already exists in the database: {link}\")\nclass, which returns a list of documents.\ngate the whole logic to these two classes, we don’t control how the content is extracted and parsed.\nWe get the page content from the extracted document, plus relevant metadata such as the title, \nWe then create a new instance of the article model, populating it with the extracted content.",
    "keywords": [
      "link",
      "class",
      "Options",
      "Selenium",
      "scroll",
      "options.add",
      "argument",
      "data",
      "import",
      "content",
      "Data Engineering",
      "repository",
      "extract",
      "crawler",
      "Chrome"
    ],
    "concepts": [
      "import",
      "imports",
      "options",
      "link",
      "self",
      "class",
      "classes",
      "content",
      "crawlers",
      "crawler"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 106-115)",
    "start_page": 106,
    "end_page": 115,
    "summary": "MediumCrawler class\nThe code begins by importing essential libraries and defining the MediumCrawler class, which \nfrom llm_engineering.domain.documents import ArticleDocument\nclass MediumCrawler(BaseSeleniumCrawler):\nWithin the MediumCrawler class, we leverage the set_extra_driver_options() method to extend \ndef extract(self, link: str, **kwargs) -> None:\ninstance, populates it with the extracted content and user information provided via kwargs, and \nThe last step is understanding how the document classes \nWe had to implement three document classes to structure our data categories.\nThese classes \nIt is best practice to structure your data in classes instead of dictionaries, as the attributes we \nour data items with classes, we can ensure each attribute is as expected.\nArticleDocument class\nPostDocument class\nRepositoryDocument class\nThese are not simple Python data classes or Pydantic models.\nall the document classes without repeating any code, we used the Object-Document Mapping\nand document classes.\nFor example, using SQLAlchemy, we defined a User ORM with the ID and name fields.\nORM is mapped to the users table within the SQL database.\nall the CRUD operations on top of the User class.\n# Define a class that maps to the users table.\nclass User(Base):\nUsing the User ORM, we can quickly insert or query users directly from Python without writing a \nshows how to save an instance of the User ORM to a SQLite database:\nThe ODM pattern is extremely similar to ORM, but instead of working with SQL databases and \nwork with NoSQL databases, the data structure is centered on collections, which store JSON-like \nTo conclude, ODM simplifies working with document-based NoSQL databases and maps ob-\nImplementing the ODM class\nThis section will explore how to implement an ODM class from scratch.\nclasses.\nHence, we will implement a base ODM class called NoSQLBaseDocument, from which all \nthe other documents will inherit to interact with the MongoDB data warehouse.\nNext, we define a type variable T bound to the NoSQLBaseDocument class.\nPython’s generic module, allowing us to generalize the class’s types.\nThe class can be found in our repository at llm_engineering/domain/base/nosql.\nWithin the NoSQLBaseDocument class, an id field is defined as a UUID4, with a default factory \nThe class also implements the __eq__ and __hash__ methods to allow \nif not isinstance(value, self.__class__):\nThe class provides methods for converting between MongoDB documents and class instances.\nfrom_mongo() class method transforms a dictionary retrieved from MongoDB into an instance of \nthe class.\ndef from_mongo(cls: Type[T], data: dict) -> T:\nreturn cls(**dict(data, id=id))\nThe save() method allows an instance of the model to be inserted into a MongoDB collection.\nretrieves the appropriate collection, converts the instance into a MongoDB-compatible document \ncollection = _database[self.get_collection_name()]\ncollection.insert_one(self.to_mongo(**kwargs))\nThe get_or_create() class method attempts to find a document in the database matching the \nIf a matching document is found, it is converted into an instance of the class.\nIf not, a new instance is created with the filter options as its initial data and saved to the database:\ndef get_or_create(cls: Type[T], **filter_options) -> T:\ninstance = collection.find_one(filter_options)\nreturn cls.from_mongo(instance)\nnew_instance = cls(**filter_options)\nThe bulk_insert() class method allows multiple documents to be inserted into the database \ndef bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\nlogger.error(f\"Failed to insert documents of type {cls.__name__}\")\nThe find() class method searches for a single document in the database that matches the given \ninstance = collection.find_one(filter_options)\nSimilarly, the bulk_find() class method retrieves multiple documents matching the filter options.\nIt converts each retrieved MongoDB document into a model instance, collecting them into a list:\ninstances = collection.find(filter_options)\nreturn [document for instance in instances if (document := cls.\nFinally, the get_collection_name() class method determines the name of the MongoDB collec-\nexception will be raised specifying that the subclass should define a nested Settings class:\n\"Document should define an Settings configuration class with ",
    "keywords": [
      "class",
      "user",
      "Data",
      "database",
      "ORM",
      "instance",
      "User ORM",
      "Python",
      "ODM",
      "ODM class",
      "Data Engineering",
      "class method",
      "return",
      "document",
      "options"
    ],
    "concepts": [
      "class",
      "class_",
      "data",
      "document classes",
      "importing",
      "import",
      "python",
      "method",
      "methods",
      "self"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 116-123)",
    "start_page": 116,
    "end_page": 123,
    "summary": "We can configure each subclass using the nested Settings class, such as defining the collection \nData categories and user document classes\nThese are the concrete classes that define our data categories.\nYou’ve seen these classes used across the chapter when working with articles, repositories, and \nWe define an enum class, where we centralize all our data category types.\nThe class can be found in the repository at llm_engineering/domain/types.py.\nNoSQLBaseDocument ODM class.\nclass Document(NoSQLBaseDocument, ABC):\nFinally, specific document types are defined by extending the Document class.\nclass ArticleDocument(Document):\nFinally, we define the UserDocument class, which is used to store and query all the users from the \nBy implementing the NoSQLBaseDocument ODM class, we had to focus solely on the fields and \nWith that, we’ve finished implementing our data collection pipeline, starting with the ZenML \nup with the ODM class and data category documents.\nThe last step is to run the data collection \npipeline and ingest raw data into the MongoDB data warehouse.\nZenML orchestrates the data collection pipeline.\nThus, leveraging ZenML, the data collection \nWe configured a different pipeline run for each author.\nPaul Iusztin’s or Maxime Labonne’s data.\nTo call the data collection pipeline to collect Maxime’s \ndata, for example, you can run the following CLI command:\npoetry poe run-digital-data-etl-maxime\nFigure 3.5 shows the user output artifact generated by this data collection pipeline.\nwe collected the links in this specific run.\nFigure 3.5: Example of the user output artifact after running the data collection pipeline using \nfrom which we collected data, the total number of links crawled for each domain, and the number \nFigure 3.6: Example of the crawled_links output artifact after running the data collection \nNow, we can download the crawled_links artifact anywhere in our code by running the following \nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML \n- https://medium.com/decodingml/sota-python-streaming-pipelines-for-\n- https://decodingml.substack.com/p/real-time-feature-pipelines-\nTo run the pipeline using Paul’s configuration, we call the following poe command:\npoetry poe run-digital-data-etl-paul\nfigured a command that calls the data collection pipeline for all the supported authors:\npoetry poe run-digital-data-etl\nWe can easily query the MongoDB data warehouse using our ODM classes.\nquery all the articles collected for Paul Iusztin:\nFirst article link: https://medium.com/decodingml/an-end-to-end-framework-\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any \nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB \nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML ",
    "keywords": [
      "data collection pipeline",
      "Data",
      "data collection",
      "collection pipeline",
      "class",
      "Paul Iusztin",
      "pipeline",
      "Data Engineering",
      "str class Settings",
      "collection",
      "run",
      "Paul",
      "user",
      "ODM",
      "class Settings"
    ],
    "concepts": [
      "data",
      "class",
      "run",
      "running",
      "user document classes",
      "mongodb",
      "https",
      "users",
      "user_full_name",
      "importing"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 124-131)",
    "start_page": 124,
    "end_page": 131,
    "summary": "the configs/ directory and find all the YAML files that start with digital_data_etl_*, such as \nFigure 3.7: Fix Selenium issues when crawling raw data\nproceed to the fine-tuning and inference sections without running the data collection ETL code.\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM \nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions \nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking \nprogrammatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.\nRAG Feature Pipeline\nRetrieval-augmented generation (RAG) is fundamental in most generative AI applications.\nRAG’s \ncore responsibility is to inject custom data into the large language model (LLM) to perform a \nthe LLM on data it wasn’t trained on (e.g., private or new data).\nWe will start this chapter with a theoretical part that focuses on the fundamentals of RAG and \nadvanced RAG system.\nThen, we will continue exploring LLM Twin’s RAG feature pipeline archi-\nFinally, we will go through a practical example by implementing the LLM Twin’s RAG \nUnderstanding RAG\nExploring the LLM Twin’s RAG feature pipeline architecture\nImplementing the LLM Twin’s RAG feature pipeline\nRAG Feature Pipeline\nUnderstanding RAG\nRAG enhances the accuracy and reliability of generative AI models with information fetched from \nGeneration: Use the augmented prompt with an LLM for generation\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized \nThe model is trained on data up to October 2023.\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and \nWhy use RAG?\nnecessary information into the prompt to answer the initial user question.\nthe augmented prompt to the LLM for the final answer.\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high \nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced con-\nRAG will act as the single source of truth for the generated answer.\nevaluate if the LLM’s answer is based on the external data or not.\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or \nDirectly injecting the necessary data to respond to user questions into the \nprompts that are fed to the LLM is enough to generate correct and valuable answers.\nright data into the prompt based on the user’s questions?\nof RAG in the next sections.\nRAG Feature Pipeline\nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and \nan LLM to generate answers\npopulate the vector DB with external data.\nThe generation pipelines use a prompt template, user input, and retrieved context to \nThe prompt is passed to an LLM to generate the answer.",
    "keywords": [
      "RAG feature pipeline",
      "RAG",
      "LLM",
      "LLM Twin",
      "RAG feature",
      "data",
      "feature pipeline",
      "RAG system",
      "pipeline",
      "answer",
      "Understanding RAG",
      "Selenium",
      "advanced RAG",
      "data collection pipeline",
      "RAG ingestion pipeline"
    ],
    "concepts": [
      "rag",
      "data",
      "pipeline",
      "pipelines",
      "https",
      "generative",
      "generated",
      "generate",
      "answer",
      "answered"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 132-139)",
    "start_page": 132,
    "end_page": 139,
    "summary": "retrieval, and generation pipelines.\nThe RAG ingestion pipeline extracts raw documents from various data sources (e.g., data ware-\nUltimately, it loads the embedded chunks into a vector DB (or other similar \nsource and embedding model.\npass the document’s content to an embedding model, this is necessary to ensure it doesn’t \nso, at the retrieval time, you will add only the essential data to the prompt.\nThe embedding component uses an embedding model to take the chunk’s content (text, \non embeddings in the What are embeddings?\nThe loading module takes the embedded chunks along with a metadata document.\nmetadata will contain essential information such as the embedded content, the URL to \nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, \nThe next step is to retrieve relevant data from the vector store correctly.\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the \nvector DB for similar vectors to the user’s input.\nThe primary function of the retrieval step is to project the user’s input into the same vector space \nas the embeddings used as an index in the vector DB.\nilar entries by comparing the embeddings from the vector storage with the user’s input vector.\nMost of the time, the cosine distance works well in non-linear complex vector spaces.\ndata and the embedding model you use.\nOne critical factor to highlight is that the user’s input and embeddings must be in the same vec-\npreprocess the user input in the same way you processed the raw documents in the RAG ingestion \nThis means you must clean, chunk (if necessary), and embed the user’s input using the \nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and \nThe final prompt results from a system and prompt template populated with the user’s query and \nretrieved_context = retrieve(user_question)\nprompt += prompt_template.format(context=retrieved_context, user_\nknow that a given answer was generated by a specific version of the LLM and prompt template(s).\nyour RAG system are the embeddings of the external data, usually stored in vector DBs, the em-\non what embeddings are and how they are computed.\nWhat are embeddings?\nsentations of objects encoded as vectors in a continuous vector space, such as words, images, or \nbeddings translate words into vectors where semantically similar words are positioned closely \ntogether in the vector space.\nFigure 4.2: What are embeddings?\nA popular method is visualizing the embeddings to understand and evaluate their geometrical \nAs the embeddings often have more than 2 or 3 dimensions, usually between 64 \nFigure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)\nmatical technique used to reduce the number of input variables or features in a data-\nWhy embeddings are so powerful\nEmbeddings come in handy when we want to feed words, images, or audio data into models.\nFor instance, when working with transformer models, you tokenize all your text input, where \nthe input to the transformer is a sequence of embeddings, which can be easily and confidently \nBased on this example, you can use embeddings to encode any categorical variable and feed it to \nSecondly, embedding your input reduces the size of its dimension and condenses all of its se-\nimages, where a CNN encoder module maps the high-dimensional meaning into an embedding, \nthat it can lead to a high-dimensional feature space if the categorical variable has \nEmbeddings help us encode categorical variables while controlling the output vec-",
    "keywords": [
      "RAG Feature Pipeline",
      "RAG ingestion pipeline",
      "RAG",
      "data",
      "embeddings",
      "RAG Feature",
      "vector",
      "user",
      "prompt",
      "input",
      "RAG ingestion",
      "Pipeline",
      "Feature Pipeline",
      "vectors",
      "Ingestion pipeline"
    ],
    "concepts": [
      "data",
      "vector",
      "vectors",
      "embedded",
      "embedding",
      "embeddings",
      "prompt",
      "retrieve",
      "retrieval",
      "retrieved"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 140-147)",
    "start_page": 140,
    "end_page": 147,
    "summary": "into the final vector embedding, a numerical image representation.\nFigure 4.4: Creating embeddings from an image using a CNN (Image source)\nEmbeddings are created by deep learning models that understand the context and semantics of \nVarious deep learning models can be used to create embeddings, varying by the data input type.\nembedding model.\nFor example, when working with text data, one of the early methods used to create embeddings \nture to smartly project your input into a dense vector space that can later be used as embeddings.\nTo quickly compute the embeddings in Python, you can conveniently leverage the Sentence \ned the embeddings for three sentences, and, ultimately, computed the cosine similarity between \n\"The dog is swimming.\"\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings, embeddings)\nLLM-Engineering/blob/main/code_snippets/08_text_embeddings.py.\nThe best-performing embedding model can change with time and your specific use case.\nfind particular models on the Massive Text Embedding Benchmark (MTEB) on Hugging Face.\nof the audio, such as a spectrogram, and then apply image embedding models to those visuals.\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the \nsame vector space.\nThis allows you to find similar images using a sentence as input, or the other \nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences.\nimg_emb = model.encode(image)\n# Output: tensor([[0.3068, 0.3300, 0.1719]])\nblob/main/code_snippets/08_text_image_embeddings.py.\nbetween two different data categories, such as the distance between the vector of a sentence and \nThese models are designed to project both data types into the same vector space, \nDue to the generative AI revolution, which uses RAG, embeddings have become extremely popu-\nlar in information retrieval tasks, such as semantic search for text, code, images, and audio, and \nThe last step to fully understanding how RAG works is to examine vector DBs and how they \nleverage embeddings to retrieve data.\nMore on vector DBs\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embed-\nTraditional scalar-based DBs struggle with the complexity of vector data, making vector \nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ \nVector DBs support CRUD operations, metadata \nHow does a vector DB work?\nVector DBs are different.\nUnder the hood, a vector DB uses \nIndexing vectors: Vectors are indexed using data structures optimized for high-dimen-\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those \nmost similar to the input vector.\nVector DBs can filter results based on metadata before or after the vector search.\n(along with the vector index), so it contains a metadata index user for filtering operations.\nAlgorithms for creating the vector index\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\nLSH: LSH maps similar vectors into buckets.\nThese algorithms enable vector DBs to efficiently handle complex and large-scale data, making \nVector DBs also share common characteristics with standard DBs to ensure high performance, \nRetrieval: This stage revolves around improving the embedding models and metadata \nfiltering to improve the vector search step.",
    "keywords": [
      "vector",
      "RAG Feature Pipeline",
      "vector DBs",
      "data",
      "embeddings",
      "RAG",
      "RAG Feature",
      "vectors",
      "image",
      "similarity",
      "DBs",
      "embedding",
      "advanced RAG",
      "Sentence",
      "models"
    ],
    "concepts": [
      "vector",
      "vectors",
      "data",
      "image",
      "images",
      "embedding",
      "embeddings",
      "models",
      "model",
      "rag"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 148-155)",
    "start_page": 148,
    "end_page": 155,
    "summary": "Pre-retrieval\nThe pre-retrieval steps are performed in two different ways:\nData indexing: It is part of the RAG ingestion pipeline.\nthe cleaning or chunking modules to preprocess the data for better indexing.\nQuery optimization: The algorithm is performed directly on the user’s query before em-\nbedding it and retrieving the chunks from the vector DB.\nAs we index our data using embeddings that semantically represent the content of a chunked \ndocument, most of the data indexing techniques focus on better preprocessing and structuring \nthe data to improve retrieval efficiency, such as:\nOptimizing index structures: It is based on different data index methods, such as various \nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used \nThus, using smaller chunks enhances the retrieval’s accuracy, \nOn the query optimization side, we can leverage techniques such as query routing, query rewriting, \nand query expansion to refine the retrieved information for the LLM further:\nQuery routing: Based on the user’s input, we might have to interact with different cate-\ngories of data and query each category differently.\ncan retrieve additional context from a vector DB using vector search queries, a standard \nHypothetical document embeddings (HyDE): This technique involves having an LLM \nthe query, for example, “Paris,” and add it to your filter to reduce your vector search space).\nBoth data indexing and query optimization pre-retrieval optimization techniques depend highly \nRetrieval\nThe retrieval step can be optimized in two fundamental ways:\nImproving the embedding models used in the RAG ingestion pipeline to encode the \ntime when you have to retrieve the most similar chunks based on user input.\nBoth strategies are aligned with our ultimate goal: to enhance the vector search step by leveraging \nthe semantic similarity between the query and the indexed data.\nWhen improving the embedding models, you usually have to fine-tune the pre-trained embedding \nInstead of fine-tuning the embedding model, you can leverage instructor models (https://\npinpoint accuracy and the retrieved information must include exact keyword matches, \nFiltered vector search: This type of search leverages the metadata index to filter for specific \nIt differs from a hybrid search in that you retrieve the data \nIn practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as \nPost-retrieval\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the \nTwo popular methods performed at the post-retrieval step are:\ninput and every retrieved chunk.\nthe data using a similarity distance between the embeddings and refine the retrieved \nFor example, if you work with multi-modal data such as text and images, most of the techniques \nvector indexing, adjusting user queries for more accurate searches, enhancing the embedding \nin mind, you can effectively optimize your RAG workflow for data processing and retrieval",
    "keywords": [
      "RAG Feature Pipeline",
      "Query",
      "Data",
      "search",
      "vector search",
      "RAG",
      "RAG Feature",
      "user",
      "LLM",
      "retrieval",
      "embedding",
      "vector",
      "model",
      "Feature Pipeline",
      "Query routing"
    ],
    "concepts": [
      "query",
      "queries",
      "retrieving",
      "retrieval",
      "retrieve",
      "retrieved",
      "search",
      "searching",
      "searches",
      "data"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 156-163)",
    "start_page": 156,
    "end_page": 163,
    "summary": "Exploring the LLM Twin’s RAG feature pipeline \nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a \nRAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) \nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and \nRAG Feature Pipeline\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and \nlogical feature store while processing the data accordingly.\nHaving the data in a feature store \nThe training pipeline will use the cleaned data from the feature store (stored \nThat is why we are designing a feature pipeline and not only a RAG ingestion \npipeline.\nbetween the data warehouse and the feature store goes into the feature pipeline namespace, con-\nin cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under \nthe feature pipeline umbrella as the artifacts are part of the logical feature store.\nwould be implementing a data validation pipeline on top of the raw data or computed features.\nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse.\nThe data \nwarehouse is populated by the data collection ETL pipeline presented in Chapter 3.\nDesigning the architecture of the RAG feature pipeline\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM \nWe will use a batch design scheduled to poll data from the MongoDB data \nFigure 4.9: The architecture of the LLM Twin’s RAG feature pipeline\nRAG Feature Pipeline\nBatch pipelines\nA batch pipeline in data systems refers to a data processing method where data is collected, pro-\nproach differs from real-time or streaming data processing, where data is processed continuously \nDuring this time, the collected data is processed in bulk.\nwarehouse, data lake, or feature store.\nBatch pipelines are particularly useful when dealing with large volumes of data that do not require \nEfficiency: Batch processing can handle large volumes of data more efficiently than re-\nComplex processing: Batch pipelines can perform complex data transformations and \nBatch versus streaming pipelines\nWhen implementing feature pipelines, you have two main design choices: batch and streaming.\nbatch architecture over a streaming one for our LLM Twin use case.\nTable 4.1 compares batch and streaming pipelines based on multiple criteria such as processing \nBatch pipeline\nStreaming pipeline\nProcesses data at regular \nProcesses data \nimmediate data processing \nand feature pipelines.\nTable 4.1: Batch versus streaming pipelines\nRAG Feature Pipeline\nFor example, streaming pipelines are extremely powerful in social media recommender systems \nBy implementing a streaming pipeline, you update \nactions as they occur, not after a few minutes or hours as a batch pipeline would process them.\nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load \nThe ETL design is widespread in data pipelines used to move data \nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline \nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming \nTwin’s feature pipeline for the following reasons:\nDoes not require immediate data processing: Even if syncing the data warehouse and \nThe whole data \nRAG Feature Pipeline\nbatch) and the quantity of data you have to process (small versus big data).\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\nIn the Change data capture: syncing the data warehouse and feature store section later in this chapter, \nMost of the RAG feature pipelines are composed of five core steps.\nRAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:",
    "keywords": [
      "RAG feature pipeline",
      "feature pipeline",
      "RAG feature",
      "data",
      "feature",
      "pipeline",
      "LLM Twin",
      "feature store",
      "RAG",
      "feature pipeline RAG",
      "pipeline RAG Feature",
      "data warehouse",
      "batch pipeline",
      "batch",
      "streaming"
    ],
    "concepts": [
      "data",
      "pipelines",
      "processing",
      "processes",
      "process",
      "processed",
      "feature pipeline",
      "features",
      "batch",
      "batches"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 164-171)",
    "start_page": 164,
    "end_page": 171,
    "summary": "Chunking: You must adopt various chunking strategies based on each data category \nThat is why you usually chunk a document based on your data \nData loading: The final step combines the embedding of a chunked document and its \nfor the features, we also push the cleaned documents (before chunking) to Qdrant.\npush data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB.\nRAG Feature Pipeline\nChange data capture: syncing the data warehouse and feature \ndata lakes, data warehouses, and feature stores getting out of sync.\nChange data capture (CDC) \nThe syncing issues also apply when building a feature pipeline.\nhow to sync the data warehouse with the feature store to have data fresh enough for your par-\nWhat happens if a record is deleted from the data warehouse?\nthe feature store?\nWhat if we want to process only the new or updated items from the data warehouse and \ndata changes.\nWith that in mind, there are different methods to detect changes in data.\nsource DB, captures all data changes, and requires no schema modification.\nin our RAG feature pipeline to sync the data warehouse and feature store more optimally when \nin the source DB; it just pulls everything from the data warehouse.\nFor more details on CDC, I recommend What is Change Data Capture?\nRAG Feature Pipeline\nWhy is the data stored in two snapshots?\nWe store two snapshots of our data in the logical feature store:\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the Mon-\nanother summarization use case where we must clean and preprocess the data differently.\ndata warehouse is generic and is modeled to specific applications only in downstream compo-\nBased on these factors, we decided to keep the cleaned data in Qdrant, \npipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, \nThis is a reminder that our logical feature store comprises the Qdrant vector \nZenML will orchestrate the batch RAG feature pipeline.\nit after the ETL data collection pipeline finishes.\nBy orchestrating the feature pipeline and integrating it into ZenML (or any other orchestration \nImplementing the LLM Twin’s RAG feature pipeline\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied every-\nThe cleaning, chunking, and embedding logic for all our data categories\nThus, let’s start with the Settings class and ZenML pipeline.\nRAG Feature Pipeline\nZenML pipeline and steps\nThe ZenML pipeline is the entry point for the RAG feature engineering pipeline.\nEngineers-Handbook/blob/main/pipelines/feature_engineering.py:\nfrom llm_engineering.interfaces.orchestrator.steps import feature_\ndef feature_engineering(author_full_names: list[str]) -> None:\nraw_documents = fe_steps.query_data_warehouse(author_full_names)\nlast_step_1 = fe_steps.load_to_vector_db(cleaned_documents)\nembedded_documents = fe_steps.chunk_and_embed(cleaned_documents)\nlast_step_2 = fe_steps.load_to_vector_db(embedded_documents)\nFigure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps \nFigure 4.12: Feature pipeline DAG in the ZenML dashboard\nRAG Feature Pipeline\nThe final puzzle piece is understanding how to configure the RAG feature pipeline dynamically.\nauthors of this book as we want to populate the feature store with data from all of us (available \nfeature_engineering.with_options(config_path=\"…/feature_engineering.yaml\")\nThus, we can call the feature engineering pipeline calling the \ncan run the feature pipeline using the following poe command:\npoetry poe run-feature-engineering-pipeline\nall the feature engineering pipeline steps is available on GitHub at \"steps/feature_engineering\".\nWe will begin with the first step, which involves querying the data warehouse for new content ",
    "keywords": [
      "RAG Feature Pipeline",
      "Feature Pipeline",
      "RAG Feature",
      "data",
      "Feature",
      "feature engineering pipeline",
      "Pipeline",
      "data warehouse",
      "feature store",
      "RAG",
      "embedding model",
      "Qdrant",
      "RAG feature engineering",
      "Change data capture",
      "model"
    ],
    "concepts": [
      "data",
      "pipeline",
      "step",
      "steps",
      "last_step_",
      "updates",
      "updated",
      "last_updated",
      "update",
      "based"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 172-179)",
    "start_page": 172,
    "end_page": 179,
    "summary": "It fetches all the raw data for the user from the data warehouse and extends the documents\nlist to include these user documents.\ndocuments = []\nlogger.info(f\"Querying data warehouse for user: {author_full_\nuser_documents = [doc for query_result in results.values() for doc \nstep_context.add_output_metadata(output_name=\"raw_documents\", \nmetadata=_get_metadata(documents))\nreturn documents\nThe _get_metadata() function takes the list of queried documents and authors and counts the \ndef _get_metadata(documents: list[Document]) -> dict:\nmetadata[collection][\"authors\"] = list()\nmetadata[collection][\"num_documents\"] = metadata[collection].\nmetadata[collection][\"authors\"].append(document.author_full_name)\nFor example, in Figure 4.13, we accessed the metadata tab of the query_data_warehouse()\ndocuments from three authors.\nCleaning the documents\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a \ndef clean_documents(\n) -> Annotated[list, \"cleaned_documents\"]:\ncleaned_documents = []\nstep_context.add_output_metadata(output_name=\"cleaned_documents\", \nmetadata=_get_metadata(cleaned_documents))\nreturn cleaned_documents\nThe computed metadata is similar to what we logged in the query_data_warehouse() step.\nChunk and embed the cleaned documents\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to \ncleaned_documents: Annotated[list, \"cleaned_documents\"],\n) -> Annotated[list, \"embedded_documents\"]:\nmetadata = {\"chunking\": {}, \"embedding\": {}, \"num_documents\": \nlen(cleaned_documents)}\nchunks = ChunkingDispatcher.dispatch(document)\nmetadata[\"chunking\"])\nmetadata[\"num_chunks\"] = len(embedded_chunks)\nstep_context.add_output_metadata(output_name=\"embedded_documents\", \nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step.\nFigure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized \nIn Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details \nFigure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding \nto group all the documents based on their data category.",
    "keywords": [
      "documents",
      "metadata",
      "step",
      "RAG Feature Pipeline",
      "data",
      "query",
      "Document",
      "list",
      "user",
      "chunks",
      "Feature Pipeline",
      "RAG Feature",
      "cleaned",
      "chunking",
      "data warehouse"
    ],
    "concepts": [
      "documents",
      "document",
      "metadata",
      "_get_metadata",
      "clean_documents",
      "cleaned_documents",
      "cleaned_document",
      "chunking",
      "chunk",
      "chunks"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 180-188)",
    "start_page": 180,
    "end_page": 188,
    "summary": "We decided to create a base class for each state of the document, resulting in having the following \nbase abstract classes:\nclass CleanedDocument(VectorBaseDocument, ABC)\nclass Chunk(VectorBaseDocument, ABC)\nclass EmbeddedChunk(VectorBaseDocument, ABC)\nNote that all of them inherit the VectorBaseDocument class, which is our custom OVM implemen-\nmakes the class abstract.\nThat is why base classes are always marked as abstract.\nEach base abstract class from above (which models the state) will have a subclass that adds \nabstract classes.\nWe will implement a specific document class for each data category and state com-\nThus, structuring the classes after the state allows us to plug another data \ncategory by inheriting these base abstract classes.\ncleaned document will be saved within the metadata of the vector DB.\nAnother fundamental aspect is the Config internal class, which defines the name of the collection \nwithin the vector DB, the data category of the entity, and whether to leverage the vector index \nclass CleanedDocument(VectorBaseDocument, ABC):\nclass Config:\nclass Config:\nclass Config:\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embed-\nclass Chunk(VectorBaseDocument, ABC):\nclass EmbeddedChunk(VectorBaseDocument, ABC):\nVectorBaseDocument OVM class.\nOur OVM base class is called VectorBaseDocument.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef from_record(cls: Type[T], point: Record) -> T:\nif cls._has_class_attribute(\"embedding\"):\npayload[\"embedding\"] = point.vector or None\nThe VectorBaseDocument class inherits from Pydantic’s BaseModel and helps us structure \nFor example, the from_record() method of the Chunk() class, \nThe from_record() method adapts a data point from Qdrant’s format to our internal structure \nThe bulk_insert() method maps each document to a point.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \ncls._bulk_insert(documents)\ncls._bulk_insert(documents)\nlogger.error(f\"Failed to insert documents in '{cls.get_\ndef _bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \nThe collection name is inferred from the Config class defined in the subclasses inheriting the OVM:\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef get_collection_name(cls: Type[T]) -> str:\nNow, we must define a method that lets us read all the records from the vector DB (without using \nThe function below scrolls the Qdrant vector DB, which returns a list of data \nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ndocuments, next_offset = cls._bulk_find(limit=limit, **kwargs)\nlogger.error(f\"Failed to search documents in '{cls.get_\nreturn documents, next_offset\ndef _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \nreturn documents, next_offset\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\ndef search(cls: Type[T], query_vector: list, limit: int = 10, \ndocuments = cls._search(query_vector=query_vector, \nlogger.error(f\"Failed to search documents in '{cls.get_\nreturn documents\ndef _search(cls: Type[T], query_vector: list, limit: int = 10, ",
    "keywords": [
      "class",
      "vector",
      "collection",
      "VectorBaseDocument",
      "RAG Feature Pipeline",
      "ABC",
      "documents",
      "VectorBaseDocument class",
      "class VectorBaseDocument",
      "type",
      "str class Config",
      "str",
      "OVM",
      "Qdrant",
      "class Config"
    ],
    "concepts": [
      "class",
      "classes",
      "document",
      "documents",
      "vector",
      "with_vectors",
      "cls",
      "type",
      "typed",
      "types"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 189-197)",
    "start_page": 189,
    "end_page": 197,
    "summary": "on to the dispatchers who clean, chunk, and embed the documents.\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, \nA handler can either clean, chunk, or embed a document.\nBased on its data category, it instantiates and calls a handler that \napplies the cleaning logic specific to that data point:\ndef dispatch(cls, data_model: NoSQLBaseDocument) -> \nhandler = cls.cleaning_factory.create_handler(data_category)\nclean_model = handler.clean(data_model)\n\"Data cleaned successfully.\",\nreturn clean_model\ncleaning handler based on the document’s data category:\ndef create_handler(data_category: DataCategory) -> \nYou have a single class that cleans any document, which \ncorrect handler based on the data category of the input document.\nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.\nInitially, we knew we wanted to clean the data, but as we knew the data category only at \ncleaning handler for its data type.\nThe last component of the RAG feature pipeline is the implementation of the cleaning, chunking, \nIn total, we will have nine Handler classes that follow the next \nUntil now, we have just modeled our entities and how the data flows in our appli-\nWe haven’t written a single piece of cleaning, chunking, or embedding code.\nThe cleaning handlers\ndef clean(self, data_model: DocumentT) -> CleanedDocumentT:\ndef clean(self, data_model: PostDocument) -> CleanedPostDocument:\ncontent=clean_text(\" #### \".join(data_model.content.\ndef clean(self, data_model: ArticleDocument) -> \ndef clean(self, data_model: RepositoryDocument) -> \ncontent=clean_text(\" #### \".join(data_model.content.\nThe handlers input a raw document domain entity, clean the content, and return a cleaned docu-\nAll the handlers use the clean_text() function to clean the text.\nthe same cleaning technique for all the data categories.\nto further optimize and create a different cleaning function for each data category.\ncleaned data used for fine-tuning will be accessed from the logical feature store, making it the \nThe chunking handlers\nchunking logic.\nThe handler takes cleaned documents as input and returns chunk entities.\n\"chunk_size\": 500,\ndef chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:\nThe handler’s chunk() method inputs cleaned article documents and returns a list of article chunk \nIt uses the chunk_text() function to split the cleaned content into chunks.\nThe chunking \nThe chunk_id \na list of chunk entities and return them.\ndef chunk(self, data_model: CleanedArticleDocument) -> \ndata_models_list = []\ncleaned_content = data_model.content\ncontent=chunk,\ndocument_id=data_model.id,\nreturn data_models_list\nIt groups sentences into a single chunk until the max_length limit is reached.\ndef chunk_article(text: str, min_length: int, max_length: int) -> \ncurrent_chunk = \"\"\nif len(current_chunk) + len(sentence) <= max_length:\ncurrent_chunk += sentence + \" \"\ncurrent_chunk = sentence + \" \"\nengineering/application/preprocessing/chunking_data_handlers.py, have a similar struc-\nThe chunk_text() function is a two-step process that has \nAt this point, we also apply the chunk_overlap logic, as we want to do it only after we ",
    "keywords": [
      "data",
      "chunk",
      "RAG Feature Pipeline",
      "data category",
      "handler",
      "RAG Feature",
      "Feature Pipeline",
      "clean",
      "class",
      "category",
      "text",
      "handlers",
      "model",
      "length",
      "return"
    ],
    "concepts": [
      "class",
      "classes",
      "clean",
      "cleaning",
      "cleaned",
      "cleans",
      "chunk",
      "chunking",
      "chunks",
      "data_model"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 198-205)",
    "start_page": 198,
    "end_page": 205,
    "summary": "tokens_per_chunk=embedding_model.max_input_length,\nmodel_name=embedding_model.model_id,\nparameters and the embedding model’s max input length.\nWe took this approach because, when calling the embedding model, \nWe implemented an embed() method, in case you want to run the inference on a single data point, \ngathers their content into a list, passes them to the embedding model, and maps the results to an \nembedding_model = EmbeddingModelSingleton()\nAbstract class for all embedding data handlers.\nAll data transformations logic for the embedding step is done here\ndef embed(self, data_model: ChunkT) -> EmbeddedChunkT:\nreturn self.embed_batch([data_model])[0]\ndef embed_batch(self, data_model: list[ChunkT]) -> \nembedding_model_input = [data_model.content for data_model in \ndata_model]\nembeddings = embedding_model(embedding_model_input, to_list=True)\nembedded_chunk = [\nself.map_model(data_model, cast(list[float], embedding))\nfor data_model, embedding in zip(data_model, embeddings, \nreturn embedded_chunk\ndef map_model(self, data_model: ChunkT, embedding: list[float]) -> \nAs you can see, we only have to implement the map_model() method, which \ntakes a chunk of input and computes the embeddings in batch mode.\ndef map_model(self, data_model: ArticleChunk, embedding: list[float]) \nid=data_model.id,\ncontent=data_model.content,\nplatform=data_model.platform,\nlink=data_model.link,\ndocument_id=data_model.document_id,\nauthor_id=data_model.author_id,\nauthor_full_name=data_model.author_full_name,\n\"embedding_model_id\": embedding_model.model_id,\n\"embedding_size\": embedding_model.embedding_size,\n\"max_input_length\": embedding_model.max_input_length,\nmodel.\nallowing us to quickly test multiple embedding models just by changing the configuration file \nThat is why I am not insisting at all on what embedding model to use.\nclass, which can quickly be configured, you can experiment with multiple embedding models \nmodel_id: str = settings.TEXT_EMBEDDING_MODEL_ID,\ndevice: str = settings.RAG_MODEL_DEVICE,\nself._model_id = model_id\nself._model = SentenceTransformer(\nself._model_id,\nself._model.eval()\ndef model_id(self) -> str:\nreturn self._model_id\ndef embedding_size(self) -> int:\ndummy_embedding = self._model.encode(\"\")\nreturn self._model.max_seq_length\nreturn self._model.tokenizer\nembeddings = self._model.encode(input_text)\nlogger.error(f\"Error generating embeddings for {self._model_\nThe embedding model class implements the singleton pattern (https://refactoring.guru/\nembedding techniques based on the data category of each document.\nKenton, J.D.M.W.C. and Toutanova, L.K., 2019, June.",
    "keywords": [
      "model",
      "embedding",
      "RAG Feature Pipeline",
      "embedding model",
      "data",
      "RAG",
      "RAG Feature",
      "list",
      "embeddings",
      "self.",
      "multiple embedding models",
      "input",
      "Feature Pipeline",
      "tokens",
      "class"
    ],
    "concepts": [
      "embedding",
      "embedded",
      "embeddings",
      "self",
      "data_model",
      "data",
      "return",
      "returns",
      "model_name",
      "model"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 206-214)",
    "start_page": 206,
    "end_page": 214,
    "summary": "the model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.\nCreating a high-quality instruction dataset\nCreating an instruction dataset\nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning \nMoreover, the quality of the data is also crucial.\nThis careful review helps ensure that the dataset is accurate and useful for training the model.\nIn this section, we will introduce a general framework to create your own instruction datasets, \ninputs of the model, used as context during fine-tuning.\nDuring fine-tuning, you can choose to train the model on the instructions and answers, \nIn this case, “inputs” contain the data the model \nTable 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\nThis example illustrates how the “system” field is used to define specific behaviors for the model, \nThe “instruction” field provides the necessary data (the concepts) and the task \nTo build an instruction dataset, we want to curate data that is representative of how the model will \nOnce we have gathered enough samples, our goal is to filter them to only keep high-quality \ndata.\nIn this context, high-quality data can be described through three main dimensions:\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering \nBy sampling data in a representative \nIn the following sections, we will see techniques to filter and evaluate instruction samples ac-\nit with high-quality data.\nCalculating an ideal number of samples is a difficult task, as both the quality of the data and the \nquality of the data is a crucial factor, and a high number of samples is always desirable.\nTo provide additional numbers, we can look at the fine-tuned models developed by companies \naimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, \nGeneral-purpose models cover more topics, which requires additional samples.\nsource community, models like OpenHermes and Dolphin use around one million samples.\non the quality of these finetunes, we recommend an instruction dataset of at least one million \nsamples to create a good general-purpose instruct model.\nOn the other hand, models fine-tuned \nHere, we differentiate task-specific models from \nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs.\nTask-specific models are designed to excel at a particular function, such as translation, summari-\nThe data required for task-specific fine-tuning is generally more manageable, \nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge \nThe data requirements for domain-specific fine-tuning can vary widely depending on the com-\nSome fields, like medicine or law, may require as much data \nor hospitality, might need fewer samples, more in line with task-specific fine-tuning.\nThe key factors determining the data needs for domain-specific models are the “size” of the \nthat domain in the model’s pre-training data.\ntraining data may require less fine-tuning, while those that are more specialized or underrep-\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and \ndomain-specific models.\nFor task-specific models, data curation often involves collecting examples \nDomain-specific data curation can be more challenging.\nrelevance of this data is crucial, as it directly impacts the model’s ability to understand and gen-\nerful models by providing a few examples of the desired task within the input prompt.\nIn practice, the line between task-specific and domain-specific models can sometimes blur.\ninstance, a model fine-tuned for medical diagnosis could be considered both task-specific (focused \nnext step consists of refining the quality of the samples through rule-based filtering, data dupli-\ncation, data decontamination, and data quality evaluation.\nRule-based filtering is a systematic approach to data quality control that relies on explicit, pre-\ndefined rules to evaluate and filter data samples.\nprimary goal of rule-based filtering is to maintain a high standard of data quality by removing \nFormat checking is recommended for datasets that include structured data or follow specific \nticularly important for datasets containing code samples, JSON structures, or other formatted \nRule-based filtering offers significant advantages in preparing instruction datasets.\nfiltering reduces the need for manual intervention and enables continuous data quality monitoring.\nData deduplication\nDataset diversity is fundamental to training models that can generalize well to new, unseen data.\nOverfitting: Models may memorize specific examples rather than learning general patterns.\nBiased performance: Overrepresented data points may skew the model’s performance \ntion removes identical samples through a straightforward process involving data normalization, \nData decontamination is the process of ensuring that the training dataset does not contain samples \nData decontamination uses techniques from data deduplication.\nAnother aspect of data decontamination is filtering out samples that may have been derived from \nthe same source as evaluation data.\nthe data they use) to identify and exclude data from specific sources that are known to be used \ninstruction dataset during the data deduplication stage.\nensure that we only remove samples from the instruction dataset, which can be ",
    "keywords": [
      "data",
      "samples",
      "models",
      "model",
      "instruction dataset",
      "instruction",
      "dataset",
      "Fine-Tuning",
      "datasets",
      "domain-specific models",
      "quality",
      "filtering",
      "data quality",
      "data decontamination",
      "rule-based filtering"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "samples",
      "sample",
      "sampling",
      "specific",
      "like",
      "datasets",
      "instruction dataset"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 215-222)",
    "start_page": 215,
    "end_page": 222,
    "summary": "Data quality evaluation\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process \nTraditional methods of data quality assessment include human annotation, which generally \nas judges, reward models, and classifiers trained for quality prediction.\nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample.\nYou are a data quality evaluator.\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\nReward models are another way to re-purpose LLMs for data quality evaluation.\ncan be broadly defined as models that take an instruction and answer pair and return a score as \nThis allows for a more fine-grained approach to data quality evaluation.\ntypes of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set \nstruction data quality, it is a good resource for finding models capable of differentiating between \nClassifiers or encoder-only models can be trained to perform data quality evaluation.\nThis model was designed as a quality filter for pretraining data but a similar \napproach can be taken to evaluate instruction samples at scale.\ner-only models are still valuable to filter out outliers or as part of an automated data pipeline, \nData exploration\ntraining data.\ninconsistencies that automated processes might miss, including formatting issues, data entry \nFigure 5.4 shows an example with Argilla, a collaborative platform for manual data quality eval-\nFigure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\nIt is often associated with data visualization, with figures that show clusters \nLet’s consider the task of building an instruction dataset about various programming languages.\nidentify sub-topics like error handling, data structures, and web frameworks.\nData generation\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary.\nWhile data can be \nSynthetic data generation using LLMs offers a more efficient \ncan produce high-quality data at a much larger scale, effectively addressing the limitations of \nmanual data creation processes.\nThe process of synthetic data generation typically begins with the preparation of a set of carefully \nThe quality of synthetically generated data largely depends on the prompts and techniques \ndiverse, relevant, and high-quality instruction-response pairs.\ncific instructions, examples, and constraints to ensure the generated data aligns with the desired \nname],\\n\\nI’m writing to ask you if you are happy to be a panelist in our workshop on \nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality.\nmodel or set of rules checks the generated pairs for accuracy, relevance, and adherence to spec-\nAn important aspect of synthetic data generation is the ability to control various attributes of the \ngenerated data.\nFurthermore, synthetic data generation can be particularly useful for addressing biases and gaps \nHowever, synthetic data generation also comes with challenges.\npotential for the generated data to inherit biases or errors from the underlying language model \ngenerated data.\nAnother consideration is the need for the generated data to be sufficiently diverse and challeng-\nIf the synthetic data is too simplistic or repetitive, it may not provide the level of complexity \nAdvanced techniques in synthetic data generation often focus on \nData augmentation\nthe quality of data samples.\nUnlike data generation, we use pre-existing instruction samples \nWhile it is possible to upsample pairs of instructions and answers, data \nThe evolved instructions can then be used to generate ",
    "keywords": [
      "Data",
      "Data quality evaluation",
      "Data quality",
      "Synthetic data generation",
      "quality",
      "Data generation",
      "quality evaluation",
      "Synthetic data",
      "models",
      "LLMs",
      "generated data",
      "answer",
      "model",
      "generation",
      "evaluation"
    ],
    "concepts": [
      "data",
      "models",
      "model",
      "instruction",
      "instruct",
      "instructions",
      "like",
      "answer",
      "answers",
      "generative"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 223-230)",
    "start_page": 223,
    "end_page": 230,
    "summary": "In-depth evolving focuses on enhancing the complexity of existing instructions.\ninstruction, making it more challenging to fulfill.\nprecision to the instruction.\nIncreasing reasoning steps: It modifies instructions to explicitly request multiple-step \nComplicating input: This involves adding more complex data formats or structures to \nthe instruction, such as XML, JSON, or code snippets.\nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset.\nIt generates entirely new instructions inspired by existing ones, focusing on creating more rare \nYou simply need to provide the instruction you want \nto evolve as input, and a powerful model like GPT-4o will return a more complex version of the \noriginal instruction.\nYou are an Instruction Rewriter that rewrites the given #Instruction# into a more complex \nPlease follow the steps below to rewrite the given “#Instruction#” into a more complex \nStep 1: Please read the “#Instruction#” carefully and list all the possible methods \nto make this instruction more complex (to make it a bit harder for well-known AI \nchange the language of the instruction!\nStep 2: Please create a comprehensive plan based on the #Methods List# generated \nin Step 1 to make the #Instruction# more complex.\nStep 3: Please execute the plan step by step and provide the #Rewritten Instruction#.\n#Rewritten Instruction# can only add 10 to 20 words into the “#Instruction#”.\nStep 4: Please carefully review the #Rewritten Instruction# and identify any \nEnsure that the #Rewritten Instruction# is only a more complex \nversion of the #Instruction#.\nJust provide the #Finally Rewritten Instruction# without \nStep 3 #Rewritten Instruction#:\nStep 4 #Finally Rewritten Instruction#:\n#Instruction#:\n{Instruction}\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language \nof instruction quality.\nUnlike Evol-Instruct, which evolves instructions, UltraFeedback uses a large pool of \ndiverse instructions and models to generate a wide range of responses.\nscores for these responses across multiple dimensions such as instruction-following, truthfulness, \ning and diverse instruction dataset.\nBy refining and evolving existing instructions and answers, \nthe resulting dataset can better train models to handle complex, multi-step tasks, and improve \nCreating our own instruction dataset\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter \n3. To create a high-quality instruction dataset, we need to address two main issues: the unstruc-\nof pairs of instructions and answers.\ning its corresponding instruction.\nHowever, using a chunk of text like a paragraph as an answer \nThe number of articles we can retrieve is limited, which constrains the size of the instruction \ninto chunks and generate three instruction-answer pairs for each chunk.\nwill do it using OpenAI’s GPT-4o-mini model, but you can also use open-source models.\nspecific templates or instructions, there’s no guarantee that the model will consistently adhere \nwill use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON \nFigure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\nallow us to interact with a model to generate the instruction data, and datasets will format \n2. We import all the required libraries as follows.\nimport json\nimport re\nfrom datasets import Dataset\nThe raw data we have is a JSON file.\nWe create a Hugging Face dataset from this JSON file \nby extracting specific fields from each article: id, content, platform, author_id, author \ndef load_articles_from_json(file_path: str) -> Dataset:\ndata = json.load(file)\ndata\"]],\ndata\"]],\nsubstack.com/p/\nsubstack.com/p/\nsubstack.com/p/\nNow that we can load our articles, we need to chunk them before turning them into pairs \nof instructions and answers.\nInstead, we will extract sentences using a regex to get chunks between 1,000 \nThe extract_substrings function processes each article in the dataset by first cleaning the \nfor article in dataset[\"content\"]:\nNext, we want to create instruction-answer pairs from the extracted chunks of text.\npairs = [(pair['instruction'], pair['answer'])\nfor pair in data['instruction_answer_pairs']]\nan LLM to transform them into pairs of instructions and answers.",
    "keywords": [
      "instruction",
      "Rewritten Instruction",
      "instruction dataset",
      "data",
      "Finally Rewritten Instruction",
      "dataset",
      "JSON",
      "text",
      "Step",
      "item",
      "Rewritten",
      "complex",
      "Methods List",
      "pairs",
      "list"
    ],
    "concepts": [
      "instructions",
      "instruction",
      "data",
      "import",
      "importance",
      "important",
      "dataset",
      "datasets",
      "model",
      "models"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 231-238)",
    "start_page": 231,
    "end_page": 238,
    "summary": "In our example, we want to create instructions like “Write a paragraph about X topic” and \nalso choose to generate five instruction-answer pairs for each extract.\nof our function for instruction generation, including our prompt.\ndef generate_instruction_answer_pairs(\ninstruction-answer pairs.\nEach instruction \\\ninstructions.\n\"instruction_answer_pairs\": [\n{{\"instruction\": \"...\", \"answer\": \"...\"}},\n4o mini model in JSON mode and a maximum of 1,200 tokens in the answer.\nparsed using the InstructionAnswerSet class to return pairs of instructions and answers.\ngenerates instruction-answer pairs based on the given \ngenerate instruction-answer pairs for each extract.\ndef create_instruction_dataset(\n) -> Dataset:\ninstruction_answer_pairs = []\nfutures = [executor.submit(generate_instruction_answer_\ninstruction_answer_pairs.extend(future.result())\ninstructions, answers = zip(*instruction_answer_pairs)\n{\"instruction\": list(instructions), \"output\": list(answers)}\nWe can create our instruction dataset by calling this function.\ndata, creates the instruction dataset, splits it into training and testing sets, and pushes \ninstruction_dataset = create_instruction_dataset(raw_dataset, \nprint(\"Instruction dataset:\")\nprint(instruction_dataset.to_pandas())\nfiltered_dataset = instruction_dataset.train_test_split(test_\nDataset({\ndataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are \nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub\nAs seen in the previous section, we could refine this instruction dataset by increasing the diver-\nconciseness and simplicity, we will keep a straightforward approach for this instruction dataset \nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instruc-\nThe goal of SFT is to turn a base model, which can only perform next-token \nprediction, into a useful assistant, capable of answering questions and following instructions.\nmodels in this book, several LLM providers offer automated fine-tuning services.\nEven worse, a study showed that fine-tuning a model on new knowledge could result in more \nInstruction dataset formats\nInstruction datasets are stored in a particular format to organize instructions and answers.\ncally, each sample in the dataset can be represented as a Python dictionary, where keys are prompt \nTable 5.5 – Examples of instruction data storage format\non raw text, this is a type of fine-tuning generally called “continual pre-training.”\nswers, which means it is limited to one instruction and one answer.\nconversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better \nOnce the instruction-answer pairs are parsed from the dataset format, we want to structure them \nChat templates offer a unified way to present the instructions and answers \nSince base models are not designed to follow instructions, they \nIf you want to fine-tune an instruct model (not recommended), you need to use \nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and \ntemplate to the instruction-answer pair shown in Table 5.1:\ninput by the model during fine-tuning.\nsystem and user part as shown in Figure 5.6, and prompt the model to answer by adding <|im_\nBecause the model has been fine-tuned with this template, it understands that the next tokens \nshould be an answer relevant to the user instruction and guided by the system prompt.\nhow fine-tuned models acquire instruction-following capabilities.",
    "keywords": [
      "instruction",
      "dataset",
      "instruction dataset",
      "SFT",
      "model",
      "Instruction dataset formats",
      "answer",
      "pairs",
      "extract",
      "prompt",
      "Fine-Tuning",
      "answers",
      "Hugging Face Hub",
      "system",
      "raw"
    ],
    "concepts": [
      "instruction",
      "instruct",
      "instructions like",
      "dataset",
      "datasets",
      "answers",
      "answer",
      "answering",
      "model",
      "models"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 239-247)",
    "start_page": 239,
    "end_page": 247,
    "summary": "<start_of_turn>model\nParameter-efficient fine-tuning techniques\nfine-tuning, LoRA, and QLoRA.\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every \nparameter in the base model.\nmain difference between continual pre-training and full fine-tuning.\nMemory usage depends on several factors, including model size, training techniques, and op-\nmodel, these are typically the weights in the attention mechanisms, feed-forward layers, \nmodel parameter.\nDuring training, gradients are computed for each parameter through backprop-\nagation and are used to update the model parameters.\nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning.\nModel \naccumulation enables larger effective batch sizes without proportional memory increase.\nwith model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the \nHowever, memory requirements remain substantial for large models even with \nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive \nare often preferred to full fine-tuning to create task and domain-specific models.\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the compu-\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced \nDramatically reduced memory usage during training\nPreservation of pre-trained model weights (non-destructive)\nwith limited computational resources, effectively democratizing the process of LLM fine-tuning.\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently.\nFigure 5.10 – LoRA adds the two trainable matrices 𝐴𝐴 and 𝐵𝐵 and keeps the pre-trained weights \nmemory savings and faster training times.\nLoRA can be applied to various parts of the model architecture.\nLoRA’s application to other key components of the model.\ncreases the number of trainable parameters and, consequently, the memory requirements.\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-\nto full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable \nparameters, LoRA drastically reduces the number compared to full fine-tuning.\nLoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\nMultiple sets of LoRA weights can be combined for different tasks or domains, allowing \nallows developers to fine-tune models on relatively small, widely available GPUs. The core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit \nof updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-\nrank matrices (adapters) to specific layers of the model.\ntraining, while the original model weights remain unchanged.\nAdditionally, it uses paged optimizers to manage memory spikes during training by leveraging \nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory \nFor example, for a 7B model, QLoRA reduces peak memory usage from 14 GB \nDuring fine-tuning, the memory savings increase \nthe cost of increased training time, with QLoRA being about 30% slower than LoRA.\nmodel performance, QLoRA shows only minor differences compared to LoRA.\nsuch as when working with very large models or on hardware with limited GPU memory.\nif training speed is crucial and sufficient memory is available, LoRA might be the preferred choice.\nproject, available hardware, and the need to balance memory usage, training speed, and model \nTraining parameters\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly \nA common starting point for transformer models is often around 1e-5.\nThe learning rate scheduler adjusts the learning rate throughout the training process.\nlater stages to fine-tune the model more precisely.\nThe specific values and decay schedule depend on your model \nThe batch size determines the number of samples processed before the model’s weights are up-\nTypical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, \nFor instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while \nTo overcome memory constraints while still benefiting from larger batch sizes, a technique called \nupdate to the model’s parameters.\nmodels or limited GPU memory.\nand then update the model as if you had processed all 32 samples at once.\nchoosing the number of steps, consider the trade-off between training speed and memory usage.\nMore accumulation steps allow for larger effective batch sizes but increase the time required for \nThe maximum sequence length determines the longest input the model can process.\nThis parameter directly impacts batch size and memory usage; a batch \nthis parameter with your GPU capabilities and the nature of your training data to optimize per-\nFor LLM fine-tuning, the typical range is 1 to 10 epochs, \ntask complexity, dataset size, and model architecture.\nFor example, a large model fine-\ntuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger \ntraining and implement early stopping if the model’s performance plateaus or degrades.\nOptimizers adjust the model’s parameters to minimize the loss function.\nmemory (but it doesn’t improve training speed).\nweight decay regularization, often leading to better training stability and model performance.\nIn situations involving extremely large models or limited GPU memory, paged ",
    "keywords": [
      "model",
      "Memory",
      "capital of France",
      "LoRA",
      "training",
      "Fine-Tuning",
      "GPU memory",
      "Batch size",
      "batch",
      "Memory usage",
      "France",
      "parameter",
      "LLM fine-tuning",
      "GPU",
      "Parameters"
    ],
    "concepts": [
      "lora",
      "memory",
      "training",
      "model",
      "models",
      "parameter",
      "parameters",
      "tuning",
      "tuned",
      "qlora"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 248-255)",
    "start_page": 248,
    "end_page": 255,
    "summary": "model to learn simpler, more generalizable features.\nfor the model to capture important patterns in the data.\nmodel architecture and dataset, so it’s generally a good practice to experiment with different \nGradient checkpointing is a technique that reduces memory consumption during training by stor-\nLet’s now fine-tune an open-source model on our custom dataset.\nThere are many efficient open-weight models we can leverage for task or domain-specific use \nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and \nPerformance: Evaluating the base model on general-purpose benchmarks or, even better, \nensure that the model has the necessary capabilities to perform well on the intended \nIn this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta.\nThere are specialized tools and libraries to fine-tune models.\nTRL: This is a library created and maintained by Hugging Face to train LLMs using SFT \ntraining (2-5x) and reduce memory use (up to 80% less memory).\nTo maximize efficiency, we will perform fine-tuning using the Unsloth library.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nLet’s now load the model to fine-tune and its corresponding tokenizer.\nthe load_in_4bit argument indicates if we want to use QLoRA (quantized pre-trained \nWe’ll use LoRA in this example because of faster training and higher quality, but you can \nmodel, tokenizer = FastLanguageModel.from_pretrained(\nNow that the model is loaded, we can define our LoRA configuration.\nFinally, we target every linear layer to maximize the quality of the fine-tuning process.\nmodel,\nNext, we need to prepare the data in the right format for fine-tuning.\nbecause the model might not correctly learn the chat template.\nthe 100,000 samples of this dataset, we will specify we only want 10,000 in the train split.\nWe concatenate these two datasets to create our final set.\ntence (EOS) token at the end of each message to ensure that the model learns to output \n8. Once the dataset is ready, we can divide it into training (95%) and test (5%) sets for val-\ndataset = dataset.train_test_split(test_size=0.05)\nThe model is now ready to be trained.\nfor our training.\nIn addition, we provide the model, tokenizer, LoRA configuration, and \nWe train \nthis model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for \nFinally, we report our training run to Comet ML for experiment tracking.\nmodel=model,\ntrain_dataset=dataset[\"train\"],\ntrainer.train()\nTraining this model on our concatenated dataset can take a few hours.\nthe fine-tuned model, but to make sure that there are no obvious errors related to the \nThis forces the model to answer the instruction instead of completing it.\nFastLanguageModel.for_inference(model)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\nHere is the answer provided by our model:\nSupervised fine-tuning is a method used to enhance a language model \nThis process is designed to align the model's \nNow that our model has been successfully fine-tuned, we can save it locally and/or push \nmodel.save_pretrained_merged(\"model\", tokenizer, save_\nmodel.push_to_hub_merged(\"mlabonne/TwinLlama-3.1-8B\", tokenizer, \nCongratulations on fine-tuning a base model from scratch!\nML to monitor your training loss, validation loss, and many other metrics.\nFigure 5.11 – Four monitored metrics during fine-tuning in Comet ML\nTraining loss: It measures how well the model is performing on the task it’s being trained \nand continuous increases in the loss value are signs that the training is failing.\nValidation loss: It measures the loss using the validation set instead of the training set; \na well-fitted model typically shows both training and validation losses decreasing and \nis expected to exist as the model will always perform slightly better on the training data.\nIf the training loss continues to decrease while the validation loss starts to increase, it’s a \nThis is still acceptable but might indicate that the model starts to overfit.\ngradient norms can indicate training instability like overfitting, especially if accompanied \nby a divergence between training and validation losses.\ndecreasing gradient norm generally means that the model is converging toward a local \nIt is often interesting to try different learning rates and select the best model based on the minimal \namined the instruction data pipeline and how to create high-quality datasets, from curation ",
    "keywords": [
      "model",
      "training",
      "dataset",
      "Weight decay",
      "loss",
      "Fine-tuning",
      "Gradient",
      "Supervised Fine-Tuning",
      "Unsloth",
      "Weight",
      "decay",
      "tokenizer",
      "models",
      "Comet",
      "training loss"
    ],
    "concepts": [
      "model",
      "models",
      "model_name",
      "dataset",
      "datasets",
      "training",
      "train",
      "trained",
      "tokens",
      "token"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 256-265)",
    "start_page": 256,
    "end_page": 265,
    "summary": "a Llama 3.1 8 B model on our custom instruction dataset.\nIn the next chapter, we will use preference alignment techniques to create a new version of Twin-\nWe will generate a new dataset with chosen and rejected answers that will help us \never, SFT struggles to capture the nuances of human preferences and the long tail of potential \nadvanced techniques for aligning AI systems with human preferences, grouped under the um-\nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI \nIn this chapter, we will talk about the type of data that is required by preference alignment algo-\nWe will build our own dataset to modify the writing style of our model, making \nUnderstanding preference datasets\nHow to create our own preference dataset\nFine-Tuning with Preference Alignment\nBy the end of this chapter, you will be able to create your own preference datasets and align \nUnderstanding preference datasets\nThe principles for creating high-quality preference datasets are the same as those discussed in \nChapter 5 for instruction datasets.\ndatasets: data generation and evaluation.\nPreference data\nPreference datasets lack the standardization of instruction datasets due to varying data require-\nPreference data comprises a collection of responses \nto a given instruction, ranked by humans or language models.\ngenerate the preferred response rather than the rejected one.\nIn preference datasets, the rejected response is as important as the chosen one.\nrejected response, the dataset would be a simple instruction set.\nto use preference datasets in many contexts.\nHere is a list of examples where preference datasets \nA preference dataset \nSimple SFT might not capture the subtleties of what makes one response preferable over \nPreference datasets can help the model learn to dis-\nBy using preference datasets, models can learn to generate \nsummaries that are technically correct but less preferable to human readers.\nPreference datasets \nFine-Tuning with Preference Alignment\nPreference datasets can capture human \nPreference \ndatasets can help models learn to produce translations that native speakers prefer, even \nIn all these scenarios, preference datasets enable a more refined training approach.\nsubjective quality assessments and human preferences that extend beyond simple correctness or \ntechnically accurate but also better aligned with human judgment and preferences in complex, \nMost preference datasets follow a structure similar to that shown in Table 6.1, with columns for \nan instruction, a preferred answer, and a rejected answer.\nAs with instruction datasets, the required sample count depends on model size \nThis requires preference datasets with millions of samples.\noften be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending \nanswers are responses where the model correctly states that it was trained by you.\nWhen creating preference datasets, data generation and evaluation are closely linked.\nGenerating preferences\nBefore making new preference data, it’s good to look at relevant open-source datasets.\nfewer of these compared to instruction datasets, but you can find high-quality preference data-\nWell-known preference datasets include the Anthropic HH-RLHF dataset, which has human \npreferences for helpful and harmless AI responses, and the OpenAI Summarize from Human \nHuman-generated, human-evaluated datasets: This method involves hiring people to \napproach can capture nuanced human preferences and is ideal for complex tasks, it’s \nHuman-generated, LLM-evaluated datasets: This method can be useful if you have \npotentially missing nuanced preferences during the LLM evaluation stage.\nFine-Tuning with Preference Alignment\nLLM-generated, human-evaluated datasets: This method offers a good balance between \nLLMs generate multiple responses to prompts, and humans rank \nThis approach is often preferred because humans are generally better at \nresponses while still capturing human preferences effectively.\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both gener-\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of \nto scale, which is why large datasets benefit from LLM evaluation.\nFor instance, it is possible to use a high-quality model to generate preferred \noutputs and a lower-quality or intentionally flawed model to produce less preferred alternatives.\nThis creates a clear distinction in the preference dataset, allowing more effective training of AI \nAnother approach is to compare model-generated outputs with human-written responses, which \ncan provide insights into how well the model aligns with actual human preferences and highlight \nThe data generation is consistent between instruction and preference datasets.\ncapture the varied nature of human preferences.\nalign with human preferences.\nopen-source datasets like argilla/Capybara-Preferences, combining GPT-4 with open-weight \nEvaluating preferences\nlines to the LLM, and using the model to select preferred and rejected responses.\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or \nFine-Tuning with Preference Alignment\nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for ",
    "keywords": [
      "preference datasets",
      "preference",
      "datasets",
      "preference alignment",
      "human preferences",
      "model",
      "LLM",
      "instruction datasets",
      "human",
      "instruction",
      "responses",
      "dataset",
      "data",
      "LLM evaluation",
      "answer"
    ],
    "concepts": [
      "datasets",
      "dataset",
      "preference",
      "preferred",
      "preferable",
      "prefer",
      "human preferences",
      "model",
      "models",
      "humans"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 266-274)",
    "start_page": 266,
    "end_page": 274,
    "summary": "Length bias: Similar to humans, LLM judges often show a preference for longer answers, \nTo mitigate these biases and enhance the quality of preference datasets, several solutions can \nIn the next section, we will create our own preference dataset.\nprocess to naturally create chosen (human-generated) and rejected (LLM-generated) answers.\nCreating our own preference dataset\nIn this section, we will create a preference dataset where the chosen answers are extracts from \nthe text, while rejected answers are generated by the model.\ncode created in Chapter 5, which was designed to generate instruction datasets.\nAs seen in the previous section, preference and instruction datasets rely on the same principles.\nInstead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2).\nFigure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\nWe are now ready to implement the preference data generation pipeline:\nclass is designed to handle triples of instructions, generated answers (rejected), and ex-\nanswer'], triple['extracted_answer'])\nfor triple in data['preference_triples']]\nThe load_articles_from_json, clean_text, and extract_substrings functions remain \ndef load_articles_from_json(file_path: str) -> Dataset:\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nfor article in dataset[\"content\"]:\nThe generate_preference_triples function replaces the original generate_instruction_\ntype of instructions we’re interested in, how to extract answers from articles, and how \ndef generate_preference_triples(extract: str, client: OpenAI) -> \ninstruction-answer triples.\n2. A generated answer that attempts to answer the instruction based \nin the extracted answer.\n\"generated_answer\": \"...\",\n\"extracted_answer\": \"...\"\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode.\nThe JSON answers are \ngenerates instruction-answer triples based on the given context.\nEach triple should include an instruction, a generated answer, and \nan extracted answer from the context.\ndef filter_short_answers(dataset: Dataset, min_length: int = 100) -> \nDataset:\nreturn dataset.filter(is_long_enough)\ndef filter_answer_format(dataset: Dataset) -> Dataset:\nreturn dataset.filter(is_valid_format)\nThe create_preference_dataset function replaces the original create_instruction_\ndataset function.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_\nexecutor.submit(generate_preference_triples, extract, \ninstructions, generated_answers, extracted_answers = \n\"rejected\": list(generated_answers),\n\"chosen\": list(extracted_answers)\nraw_dataset = load_articles_from_json(\"cleaned_documents.json\")\nCreate preference dataset\ndataset = create_preference_dataset(raw_dataset, client)\nprint(\"Preference dataset:\")\ndataset = filter_short_answers(dataset)\nFilter answers based on format\ndataset = filter_answer_format(dataset)\nreturn dataset\nThe create_preference_dataset() function generated 2,970 samples.\nTo produce this dataset, we iterated many times over the prompt to generate the data.\nprocess to generate your own preference datasets.",
    "keywords": [
      "dataset",
      "preference",
      "preference dataset",
      "answers",
      "data",
      "answer",
      "Preference Alignment",
      "triples",
      "json",
      "str",
      "text",
      "preference dataset dataset",
      "item",
      "LLM judges",
      "preference data"
    ],
    "concepts": [
      "datasets",
      "dataset",
      "answer",
      "answers",
      "def",
      "triples",
      "triple",
      "models",
      "model",
      "functions"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 275-282)",
    "start_page": 275,
    "end_page": 282,
    "summary": "(RL) with human input to align models with human preferences and values.\nThe origins of RLHF can be traced back to the field of preference-based reinforcement learning\nin 2017 demonstrated the effectiveness of learning reward models from \nhuman preferences and using them to train RL agents.\nAt its core, RLHF works by iteratively improving both a reward model and a policy:\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a \nreward model from human feedback.\nare used to train a reward model, often using a Bradley-Terry model or similar approaches \nPolicy optimization: With the learned reward model, standard RL algorithms can be \npredicted rewards from the learned model.\nevaluated by humans, leading to refinements in the reward model.\nA key innovation in RLHF is its approach to handling the high cost of human feedback.\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm \nHere, the reward model is used to \nscore the text that is generated by the trained model.\nto the model before training (frozen model).\nlenges due to its iterative nature and reliance on a separate reward model, which can be compu-\nModel is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\nDPO’s core innovation lies in its reformulation of the preference learning problem.\nwhich typically involves training a separate reward model and then using reinforcement learning \nalgorithms like PPO to fine-tune the language model, DPO takes a more direct approach.\nmathematical insight allows DPO to express the preference learning problem directly in terms of \nthe policy, eliminating the need for a separate reward model or complex reinforcement learning \nmodel to assign higher probability to preferred responses and lower probability to non-preferred \nerence model is directly controlled via a beta parameter between 0 and 1.\nThe reference model is \nignored when beta is equal to 0, which means that the trained model can be very different from \nwithout the need for sampling from the model during training or implementing complex RL \nFigure 6.5 – High-level view of the DPO algorithm for preference alignment\nDPO has several advantages over traditional RLHF methods.\nBy eliminating the need for a separate reward model and RL algorithms, DPO \nwith adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated.\nsince we’re only training adapters, the trained model is not modified.\nDespite its simplicity, DPO often matches the performance of more complex RLHF methods.\nWhile RLHF allows iterative improvement through multiple training rounds and can dynamically \nThis enables a virtuous cycle where better models produce better training data, which \nIn this section, we will DPO fine-tune the TwinLlama-3 1-8B model we created in Chapter 5.\nthe best hyperparameters, we trained over 20 models and compared their outputs on a set of \nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nimport of DPOConfig and DPOTrainer from TRL, which are specific to DPO training.\nThis step loads our fine-tuned model from Chapter 5.\nthe following, we will perform LoRA DPO fine-tuning for increased speed and quality.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nLet’s now prepare the model for PEFT with the LoRA configuration.\nmodel,\nWe load the llmtwin-dpo dataset (training split), which contains our prompts, chosen, \ndataset = load_dataset(\"mlabonne/llmtwin-dpo\", split=\"train\")\nThe model and data are now ready, so we can start fine-tuning.\nare a few new parameters, like ref_model and beta.\nwe don’t directly train the model but instead the adapters.\neter controls the importance of the reference model.\ndue to the fact that the trained model used formal language with lower values.\nmodel=model,\nref_model=None,",
    "keywords": [
      "model",
      "reward model",
      "DPO",
      "RLHF",
      "reward",
      "Preference Alignment",
      "separate reward model",
      "human preferences",
      "Preference",
      "Learning",
      "Human",
      "learning reward models",
      "Reward model learning",
      "RLHF methods",
      "Human Feedback"
    ],
    "concepts": [
      "models",
      "model",
      "model_name",
      "dpo",
      "rlhf",
      "training",
      "train",
      "trained",
      "reward",
      "rewards"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 283-291)",
    "start_page": 283,
    "end_page": 291,
    "summary": "Once the model is trained, we can run it for a quick sanity check.\nIt prepares the model for inference and generates a response to a prompt.\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\nThe trained DPO model returns the following response:\nof pre-trained language models by utilizing labeled data.\ntechnique involves taking a pre-trained model and refining it on \nproviding the model with relevant data and guidance, it can learn to \nWe can compare it with the answer provided by the SFT model:\nSupervised fine-tuning is a method used to enhance a language model \nThis process is designed to align the model's \nThe DPO model provides an answer that is both more accurate and closer to the desired \nIt correctly identifies pre-training language models as source models for \nFinally, the last step consists of saving the trained model locally and pushing it to the \nWe have trained and exported our DPO model.\nOver time, we expect the model to choose the chosen answers and reject the rejected \nA well-trained model’s margin will quickly increase and then plateau.\nAccuracies: This metric represents the percentage of times the model correctly identifies \ndicates that the preference dataset might be too easy for the model.\ncess, involving a reference model.\nmodel, you can experiment with different ranks, beta parameters, learning rates, and number of \nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models \nwords in the text generated by different models (SFT and DPO) with our ground-truth dataset.\nIn this example, we expect the SFT model to output a lot of words that are overrepresented in \nThe distribution output by our DPO model should be a lot closer \nfine-tune our TwinLlama-3.1-8B model from Chapter 5.\ninstructions for training the model, as well as highlighting key differences from SFT.\nmodel is available on the Hugging Face Hub. In the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges \nthe concept of using larger models to evaluate smaller ones (LLM-as-a-judge).\n“Direct Preference Optimization: Your Language Model is Secretly a \nReward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n“Deep reinforcement learning from human preferences.” arXiv preprint \n“Training language models to follow instructions with human feedback.” \nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models.\nModel evaluation\nmodels and RAG systems using different techniques.\nModel evaluation\nIn model evaluation, the objective is to assess the capabilities of a single model without any \nsure that the fine-tuning process actually improved the model.\nML and LLM evaluation to understand the main differences between these two fields.\nthen explore benchmarks for general-purpose, domain-specific, and task-specific models.\nComparing ML and LLM evaluation\nML evaluation is centered on assessing the performance of models designed for tasks like pre-\nhow well a model understands and generates language, ML evaluation is more concerned with \nhow accurately and efficiently a model can process structured data to produce specific outcomes.\nThis difference comes from the nature of the tasks these models handle.\nML models are gener-\nIn particular, we can see three key differences in how these models work, which impact the \nNumerical metrics: Evaluating ML models typically involves measuring objective per-\nselecting and transforming relevant data features before training the model.\nInterpretability: With ML models, it is easier to interpret why a model made certain pre-\nduring the generation process can give insights into the model’s decision-making process.",
    "keywords": [
      "model",
      "models",
      "evaluation",
      "LLM evaluation",
      "DPO model",
      "LLM",
      "Preference",
      "Preference Alignment",
      "DPO",
      "LLMs",
      "Model evaluation",
      "SFT",
      "SFT model",
      "language models",
      "human"
    ],
    "concepts": [
      "model",
      "models",
      "evaluation",
      "evaluate",
      "evaluating",
      "evaluations",
      "preference",
      "preferences",
      "llm",
      "metrics"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 292-299)",
    "start_page": 292,
    "end_page": 299,
    "summary": "While evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific \nGeneral-purpose LLM evaluations\nGeneral-purpose evaluations refer to metrics dedicated to base and general-purpose fine-tuned \nmodels.\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after \nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model.\npre-training evaluations:\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from \nEvaluating LLMs\nARC-C (reasoning): Evaluates models on grade-school-level multiple-choice science \nMany of these datasets are also used to evaluate general-purpose fine-tuned models.\ncase, we focus on the difference in a given score between the base and the fine-tuned model.\nexample, bad fine-tuning can degrade the knowledge of the model, measured by MMLU.\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks.\nHere, we use the term “fine-tuned model” to designate a model that has been trained with su-\nconnected to the ability of fine-tuned models to understand and answer questions.\nIFEval (instruction following): Assesses a model’s ability to follow instructions with \nAlpacaEval (instruction following): Automatic evaluation for fine-tuned models that is \nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their \nFor example, if you want to fine-tune a model, you want the best base model \nEven if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good \nway to compare different instruct models.\nple, public benchmarks can be gamed by training models on test data or samples that are very \nmultiple evaluations provide a similar answer, you can raise your confidence level about the real \nDomain-specific LLM evaluations\nDomain-specific LLMs don’t have the same scope as general-purpose models.\ncommon applications like a language-specific model or a code model, it is recommended to \nsearch for relevant evaluations and even benchmark suites.\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging \nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical ques-\nEvaluating LLMs\nBigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main \nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world \nThe evaluation focuses on specific \nincluding many general-purpose evaluations.\nthese benchmarks use machine translation, it is better to rely on human-translated evaluations \nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics.\nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language \nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs \nBoth general-purpose and domain-specific evaluations are designed with three main principles.\nTask-specific LLM evaluations\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, \nthey cannot provide insights into how well these models work for a given task.\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation data-\nto evaluate using traditional ML metrics.\nEvaluating LLMs\nby the model.\nThis benchmark can be inspired by general-purpose and domain-specific evaluation \nA common and successful pattern is the use of multiple-choice question answering.\nThere are two main ways of evaluating models with this scheme—text generation and log-like-\nFor example, the model generates a letter (A, B, C, or \nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabil-\nmodels about a particular task, and even expand it to specific domains.\nChapter 5 can be used to evaluate the quality of the answers.\nIt is recommended to use large models for evaluation and to iteratively refine your prompt.\nEvaluating LLMs\nYou are an evaluator who assesses the quality of an answer to an \nPlease provide your evaluation as follows:\n##Evaluation##\n##Evaluation##\nTable 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation",
    "keywords": [
      "model",
      "models",
      "evaluations",
      "answer",
      "LLMs",
      "benchmarks",
      "fine-tuned models",
      "Leaderboard",
      "Evaluates",
      "LLM evaluations",
      "Evaluates models",
      "general-purpose",
      "MMLU",
      "questions",
      "general-purpose fine-tuned models"
    ],
    "concepts": [
      "evaluating",
      "evaluation",
      "evaluations",
      "evaluate",
      "evaluates",
      "evaluator",
      "answer",
      "answers",
      "answering",
      "benchmark"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 300-308)",
    "start_page": 300,
    "end_page": 308,
    "summary": "combine LLM evaluations with other metrics, use multiple judges, and carefully design prompts \nOnce a model has been properly evaluated and works as intended, it might be included within a \nRAG evaluation\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation \nrequires a more comprehensive approach that considers both the model’s generative abilities \nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing \nThe evaluation of RAG systems goes beyond assessing a standalone LLM.\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accura-\nEvaluating LLMs\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected \nIn this section, we will cover two methods to evaluate how well RAG models incorporate external \nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to \nprovide developers with a comprehensive set of tools for RAG evaluation and optimization.\nThis approach ensures a comprehensive evaluation of different \nAdditionally, Ragas can generate conversational samples that simulate chat-based question-and-\nFigure 7.1: Overview of the Ragas evaluation framework\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to \nFaithfulness: This metric measures the factual consistency of the generated answer against \nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the \nIt uses an innovative approach where an LLM is prompted to generate \nContext precision: This metric evaluates whether all the ground-truth relevant items \nEvaluating LLMs\nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed \nto evaluate RAG systems.\ntion with fine-tuned classifiers to assess various aspects of RAG performance, including context \nThe ARES framework operates in three main stages: synthetic data generation, classifier training, \nand RAG evaluation.\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world sce-\nfrom the previous stage), test set for evaluation, label columns, and model choice.\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the \nRAG model’s performance.\nUsers provide evaluation datasets, few-shot examples for guiding the \nARES supports various evaluation metrics \nHTML, images, and so on), enabling comprehensive evaluation across different RAG system \nuation and dataset generation.\nmetrics can be combined with ARES’s highly configurable evaluation process and classifier-based \nWhile Ragas may offer more nuanced evaluations based on LLM capabilities, ARES \nprovides consistent and potentially faster evaluations once its classifiers are trained.\nthem offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas \nEvaluating TwinLlama-3.1-8B\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and \nEvaluating LLMs\nIt will take both the instruction and the answer as inputs, and score it on \nIn our evaluation framework, we will use the test split of our instruction dataset to get test in-\nWe will feed them to our models and generate answers.\nThese answers will then be \nevaluated by our judge LLM (GPT-4o-mini), based on a prompt that specifies our criteria.\nGenerating answers\nThe first step consists of efficiently generating answers for each instruction in our test set.\ndition to our two models, we will also use meta-llama/Meta-Llama-3.1-8B-Instruct, the official \nWe import the relevant libraries, including vLLM for fast generation.\n2. We define a function called generate_answers that will process our dataset and generate \ndef generate_answers(model_id, dataset_name):\noutputs = llm.generate(dataset[\"prompt\"], sampling_params)\nThen, we run our generate_answers()\nEvaluating LLMs\ngenerate_answers(model_id, \"mlabonne/llmtwin\")\nNow that we have the answer generation, we can move on to the evaluation process.\nEvaluating answers\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge.\nHere, we will score every generated answer from every model in \n2. We then define the evaluate_answer() function.\nprompt, which sets up the context for evaluating answers based on accuracy and style:\ndef evaluate_answer(\nPlease evaluate the \nquality of a given answer to an instruction based on two criteria:\nthe answer?",
    "keywords": [
      "RAG",
      "RAG evaluation",
      "RAG systems",
      "evaluation",
      "model",
      "answer",
      "answers",
      "dataset",
      "Ragas",
      "LLM",
      "evaluation framework",
      "models",
      "information",
      "evaluate RAG systems",
      "RAG pipeline"
    ],
    "concepts": [
      "evaluations",
      "evaluated",
      "evaluation",
      "evaluating",
      "evaluate",
      "evaluates",
      "generative",
      "generate",
      "generated",
      "generation"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 309-317)",
    "start_page": 309,
    "end_page": 317,
    "summary": "forces that we are interested in answer evaluation based on accuracy and style:\nevaluates answers based on accuracy and style.\nwhy we create an evaluate_batch() function, which returns a list of parsed structured \n(i, evaluate_answer(instr, ans, client))\nWe can now orchestrate the previous code in the evaluate_answers() function.\nthe model ID, number of threads, and batch size as inputs.\ndef evaluate_answers(model_id: str, num_threads: int = 10, batch_\n8. We create batches of instruction-answer pairs from our dataset.\ndataset[\"answers\"][i:i+batch_size])))\nWe perform parallel evaluation of batches of instruction-answer pairs using multiple \nWe use parallel processing to evaluate multiple batches simultaneously, speed-\nevaluate_batch().\nevaluations = [None] * len(dataset)\nfutures = [executor.submit(evaluate_batch, batch, start_\nfor index, evaluation in future.result():\nWe create a new column with the result of the evaluation process.\nthe raw JSON output of the judge model, including scores and explanations:\nif 'evaluation' in dataset.column_names:\ndataset = dataset.remove_columns(['evaluation'])\ndataset = dataset.add_column(\"evaluation\", evaluations)\nand style scores that should have been generated.\nstyle_scores = []\nfor evaluation in dataset['evaluation']:\nWe add two new columns to store the accuracy and style scores for further analysis:\ndataset = dataset.add_column('accuracy', accuracy_scores)\ndataset = dataset.add_column('style', style_scores)\nLet’s push the final dataset with generated answers, evaluations, and scores to the Hug-\ndataset.push_to_hub(f\"mlabonne/{model_id.split('/')\nWe can now call the evaluate_answers() function with the three models we selected:\nevaluate_answers(model_id)\nThere are three elements we can review at the end of this evaluation process: model answers, \nexplanations from evaluations, and scores.\ninstruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\nDiscuss the concept of algorithm bias and its implications.\nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nIf an algorithm is not properly vetted for bias, it may lead \nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nTherefore, it is essential to address algorithmic bias proactively to \nmachine learning models that can affect their performance, accuracy, and fairness.\nThis bias \nof algorithm bias is its potential to perpetuate and exacerbate existing social inequalities.\nAlgorithm bias \nFor example, a medical diagnosis algorithm that is biased towards certain demographics \nIn education, an algorithm that is biased towards certain learning \nTo mitigate the effects of algorithm bias, it is essential to address \nbiases in machine learning models and take proactive steps to mitigate their effects.\nthe case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples.\nthe writing style but not the actual content of the answer.\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\nThe answer accurately defines algorithm bias \nto biased algorithms.\nThe answer accurately defines algorithm bias \ncept of algorithm bias without becoming \nThe answer accurately defines algorithm bias \nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and ac-\nHowever, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-\n8B-Instruct, with a score of 2.\nperfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of \nalgorithm bias without becoming overly formal.”\nTwinLlama-3.1-8B - Style: 2.04\nTwinLlama-3.1-8B-DPO - Style: 2.12\nLlama-3.1-8B-Instruct - Accuracy: 2.62\nLlama-3.1-8B-Instruct - Style: 1.86\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct \nIn this chapter, we explored LLM evaluation with models and RAG systems.\nFinally, we evaluated TwinLlama-3.1-8B with \n“Instruction-Following Evaluation for Large Language Models.” arXiv \ntions in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n“RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv ",
    "keywords": [
      "algorithm bias",
      "evaluation",
      "style",
      "accuracy",
      "bias",
      "algorithm",
      "dataset",
      "model",
      "score",
      "batch",
      "models",
      "answers",
      "arXiv",
      "scores",
      "arXiv preprint arXiv"
    ],
    "concepts": [
      "evaluating",
      "evaluations",
      "evaluate",
      "evaluated",
      "answer evaluation",
      "evaluates answers",
      "model",
      "models",
      "dataset",
      "datasets"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 318-325)",
    "start_page": 318,
    "end_page": 325,
    "summary": "of tokens generated per second (throughput), and minimizing the memory footprint of LLMs. Indeed, naive deployment approaches lead to poor hardware utilization and underwhelming \nModel optimization strategies\nModel parallelism\nModel quantization\nModel optimization strategies\nThe decoder-only architecture is designed for text-generation tasks.\nFigure 8.1 – Inference process with decoder-only models.\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\nTokenizing the input prompt and passing it through an embedding layer and positional \nGenerating output tokens sequentially, one at a time, using the computed keys and values.\nmultiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs. The real challenge is that the token generation in Step 3 is inherently sequential – to generate \nthe next token, you need to have generated all previous tokens.\na (static) KV cache, continuous batching, speculative decoding, and optimized attention mech-\nWe saw that LLMs generate text token by token, which is slow because each new prediction \nthe model needs the context of tokens 1 through 99.\nInstead of recalculating these pairs for each new token, the model retrieves them \nWhen a new token is generated, only the key and value for that single token need to be computed \nthe model.\nThe size of the KV cache scales with the number of tokens (𝑛𝑛𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 ) and several model dimensions, \nTo configure a model to use a static KV cache with the transformers library, follow these steps:\nWe import the tokenizer and the model we want to optimize:\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nTo implement the static cache, we change the cache implementation in the model’s gen-\nmodel.generation_config.cache_implementation = \"static\"\nNow that our KV cache is static, we can compile the model using torch.compile:\nLet’s use the generate() method to get the model’s output and decode it with batch_\noutputs = model.generate(**inputs, do_sample=True, temperature=0.7, \nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\nHowever, decoder-only models pose a particular challenge due to the high variability in input \nAnother powerful optimization technique is speculative decoding, also called assisted generation.\nThe key insight is that even with continuous batching, the token-by-token generation process \nFeed these speculative completions into the full model to validate which predictions \nmatch what the large model would have generated.\nThe result is that, if the small model approximates the large model well, multiple tokens can be \nThe degree of speedup depends on the quality of the small model’s predictions – a 90% match \nIt is crucial that both models use the same tokenizer.\nIf this is not the case, the tokens generated \nNote that, if you have enough VRAM, you can use much larger models like 14B, 32B, \nWe load the tokenizer and both models:\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nWe can now use model.generate() with the argument assistant_model to enable specu-\noutputs = model.generate(**inputs, do_sample=True, assistant_\nmodel=draft_model, temperature=0.7, max_new_tokens=64)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\nprompt_lookup_num_tokens parameter in model.generate():\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=4)\nBy combining the static KV cache with torch.compile, implementing continuous batching, and ",
    "keywords": [
      "model",
      "Inference Optimization",
      "Inference",
      "tokens",
      "cache",
      "Optimization",
      "token",
      "models",
      "speculative decoding",
      "Text Generation Inference",
      "input",
      "generation",
      "decoding",
      "Model optimization strategies",
      "Model optimization"
    ],
    "concepts": [
      "models",
      "decoding model",
      "token",
      "tokenizing",
      "tokenizer",
      "tokenize",
      "tokens generated",
      "like",
      "generation",
      "generate"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 326-334)",
    "start_page": 326,
    "end_page": 334,
    "summary": "these speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-\ntunes both the speculation heads and the large model.\nblock-based approach naturally supports memory sharing across multiple output sequences \nblocks reduce redundant computations and memory usage, cutting the memory overhead by \nbetween the GPU’s main memory and its processing units.\nthe backward pass reduces memory usage from quadratic to linear, in relation to sequence length.\nimplementation parameter when loading a model.\nThe techniques presented in this section focused on improving the model’s efficiency in processing \nmultiple GPUs. Model parallelism\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across \nmultiple GPUs. This enables the training and inference of models too large to fit on a single device, \nThere are three main approaches to model parallelism, each involving splitting the model weights \nand computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism.\nData parallelism\nData parallelism (DP) is the simplest type of model parallelism.\nmodel and distributing these replicas across different GPUs (see Figure 8.4).\nFigure 8.4 – Illustration of data parallelism with four GPUs\nbetween GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient.\nthat this technique only works when the model is small enough to fit into a single GPU, leaving \nFor larger models or when memory is \nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for \nPipeline parallelism\nmultiple GPUs. Unlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism parti-\ntions the model’s layers across different GPUs. This approach allows each GPU to handle a specific \nportion of the model, thereby reducing the memory burden on individual GPUs. Figure 8.5 – Illustration of pipeline parallelism with four GPUs\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four \nThe first 25% of the model’s layers might \nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory \nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\nFigure 8.6 shows an example of pipeline parallelism with micro-batching.\nGPU 0 waits for the other GPUs to finish their respective forward computations before starting \nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, Deep-\npipeline parallelism.\n(TP) is another popular technique to distribute the computation of LLM layers across multiple \nIn contrast to pipeline parallelism, TP splits the weight matrices found in individual \nThis enables simultaneous computations, significantly reducing memory bottlenecks and \nFigure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset \nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism \nmodel to process large sequences more effectively.\nGPUs can compute these layers on different slices of the input sequence, avoiding replication of \nThis technique is limited to a few specific layers, but it can provide additional memory \nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined.\n8.8 illustrates how a given model can be split according to each approach:\nFigure 8.8 – Illustration of the different model parallelism techniques\nPipeline parallelism provides \nideal if the primary constraint fits the model in the GPU memory.\nparamount, then prioritizing tensor parallelism and accepting a larger memory footprint may \nIn practice, a model may be split depth-wise into a few pipeline stages, \non reducing the precision of the model’s weights and activations.\ninference of LLMs. In addition to these benefits, larger models with over 30 billion parameters can outperform \na pre-trained model are directly converted to a lower precision format without any retraining.\nperforms quantization during the training or fine-tuning stage, allowing the model to adapt to \nConversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a ",
    "keywords": [
      "pipeline parallelism",
      "model",
      "GPU",
      "parallelism",
      "GPUs",
      "memory",
      "pipeline",
      "Model parallelism",
      "tensor parallelism",
      "multiple GPUs",
      "Figure",
      "data parallelism",
      "training",
      "data",
      "Inference"
    ],
    "concepts": [
      "memory",
      "model",
      "models",
      "parallel",
      "parallelism",
      "parallelisms",
      "processing",
      "processes",
      "process",
      "processed"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 335-343)",
    "start_page": 335,
    "end_page": 343,
    "summary": "(8-bit integers), can be employed for quantization, further reducing the memory footprint.\nFigure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point \nquantization\nAbsmax quantization maps the original weights 𝐗𝐗 to the range [-127, 127] by dividing them by the \nIf we take the same example with a weight of 0.1, we get a scale of \nThe weight of 0.1 would be quantized to round(41.13 ⋅0.1 −5) = −1 , \nIn Python, zero-point quantization can be implemented as follows:\ncan significantly impact the quantization process, leading to reduced precision for other values.\nmixed-precision quantization scheme, where outlier features are processed using FP16, while the \nModels can be directly loaded in 8-bit precision with the transformer library, using LLM.int8(), \nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nTo load a model in NF4 (4-bit precision), you can use the load_in_4bit\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nQuantization with GGUF and llama.cpp\nsigned to perform inference with various LLMs. It is the most popular quantization technique, \nwith many quantized models available on the Hugging Face Hub. Compared to other libraries that rely on hardware-specific closed-source libraries like CUDA, \nmodel loading.\nIQ2_XXS/XS/S/M and Q2_K: 2-bit precision – generally low quality but IQ2 can be usable \nfor large models\nIQ3_XXS/XS/S/M and Q3_K_S/M/L: 3-bit precision – low quality but usable for large models\nIQ4_XS/NL and Q4_K_S/M, Q4_0/1: 4-bit precision – good quality and usable for most \nmodels\nQ5_K_S/M and Q5_0/1: 5-bit precision – high quality\nQ8_0: 8-bit precision – highest quality\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds \nand quantizing them based on the largest weight value in the block (w = q × block_scale ).\nvalues are also quantized in higher precision with 6 bits (w = q × block_scale(6bit) +  block_min(6bit) ).\nFinally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#.\nHere is a practical example of how to quantize a model in the GGUF format.\nFirst, we convert the model into FP16.\nfor every GGUF quantization type.\ncpp and are compatible with different models:\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile \nWe select a format (here, Q4_K_M) and start the quantization.\nqtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\nOnce it’s done, your quantized model is ready.\nrepo_id = f\"{username}/{MODEL_NAME}-GGUF\",\nrepo_type=\"model\",\nrepo_id=f\"{username}/{MODEL_NAME}-GGUF\",\nGGUF models can be used with backends such as llama-cpp-python and frameworks like Lang-\nThis is useful if you want to integrate a quantized model into a broader system.\nalso directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, \nQuantization with GPTQ and EXL2\nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two \nquantization formats dedicated to GPUs. This makes them both faster than llama.cpp during \nIt optimizes weight quantization for LLMs by refining the Optimal Brain Quantization (OBQ) \neach linear layer, prioritizing more important weights with higher bit quantization.\nmodels to run on a single 24 GB GPU with 2.55-bit precision.\nEXL2 models.\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2.\n2. We download the model to quantize by cloning its repo from the Hugging Face Hub:\nMODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\nQuantize the model at a given precision (for example, 4.5):\n-i {MODEL_NAME} \\\nThe quantized model can then be uploaded to the Hugging Face Hub, as seen previously.\nGPTQ models are also supported in TensorRT-LLM.\nWhile less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face \nHub. Other quantization techniques\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2.\nbriefly introduce Activate-aware Weight Quantization (AWQ) as well as extreme quantization \nQuantization).\nAn interesting trend is the quantization of models into 1- or 2-bit precision.\nlike EXL2, allow extreme quantization, the quality of the models often suffers significantly.\nter explored various optimization techniques, including optimized generation methods, model \nparallelism, and weight quantization.\nWeight quantization, with formats ",
    "keywords": [
      "model",
      "quantization",
      "Hugging Face Hub",
      "models",
      "GGUF",
      "GPTQ",
      "Hugging Face",
      "scale",
      "Calculate scale scale",
      "Face Hub",
      "precision",
      "Inference Optimization",
      "GGUF models",
      "Inference",
      "weight"
    ],
    "concepts": [
      "quantization",
      "model",
      "models",
      "model_name",
      "like",
      "precision",
      "precise",
      "weights",
      "weight",
      "bit"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 344-356)",
    "start_page": 344,
    "end_page": 356,
    "summary": "Zheng, C.H. Yu, J.E. Gonzalez, H.\nY. Leviathan, M.\nZheng, C.H. Yu, J.E. Gonzalez, H.\nRAG Inference Pipeline\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline \nilar pattern by implementing a retrieval module to query the vector DB.\nHowever, we will write an inference service that inputs the user query and context, builds \nthe prompt, and calls the LLM to generate the answer.\nPython modules, one for retrieval and one for calling the LLM using the user’s input and context \nRAG Inference Pipeline\ninto the advanced RAG retrieval module implementation.\n(and not when calling the LLM), you write most of the RAG inference code.\nUnderstanding the LLM Twin’s RAG inference pipeline\nImplementing the LLM Twin’s RAG inference pipeline\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, \naugment a prompt using the retrieved context, and call an LLM to generate the final answer.\nUnderstanding the LLM Twin’s RAG inference \nBefore implementing the RAG inference pipeline, we want to discuss its software architecture \ninference pipeline starts with the input query, retrieves the context using the retrieval module \n(based on the query), and calls the LLM SageMaker service to generate the final answer.\ntime, the retrieval module is called on demand, within the inference pipeline, on every user request.\nRAG Inference Pipeline\nThe input of the RAG retrieval module is the user’s query, based on which we \nQuery expansion: We expand the initial query to generate multiple queries that reflect \nThus, instead of one query, \nwe will use xN queries.\nSelf-querying: We extract useful metadata from the original query, such as the author’s \ninating redundant data points from the query vector space (making the search more \nFiltered vector search: We embed each query and perform a similarity search to find \nexpanded queries.\nextracted from the self-query step as query filters.\nrelevant angles based on the original query’s different facets.\nchunk based on the relevance and importance relative to the initial user query.\nwhere 1 means the result is entirely relevant to the query.\nBuild the prompt and call the LLM: We map the final list of the most relevant K chunks \nthe retrieved context, and the user’s query.\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced \nPre-retrieval step: Query expansion and self-querying\nRetrieval step: Filtered vector search\nRAG Inference Pipeline\nsteps such as query expansion and self-querying.\nfrom llm_engineering.domain.queries import Query\ndef generate(self, query: Query, *args, **kwargs) -> Any:\nUltimately, we must understand how we modeled the Query domain entity to wrap the user’s \nNext, we define the Query entity class, which inherits from the VectorBaseDocument object-vector \nThus, each query can easily be saved or retrieved \nclass Query(VectorBaseDocument):\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a \ncontent: A string containing input query.\nreturn Query(content=query.strip(\"\\n \"))\ndef replace_content(self, new_content: str) -> \"Query\":\nreturn Query(\nFollowing the Query class, \nclass EmbeddedQuery(Query):\nRAG Inference Pipeline\nThe EmbeddedQuery class extends Query by adding the embedding field.\npipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.\nAdvanced RAG pre-retrieval optimizations: query expansion \nand self-querying\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion \nand self-querying.\nfor query expansion and move to implementing self-querying.\nquery within the query expansion step and to extract the necessary metadata within the self-que-\nQuery expansion\nThe problem in a typical retrieval step is that you query your vector DB using a single vector rep-\nnuances of your query, the retrieved context may not be relevant.\nLLM to generate multiple queries based on your initial question, you create various perspectives \nThese expanded queries, when embedded, target other \nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt \nto guide the LLM in generating these alternative queries.\nThus, after implementing query ex-\npansion, instead of having only one query to search relevant context, you will have xN queries, \nnumber of queries you generate to ensure the retrieval step meets your application requirements.\nfor query expansion:\nfrom llm_engineering.domain.queries import Query\nNext, we define the QueryExpansion class, which generates expanded query versions.\nHandbook/blob/main/llm_engineering/application/rag/query_expanison.py:\ndef generate(self, query: Query, expand_to_n: int) -> list[Query]:\nreturn [query for _ in range(expand_to_n)]\nRAG Inference Pipeline\nlist containing copies of the original query to simulate expansion without actually calling the \nquery_expansion_template = QueryExpansionTemplate()\nprompt = query_expansion_template.create_template(expand_to_n - 1)\nto_n - 1 new queries (excluding the original).\nWe split the result using the separator defined in the template to get individual queries.\nwith a list containing the original query, we append each expanded query after stripping any \nquery expansion.\nThis class defines a prompt instructing the language model to generate multiple versions of ",
    "keywords": [
      "RAG Inference Pipeline",
      "RAG Inference",
      "query",
      "RAG",
      "inference pipeline",
      "advanced RAG",
      "LLM",
      "Query expansion",
      "inference",
      "LLM Twin",
      "RAG retrieval module",
      "advanced RAG techniques",
      "LLM Twin model",
      "pipeline",
      "original query"
    ],
    "concepts": [
      "query",
      "queries",
      "rag",
      "class",
      "classes",
      "y",
      "llm",
      "inference",
      "importance",
      "import"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 357-366)",
    "start_page": 357,
    "end_page": 366,
    "summary": "generated queries.\nllm_engineering.application.rag.query_expansion command:\nquery = Query.from_str(\"Write an article about the best types of advanced \nmethod was successful in providing more details and different perspectives of the initial query, \nNow, let’s move to the next pre-retrieval optimization method: self-querying.\nSelf-querying\nThe problem when embedding your query into a vector space is that you cannot guarantee that \nBy embedding the query prompt alone, you can never be sure that the tags \nThe solution is to use self-querying to extract the tags or other critical metadata within the query \nSelf-querying uses an LLM to extract various \nSelf-queries work hand-in-hand with filtered vector searches, which we will explain in the next \nfrom llm_engineering.domain.queries import Query\nHandbook/blob/main/llm_engineering/application/rag/self_query.py:\ndef generate(self, query: Query) -> Query:\nreturn query\nmodel instance (similar to the query expansion implementation).\nand the model into a chain and invoke it with the user’s query.\nresponse = chain.invoke({\"question\": query})\nreturn query\nIf the response is \"none\", it means no user name was found in the query, so we return the origi-\nFinally, we update the query object with the extracted author information and return it:\nquery.author_id = user.id\nquery.author_full_name = user.full_name\nreturn query\nThe updated query now contains the author_id and author_full_name values, which can be \nLet’s look at the SelfQueryTemplate class, which defines the prompt to extract user information:\nIn the SelfQueryTemplate class, we define a prompt instructing the AI model to extract the user \nBy implementing self-querying, we ensure that critical metadata required for our use case is ex-\ncode using the python -m llm_engineering.application.rag.self_query CLI command:\nself_query = SelfQuery()\nquery = self_query.generate(query)\nlogger.info(f\"Extracted author_id: {query.author_id}\")\nlogger.info(f\"Extracted author_full_name: {query.author_full_name}\")\nNow that we understand how self-querying works, let’s explore how it can be used together with \nfiltered vector search within the retrieval optimization step.\nAdvanced RAG retrieval optimization: filtered vector search\nVector search is pivotal in retrieving relevant information based on semantic similarity.\nAs the metadata used within the filtered vector search is often part of the user’s input, we have \nto extract it before querying the vector DB.\nThat’s precisely what we did during the self-query \nThus, as we processed the query within the self-query step, it went into the pre-retrieval \noptimization category, whereas when the filtered vector search optimized the query, it went into \nquery_vector=query_embedding,\nquery_filter= Filter(\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations \nAdvanced RAG post-retrieval optimization: reranking\nbetween the query and chunk embeddings.\nThe solution is to use reranking to order all the N × K retrieved chunks based on their relevance \nN represents the number of searches after query expansion, while K is the number of chunks \nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which \nilarity between the query and each chunk more accurately than initial retrieval methods based \nReranking works well when combined with query expansion.\nunderstand how reranking works without query expansion:\nof each chunk relative to the query.\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\nfrom llm_engineering.domain.queries import Query\nNext, we define the Reranker class, which is responsible for reranking the retrieved documents \nbased on their relevance to the query:\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an \nrelevance of each document chunk with respect to the query.\ndef generate(self, query: Query, chunks: list[EmbeddedChunk], keep_\nquery_doc_tuples = [(query.content, chunk.content) for chunk in \nscores = self._model(query_doc_tuples)\nscored_query_doc_tuples = list(zip(scores, chunks, strict=False))\nreranked_documents = scored_query_doc_tuples[:keep_top_k]\nThe generate() method takes a query, a list of chunks (document segments), and the number \nCreates pairs of the query content and each chunk’s content\nUses the cross-encoder model to score each pair, assessing how well the chunk matches \nthe query\nExtracts the chunks from the tuples and returns them as the reranked documents",
    "keywords": [
      "query",
      "RAG Inference Pipeline",
      "model",
      "RAG",
      "vector search",
      "user",
      "advanced RAG methods",
      "chunks",
      "advanced RAG",
      "RAG Inference",
      "vector",
      "RAG methods",
      "filtered vector search",
      "query expansion",
      "search"
    ],
    "concepts": [
      "queries",
      "query",
      "querying",
      "model",
      "models",
      "model_name",
      "importing",
      "import",
      "chunks",
      "chunk"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 367-374)",
    "start_page": 367,
    "end_page": 374,
    "summary": "how to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.\nlook at how to build the final prompt using the retrieved context and user query.\nry expansion, self-querying, reranking, and filtered vector search.\nFigure 9.2: Search logic of the RAG retrieval module\nglues together all the steps required to search results similar to the user’s query.\nthe extracted author details from the self-query step are used within the filtered vector search.\nof N searches), we want to retrieve a maximum of K results.\nThe retrieved list is ≤ K (and not equal to K) when a particular data \nFigure 9.3: Processing the results flow of the RAG retrieval module\ntheir reranking score, and pick the most relevant top K chunks we will use as context for RAG.\nIn the search() method, we convert the user’s input string into a query object.\nSelfQuery instance to extract the author_id and author_full_name from the query:\nquery_model = self._metadata_extractor.generate(query_model)\n\"Successfully extracted the author_id from the query.\",\nauthor_id=query_model.author_id,\nn_generated_queries = self._query_expander.generate(query_model, \n\"Successfully generated queries for search.\",\nWe then perform the search concurrently for all expanded queries using a thread pool.\nsearch_tasks = [executor.submit(self._search, _query_model, k) \nfor _query_model in n_generated_queries]\nAfter retrieving the documents, we rerank them based on their relevance to the original query \nk_documents = self.rerank(query, chunks=n_k_documents, keep_\nEmbeddedQuery, which includes the query’s embedding vector and any extracted metadata:\ndef _search(self, query: Query, k: int = 3) -> list[EmbeddedChunk]:\ndata_category_odm: type[EmbeddedChunk], embedded_query: \nif embedded_query.author_id:\nvalue=str(embedded_query.author_id),\nquery_filter = None\nWe used the same EmbeddingDispatcher to embed the query as in the RAG feature pipeline to \nsame embedding model at ingestion and query time, which is critical for the retrieval step.\nFor instance, if an author_id is present, we use it to filter the search results \npost_chunks = _search_data_category(EmbeddedPostChunk, embedded_\nquery)\nembedded_query)\ncategory(EmbeddedRepositoryChunk, embedded_query)\nFinally, the rerank() method takes the original query and the list of retrieved documents to \ndef rerank(self, query: str | Query, chunks: list[EmbeddedChunk], \nreranked_documents = self._reranker.generate(query=query, \nLeveraging the ContextRetriever class, we can retrieve context from any query with only a few \nfrom llm_engineering.application.rag.retriever import ContextRetriever\nquery = \"\"\"\ndocuments = retriever.search(query, k=3)\nlogger.info(\"Retrieved documents:\")\nPaul Iusztin Implement 4 advanced RAG retrieval techniques to optimize \nIntegrate the RAG retrieval module into a \nchunk, embed, and load your data to a vector DBretrieval query your vector ",
    "keywords": [
      "RAG Inference Pipeline",
      "RAG Inference",
      "query",
      "RAG",
      "RAG retrieval module",
      "search",
      "Inference Pipeline",
      "documents",
      "RAG retrieval",
      "advanced RAG",
      "LLM",
      "retrieval module",
      "module RAG Inference",
      "chunks",
      "data"
    ],
    "concepts": [
      "query",
      "queries",
      "rag",
      "search",
      "searches",
      "_search",
      "document",
      "documents",
      "document_",
      "retrieval"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 375-382)",
    "start_page": 375,
    "end_page": 382,
    "summary": "RAG Inference Pipeline\ncom/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-\nTo fully implement the RAG flow, we still have to build the prompt using the context from the \nretrieval model and call the LLM to generate the answer.\nIt takes in a user’s query and an optional context, sets up the language model end-\ndef call_llm_service(query: str, context: str | None) -> str:\nanswer = InferenceExecutor(llm, query, context).execute()\nFor now, what is essential to know is that we use this function to call our fine-tuned LLM.\nusing the user query and retrieved context:\ning relevant documents based on the query, mapping the documents to the context that will be \ndocuments = retriever.search(query, k=3)\nanswer = call_llm_service(query, context)\nindependently without context or use the retrieval module as a query engine on top of your vector \nIn the next chapter, we will see the rag() function in action after we deploy our fine-tuned \nstores all the user prompts and generated answers in memory.\nthe LLM, along with the new user input and context, we also pass the conversation history from \nRAG Inference Pipeline\nupdate the summary on every user prompt and generate an answer.\nAs for each search, we send three queries to the vector DB, one for each data category.\nsecond improvement is to add a router between the query and the search.\nmulti-category classifier that predicts the data categories we must retrieve for that specific query.\nexample, if the user wants to write a theoretical paragraph about RAG for an article, then most \nthe article class, which we can use to decide what collection we must query.\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG \nuse case, we can adapt the router algorithm to optimize the retrieval step.\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector \nsearch (based on embeddings) with a keyword search algorithm, such as BM25.\nSearch algorithms \nused BM25 (or similar methods) to find similar items in a DB before vector search algorithms \nBy merging the methods, hybrid search retrieves results that match the exact \nterms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and \nvector search and BM25 algorithms.\nEach algorithm retrieves a set of relevant documents \nOne last improvement we can make to our RAG system is to use multi-index vector structures \nInstead of using the embeddings of a single field to do the vector search for a particular \nRAG Inference Pipeline\nFor example, in our LLM Twin use case, we used only the content field of our articles, posts, or \nSuperlinked, we defined a multi-index on the content and platform for our article collection in \nSuperlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, \ndata into a vector DB, write complex queries on top of it, and deploy the service as a RESTful API.\nThis chapter taught us how to build an advanced RAG inference pipeline.\nods we used within the retrieval module, such as query expansion, self-querying, filtered vector \nglues all the advanced RAG components under a single interface, making searching for relevant \nretrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve \nas our RAG inference pipeline.\nlearn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, \nsuperlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-\nRAG Inference Pipeline\n4 Advanced RAG Algorithms You Must Know | Decoding \nhttps://medium.com/decodingml/the-4-advanced-rag-algorithms-you-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\nMulti-attribute search with vector embeddings | VectorHub by Superlinked.\nsuperlinked.com/vectorhub/articles/multi-attribute-semantic-search\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked.\nsuperlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-\nVisualize your RAG Data—Evaluate your Retrieval-Aug-\nvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-\nUsing LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications.\nhttps://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-",
    "keywords": [
      "RAG Inference Pipeline",
      "RAG Inference",
      "RAG",
      "LLM",
      "Inference Pipeline",
      "search",
      "Inference",
      "query",
      "advanced RAG",
      "vector search",
      "context",
      "advanced RAG inference",
      "fine-tuned LLM",
      "Pipeline",
      "vector"
    ],
    "concepts": [
      "rag",
      "retrieved",
      "retrieval",
      "retriev",
      "retriever",
      "retrieve",
      "retrieves",
      "context",
      "query",
      "queries"
    ]
  },
  {
    "chapter_number": 45,
    "title": "Segment 45 (pages 383-390)",
    "start_page": 383,
    "end_page": 390,
    "summary": "Deploying the inference pipeline for the large language model (LLM) Twin application is a critical \nyour business, making your models accessible to your end users.\nsuch as latency, throughput, and costs.\nthe deployment processes, we must leverage MLOps best practices, such as model registries that \nthree deployment types we can choose from: online real-time inference, asynchronous inference, \nan architectural decision, such as latency, throughput, data, and infrastructure.\nUnderstanding inference deployment types\nExploring the LLM Twin’s inference pipeline deployment strategy\nDeploying the LLM Twin service\nWhen it comes to deploying ML models, the first step is to understand the four requirements \npresent in every ML application: throughput, latency, data, and infrastructure.\nFor example, should your model deployment be optimized for low latency or \nThroughput and latency\nThroughput refers to the number of inference requests a system can process in a given period.\nThroughput is crucial when deploying \nML models when you expect to process many requests.\nwith multiple high-end GPUs.Latency is the time it takes for a system to process a single inference \nis the average response time from when a user sends a request, and the service provides a result \nMeanwhile, the throughput is the average number of requests the API processes and \nLow-latency systems require optimized and often more costly infrastructure, such as faster pro-\nA lower latency translates to higher throughput when the service processes multiple queries in \nFor example, if the service takes 100 ms to process requests, this translates to \na throughput of 10 requests per second.\nIf the latency reaches 10 ms per request, the throughput \nprocess 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests \nIf you process 60 requests in 200 ms, the latency is 200 ms, while the throughput \nThus, even when batching requests at serving time, it’s essential \nData is the foundation of the inference process.\nThe type and size of the data directly impact latency and throughput, as more complex or exten-\nthat supports the deployment and operation of the ML models.\nnecessary resources for deploying, scaling, and maintaining ML models.\nFor high throughput, the systems require scalable infrastructure to manage large data \nInfrastructure must be optimized to reduce processing time to achieve low latency, such \nlow latency while batching your requests, you often have to sacrifice high throughput \nAs you process fewer requests per second, it results in idle computing, which \nLet’s move on to the three deployment architectures we can leverage to serve our models.\nUnderstanding inference deployment types\nYou must consider how the data is accessed and the infrastructure you are working with.\nFor example, serving your model in \nOtherwise, you have to serve your model in real-time, which is more infrastruc-\nFigure 10.1: The three fundamental architectures of inference deployment types\nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which \nimmediately processes the request and returns the result in the same response.\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML \nrecommendation engines in platforms like TikTok. The simplicity of real-time inference, with its direct client-server interaction, makes it an attrac-\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the \nInstead, the ML service processes the request asynchronously.",
    "keywords": [
      "Inference Pipeline Deployment",
      "LLM Twin",
      "Inference",
      "Deployment",
      "LLM Twin service",
      "Inference Pipeline",
      "inference deployment types",
      "throughput",
      "latency",
      "Pipeline Deployment",
      "deployment types",
      "requests",
      "LLM",
      "data",
      "model"
    ],
    "concepts": [
      "data",
      "latency",
      "deployment",
      "deploying",
      "deployed",
      "model",
      "models",
      "processes",
      "process",
      "processed"
    ]
  },
  {
    "chapter_number": 46,
    "title": "Segment 46 (pages 391-399)",
    "start_page": 391,
    "end_page": 399,
    "summary": "In a batch transform architecture, the ML service pulls data from a storage \nUnlike the asynchronous inference architecture, a batch transform design is optimized for high \nthis approach can significantly reduce costs, as processing data in big batches is the most eco-\nMoreover, the batch transform architecture is the simplest way to serve a model, \nThe client pulls the results directly from data storage, decoupling its interaction with the ML \nTaking this approach, the client never has to wait for the ML service to process its input, \nTo conclude, we examined the three most common architectures for serving ML models.\nstarted with online real-time inference, which serves clients when they request a prediction.\nMonolithic versus microservices architecture in \nIn the previous section, we saw three different methods of deploying the ML service.\nences in architecture were mainly based on the interaction between the client and the ML service, \nBut another aspect to consider is the architecture of the ML service itself, which can be imple-\nFigure 10.2: Monolithic versus microservices architecture in model serving\nMonolithic architecture\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-pro-\ncessing steps) are bundled into a single service in a monolithic architecture.\nOne key challenge of a monolithic architecture is the difficulty of scaling components independent-\nThe LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound.\nresource use, with the GPU being idle when the business logic is executed and vice versa.\nMicroservices architecture\nA microservices architecture breaks down the inference pipeline into separate, independent ser-\nvices—typically splitting the LLM service and the business logic into distinct components.\nFor instance, since the LLM service might require more GPU resources \nThis optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus \nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service \ncomponents, you will run only what is required on the GPU machine and not block the GPU VM \nFigure 10.3: Scaling microservices independently based on compute requirements\nNote that the proposed design for decoupling the ML model and business logic into two services \narchitecture for your application needs.\nThe choice between monolithic and microservices architectures for serving ML models largely \nif the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, \nmore complex systems where different components have varying scaling needs or require distinct \nas GPU-intensive LLM services.\nthe system for keeping these machines busy all the time or quickly scaling down when the GPU \ncomplex and costly, you must design the monolith application with this in mind.\nservices when the time comes.\nML and business logic into two different Python modules that don’t interact with each other.\nif you start with a monolith and down the line you want to move to a microservices architecture, \nIn summary, monolithic architectures offer simplicity and ease of maintenance but at the cost of \nAt the same time, microservices provide the agility to scale and innovate \nExploring the LLM Twin’s inference pipeline \nstrategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to \nthe selection of an online real-time inference deployment architecture.\nOn the monolith versus microservice aspect, we will split the ML service between a REST API \nserver containing the business logic and an LLM microservice optimized for running the given \nAs the LLM requires a powerful machine to run the inference, and we can further optimize \nwith the microservice architecture.\nThe LLM microservice is strictly optimized for the RAG generation component.\nIn summary, our approach involves implementing an online real-time ML service using a micro-\nservice architecture, which effectively splits the LLM and business logic into two distinct services.\nFigure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\nA fine-tuned LLM generated by the training pipeline, which is pulled from our model \nThe prompt is sent to the LLM microservice through an HTTP request.\nNow, let’s explore what tech stack we used to implement the architecture presented in Figure \nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast.\nThe LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s ",
    "keywords": [
      "Inference Pipeline Deployment",
      "LLM",
      "Inference Pipeline",
      "LLM microservice",
      "architecture",
      "business logic",
      "service",
      "Inference",
      "microservices architecture",
      "Pipeline",
      "Pipeline Deployment",
      "GPU",
      "microservices",
      "model",
      "business"
    ],
    "concepts": [
      "microservices",
      "microservice",
      "inference",
      "infer",
      "service",
      "services",
      "architectures",
      "architecture",
      "time",
      "requirements"
    ]
  },
  {
    "chapter_number": 47,
    "title": "Segment 47 (pages 400-411)",
    "start_page": 400,
    "end_page": 411,
    "summary": "The SageMaker Inference deployment is composed of the following components that we will \nenable real-time predictions from deployed models.\nEndpoint configurations are used when creating \nmodel and configuration to an endpoint.\nYou can deploy multiple models to an endpoint, \nOnce deployed, models are easily accessible via the \nTogether, these components create a robust infrastructure for deploying and managing ML models \ndeploy the inference pipeline.\nInference Pipeline Deployment\nDeploying the LLM Twin service\nwe will deploy the LLM microservice using AWS SageMaker and the business microservice using \nDeploy our fined-tuned LLM Twin model to AWS SageMaker\nWrite an inference client to interact with the deployed model\nImplementing the LLM microservice using AWS SageMaker\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon \nSageMaker as an online real-time inference endpoint.\ninference container, known as the Hugging Face LLM DLC, to deploy our LLM.\nThese containers are designed to simplify the process of training and deploying \nInference Pipeline Deployment\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, \nwill be hosted on AWS SageMaker under inference endpoints that can be accessed through HTTP \nWe will start by deploying the \nLLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.\nConfiguring SageMaker roles\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and \nroles to access and deploy the SageMaker infrastructure.\nthe deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3.\ndeploy everything related to SageMaker to ensure we modify only the resources associated \nllm_engineering/infrastructure/aws/roles/create_sagemaker_role.py.\nWe will attach this role to the SageMaker deployment, \ndeployment.\nDeploying the LLM Twin model to AWS SageMaker\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which \nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command: poe \ndeploy-inference-endpoint.\nexcept for creating the SageMaker AWS IAMs we created and configured in the previous step.\nInference Pipeline Deployment\nmate the deployment process, starting with the create_endpoint() function.\ntest the CLI command and check the AWS console to see whether the deployment was successful.\nThe SageMaker deployment code is available at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.\nFigure 10.5: AWS SageMaker deployment steps\nmain function that deploys the LLM Twin model to AWS SageMaker.\nfunction, which is later passed to the deployment strategy class, along with an instance of the \nresource manager and deployment service:\ndef create_endpoint(endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED):\ndeployment_service = DeploymentService(resource_manager=resource_\nconfig=hugging_face_deploy_config,\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,\nendpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,\nthe deployment process.\nInference Pipeline Deployment\nNext, we implement the endpoint_config_exists method, checking whether a specific Sage-\nself.sagemaker_client.describe_endpoint_\nSageMaker endpoint:\nself.sagemaker_client.describe_endpoint(EndpointName=endpoint_\nwhich will interface with AWS SageMaker and an instance of the ResourceManager class we \nThe deploy() method is the heart of the DeploymentService class.\nentire process of deploying a model to a SageMaker endpoint.\ndef deploy(\nif self.resource_manager.endpoint_config_exists(endpoint_config_\nself.prepare_and_deploy_model(\nInference Pipeline Deployment\nlogger.info(f\"Successfully deployed/updated model to endpoint \n{endpoint_name}.\")\nlogger.error(f\"Failed to deploy model to SageMaker: {e}\")\nThe deploy method begins by checking whether the endpoint configuration already exists using \ndeploy_model() method, which is responsible for the actual deployment of the model to the \nspecified SageMaker endpoint.\nThe prepare_and_deploy_model() method is a static method within the DeploymentService\nThis method is focused on setting up and deploying the Hugging Face model to SageMaker:\ndef prepare_and_deploy_model(\nhuggingface_model.deploy(\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from \nSageMaker designed to handle Hugging Face models.\nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy \nThis deployment process involves specifying the type of instance used, the number of \ncludes optional resources for more complex deployments, such as the initial_instance_count\nThe class is initialized only with an instance of a deployment service, \ndef __init__(self, deployment_service):\nhow the Hugging Face model should be deployed to AWS SageMaker:\ndef deploy(\nInference Pipeline Deployment\nlogger.info(\"Starting deployment using Sagemaker Huggingface \nrole_arn: The AWS IAM role that provides permissions for the SageMaker deployment.\nendpoint_name and endpoint_config_name: Names for the SageMaker endpoint and its \nresources: Optional resources dictionary used for multi-model endpoint deployments.\nendpoint_type: This can either be MODEL_BASED or INFERENCE_COMPONENT, determining \nself.deployment_service.deploy(",
    "keywords": [
      "endpoint",
      "Inference Pipeline Deployment",
      "AWS",
      "LLM Twin model",
      "AWS SageMaker",
      "Hugging Face",
      "LLM",
      "inference pipeline",
      "deployment",
      "SageMaker",
      "Inference",
      "LLM Twin",
      "model",
      "Hugging Face model",
      "config"
    ],
    "concepts": [
      "deployed",
      "deploy",
      "deploying",
      "deployments",
      "deploys",
      "inference deployment",
      "sagemaker",
      "models",
      "model",
      "endpoint"
    ]
  },
  {
    "chapter_number": 48,
    "title": "Segment 48 (pages 412-421)",
    "start_page": 412,
    "end_page": 421,
    "summary": "sources are leveraged when setting up multi-endpoint configurations that use multiple replicas \nInference Pipeline Deployment\nwhen the model is deployed to a SageMaker endpoint.\ndeploying the endpoint, we suggest modifying them and seeing how the performance of the LLM \nUltimately, let’s review the settings configuring the LLM engine.\npoetry poe deploy-inference-endpoint\nThat’s all you need to deploy an inference pipeline to AWS SageMaker.\nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker \nin the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.\nFigure 10.6: AWS SageMaker Inference endpoints example\nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve gen-\nInference Pipeline Deployment\nFigure 10.7: AWS SageMaker twin inference endpoint example\nCalling the AWS SageMaker Inference endpoint\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service.\ninference endpoint through HTTP requests, and decode the results in a way the client can work \nAll the AWS SageMaker Inference code is available on GitHub at llm_engineering/model/\ninference.\ntext = \"Write me a post about AWS SageMaker inference endpoints.\"\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE\nSageMaker endpoint:\nclass LLMInferenceSagemakerEndpoint(Inference):\ninference_component_name: Optional[str] = None,\nself.inference_component_name = inference_component_name\nOne of the key features of the class is its ability to generate a default payload for inference requests.\ndef _default_payload(self) -> Dict[str, Any]:\n\"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n\"top_p\": settings.TOP_P_INFERENCE,\n\"temperature\": settings.TEMPERATURE_INFERENCE,\nfor inference.\nThe parameters section includes settings that influence the model’s behavior during \nthe application’s settings, ensuring consistency across different inference tasks.\nThe class allows customization of the payload through the set_payload() method, which enables \nthe user to modify the inputs and parameters before sending an inference request:\ndef set_payload(self, inputs: str, parameters: Optional[Dict[str, Any]] = \nInference Pipeline Deployment\nAdditionally, it allows for modifying inference parameters if any are provided.\nUltimately, we leverage the inference() method to call the SageMaker endpoint with the cus-\ndef inference(self) -> Dict[str, Any]:\nif self.inference_component_name not in [\"None\", None]:\ninvoke_args[\"InferenceComponentName\"] = self.inference_\nlogger.exception(\"SageMaker inference failed.\")\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint.\nwe previously presented to send HTTP requests to the AWS SageMaker endpoint.\nThe llm parameter accepts any instance that implements the Inference interface, such \nas the LLMInferenceSagemakerEndpoint class, which is used to perform the inference.\nllm: Inference,\nsent to the LLM by formatting the prompt with the user’s query and context.\nthe model from generating repetitive text, and the temperature setting that controls the ran-\nOnce the payload and parameters are set, the method calls the inference function from \nInference Pipeline Deployment\nself.llm.set_payload(\n\"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n\"temperature\": settings.TEMPERATURE_INFERENCE,\nanswer = self.llm.inference()[0][\"generated_text\"]\npoetry run python -m llm_engineering.model.inference.test\npoetry poe test-sagemaker-endpoint\nmicroservice will send HTTP requests to the LLM microservice defined above and call the RAG \nclasses represent the request and response structure for the FastAPI endpoints:\nllm_service() function wraps the inference logic used to call the SageMaker LLM microservice:\ndef call_llm_service(query: str, context: str | None) -> str:\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\nanswer = InferenceExecutor(llm, query, context).execute()\nAlso, as the LLM inference logic is moved to a different microservice, the call_llm_service()\nanswer = call_llm_service(query, context)\nInference Pipeline Deployment\npoetry poe run-inference-ml-service",
    "keywords": [
      "AWS SageMaker Inference",
      "Inference",
      "Inference Pipeline Deployment",
      "LLM",
      "AWS SageMaker",
      "SageMaker Inference endpoint",
      "endpoint",
      "SageMaker Inference",
      "model",
      "Inference Pipeline",
      "Inference endpoint",
      "AWS",
      "SageMaker",
      "SageMaker endpoint",
      "RAG"
    ],
    "concepts": [
      "infer",
      "inference",
      "inference_",
      "setting",
      "settings",
      "sets",
      "set",
      "parameters",
      "parameter",
      "sagemaker"
    ]
  },
  {
    "chapter_number": 49,
    "title": "Segment 49 (pages 422-429)",
    "start_page": 422,
    "end_page": 429,
    "summary": "So far, the SageMaker LLM microservice has used a static number of replicas to serve our users, \nOnce you’re done testing your inference pipeline deployment, deleting all your AWS \nSageMaker resources used to deploy the LLM is essential.\nAs almost all AWS re-\nyou’re done testing your SageMaker infrastructure (or any AWS resource).\nwe have provided a script that deletes all the AWS SageMaker resources for you:\nFor example, let’s assume we requested four copies (replicas) with the following \nThe solution is to implement an autoscaling strategy that scales the number \nof replicas up and down dynamically based on various metrics, such as the number of requests.\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints \nscale in and out based on the number of requests.\nreplica so the server remains responsive to new user requests or even scales down to zero if the \nwe have to keep two replicas online, and when the number of requests spikes to 100 per second, \nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference \nSageMaker provides a feature called Application Auto Scaling that allows you to scale \nThe first step in enabling autoscaling for your resources is to register a scalable target with the \nApplication Auto Scaling feature AWS provides.\nresource you intend to scale, as well as setting the boundaries within which the scaling should \nFor instance, when working with SageMaker Inference components, you’ll define the following:\nResource ID: This serves as a unique identifier for the resource you want to scale, typically \nincluding the name of the SageMaker Inference component.\nScalable dimension: This specifies the resources to be scaled, such as the desired number \ning strategies, such as minimum and maximum limits of the number of replicas.\nBy registering a scalable target, you prepare your SageMaker Inference component for future \nOnce your scalable target is registered, the next step is defining how the scaling should occur.\nThis is where creating a scaling policy comes in.\nA scaling policy defines specific rules that trigger \nscaling events.\nIn the context of our SageMaker Inference component, the scalable policy might include the \nthe resource’s capacity to maintain a specific target value for a chosen metric.\nspecifying cooldown periods that control how quickly scaling actions can occur after \nThe scaling policy defines the rules for your scaling-in and scaling-out strategy.\ntarget value, it triggers actions to scale the number of inference component copies up or down, \nOnce defined, Application Auto Scaling creates and manages the necessary \nFor instance, consider an application running on SageMaker.\nGPU usage exceeds the target, the system scales out, adding resources to manage the increased \nConversely, when GPU usage drops below the target, the system scales in, reducing capacity \nOne significant advantage of setting up target tracking policies using Application Auto Scaling is \ndefine scaling adjustments manually.\nMinimum and maximum scaling limits\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish \nyour maximum and minimum scaling limits before creating your scaling policy.\nNext, configure the maximum value, which defines the upper limit of resources your model can \nscale up to.\nThus, you can scale up as much as your application needs within the \nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to \nof instances during scale-in requests and restricts the creation of new replicas during scale-out \nOnce you understand how to configure scaling policies for SageMaker, you can imme-\nFor a step-by-step guideline on how to configure autoscaling for the AWS SagaMak-\nAWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-\nIf your scaling policy or cooldown period is too sensitive, you may \nfor instance, that you expect your system to support an average of 100 users per minute and scale \nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Infer-\nmicroservice deployed on AWS SageMaker.\nWe also reviewed a popular autoscaling strategy that scales in and out \nbased on a given set of metrics and saw how to implement it in AWS SageMaker.\nuser-in-aws\nhttps://www.oreilly.com/library/view/machine-learning-",
    "keywords": [
      "AWS",
      "inference pipeline deployment",
      "AWS SageMaker",
      "Application Auto Scaling",
      "AWS SageMaker resources",
      "Scaling",
      "SageMaker Inference",
      "AWS SageMaker Inference",
      "inference",
      "SageMaker",
      "AWS Elastic Container",
      "inference pipeline",
      "AWS EKS",
      "SageMaker Inference component",
      "number"
    ],
    "concepts": [
      "scales",
      "scale",
      "scaling",
      "scaled",
      "aws",
      "inference",
      "infer",
      "sagemaker",
      "resources",
      "resource"
    ]
  },
  {
    "chapter_number": 50,
    "title": "Segment 50 (pages 430-437)",
    "start_page": 430,
    "end_page": 437,
    "summary": "MLOps and LLMOps\nmodels (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to \nBut MLOps is not just about these components; it takes an ML \napplication to the next level by automating data collection, training, testing, and deployment.\nThus, the end goal of MLOps is to automate as much as possible and let users focus on the most \nadopt these pre-trained foundational models for their use cases, focusing on LLMOps problems \nMLOps and LLMOps\ndeployment (CI/CD) pipeline to test the integrity of our code and automate the deployment \nprocess, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline \nstarting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging \ninto LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as \nThe path to LLMOps: Understanding its roots in DevOps and MLOps\nDevOps and MLOps\nThen, we will move to MLOps to understand how \nThus, DevOps was born to automate the process of shipping software at scale.\ncifically, DevOps is used in software development, where you want to completely automate your \nSuperior quality and security: DevOps ensures swift software development while main-\nMLOps and LLMOps\nDeployment environments: To thoroughly test your code before shipping it to produc-\nVersion control: Used to track, manage, and version every change made to the source code.\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field \nMLOps\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML.\napplication, such as the data, model, and, finally, the code.\nMLOps and LLMOps\nthe code, modifications in the data, or adjustments to the model.\nFigure 11.2: Relationship between data, model, and code changes\nIn that case, you must train (or fine-tune) a new model, resulting \na few examples that can trigger a change in the data and indirectly in the model:\nAfter deploying the ML model, its performance might decay as time passes, so we need \ndata or re-label it, which generates a new set of models.\nSo, what is MLOps?\nthat makes data and models their first-class citizen while preserving the DevOps methodology.\nLike DevOps, MLOps originates from the idea that isolating ML model development from its \ndeployment process (ML operations) diminishes the system’s overall quality, transparency, and \nMLOps core components\ner on the MLOps core components now that we better understand the field.\ncontrol and CI/CD, MLOps revolves around:\nModel registry: A centralized repository for storing trained ML models (tools: Comet \nFeature store: Preprocessing and storing input data as features for both model training \nML metadata store: This store tracks information related to model training, such as model \nconfigurations, training data, testing data, and performance metrics.\nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools: ZenML, \nMLOps and LLMOps\nmanual processes to automated pipelines through CT and CI/CD.\nretraining and deployment of ML models in response to triggers such as new data, per-\nautomation ensures that our ML systems are robust, scalable, and adaptable to changing \nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, \nCode is tracked using tools like Git, models are \nversioned through model registries, and data versioning can be managed using solutions \nExperiment tracking: As training ML models is an iterative and experimental process \nTesting: MLOps suggests that along with testing your code, you should also test your \ndata and models through unit, integration, acceptance, regression, and stress tests.\nMonitoring: This stage is vital for detecting performance degradation in served ML models \nmodel metrics and detecting drifts, we can maintain the health of ML systems in produc-",
    "keywords": [
      "MLOps",
      "DevOps",
      "LLM Twin",
      "model",
      "data",
      "LLMOps",
      "code",
      "models",
      "LLM",
      "software",
      "pipeline",
      "LLMs",
      "training",
      "Twin",
      "core"
    ],
    "concepts": [
      "mlops",
      "devops",
      "model",
      "models",
      "ml",
      "data",
      "tool",
      "tools",
      "tooling",
      "operations"
    ]
  },
  {
    "chapter_number": 51,
    "title": "Segment 51 (pages 438-445)",
    "start_page": 438,
    "end_page": 445,
    "summary": "MLOps engineering\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description \nan MLOps engineer, you have a lot of work to do on the infrastructure side.\nseen in this section, an MLOps engineer still has to implement things such as experiment tracking, \nthese into the code and the MLOps engineer focus on making them work on their infrastructure.\nMLOps\ndata scientist/ML researcher, ML engineer, and MLOps engineer.\nMLOps and LLMOps\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of \nHowever, the MLOps engineer plays a pivotal role in this \nLLMOps encompasses the practices and processes essential for managing and running LLMs.\nassociated with LLMs. While MLOps addresses the principles and practices of managing various \nML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly \ncomplex training requirements, prompt management, and non-deterministic nature of generating \nWhen training LLMs from scratch, the data and model dimensions of an ML system grow sub-\nstantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns \nsive datasets required for training LLMs. It involves big data techniques for processing, \nThe massive size of LLMs directly impacts model training.\nWhen training an LLM from \noptimizing your processes and infrastructure to support data, model, or tensor parallelism.\nAt its core, LLMOps is MLOps at scale.\nIt uses the same MLOps principles but is applied to big data \nand huge models that require more computing power to train and run.\ntions now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or \nThus, for most LLM applications out there, your development steps will involve the selection of a \nfoundation model, which you further have to optimize by using prompt engineering, fine-tuning, \ndive into some popular components of LLMOps that can improve prompt engineering, fine-tun-\nMLOps and LLMOps\nyour LLM can produce harmful output or receive dangerous input, so you should monitor and \nyour system (model jailbreaking), and accepting violent or unethical prompts.\napplications don’t want to accept violent or unethical queries from users, such as asking \nOutput guardrails: At the output of an LLM response, you want to catch failed outputs \nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data \nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the \nthese tools, you usually want to track the user input, the prompt templates, the input variables, \nthe generated response, the number of tokens, and the latency.\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we \nThus, when it comes to tracking the latency of generating an answer, the final user experience \nTime per Output Token (TPOT): The time it takes to generate each output token\nMLOps and LLMOps\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting \nyour LLMs. Ultimately, you can compute metrics that validate your model’s performance for each input, \nhave multiple intermediate steps from the user query to the final general answer.\nand the final prompt sent to the model.\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated \nthe application can behave unexpectedly if the number of generated tokens suddenly fluctuates \nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong \nDeploying the LLM Twin’s pipelines to the cloud\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud.\nNote that the training and inference pipelines already work with AWS SageMaker.\nSageMaker training and inference components are more costly to run (which we \nMLOps and LLMOps\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, ",
    "keywords": [
      "LLM",
      "MLOps",
      "LLMs",
      "MLOps engineer",
      "training LLMs",
      "LLMOps",
      "data",
      "LLM Twin",
      "training",
      "output",
      "model",
      "LLM systems",
      "input",
      "prompt",
      "models"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "prompt",
      "prompts",
      "training",
      "trained",
      "train",
      "output",
      "outputting"
    ]
  },
  {
    "chapter_number": 52,
    "title": "Segment 52 (pages 446-455)",
    "start_page": 446,
    "end_page": 455,
    "summary": "Build a Docker image that contains all the system dependencies, the project dependencies, \nPush the Docker image to ECR, where SageMaker can access it.\nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS \nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2.\nBased on the pulled image, it creates a Docker container that executes the pipeline step.\nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and \nWe will show you how to create and integrate a free MongoDB cluster into our projects.\nIn the left panel, go to Deployment | Database and click Build a Cluster.\nWhat AWS cloud region should I choose?\nTo test that your newly created MongoDB cluster works fine, we must connect to it from \nThe final step is to return to your project and open your .env file.\ncloud MongoDB cluster we just created.\nThus, to create a Qdrant cluster \nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\nIn the left panel, go to Clusters and click Create.\nClick Create and choose your twin cluster to create a new access token.\nGo back to the Clusters section of Qdrant and open your newly created twin cluster.\nwill have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\nQDRANT_CLOUD_URL=<the endpoint URL found at step 7>\nQDRANT_APIKEY=<the access token created at step 5>\ncloud Qdrant cluster we just created.\nend data pipeline with the cloud version of MongoDB and Qdrant as follows:\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.\nSetting up the ZenML cloud\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process.\nup a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, \nwe will bundle our code in a Docker image to run it in AWS SageMaker.\nLet’s start with setting up the ZenML cloud:\nML cloud tenant, return to the project and run the zenml connect command provided \nTo ensure everything works fine, run a random pipeline from your code.\nGo to the Pipelines section in the left panel of the ZenML dashboard.\nTo ship the code to AWS, you must create a ZenML stack.\nworking locally, ZenML offers a default stack that allows you to quickly develop your code and \ninfrastructure environments, such as local and AWS runs, which we will showcase in this section.\nWith that in mind, let’s create an AWS stack for our project.\nThen, choose AWS as your cloud provider.\nNow ZenML will create a set of IAM roles to give permissions to all the other components \nThen run poetry lock --no-update && poetry install\nleverages CloudFormation (an infrastructure as code, or IaC, tool) to create all the AWS \nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML \nAWS resources or to connect ZenML with your current infrastructure.\nand deploying Docker container images easy.\nCloudFormation is a service that allows you to model and set up your AWS re-\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker \nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing.\nThe last step is to find a way to take our code and run it on top of this infrastructure.\npopular solution is Docker, a tool that allows us to create an isolated environment (a container) \nthat contains everything we need to run our application, such as system dependencies, Python \nWe defined our Docker image at the project’s root in the Dockerfile.\nBefore digging into the code, if you want to build the Docker image your-\nself, ensure that you have Docker installed on your machine.\nby following the instructions provided here: https://docs.docker.com/engine/install.\nPoetry to be installed is specified, and a few environment variables are set to ensure that package \nNext, we install Google Chrome in the container.\nthe stable version of Google Chrome is installed.\nFollowing the Chrome installation, other essential system dependencies are installed.\nPoetry, the dependency management tool, is then installed using pip.\nAfter installation, Poetry \nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\"\nRUN poetry config installer.max-workers 20\nWith the dependency files in place, the project’s dependencies are installed using Poetry.\ninstalled directly into the container’s Python environment.\nRUN poetry config virtualenvs.create false && \\\npoetry install --no-root --no-interaction --no-cache --without dev && \nIn the final step, the entire project directory from the host machine is copied into the container’s \nThis step ensures that all the application files are available within the container.\nOne important trick when writing a Dockerfile is to decouple your installation steps from copy-\nproject dependencies but mostly change your code, copying your project files in the last step makes \nIt allows the project to run smoothly in any environment that supports Docker.\nThe last step is to build the Docker image and push it to the ECR created by ZenML.\nDocker image from the root of the project, run the following:\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works \nThe tag of the newly created Docker image is llmtwin.\npoetry poe build-docker-image",
    "keywords": [
      "AWS",
      "Docker image",
      "Docker",
      "Qdrant",
      "ZenML",
      "ZenML cloud",
      "create",
      "step",
      "AWS infrastructure",
      "Qdrant cluster",
      "cloud",
      "run",
      "cluster",
      "MongoDB",
      "poetry"
    ],
    "concepts": [
      "aws",
      "mongodb",
      "zenml",
      "docker",
      "poetry",
      "creates",
      "create",
      "created",
      "creating",
      "step"
    ]
  },
  {
    "chapter_number": 53,
    "title": "Segment 53 (pages 456-464)",
    "start_page": 456,
    "end_page": 464,
    "summary": "Figure 11.7: AWS ECR example\ndocker tag llmtwin ${AWS_ECR_URL}:latest\ndocker push ${AWS_ECR_URL}:latest\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository.\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.\nRun the pipelines on AWS\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps.\nzenml stack set aws-stack\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9.\nWe’ve configured the pipeline to always use the latest Docker image available in ECR.\nThe last step is setting up to run the pipelines asynchronously so we don’t have to wait until they \nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our \nRun the end-to-end-data-pipeline with the \npoetry poe run-end-to-end-data-pipeline\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run.\nthe ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10.\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\nTo find even more details about the runs, you can go to AWS SageMaker.\nTo run other pipelines, you have to update the settings.docker.parent_image\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\na ZenML pipeline on SageMaker\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline \nIf you want to run the pipelines locally again, use the following CLI command:\nprocess and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML.\nAs mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to \nLLM Twin’s CI/CD pipeline flow\nAs illustrated in Figure 11.14, the CI pipeline is triggered when the PR opens.\nImplementing a CI pipeline ensures that new features follow the repository’s standards and \nFigure 11.14: CI/CD pipelines flow\nThe CD pipeline runs after the branch is merged.\ninto staging, the CD pipeline takes the code from the staging branch, builds a new Docker im-\nage, and pushes it to the AWS ECR Docker repository.\nWhen running future pipeline runs in \nthe staging environment, it will use the latest Docker image that was built by the CD pipeline.\nfurther manually test the new feature along with what is automatically tested in the CI pipeline.",
    "keywords": [
      "AWS ECR",
      "AWS",
      "AWS ECR Docker",
      "AWS ECR dashboard",
      "AWS ECR ZenML",
      "ECR",
      "ECR URL",
      "AWS ECR repository",
      "pipeline",
      "Docker image",
      "Figure",
      "docker",
      "main AWS ECR",
      "LLM Twin",
      "ZenML"
    ],
    "concepts": [
      "aws",
      "zenml",
      "pipeline",
      "pipelines",
      "running",
      "run",
      "runs",
      "docker",
      "step",
      "steps"
    ]
  },
  {
    "chapter_number": 54,
    "title": "Segment 54 (pages 465-475)",
    "start_page": 465,
    "end_page": 475,
    "summary": "GitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their \nworkflows directly within a GitHub repository.\ncode directly from GitHub by defining workflows in YAML files.\nrepository’s .github/workflows directory.\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same \ncurrent GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\nmight trigger a workflow every time code is pushed to a specific branch.\nhow GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.\nGitHub Actions CI YAML file\nThe YAML file sits under .github/workflows/ci.yaml.\nHence, the CI workflow will automatically run \nThe concurrency section ensures that only one instance of this workflow runs for a given reference \ntrue line ensures that if a new workflow run is triggered before the previous one finishes, the \nThe workflow defines two separate jobs: qa and test.\nThe first job, named QA, is responsible for quality assurance tasks like code checks and format-\nWithin the qa job, the first step is to check out the repository’s code using the \nThis step is necessary to ensure that the job has access to the code \nsteps in the job will run in the correct Python environment.\nThe workflow then installs Poetry using the abatilo/actions-poetry@v2 action, specifying the \nuses: abatilo/actions-poetry@v2\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the \nThe qa job then runs several quality checks on the code.\nrun: poetry poe gitleaks-check\nFollowing the gitleaks check, the workflow runs a linting process to enforce coding standards \nrun: poetry poe lint-check\nThe last step in the qa job is a format check, which ensures that the Python code is properly for-\nrun: poetry poe format-check\nThe second job defined in the workflow is the test job, which also runs on the latest version \nLike the qa job, it starts by checking out the code from the repository and installing \nFinally, the test job runs the project’s tests using the poetry poe test command.\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting \nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository.\ncommit with the message feat: Add Docker image and CD pipeline and ran the two jobs de-\nFigure 11.15: GitHub Actions CI pipeline run example\nThe CD pipeline will automate the Docker steps we manually performed in the Deploying the \nPush the Docker image to AWS ECR.\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under .github/workflows/\ntrigger is any push to the repository’s main branch.\nThis workflow will automatically run when \nnew code is pushed to the main branch, usually when a PR is merged into the main branch.\nThe workflow then defines a single job named Build & Push Docker Image:\nname: Build & Push Docker Image\nThe first step within the job is to check out the repository’s code.\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends \nrepository’s secrets to authenticate the workflow with AWS.\nnecessary permissions to push Docker images to the ECR repository.\nuses: aws-actions/configure-aws-credentials@v1\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR.\nuses: aws-actions/amazon-ecr-login@v1\nThe final step in the workflow involves building the Docker image and pushing it to the Ama-\nThis is accomplished using the docker/build-push-action@v6 action.\n- name: Build images & push to ECR\nuses: docker/build-push-action@v6\nTo conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to \nThe Docker image is pushed with latest and the commit’s SHA tag.\nAs seen before, we only have the Build & Push Docker Image job here.\nFigure 11.16: GitHub Actions CD pipeline run example\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.\nTest out the CI/CD pipeline\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full \nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point \nThese secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create \nTo trigger the CD pipeline, merge the PR into the main branch.\nFor the AWS_ECR_NAME, you should configure only the name of the repository (e.g., \nAfter the CD GitHub Actions are complete, check the ECR repository to see whether the Docker \nFigure 11.18: GitHub Actions secrets\nIf you need more details on how to set up GitHub Actions secrets, we recommend checking out \nactions/security-guides/using-secrets-in-github-actions\nCT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, ",
    "keywords": [
      "GitHub Actions",
      "Actions",
      "Docker image",
      "Docker",
      "workflow",
      "Push Docker Image",
      "GitHub",
      "aws",
      "code",
      "ECR",
      "pipeline",
      "Poetry",
      "GitHub Actions secrets",
      "GitHub Actions workflow",
      "run"
    ],
    "concepts": [
      "actions",
      "action",
      "code",
      "coding",
      "github",
      "jobs",
      "job runs",
      "run",
      "running",
      "workflows"
    ]
  },
  {
    "chapter_number": 55,
    "title": "Segment 55 (pages 476-483)",
    "start_page": 476,
    "end_page": 483,
    "summary": "for our pipelines and a way to monitor their execution.\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our \nFigure 11.19: CT pipeline\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines \nand how the pipelines are triggered by each other.\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline.\nREST API triggers: You can call a pipeline by an HTTP request.\nwhen integrating your ML pipelines with other components.\nZenML’s documentation: https://docs.zenml.io/v/docs/how-to/trigger-pipelines/\ntrigger-a-pipeline-from-rest-api.\nScheduled triggers: Another common approach is to schedule your pipeline to run con-\nfollowing example from ZenML, the pipeline is scheduled every hour:\nWe chose a manual trigger for our LLM Twin use case as we don’t have other components to lever-\nWhen it finds any, it generates a new config and triggers the pipelines through the REST API.\nother option is implementing the watcher as an additional pipeline and leveraging the schedule \ntriggers to look daily for new data.\nThe conclusion is that once you can manually trigger all your ML pipelines through a single \nTrigger downstream pipelines\ncollection pipeline has finished, it will trigger the feature pipeline.\nWhen the feature pipeline has \nbeen completed successfully, it triggers the dataset generation pipeline, and so on.\nthe logic more complex, like scheduling the generate instruct dataset pipeline to run daily, check-\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything \n@pipeline\nand deploy logic to the parent pipeline to implement an end-to-end flow.\nTo run the end-to-end pipeline, use the following poe command:\npoetry poe run-end-to-end-data-pipeline\npipeline isolated and use triggers to start downstream pipelines.\nFigure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\nwe have more, we avoided that limitation by compressing all the steps into a single pipeline.\nly trigger a pipeline from another pipeline, as you can see in the code snippet below where we \ntriggered the feature engineering pipeline after the data collection ETL:\nfrom zenml import pipeline, step\n@pipeline \ntrigger_feature_engineering_pipeline(user)\ndef trigger_feature_engineering_pipeline(user):\nClient().trigger_pipeline(\"feature_engineering\", run_configuration=run_\n@pipeline\ndef llm_chain(input_text: str) -> str:\ndef llm_chain(input_text):\n\"value\": compute_llm_judge_score(…),\nprompt that already contains the user’s input and context and returns an answer that is usually \nFigure 11.21: Inference pipeline serving architecture",
    "keywords": [
      "pipeline",
      "LLM",
      "pipelines",
      "data",
      "str",
      "LLM Twin",
      "triggers",
      "user",
      "trigger",
      "REST API triggers",
      "ZenML",
      "REST API",
      "author",
      "input",
      "feature"
    ],
    "concepts": [
      "pipelines",
      "pipeline",
      "zenml",
      "trigger",
      "triggered",
      "triggers",
      "user_full_name",
      "user",
      "def",
      "data"
    ]
  },
  {
    "chapter_number": 56,
    "title": "Segment 56 (pages 484-491)",
    "start_page": 484,
    "end_page": 491,
    "summary": "def call_llm_service(query: str, context: str | None) -> str:\nanswer = InferenceExecutor(llm, query, context).execute()\ndef rag(query: str) -> str:\nanswer = call_llm_service(query, context)\npost-processing steps, such as the ContextRetriever search function:\nMLOps and LLMOps\nquery_model = Query.from_str(query)\nquery_model = self._metadata_extractor.generate(query_model)\ndef generate(self, query: str) -> str:\ndef rag(query: str) -> str:\nanswer, prompt = call_llm_service(query, context)\nModel configuration: Here, we should consider both the LLM and other models used \nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such \nFor example, you can add a callback in your training pipeline to trigger \nfrom zenml import get_pipeline_context, pipeline\ndef training_pipeline(…):\nMLOps and LLMOps\nZenML and most orchestrators simplify implementing an alerter, as it’s a critical component \npipeline is, the three core dimensions of an ML application (code, data, model), and that, after \ndeployment, it is more critical than ever to implement a monitoring and alerting layer due to \nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud.\nThe final step was to add LLMOps to our LLM Twin project.\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an \nto any LLM-based application.\nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with \ndata collection and fine-tuning until deploying the LLM microservice and RAG service.\nMLOps: Continuous delivery and automation pipelines in machine learning.\nhttps://cloud.google.com/architecture/mlops-continuous-\nhttps://ml-ops.org/content/mlops-principles\nhttps://ml-ops.org/content/mlops-principles\n(2024c, July 5).\nhttps://ml-ops.org/content/motivation\nmadewithml.com/courses/mlops/monitoring/\nTesting Machine Learning Systems: Code, Data and Models.\nhttps://madewithml.com/courses/mlops/testing/\nMLOps and LLMOps\nUnderstanding LLMOps: Large Language Model Operations.\nGitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps over-\nview of ZenML pipelines from a Hugging Face model repository to a deployed AWS SageMaker \nGitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/\nBuilding robust and scalable ML systems requires more than creating powerful models.\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual \nan ML application.\nThe data scientist manually performs each pipeline step, such as data \npreparation and validation, model training, and testing.\nJupyter notebooks to train their models.\nthe data and train the models.\nContinuous training (CT): The next level involves automating model training.\nknown as continuous training, which triggers model retraining whenever required.\npoint, you often automate your data and model validation steps.\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable \nautomatic building, testing, and deployment of data, ML models, and training pipeline \nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can \nquickly move from a manual process to CI/CD/CT.\nOnce they improve the model by tinkering with how the data is processed \nor the model architecture, they push the code to the code repository, which triggers the CI/CD \npipeline to build, test, package, and deploy the new changes to the FTI pipelines.",
    "keywords": [
      "query",
      "llm",
      "model",
      "str",
      "data",
      "pipeline",
      "MLOps",
      "LLM Twin",
      "track def rag",
      "track def",
      "context",
      "models",
      "application",
      "rag",
      "LLMOps"
    ],
    "concepts": [
      "model",
      "models",
      "mlops",
      "https",
      "ml",
      "llmops",
      "com",
      "data",
      "pipeline",
      "pipelines"
    ]
  },
  {
    "chapter_number": 57,
    "title": "Segment 57 (pages 492-500)",
    "start_page": 492,
    "end_page": 500,
    "summary": "By now, we understand that the whole ML system changes if the code, model, or data changes.\nwe adopt to track the code, model, and data separately?\nmodels used within your system.\nmodel, such as what data it was trained on, its architecture, performance, latency, and \nVersioning the data isn’t as straightforward as versioning the code and model because it \nsolutions are based on Git-like systems, such as Data Version Control (DVC), that track \ncreating a new version for every change made to your data.\nTraining ML models is an entirely iterative and experimental process.\nmodel.\n4. Testing\nThe same trend is followed when testing ML systems.\nall three dimensions: the data, the model, and the code.\noverall user experience—for example, testing an entire ML pipeline, from data ingestion \nto model training and inference, ensuring the system produces the correct outputs for \nWhat do we test?\nYou want to test that you get an expected output for a given \nTest examples\nWhen we talk about data tests, we mainly refer to data validity.\nThus, by writing integration or system tests for your feature pipeline, \nTesting data validity depends a lot on your application and data type.\nModel tests are the trickiest, as model training is the most non-deterministic process of an ML \nSome standard model test techniques involve checking:\nThe shapes of the input and model output tensors\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which \ntries to adopt the strategy from code testing and treats the model as a black box while looking \nsolely at the input data and expected outputs.\nThis makes the behavioral testing methods model \nA fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models \nquick overview, the paper proposes that you test your model against three types of tests.\na model that extracts the main subject from a sentence as an example:\nmodel(text=\"The advancements in AI are changing the world rapidly.\")\nmodel(text=\"The progress in AI is changing the world rapidly.\")\nexample where we know the outputs should change based on the provided inputs:\nfor example, below is a set of simple examples that we expect the model should always \nData, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/\ntesting/.\nmeans that our ML model will constantly be exposed to a level of degradation.\nbecause the data from production might differ from the data the model was trained on.\nIntuitively, monitoring detects the model’s performance degradation, which triggers an alarm that \nWhy retrain the model?\nAs the model performance degrades due to a drift in the training dataset \nand what it inputs from production, the only solution is to adapt or retrain the model on a new \ndifferent aspects of your application, such as the infrastructure, data, and model.\nModel metrics\nmodel.\nTherefore, moving on to the next layer of metrics that focus on the model’s performance \nwell as essential business metrics influenced by the model, such as ROI and click rate.\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance \nDrifts are proxy metrics that help us detect potential issues with the production model in time \ndata drift →P(X) ≠Pref(X) \nTable A.1: Relationship between data, model, and code changes\nData drift\nmodel cannot handle the changes in feature space, leading to potentially unreliable predictions.\nDrift can result from natural real-life changes or systemic problems like missing data, pipeline \nFigure A.3: Data drift examples\nWhen data begins to drift, the degradation in our model’s performance might not be immediately \nchance to consider retraining before the drift affects the model’s performance.\nIn addition to changes in input data (data drift), we might also encounter shifts in output dis-\nWhile retraining the model can help reduce performance \nand model head to support the new schema of the output class.\nIn addition to changes in input and output data, their relationship can also shift.\nenon, known as concept drift, makes our model ineffective because the patterns it previously \nFor example, this happens when using the model in a different geographic area.",
    "keywords": [
      "model",
      "data",
      "tests",
      "system",
      "drift",
      "data drift",
      "output",
      "Testing",
      "test",
      "code",
      "System tests",
      "metrics",
      "outputs",
      "version",
      "models"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "tests",
      "testing",
      "test",
      "outputs",
      "output",
      "versions",
      "versioning"
    ]
  },
  {
    "chapter_number": 58,
    "title": "Segment 58 (pages 501-508)",
    "start_page": 501,
    "end_page": 508,
    "summary": "How to detect and measure drifts\nA reference window: This is the collection of data points used as a baseline to compare \nagainst the production data distributions for drift identification.\nA test window: This collects data points gathered while the ML system is in production.\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between \nFor example, you can use the Kolmogorov-Smirnov (KS) test to monitor a \nwindow distribution.\nWhen working with text data in an embedding representation, we have to model a multivariate \nmean of the embeddings of the two windows.\nMonitoring involves the collection and visualization of data, whereas observability provides in-\nOn the other hand, a system is considered observable if it generates meaningful data about its \nTweaking the p-value of the statistical tests that check for drifts.\nA lower p-value means \nThese thresholds and p-values depend on your application.\ntriggered the alarm, with what value, the time it happened, and anything else that makes sense \nand train the model on the newly shifted dataset to solve the drift.\nyou use the same dataset and hyperparameters, you might end up with a model with a differ-\nThis aspect can be solved by always using a seed before generating random \nrandom values or randomly remove data or labels.\nJoin our book’s Discord space\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\nIf you enjoyed this book, you may be interested in these other books by Packt:\nOther Books You May Enjoy\nCustomize and scale RAG-driven generative AI systems across domains\nControl and build robust generative AI systems grounded in real-world data\nCombine text and image data for richer, more informative AI responses\nOther Books You May Enjoy\nUnderstand the implications of LFMs for AI research and industry applications",
    "keywords": [
      "data",
      "system",
      "reference window",
      "books",
      "model",
      "test",
      "drift",
      "window",
      "reference",
      "data points",
      "LLM",
      "alarm",
      "production data distributions",
      "drifts",
      "windows"
    ],
    "concepts": [
      "data",
      "ai",
      "monitor",
      "monitoring",
      "randomly",
      "random",
      "randomness",
      "packt",
      "values",
      "value"
    ]
  },
  {
    "chapter_number": 59,
    "title": "Segment 59 (pages 509-516)",
    "start_page": 509,
    "end_page": 516,
    "summary": "LLM Twin model, deploying to  375-385\nCI/CD pipeline  462\nCI pipeline, LLM Twin\ndeployment (CI/CD) pipeline  31, 402\nversus LLM Twin  4\ndata collection pipeline  19\ndata deduplication  184, 185\ndata drift  470\ndata evaluation  233\ndata exploration  189-191\ndata generation  191-233\npreference data, evaluating  235-237\npreference data, generating  233, 234\ndata quality evaluation  186-189\ndata tests  466\ndata  357\ndomain-specific LLM evaluations  265-267\ndownstream pipelines\nhuman-generated, LLM-evaluated \nLLM-generated, human-evaluated \nLLM-generated, LLM-evaluated datasets  234\ndata drift  470\nend-to-end RAG inference pipeline\nETL pipeline\nconnecting, to feature pipeline  60\nExtract, Transform, Load (ETL) pipeline  55\nfeature pipeline  14, 19, 20\nfeature pipeline  14\ninference pipeline  14\ntraining pipeline  14\nused, for building LLM system  462, 463\nFTI pipeline design\nLLM Twin architecture, designing  17\nFTI pipelines architecture\ninference pipeline  14\ngeneral-purpose LLM evaluations  263-265\nhandlers  162, 163\nhuman-generated, LLM-evaluated \ninference pipeline  22\nversus training pipeline  371, 372\ndata deduplication  184, 185\ndata exploration  189-191\ndata generation  191, 193\ndata quality evaluation  186-189\nlarge language model (LLM)  1, 99, 355, 401\nLLM evaluation  235\nLLM-generated, human-evaluated \nLLM-generated, LLM-evaluated datasets  234\nLLM system\nLLM Twin  2, 5, 6\nCD pipeline  442-444\nCI/CD pipeline flow  434, 435\nCI/CD pipeline, testing  445\nCI pipeline  438\nCT pipeline  446, 448\ninference pipeline deployment \nRAG feature pipeline architecture  127, 139\nLLM Twin architecture  23\ndata collection pipeline  19\ndesigning, with FTI pipeline design  17\nfeature pipeline  19, 20\ninference pipeline  22\ntraining pipeline  21, 22\nLLM Twin model\nLLM Twin RAG feature pipeline\nhandlers  162\nsetting  139\nZenML pipeline and steps  140, 141\nLLM Twin's data collection pipeline\nZenML pipeline and steps  61-65\nLLM Twin service\nLLM Twin's pipelines, cloud deployment  415\nMongoDB, setting up  418, 419\npipelines, running on AWS  428-431\nQdrant, setting up  419, 420\npipeline on SageMaker  432, 433\nZenML, setting up  421-423\nlow latency  358\nminimum viable product (MVP)  6\nML evaluation\nvesus, LLM evaluation  262, 263\nML models\nCI/CD pipeline  462\nML pipeline orchestrator  407\nML pipeline automation\nML pipelines\nmodel evaluation  261\ndomain-specific LLM evaluations  265-267\ngeneral-purpose LLM evaluations  263-265\nML, versus LLM evaluation  262, 263\ntask-specific LLM evaluations  267-271",
    "keywords": [
      "LLM Twin",
      "LLM",
      "pipeline",
      "data",
      "LLM Twin RAG",
      "LLM Twin architecture",
      "Twin",
      "LLM Twin model",
      "LLM evaluations",
      "LLM Twin pipelines",
      "RAG",
      "LLM Twin service",
      "LLM Twin data",
      "inference",
      "advanced RAG"
    ],
    "concepts": [
      "data",
      "pipeline",
      "pipelines",
      "llm",
      "tests",
      "testing",
      "test",
      "models",
      "model deploying",
      "evaluation"
    ]
  },
  {
    "chapter_number": 60,
    "title": "Segment 60 (pages 517-523)",
    "start_page": 517,
    "end_page": 523,
    "summary": "data parallelism (DP)  299\npipeline parallelism (PP)  300, 301\nmodel quantization  303, 304\nmodel tests  466\nMongoDB, as data warehouse\nmonolithic batch pipeline architecture  10\nNoSQL data warehouse documents  79, 80\ndata categories and user document \nODM class, implementing  82-87\noutput test  465\nparameter-efficient fine-tuning techniques\ntechniques  211\npipeline parallelism (PP)  300\npolicy optimization  246\npreference-based reinforcement \ncode generation  231\ndata evaluation  233\ndata generation  233\ndata quantity  232\ndata indexing  119\nquery optimizing  119\ndata category  151\nPydantic Settings\ntechniques  313, 314\nquery optimization  120\nRAG evaluation  271, 272\nRAG feature pipeline\ndata extraction  134\ndata loading  135\ndata warehouse and feature store, \nRAG feature pipeline architecture\nbatch pipelines  130\nbatch pipelines, versus streaming \npipelines  130-134\ninference pipeline  127\nraw data  128\nRAG inference pipeline\npolicy optimization  246\nreward model learning  246\nembeddings, creating  111-114\nretrieval-augmented generation (RAG) \npipeline  206, 261\nreward model learning  246\nmodel  371\ntechniques  211\nsystem tests  464\ntest job  438\ntest types  465\nsystem tests  464\ntest window  472\ntraining pipeline  14, 21, 22\nversus inference pipeline  371, 372\ntriggers\ndata  463\nmodel  463\ntest window  472\nZenML pipeline  140-142\ncleaned documents, embedding  147-150\ndata warehouse, querying  143-145\nDownload a free PDF copy of this book\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.",
    "keywords": [
      "data",
      "pipeline",
      "RAG",
      "tests",
      "Index",
      "LLM Leaderboard",
      "model",
      "inference",
      "techniques",
      "parallelism",
      "Leaderboard",
      "ODM",
      "Arabic LLM Leaderboard",
      "Portuguese LLM Leaderboard",
      "generation"
    ],
    "concepts": [
      "data",
      "pipeline",
      "pipelines",
      "tests",
      "test",
      "testing",
      "model",
      "models",
      "optimal",
      "optimized"
    ]
  }
]