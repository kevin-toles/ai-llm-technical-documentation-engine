[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 2-10)",
    "start_page": 2,
    "end_page": 10,
    "summary": "Packt Publishing has endeavored to provide trademark information about all of the companies and products \nFrom data engineering and model fine-tuning to advanced topics like RAG pipelines \na book; it’s a roadmap to becoming a proficient LLM engineer in today’s AI-driven landscape.\nPaul Iusztin is a senior ML and MLOps engineer with over seven years of experience building \n• 2\nWhy building an LLM Twin matters • 3\n• 5\n• 6\nDefining the LLM Twin MVP • 7\nThe problem with building ML systems • 8\nThe issue with previous solutions • 10\nThe solution – ML pipelines for ML systems • 13\nThe feature pipeline • 14\nThe training pipeline • 14\nThe inference pipeline • 14\nListing the technical details of the LLM Twin architecture • 16\nHow to design the LLM Twin architecture using the FTI pipeline design • 17\nData collection pipeline • 19\nFeature pipeline • 19\nTraining pipeline • 21\nInference pipeline • 22\nFinal thoughts on the FTI design and the LLM Twin architecture • 22\nHugging Face: model registry • 31\nOrchestrator • 33\nHow to run and configure a ZenML pipeline • 43\nComet ML: experiment tracker • 45\n• 51\nDesigning the LLM Twin’s data collection pipeline                                                             56\nImplementing the LLM Twin’s data collection pipeline • 61\nZenML pipeline and steps • 61\n• 66\nThe crawlers • 69\nBase classes • 69\nGitHubCrawler class • 73\nCustomArticleCrawler class • 75\nTroubleshooting • 94\n• 100\nHallucinations • 101\nOld information • 101\nIngestion pipeline • 104\nRetrieval pipeline • 105\nGeneration pipeline • 105\n• 107\nWhy embeddings are so powerful • 109",
    "keywords": [
      "LLM Twin",
      "LLM Twin architecture",
      "LLM",
      "Packt Publishing",
      "LLM Twin Concept",
      "LLM Engineer",
      "LLM Twin MVP",
      "LLM Twin product",
      "Twin",
      "pipeline",
      "LLMs",
      "book",
      "LLM Twin matters",
      "Twin architecture",
      "Hugging Face"
    ],
    "concepts": [
      "pipelines",
      "pipeline",
      "editor",
      "models",
      "model",
      "rag",
      "products",
      "product",
      "production",
      "classes"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 11-18)",
    "start_page": 11,
    "end_page": 18,
    "summary": "• 111\n• 115\nRetrieval • 122\n• 128\nBatch pipelines • 130\n• 138\nOrchestration • 138\nSettings • 139\nOVM • 154\nThe handlers • 162\nData exploration • 189\nData generation • 191\nWhen to fine-tune • 206\nFull fine-tuning • 211\nLoRA • 213\nQLoRA • 215\nOptimizers • 218\nPreference data • 230\nData generation and evaluation • 233\nGenerating preferences • 233\nEvaluating preferences • 235\nRagas • 272\nARES • 274\nEvaluating answers • 278\nData parallelism • 299\nPipeline parallelism • 300\nBringing everything together into the RAG inference pipeline • 346\nData • 357\n• 373\nDevOps • 403\nMLOps • 405\nLLMOps • 410\nGuardrails • 411\nPrompt monitoring • 413\non SageMaker • 432\nThe CI pipeline • 438\nThe CD pipeline • 442\nThe CT pipeline • 446\nPrompt monitoring • 451\nAlerting • 457\n• 465\nLogs • 468\nMetrics • 468\nSystem metrics • 469\nDrifts • 469\nobservability • 472\nAlerts • 473",
    "keywords": [
      "LLM Twin",
      "RAG Inference Pipeline",
      "Table of Contents",
      "LLM",
      "RAG feature pipeline",
      "Inference Pipeline",
      "pipeline",
      "data",
      "RAG",
      "RAG Inference",
      "Twin",
      "LLM Twin model",
      "Inference",
      "advanced RAG",
      "LLM evaluations"
    ],
    "concepts": [
      "pipeline",
      "data",
      "evaluation",
      "evaluating",
      "evaluations",
      "optimization",
      "optimized",
      "optimizations",
      "inference",
      "contents"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 19-27)",
    "start_page": 19,
    "end_page": 27,
    "summary": "By incorporating MLOps practices, LLM projects can \nThe LLM Engineer’s Handbook is a comprehensive guide to applying best practices to the new \nfield of LLM engineering.\nThe book covers topics \nsuch as data engineering, supervised fine-tuning, model evaluation, inference optimization, and \nTo illustrate these concepts in action, an end-to-end project called the LLM Twin will be developed \nusing various aspects of LLM engineering and MLOps.\nfine-tune models for specific tasks, optimize inference performance, and implement RAG pipelines.\nThey will learn how to evaluate LLM performance, align models with human preferences, and \ndeploy LLM-based applications.\nThe book also covers essential MLOps principles and practices, \nenabling readers to build scalable, reproducible, and robust LLM applications.\nWho this book is for\nmenting and deploying LLM-based systems.\nWhat this book covers\nChapter 1, Understanding the LLM Twin Concept and Architecture, introduces the LLM Twin project, \nwhich is used throughout the book as an end-to-end example of a production-level LLM appli-\nLLM Twin use case.\nRAG theory by architecting and implementing LLM Twin’s RAG feature pipeline using software \nmodels and LLM systems.\nimplementing the LLM Twin’s RAG inference pipeline and a custom retrieval module similar to \nasynchronous and batch inference, which will help in architecting and deploying the LLM Twin \nMLOps. This chapter explains how to deploy the LLM Twin project to the cloud, such as the ML \nIt also adds a prompt monitoring layer on top of LLM Twin’s inference pipeline.\nTo get the most out of this book\ngramming is particularly beneficial, as the book’s examples and code snippets are predominantly \nstrictly necessary, as the book provides explanations for many fundamental AI and ML concepts.\nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/\nLLM-Engineers-Handbook.\nbooks directly into your application.\nUnderstanding the LLM Twin \nlarge language model (LLM) product.\nand production machine learning (ML) is to get your hands dirty and build systems.\nThis book \nwill show you how to build an LLM Twin, an AI character that learns to write like a particular \nMost of the concepts learned while implementing your LLM Twin can be applied in other LLM-\nIn our case, what exactly is an LLM Twin, \nused to build the LLM system.\nUnderstanding the LLM Twin Concept and Architecture\nmuch ML knowledge, it is critical to go through them to understand “how” to build the product \nUnderstanding the LLM Twin concept\nPlanning the MVP of the LLM Twin product\nBuilding ML systems with feature/training/inference pipelines\nDesigning the system architecture of the LLM Twin\nthe book.\nUnderstanding the LLM Twin concept\nThe concept of an LLM Twin is new.\nWhat is an LLM Twin?\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and \npersonality into an LLM, which is a complex AI model.\ninto an LLM.\nInstead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned \nNaturally, as an ML model reflects the data it is trained on, this LLM will incorporate \nThus, this LLM will not be you; \nIt is essential to understand that an LLM reflects the data it was trained on.\nTo adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various ",
    "keywords": [
      "LLM Twin",
      "LLM Twin Concept",
      "LLM",
      "LLM Twin project",
      "Twin",
      "book",
      "LLM Twin product",
      "LLM engineering",
      "RAG Inference Pipeline",
      "implementing LLM Twin",
      "Twin Concept",
      "LLMs",
      "LLM Twin fine-tuned",
      "RAG",
      "LLM Engineer"
    ],
    "concepts": [
      "chapters",
      "models",
      "model",
      "production",
      "product",
      "code",
      "engineer",
      "engineers",
      "engines",
      "books"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 28-36)",
    "start_page": 28,
    "end_page": 36,
    "summary": "Here are some scenarios of what you can fine-tune an LLM on to become your twin:\nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\nUltimately, the LLM reflects \nDo we have enough digital data to project ourselves into an LLM?\n“Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and \nWhy building an LLM Twin matters\nThe biggest issue with creating a personal brand is that writing content on plat-\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Sub-\nthe skeleton of our main idea to the LLM Twin and let it do the grunt work.\nUnderstanding the LLM Twin Concept and Architecture\non the concrete features in the Planning the MVP of the LLM Twin product section).\nect ourselves into a content-writing LLM Twin that will help us automate our writing process.\nwill likely fail if we try to use this particular LLM in a different scenario, as this is where we will \nspecialize the LLM through fine-tuning, prompt engineering, and RAG.\nSo, why does building an LLM Twin matter?\nAlso, it is critical to understand that building an LLM Twin is entirely moral.\nThe LLM will be \nfine-tuned only on our personal digital data.\nEveryone will have their own LLM Twin with restricted access.\nWhat’s the difference between a co-pilot and an LLM Twin?\nstance, an LLM Twin is an LLM that learns to mimic your voice, personality, \nwrites like you is your LLM Twin co-pilot.\nThe key of the LLM Twin stands in the following:\nHow we feed the data into the LLM\nThe LLM itself is important, but we want to highlight that using ChatGPT’s web interface is \nThe solution is to build an LLM system that encapsulates and automates all the following steps \nUnderstanding the LLM Twin Concept and Architecture\nLLM fine-tuning\ninterface, it can be integrated into the LLM Twin system we will learn to build.\nsuccessful ML products is to be data-centric and make your architecture model-agnostic.\nyou can quickly experiment with multiple models on your specific data.\nPlanning the MVP of the LLM Twin product\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define \nthe product’s features.\nAn MVP is a version of a product that includes just enough features to draw in early users and test \nprovide an end-to-end user journey without half-implemented features, even if the product is \nDefining the LLM Twin MVP\nAs a thought experiment, let’s assume that instead of building this project for this book, we want \nin defining our LLM Twin MVP and what features we want to pick.\nTo keep it simple, we will build the features that can do the following for the LLM Twin:\nFine-tune an open-source LLM using the collected data\nHave a simple web interface to interact with the LLM Twin and be able to do the following:\nThat will be the LLM Twin MVP.\nEven if we focus only on the core features of the LLM Twin defined in this section, we \nwill build the product with the latest LLM research and best software engineering \nand scalable LLM application.\nUnderstanding the LLM Twin Concept and Architecture\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives.\nof the LLM Twin.\nBuilding ML systems with feature/training/inference \nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system \nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.\nBuilding production-ready ML systems is much more than just training a model.\nHowever, training a model becomes complex when deciding on the correct architecture and \nAt this point, we want to focus on how to design a production-ready architecture.\ndata science team is often responsible for training the model.\nUnderstanding the LLM Twin Concept and Architecture\non a monolithic batch architecture that couples the feature creation, model training, and infer-\npassed to the model are computed differently at training and inference time.\nIt’s hard to share the work between multiple teams between the features, training, and \nwhole state through the client request so the features can be computed and passed to the model.\nAnother example would be when implementing an LLM with RAG support.",
    "keywords": [
      "LLM Twin",
      "LLM Twin MVP",
      "LLM Twin Concept",
      "LLM",
      "LLM Twin architecture",
      "LLM Twin product",
      "twin",
      "LLM Twin system",
      "LLM Twin co-pilot",
      "LLM Twin matters",
      "LLM Twin stands",
      "Twin MVP",
      "content-writing LLM Twin",
      "LLM Twin defined",
      "data"
    ],
    "concepts": [
      "data",
      "content",
      "users",
      "user",
      "architecture",
      "llm",
      "model",
      "models",
      "product",
      "production"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 37-46)",
    "start_page": 37,
    "end_page": 46,
    "summary": "Figure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/\nBut here is where the FTI pipeline architectures kick in.\ncan follow to compute the features, train the model, and make predictions.\ncritical steps that any ML system requires, the pattern is known as the FTI pipeline.\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, \nAs shown in Figure 1.5, we have the feature, training, and inference pipelines.\nFigure 1.5: FTI pipelines architecture\nBefore going into the details, it is essential to understand that each pipeline is a different com-\nThus, each pipeline can be written using \nThe feature pipeline\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels \nrequired by the model for training or inference.\nThus, we can easily send the features to the training and inference pipelines.\nAs the data is versioned, we can always ensure that the training and inference time features match.\nThe training pipeline\nThe training pipeline takes the features and labels from the features stored as input and outputs \nfeature stores, but this time, the model is the first-class citizen.\nThus, the model registry will store, \nversion, track, and share the model with the inference pipeline.\nversion used to train the model.\nThus, we will always know what data the model was trained on.\nThe inference pipeline\nThe inference pipeline takes as input the features and labels from the feature store and the trained \nAdditionally, the features, labels, and models are \nThe feature pipeline takes in data and outputs the features and labels saved to the feature \nThe training pipeline queries the features store for features and labels and outputs a \nThe inference pipeline uses the features from the feature store and the model from the \ncontain only three pipelines.\nFor example, the feature pipeline \ntraining pipeline can be composed of the training and evaluation components.\nThe FTI pipelines act as logical layers.\nlines interact with each other through the feature store and model registries.\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how \nStore the vectorized data into a vector DB \nTo learn more about the FTI pipeline pattern, consider reading From MLOps to ML \nSystems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder \nfti-pipelines.\nHow can we apply the FTI pipeline design to implement the preceding list of requirements?\nHow to design the LLM Twin architecture using the FTI \npipeline design\nthree, as the FTI pipeline design clearly states?” That is a great question.\nWe must also implement the data pipeline along the three feature/training/inference \npipelines.\nThe data engineering team owns the data pipeline\nThe ML engineering team owns the FTI pipelines.\nThis includes defining the data collection and FTI pipelines.\nData collection pipeline\nThe data collection pipeline involves crawling your personal data from Medium, Substack, Linke-\ndIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to \nThe output of this component will be a NoSQL DB, which will act as our data warehouse.\nattach an additional ETL in the data collection pipeline, and everything else will work without \nFeature pipeline\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data ware-\nIt is critical to highlight that the data collection pipeline is designed to crawl data \nHere are some custom properties of the LLM Twin’s feature pipeline:\nIt processes three types of data differently: articles, posts, and code\ncretely, a specialized feature store, we used the vector DB, plus some additional logic to check all \nquery the vector DB for new data points without any vector search logic.\nThe training pipeline will use the \ninstruct datasets as artifacts, and the inference pipeline will query the vector DB for additional \nTo conclude, we take in raw article, post, or code data points, process them, and store them in \na feature store to make them accessible to the training and inference pipelines.\nTraining pipeline\nThe training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with \nit, and stores the tuned LLM weights in a model registry.\ndataset is available in the logical feature store, we will trigger the training pipeline, consume the \nThe proposed LLM is then stored in the model registry.\nThe testing pipeline is triggered for a more detailed analysis than during fine-tuning.\nmodel is ultimately tagged as accepted and deployed to the production inference pipeline.\nHow do you implement an LLM agnostic pipeline?\ndata collection pipeline to crawl data every week.",
    "keywords": [
      "LLM Twin",
      "LLM Twin Concept",
      "feature store",
      "LLM",
      "pipeline",
      "data",
      "LLM Twin architecture",
      "FTI pipeline",
      "feature pipeline",
      "feature",
      "model",
      "features",
      "FTI",
      "Data collection pipeline",
      "training pipeline"
    ],
    "concepts": [
      "data",
      "pipeline",
      "pipelines",
      "llm",
      "features",
      "feature",
      "chapters",
      "differ",
      "different",
      "differently"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 47-58)",
    "start_page": 47,
    "end_page": 58,
    "summary": "It loads a fine-tuned LLM from the model registry, and from the logical feature \nIt uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\ncollection and feature pipeline are mostly CPU-based and do not require powerful machines.\ntraining pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it.\nMLOps side, we will walk you through using a computing platform, orchestrator, model registry, \nFrom MLOps to ML Systems with Feature/Training/Inference \nhttps://www.hopsworks.ai/post/mlops-to-ml-systems-with-\nMLOps: Continuous delivery and automation pipelines in machine learning.\nhttps://cloud.google.com/architecture/mlops-continuous-\n5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024.\nhttps://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-\nTooling and Installation\nimplementing and deploying the LLM Twin project.\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage \nmultiple Python versions, create a virtual environment, and install the pinned dependencies re-\ncode yourself): https://github.com/PacktPublishing/LLM-Engineers-Handbook.\nNext, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, \nsuch as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and \nWe will also understand how to manage a project with multiple ML \npipelines using ZenML, an orchestrator bridging the gap between ML and MLOps. Also, we will \nTooling and Installation\nPython ecosystem and project installation\nBy the end of this chapter, you will be aware of all the tools we will use across the book.\nwill have learned how to install the LLM-Engineers-Handbook repository, set up the rest of the \ntools, and use them if you run the code while reading the book.\nPython ecosystem and project installation\nAny Python project needs three fundamental tools: the Python interpreter, dependency manage-\nversion (Python 3.11.8) to run the LLM Twin project using pyenv, making the installation process \nInstead of installing multiple global Python versions, we recommend managing them using pyenv, \na Python version management tool that lets you manage multiple Python versions between \nAfter you have installed pyenv, you can install the latest version of Python 3.11, using pyenv, as \nNow list all installed Python versions to see that it was installed correctly:\nHowever, we aim to use Python 3.11.8 locally only in our repository.\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git \nBecause we defined a .python-version file within the repository, pyenv will know to pick up \nthe version from that file and use it locally whenever you are working within that folder.\npython --version\nTo create the .python-version file, you must run pyenv local 3.11.8 once.\nalways know to use that Python version while working within a specific directory.\nNow that we have installed the correct Python version using pyenv, let’s move on to Poetry, which \nwe will use as our dependency and virtual environment manager.\nPoetry: dependency and virtual environment management\nPoetry is one of the most popular dependency and virtual environment managers within the \nFor example, this is a simple Poetry requirements file that \nuses Python 3.11 and the requests and numpy Python packages.\n[tool.poetry.dependencies]\nTooling and Installation\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version \nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in \nwhich it installs the specified Python version and requirements.\nprojects in the global Python environment, that will not work, as Project B will override Project A’s \nisolate each project in its own Python environment with its own Python dependencies, avoiding \nYou can install Poetry from here: https://python-poetry.org/docs/.\nWe use Poetry 1.8.3 \nOnce Poetry is installed, navigate to your cloned LLM-Engineers-Hand-\nbook repository and run the following command to install all the necessary Python dependencies:\npoetry install --without aws\nas follows: poetry run <your command>.\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the \npoetry.lock file based on the definitions added to the project.toml file.\ntoml file may specify version ranges (e.g., requests = \"^2.25.1\"), the poetry.lock file records \nversions, the poetry.lock file ensures that all project installations use the same versions of each \nOther tools similar to Poetry are Venv and Conda for creating virtual environments.\nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands \nfile that Poetry already uses for dependencies.\nYou can install Poe the Poet as a Poetry plugin, as follows:\nTooling and Installation\nAssuming you have pyenv and Poetry installed, here are all the commands you need to run to \nclone the repository and install the dependencies and Poe the Poet as a Poetry plugin:\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.gitcd \npoetry install --without aws\nthey are useful only if you plan to run the repository: https://github.com/PacktPublishing/\nNow that we have installed our Python project, let’s present the MLOps tools we will use in the \nThis section will quickly present all the MLOps and LLMOps tools we will use throughout the \nbook and their role in building ML systems using MLOps best practices.\nbook, we don’t aim to detail all the MLOps components we will use to implement the LLM Twin \nuse case, such as model registries and orchestrators, but only provide a quick idea of what they \nAs we develop the LLM Twin project throughout the book, you will \nunderstand it after you go through the LLM Twin use case implementation.\nRun poetry poe local-infrastructure-up to locally spin up ZenML (http://127.0.0.1:8237/) \nYou can read more details on how to run everything locally in the LLM-Engineers-Handbook re-\npository README: https://github.com/PacktPublishing/LLM-Engineers-Handbook.\nfine-tuned LLM Twin models with anyone who reads the book.\nTooling and Installation\nMost ML tools provide model registry features.\nregistry that integrates the most with your project’s tooling and requirements.\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that \npipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts \nFor example, in our LLM Twin use case, we used the AWS stack:\nThe local version of the ZenML server comes installed as a Python package.\npoetry install, it installs a ZenML debugging server that you can use locally.\nwill show you how to use their cloud serverless option to deploy the ML pipelines to AWS.\nscalability, making complex ML pipelines more reliable and easier to manage.\nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented \nfor the LLM Twin project.",
    "keywords": [
      "LLM Twin",
      "LLM Twin project",
      "Python",
      "LLM Twin architecture",
      "Poetry",
      "LLM",
      "LLM Twin Concept",
      "Python version",
      "Twin",
      "project",
      "pipeline",
      "LLM Twin models",
      "Python project",
      "Twin project",
      "LLM Twin MVP"
    ],
    "concepts": [
      "python",
      "tools",
      "tool",
      "tooling",
      "poetry",
      "zenml",
      "versions",
      "version",
      "installation",
      "install"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 59-66)",
    "start_page": 59,
    "end_page": 66,
    "summary": "def digital_data_etl(user_full_name: str, links: list[str]) -> None:\nTo visualize the pipeline run, you can go to your ZenML dashboard (at http://127.0.0.1:8237/) \nFigure 2.2: ZenML Pipelines dashboard\nAfter clicking on the digital_data_etl pipeline, you can visualize all the previous and current \npipeline runs, as seen in Figure 2.3.\nFigure 2.3: ZenML digital_data_etl pipeline dashboard.\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or \nis still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure \nFigure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, \nlet’s quickly look at how to define a ZenML step.\nfrom zenml import get_step_context, step\nWithin a ZenML step, you can define any Python logic your use case needs.\nWe also defined the pipelines and steps folders, where \nmodule, we only aggregated ZenML steps to glue them into the final pipeline.\nOne last thing to consider when writing ZenML steps is that if you return a value, it should be se-\nAs mentioned in the previous section, ZenML transforms any step output into an artifact.\nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an ar-\nFigure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\nFigure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\nA more interesting example of an artifact and its metadata is the generated dataset artifact.\nFigure 2.9, we can visualize the metadata of the instruct_datasets artifact, which was auto-\nFigure 2.9: ZenML metadata example for the instruct_datasets artifact\nfrom zenml import ArtifactConfig, get_step_context, step",
    "keywords": [
      "etl pipeline",
      "pipeline",
      "ZenML",
      "etl pipeline run",
      "user",
      "step",
      "pipeline run",
      "artifact",
      "etl",
      "data",
      "digital",
      "run",
      "metadata",
      "Tooling and Installation",
      "code"
    ],
    "concepts": [
      "pipeline",
      "pipelines",
      "imports",
      "steps",
      "step",
      "user",
      "data",
      "artifacts",
      "artifact",
      "models"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 67-75)",
    "start_page": 67,
    "end_page": 75,
    "summary": "instruct_dataset_categories = list(datasets.train.keys())\n\"data_categories\": instruct_dataset_categories,\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.\nHow to run and configure a ZenML pipeline\nAll the ZenML pipelines can be called from the run.py file, accessed at tools/run.py in our GitHub \nFor example, to call the digital_data_etl pipeline to crawl Maxime’s content, \npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\npoetry poe run-digital-data-etl-maxime\npoetry poe run-digital-data-etl-paul\n\"run_name\": f\"digital_data_etl_run_{dt.now().\ndigital_data_etl.with_options()(**run_args_etl)\nample, the configs/digital_data_etl_maxime_labonne.yaml configuration file looks as follows:\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nconsider ZenML the best trade-off between ease of use, features, and costs.\nFigure 2.11: Comet ML training metrics example\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your \nexperiment tracker, we made the training experiments tracked with Comet ML public while \nYou can access them here: https://www.comet.com/mlabonne/\nDatabases for storing unstructured and vector data\nWe also want to present the NoSQL and vector databases we will use within our examples.\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before \nWe will use Qdrant to store the data from MongoDB after it’s processed and transformed for \nhttps://superlinked.com/vector-db-comparison, which compares all the top vector databases \nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have \none), an AWS access key, and the CLI.\nBut for our MVP, AWS, it’s the perfect option as it provides robust features for every-\nSetting up an AWS account, an access key, and the CLI\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is \nby redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/\nAfter successfully creating an AWS account, you can access the AWS console at http://console.\naws.amazon.com.\nNext, we must generate access keys to access AWS programmatically.\nfirst to create an IAM user with administrative access as described in this AWS official tutorial: \nhttps://docs.aws.amazon.com/streams/latest/dev/setting-up.html\nNext, you have to create an access key for the IAM user you just created using the following tutorial: \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html.\naws_access_key_id = <your_access_key_id>\naws_secret_access_key = <your_secret_access_key>\nAlso, be cautious with who you share them, as they could be used to access your AWS \nThe last step is to install the AWS CLI and configure it with your newly created access keys.\ncan install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/\nAfter installing the AWS CLI, you can configure it by running aws configure.\nof our AWS configuration:\naws_access_key_id = *************\naws_secret_access_key = ************\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://\ndocs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html.\nAlso, to configure the project with your AWS credentials, you must fill in the following variables \nAWS_ACCESS_KEY=\"<your_aws_access_key>\"\nSageMaker is a fully managed machine learning service by AWS that enables developers and data \nAll the cloud services used across the book stick to their freemium option, except AWS.\nThus, if you use a personal AWS account, you will be responsible for AWS costs as you \non our tests, the AWS costs can vary between $50 and $100 using the specifications \nat https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/",
    "keywords": [
      "AWS",
      "AWS CLI",
      "AWS account",
      "access",
      "CLI",
      "data",
      "Comet",
      "AWS access key",
      "Tooling and Installation",
      "key",
      "dataset",
      "samples",
      "datasets",
      "etl",
      "run"
    ],
    "concepts": [
      "tooling",
      "tools",
      "tool",
      "training",
      "train",
      "dataset",
      "run",
      "running",
      "user",
      "users"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 76-83)",
    "start_page": 76,
    "end_page": 83,
    "summary": "and to deploy our custom LLM Twin model as a REST API that can be accessed in real time from \nThat’s why SageMaker is mainly used by data scientists and machine \nregarding costs, following a pay-as-you-go pricing model similar to most AWS services.\nployment, use EKS, AWS’s Kubernetes self-managed service.\nto the virtual machines, allowing you to fully customize how you build your ML pipelines, how \norchestrator to manage all our ML pipelines and artifacts, and metadata to manage all our files \nWe also understood what type of databases we need to implement the LLM Twin \nIn the next chapter, we will explore the implementation of the LLM Twin project by starting with \nthe data collection ETL that scrapes posts, articles, and repositories from the internet and stores \nthem in a data warehouse.\nhttps://github.com/comet-ml/opik\nhttps://neptune.ai/blog/best-workflow-and-pipeline-\nneptune.ai/blog/ml-model-registry\nData Engineering\ndesign and implement the data collection pipeline to gather the raw data we will use in all our \nAs this is not a book on data engineering, we \nraw data.\nThus, implementing our data pipeline will connect the dots \nimplement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, \nsuch as Medium, Substack, or GitHub, and aggregates the gathered data into a MongoDB data \nWe will show you how to implement various crawling methods, standardize the data, \nand load it into a data warehouse.\nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture \na data layer on top of MongoDB to structure all our documents and interact with the database.\nData Engineering\nFinally, we will explore how to run the data collection pipeline using ZenML and query the col-\nlected data from MongoDB.\nDesigning the LLM Twin’s data collection pipeline\nImplementing the LLM Twin’s data collection pipeline\nGathering raw data into the data warehouse\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, \ntransform, and load raw data ready to be ingested into the ML application.\nDesigning the LLM Twin’s data collection pipeline\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL \ndata from and how we will design our data structures and processes.\nunderstanding how our data collection pipeline maps to an ETL process.\nWe extract data from various sources.\nWe will crawl data from platforms like Medium, \nSubstack, and GitHub to gather raw data.\nWe load the transformed data into a data warehouse or database.\nFor our project, we use MongoDB as our NoSQL data warehouse.\nFigure 3.1: LLM Twin’s data collection ETL pipeline architecture\nauthor in a MongoDB data warehouse.\nData Engineering\nHence, the signature of the data collection pipeline will look as follows:\nOutput: A list of raw documents stored in the NoSQL data warehouse\nWe will use user and author interchangeably, as in most scenarios across the ETL pipeline, a \nHowever, within the data warehouse, we have only \nWe implemented four different crawlers for three different data categories, as seen in \nFirst, we will explore the three fundamental data categories we will work with across \nFigure 3.2: The relationship between the crawlers and the data categories",
    "keywords": [
      "LLM Twin",
      "data",
      "data collection pipeline",
      "data collection",
      "data collection ETL",
      "ETL pipeline",
      "LLM",
      "AWS",
      "data warehouse",
      "pipeline",
      "Twin",
      "LLM Twin model",
      "LLM Twin project",
      "collection pipeline",
      "Bedrock"
    ],
    "concepts": [
      "data",
      "pipeline",
      "pipelines",
      "uses",
      "model",
      "models",
      "platform",
      "platforms",
      "tooling",
      "tools"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 84-97)",
    "start_page": 84,
    "end_page": 97,
    "summary": "Medium crawler: Used to collect data from Medium.\nlogs in to Medium and crawls the HTML of the article’s link.\nCustom article crawler: It performs similar steps to the Medium crawler but is a more \nWe will use this crawler as a \nsafety net when the link’s domain isn’t associated with the other supported crawlers.\nexample, when providing a Substack link, it will default to the custom article crawler, but \nwhen providing a Medium URL, it will use the Medium crawler.\nGitHub crawler: This collects data from GitHub. It outputs a repository document.\nLinkedIn crawler: This is used to collect data from LinkedIn. It outputs multiple post \neach crawler accesses a specific platform or site in a particular way and extracts HTML from it.\nment a new crawler that outputs a post document, and that’s it.\nThus, the data collection pipeline can write data for MongoDB, and the feature \nMongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of docu-\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s \nImplementing the LLM Twin’s data collection pipeline\nThus, let’s start by looking into the ZenML digital_data_etl pipeline.\ntation of each crawler used to collect data from various sites and the MongoDB documents used \nIn the code snippet below, we can see the implementation of the ZenML digital_data_etl\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user \nthrough all the links and crawl each independently.\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nlast_step = crawl_links(user=user, links=links)\nFigure 3.3 shows a run of the digital_data_etl pipeline on the ZenML dashboard.\nphase is to explore the get_or_create_user and crawl_links ZenML steps individually.\nFigure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\nfrom llm_engineering.domain.documents import UserDocument\ncurrent step context and add metadata about the user to the output, which will be reflected in \nstep_context.add_output_metadata(output_name=\"user\", metadata=_get_\nWe will move on to the crawl_links ZenML step, which collects the data from the provided links.\nfrom llm_engineering.application.crawlers.dispatcher import \nfrom llm_engineering.domain.documents import UserDocument\nFollowing the imports, the main function inputs a list of links written by a specific author.\nthis function, a crawler dispatcher is initialized and configured to handle specific domains such \ndef crawl_links(user: UserDocument, links: list[str]) -> \nAnnotated[list[str], \"crawled_links\"]:\nIt attempts to crawl and extract data for each link, updating the count of \nsuccessfull_crawl, crawled_domain = _crawl_link(dispatcher, link, \nmetadata = _add_to_metadata(metadata, crawled_domain, successfull_\nstep_context.add_output_metadata(output_name=\"crawled_links\", \nappropriate crawler based on the link’s domain.\nextraction and returns a tuple indicating the crawl’s success and the link’s domain:\ndef _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: \ncrawler = dispatcher.get_crawler(link)\ncrawler_domain = urlparse(link).netloc\ncrawler.extract(link=link, user=user)\nreturn (True, crawler_domain)\nreturn (False, crawler_domain)\ndef _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) \nAs seen in the abovementioned _crawl_link() function, the CrawlerDispatcher class knows \nwhat crawler to initialize based on each link’s domain.\nthe crawler’s extract() method.\n3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers.\nThe CrawlerDispatcher class knows how to extract the domain of each link and initialize the \nproper crawler that collects the data from that site.\ncom domain when providing a link to an article, it will build an instance of the MediumCrawler\ncrawlers\nimporting our crawler classes:\nThe CrawlerDispatcher class is defined to manage and dispatch appropriate crawler instances \nself._crawlers = {}\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, Linke-\ndef register_medium(self) -> \"CrawlerDispatcher\":\ndef register_linkedin(self) -> \"CrawlerDispatcher\":\nfore it’s added as a key to the self._crawlers registry of the dispatcher.\nwe will use the key of the dictionary as the domain pattern to match future links with a crawler:\ndef register(self, domain: str, crawler: type[BaseCrawler]) -> None:\nself._crawlers[r\"https://(www\\.)?{}/*\".format(re.escape(domain))] \n= crawler\ndef get_crawler(self, url: str) -> BaseCrawler:\nfor pattern, crawler in self._crawlers.items():\nreturn crawler()\nThe next step in understanding how the data collection pipeline works is analyzing each crawler \nThe crawlers\nBefore exploring each crawler’s implementation, we must present their base class, which defines \nFor example, in the _crawl_link()\ncrawler = dispatcher.get_crawler(link)\ncrawler.extract(link=link, user=user)\nNote how we called the extract() method without caring about what specific type of crawler \ndef extract(self, link: str, **kwargs) -> None: ...\ndefines a model attribute at the class level that represents the data category document type used \nreusable functionality that uses Selenium to crawl various sites, such as Medium or LinkedIn.\nThe code begins by setting up the necessary imports and configurations for web crawling using \nfrom llm_engineering.domain.documents import NoSQLBaseDocument\nFor the Selenium-based crawlers to work, you must install Chrome on your machine \noptions.add_argument(f\"--user-data-dir={mkdtemp()}\")\noptions.add_argument(f\"--data-path={mkdtemp()}\")\nWe’ve understood what the base classes of our crawlers look like.\nimplementation of the following specific crawlers:\nYou can find the implementation of the above crawlers in the GitHub repository at \n/llm_engineering/application/crawlers.",
    "keywords": [
      "data",
      "user",
      "crawler",
      "link",
      "Data Engineering",
      "links",
      "crawlers",
      "data warehouse",
      "metadata",
      "domain",
      "crawl",
      "pipeline",
      "step",
      "LLM",
      "Medium"
    ],
    "concepts": [
      "crawlers",
      "crawler",
      "importing",
      "imports",
      "data",
      "user",
      "link",
      "links",
      "options",
      "returns"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 98-105)",
    "start_page": 98,
    "end_page": 105,
    "summary": "GitHubCrawler class\nThe GithubCrawler class is designed to scrape GitHub repositories, extending the functionality \nclass GithubCrawler(BaseCrawler):\nNext, we implement the extract() method, where the crawler first checks if the repository has \ndef extract(self, link: str, **kwargs) -> None:\nlogger.info(f\"Repository already exists in the database: {link}\")\nIf the repository is new, the crawler extracts the repository name from the link.\nlogger.info(f\"Starting scrapping GitHub repository: {link}\")\nitory content, name, link, platform information, and author details.\nlogger.info(f\"Finished scrapping GitHub repository: {link}\")\nCustomArticleCrawler class\nThe CustomArticleCrawler class takes a different approach to collecting data from the in-\nIt leverages the AsyncHtmlLoader class to read the entire HTML from a link and the \nHtml2TextTransformer class to extract the text from that HTML.\nBoth classes are made available \nNext, we define the CustomArticleCrawler class, which inherits from BaseCrawler.\nmethod, we first check if the article exists in the database to avoid duplicating content:\nclass CustomArticleCrawler(BaseCrawler):\ndef extract(self, link: str, **kwargs) -> None:\nlogger.info(f\"Article already exists in the database: {link}\")\nWe use the AsyncHtmlLoader class to load the \nclass, which returns a list of documents.\ngate the whole logic to these two classes, we don’t control how the content is extracted and parsed.\nThese two classes follow the LangChain paradigm, which provides high-level \nlogger.info(f\"Starting scrapping article: {link}\")\nWe get the page content from the extracted document, plus relevant metadata such as the title, \n\"Content\": doc_transformed.page_content,\nWe then create a new instance of the article model, populating it with the extracted content.\nlogger.info(f\"Finished scrapping custom article: {link}\")       \nMediumCrawler class\nThe code begins by importing essential libraries and defining the MediumCrawler class, which \nclass MediumCrawler(BaseSeleniumCrawler):\nWithin the MediumCrawler class, we leverage the set_extra_driver_options() method to extend \nThe extract() method implements the core functionality, first checking whether the article \nIf the article is new, the method proceeds to navigate to the article’s link and scroll through the \ndef extract(self, link: str, **kwargs) -> None:\nlogger.info(f\"Article already exists in the database: {link}\")\nlogger.info(f\"Starting scrapping Medium article: {link}\")\nself.driver.get(link)\nAfter fully loading the page, the method uses BeautifulSoup to parse the HTML content and \ninstance, populates it with the extracted content and user information provided via kwargs, and \ncontent=data,\nlogger.info(f\"Successfully scraped and saved article: {link}\")\nThe last step is understanding how the document classes \nWe had to implement three document classes to structure our data categories.\nThese classes \nIt is best practice to structure your data in classes instead of dictionaries, as the attributes we \nour data items with classes, we can ensure each attribute is as expected.\nArticleDocument class\nPostDocument class\nRepositoryDocument class\nThese are not simple Python data classes or Pydantic models.\nall the document classes without repeating any code, we used the Object-Document Mapping\nand document classes.\nMost modern Python applications use ORMs when interacting with the database.\nORM is mapped to the users table within the SQL database.\nall the CRUD operations on top of the User class.",
    "keywords": [
      "link",
      "content",
      "Data",
      "article",
      "Python",
      "ORM",
      "HTML",
      "database",
      "repository",
      "user",
      "Data Engineering",
      "extract",
      "model",
      "classes",
      "Selenium"
    ],
    "concepts": [
      "data",
      "classes",
      "importing",
      "python",
      "link",
      "content",
      "user",
      "users",
      "html",
      "orm"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 106-115)",
    "start_page": 106,
    "end_page": 115,
    "summary": "# Define a class that maps to the users table.\nclass User(Base):\nImplementing the ODM class\nclasses.\nHence, we will implement a base ODM class called NoSQLBaseDocument, from which all \nNext, we define a type variable T bound to the NoSQLBaseDocument class.\nPython’s generic module, allowing us to generalize the class’s types.\nThe class can be found in our repository at llm_engineering/domain/base/nosql.\nclass NoSQLBaseDocument(BaseModel, Generic[T], ABC):\nWithin the NoSQLBaseDocument class, an id field is defined as a UUID4, with a default factory \nThe class also implements the __eq__ and __hash__ methods to allow \nif not isinstance(value, self.__class__):\nThe class provides methods for converting between MongoDB documents and class instances.\nfrom_mongo() class method transforms a dictionary retrieved from MongoDB into an instance of \nthe class.\ndef from_mongo(cls: Type[T], data: dict) -> T:\nreturn cls(**dict(data, id=id))\nThe save() method allows an instance of the model to be inserted into a MongoDB collection.\nretrieves the appropriate collection, converts the instance into a MongoDB-compatible document \nThe get_or_create() class method attempts to find a document in the database matching the \nIf a matching document is found, it is converted into an instance of the class.\nIf not, a new instance is created with the filter options as its initial data and saved to the database:\ninstance = collection.find_one(filter_options)\nThe bulk_insert() class method allows multiple documents to be inserted into the database \ndef bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\nlogger.error(f\"Failed to insert documents of type {cls.__name__}\")\nThe find() class method searches for a single document in the database that matches the given \ninstance = collection.find_one(filter_options)\nSimilarly, the bulk_find() class method retrieves multiple documents matching the filter options.\nIt converts each retrieved MongoDB document into a model instance, collecting them into a list:\ninstances = collection.find(filter_options)\nreturn [document for instance in instances if (document := cls.\nFinally, the get_collection_name() class method determines the name of the MongoDB collec-\nexception will be raised specifying that the subclass should define a nested Settings class:\ndef get_collection_name(cls: Type[T]) -> str:\n\"Document should define an Settings configuration class with \nWe can configure each subclass using the nested Settings class, such as defining the collection \nData categories and user document classes\nNoSQLBaseDocument base class.\nThese are the concrete classes that define our data categories.\nposts within the crawler classes.\nWe begin by importing the essential Python modules and the ODM base class:\nWe define an enum class, where we centralize all our data category types.\nclass DataCategory(StrEnum):\nThe class can be found in the repository at llm_engineering/domain/types.py.\nThe Document class is introduced as an abstract base model for other documents on top of the \nNoSQLBaseDocument ODM class.\nclass Document(NoSQLBaseDocument, ABC):\nFinally, specific document types are defined by extending the Document class.\nclass RepositoryDocument(Document):\nclass Settings:\nclass PostDocument(Document):\nclass Settings:\nclass ArticleDocument(Document):\nclass Settings:\nFinally, we define the UserDocument class, which is used to store and query all the users from the \nclass UserDocument(NoSQLBaseDocument):\nclass Settings:\nBy implementing the NoSQLBaseDocument ODM class, we had to focus solely on the fields and \nFor example, when creating an instance of the ArticleDocument class, if the provided \nup with the ODM class and data category documents.\nFigure 3.5 shows the user output artifact generated by this data collection pipeline.",
    "keywords": [
      "ODM class",
      "data",
      "User",
      "ODM",
      "collection",
      "data collection pipeline",
      "NoSQLBaseDocument ODM class",
      "document",
      "instance",
      "str class Settings",
      "data collection",
      "database",
      "class Settings",
      "Data Engineering",
      "settings"
    ],
    "concepts": [
      "classes",
      "documents",
      "document",
      "importing",
      "collections",
      "collection",
      "collecting",
      "collect",
      "collected",
      "data"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 116-123)",
    "start_page": 116,
    "end_page": 123,
    "summary": "Figure 3.5: Example of the user output artifact after running the data collection pipeline using \nfrom which we collected data, the total number of links crawled for each domain, and the number \nData Engineering\nFigure 3.6: Example of the crawled_links output artifact after running the data collection \nNow, we can download the crawled_links artifact anywhere in our code by running the following \nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML \n- https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-\n- https://medium.com/decodingml/sota-python-streaming-pipelines-for-\n- https://decodingml.substack.com/p/real-time-feature-pipelines-\n- https://decodingml.substack.com/p/building-ml-systems-the-right-\n- https://decodingml.substack.com/p/reduce-your-pytorchs-code-\nTo run the pipeline using Paul’s configuration, we call the following poe command:\npoetry poe run-digital-data-etl-paul\ndigital_data_etl_paul_iusztin.yaml\nfigured a command that calls the data collection pipeline for all the supported authors:\npoetry poe run-digital-data-etl\nWe can easily query the MongoDB data warehouse using our ODM classes.\nFirst article link: https://medium.com/decodingml/an-end-to-end-framework-\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any \nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB \nyou can use this plugin for VSCode: https://www.mongodb.com/products/tools/vs-code.\nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML \nThe raw data stored in the MongoDB database is central to all future steps.\ncommenting out the Medium links added to the data collection YAML configs.\nthe configs/ directory and find all the YAML files that start with digital_data_etl_*, such as \ndigital_data_etl_maxime_labonne.yaml.\nFigure 3.7: Fix Selenium issues when crawling raw data\nproceed to the fine-tuning and inference sections without running the data collection ETL code.\nTo import all the data within this directory, run:\npoetry poe run-import-data-warehouse-from-json\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM \nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions \nWe learned how to crawl data in three ways: using CLI commands in \nAt the end of the chapter, we learned how to run ZenML pipelines with different YAML configura-\ndata warehouse through the ODM classes.\nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking \nhttps://realpython.com/introduction-to-mongodb-and-python/\nRAG Feature Pipeline\nThen, we will continue exploring LLM Twin’s RAG feature pipeline archi-\nFinally, we will go through a practical example by implementing the LLM Twin’s RAG \nExploring the LLM Twin’s RAG feature pipeline architecture\nImplementing the LLM Twin’s RAG feature pipeline",
    "keywords": [
      "data collection pipeline",
      "data",
      "RAG feature pipeline",
      "data collection",
      "LLM Twin",
      "RAG",
      "pipeline",
      "Paul Iusztin",
      "collection pipeline",
      "RAG feature",
      "llm",
      "MongoDB data warehouse",
      "Paul",
      "Data Engineering",
      "feature pipeline"
    ],
    "concepts": [
      "pipelines",
      "running",
      "run",
      "python",
      "mongodb",
      "rag",
      "yaml",
      "links",
      "link",
      "user"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 124-132)",
    "start_page": 124,
    "end_page": 132,
    "summary": "RAG enhances the accuracy and reliability of generative AI models with information fetched from \nAugmented: Add the data as context to the prompt\nGeneration: Use the augmented prompt with an LLM for generation\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized \nThe model is trained on data up to October 2023.\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and \nnecessary information into the prompt to answer the initial user question.\nthe augmented prompt to the LLM for the final answer.\ncontext to answer the user question.\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high \nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced con-\nRAG will act as the single source of truth for the generated answer.\nevaluate if the LLM’s answer is based on the external data or not.\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or \nDirectly injecting the necessary data to respond to user questions into the \nprompts that are fed to the LLM is enough to generate correct and valuable answers.\nright data into the prompt based on the user’s questions?\nRetrieval pipeline: A module that queries the vector DB and retrieves relevant entries to \nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and \nan LLM to generate answers\npopulate the vector DB with external data.\nThe question is passed to the retrieval module, which preprocesses the user’s input and \nThe generation pipelines use a prompt template, user input, and retrieved context to \nThe prompt is passed to an LLM to generate the answer.\nThe answer is shown to the user.\nYou must implement RAG in your generative AI application when you need access to any type of \nhave to implement a RAG strategy in your generative AI project.\nretrieval, and generation pipelines.\nThe RAG ingestion pipeline extracts raw documents from various data sources (e.g., data ware-\nUltimately, it loads the embedded chunks into a vector DB (or other similar \nso, at the retrieval time, you will add only the essential data to the prompt.\nThe embedding component uses an embedding model to take the chunk’s content (text, \nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, \nThe next step is to retrieve relevant data from the vector store correctly.\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the \nvector DB for similar vectors to the user’s input.\nThe primary function of the retrieval step is to project the user’s input into the same vector space \nas the embeddings used as an index in the vector DB.\nilar entries by comparing the embeddings from the vector storage with the user’s input vector.\nThese entries then serve as content to augment the prompt that is passed to the LLM to generate \ndata and the embedding model you use.\npreprocess the user input in the same way you processed the raw documents in the RAG ingestion \nThis means you must clean, chunk (if necessary), and embed the user’s input using the \nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and \nThe final prompt results from a system and prompt template populated with the user’s query and \nand how they are used together with the retrieval logic and the LLM to generate the final answer:\nYou are a helpful assistant who answers all the user's questions politely.\nAnswer the user's question using only the provided context.\nretrieved_context = retrieve(user_question)\nprompt += prompt_template.format(context=retrieved_context, user_\nanswer  = llm(prompt)\nknow that a given answer was generated by a specific version of the LLM and prompt template(s).\nyour RAG system are the embeddings of the external data, usually stored in vector DBs, the em-",
    "keywords": [
      "RAG Feature Pipeline",
      "RAG",
      "RAG ingestion pipeline",
      "LLM",
      "data",
      "RAG Feature",
      "user",
      "prompt",
      "answer",
      "RAG system",
      "Pipeline",
      "RAG ingestion",
      "vector",
      "Feature Pipeline",
      "embeddings"
    ],
    "concepts": [
      "rag",
      "data",
      "prompt",
      "prompts",
      "retrieval",
      "retrieves",
      "retrieved",
      "retrieve",
      "vector",
      "vectors"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 133-140)",
    "start_page": 133,
    "end_page": 140,
    "summary": "Embeddings come in handy when we want to feed words, images, or audio data into models.\nFor instance, when working with transformer models, you tokenize all your text input, where \nBased on this example, you can use embeddings to encode any categorical variable and feed it to \nimages, where a CNN encoder module maps the high-dimensional meaning into an embedding, \nEach category is represented as a unique binary vector.\neach categorical variable, a binary vector is created with a length equal to the number \nEmbeddings help us encode categorical variables while controlling the output vec-\ninto the final vector embedding, a numerical image representation.\nFigure 4.4: Creating embeddings from an image using a CNN (Image source)\nHow are embeddings created?\nEmbeddings are created by deep learning models that understand the context and semantics of \nyour input and project it into a continuous vector space.\nVarious deep learning models can be used to create embeddings, varying by the data input type.\nembedding model.\nFor example, when working with text data, one of the early methods used to create embeddings \nture to smartly project your input into a dense vector space that can later be used as embeddings.\nprovides a user-friendly interface, making the embedding process straightforward and efficient.\ned the embeddings for three sentences, and, ultimately, computed the cosine similarity between \n\"The dog is swimming.\"\nembeddings = model.encode(sentences)\nsimilarities = model.similarity(embeddings, embeddings)\nThe best-performing embedding model can change with time and your specific use case.\nfind particular models on the Massive Text Embedding Benchmark (MTEB) on Hugging Face.\nof the audio, such as a spectrogram, and then apply image embedding models to those visuals.\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the \nsame vector space.\nThis allows you to find similar images using a sentence as input, or the other \nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences.\nimg_emb = model.encode(image)\ntext_emb = model.encode(\nblob/main/code_snippets/08_text_image_embeddings.py.\nmost digital data categories, such as words, sentences, documents, images, videos, and graphs.\nbetween two different data categories, such as the distance between the vector of a sentence and \nThese models are designed to project both data types into the same vector space, \nApplications of embeddings\nThe last step to fully understanding how RAG works is to examine vector DBs and how they \nleverage embeddings to retrieve data.\nMore on vector DBs\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embed-\nTraditional scalar-based DBs struggle with the complexity of vector data, making vector \nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ \nHow does a vector DB work?\nVector DBs are different.\nUnder the hood, a vector DB uses \nIndexing vectors: Vectors are indexed using data structures optimized for high-dimen-\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those \nmost similar to the input vector.\nVector DBs can filter results based on metadata before or after the vector search.\nAlgorithms for creating the vector index\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\nLSH: LSH maps similar vectors into buckets.\nThese algorithms enable vector DBs to efficiently handle complex and large-scale data, making \nVector DBs also share common characteristics with standard DBs to ensure high performance, ",
    "keywords": [
      "vector",
      "embeddings",
      "vector DBs",
      "RAG Feature Pipeline",
      "data",
      "vectors",
      "image",
      "embedding",
      "RAG Feature",
      "models",
      "DBs",
      "similarity",
      "Feature Pipeline",
      "vector space",
      "input"
    ],
    "concepts": [
      "vectors",
      "vector",
      "data",
      "embeddings",
      "embedding",
      "images",
      "image",
      "models",
      "model",
      "useful"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 141-148)",
    "start_page": 141,
    "end_page": 148,
    "summary": "Are the retrieved documents relevant to the user’s question?\nIs the retrieved context enough to answer the user’s question?\nDoes the latency of the retrieval step match our requirements?\nWhat do we do if we can’t generate a valid answer using the retrieved information?\ndata and generate answers relative to the user’s question.\nPre-retrieval: This stage focuses on how to structure and preprocess your data for data \nRetrieval: This stage revolves around improving the embedding models and metadata \nPost-retrieval: This stage mainly targets different ways to filter out noise from the retrieved \nPre-retrieval\nThe pre-retrieval steps are performed in two different ways:\nData indexing: It is part of the RAG ingestion pipeline.\nQuery optimization: The algorithm is performed directly on the user’s query before em-\nbedding it and retrieving the chunks from the vector DB.\nAs we index our data using embeddings that semantically represent the content of a chunked \ndocument, most of the data indexing techniques focus on better preprocessing and structuring \nthe data to improve retrieval efficiency, such as:\nfilter results efficiently during retrieval.\nOptimizing index structures: It is based on different data index methods, such as various \nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used \nThus, using smaller chunks enhances the retrieval’s accuracy, \nOn the query optimization side, we can leverage techniques such as query routing, query rewriting, \nand query expansion to refine the retrieved information for the LLM further:\nQuery routing: Based on the user’s input, we might have to interact with different cate-\ngories of data and query each category differently.\nAs illustrated in Figure 4.6, let’s assume that, based on the user’s input, to do RAG, we \ncan retrieve additional context from a vector DB using vector search queries, a standard \nThis can help the retrieval stage identify relevant \nthe query, for example, “Paris,” and add it to your filter to reduce your vector search space).\nBoth data indexing and query optimization pre-retrieval optimization techniques depend highly \nRetrieval\nThe retrieval step can be optimized in two fundamental ways:\nImproving the embedding models used in the RAG ingestion pipeline to encode the \ntime when you have to retrieve the most similar chunks based on user input.\nthe semantic similarity between the query and the indexed data.\nInstead of fine-tuning the embedding model, you can leverage instructor models (https://\nOn the other side of the spectrum, here is how you can improve your retrieval by leveraging classic \npinpoint accuracy and the retrieved information must include exact keyword matches, \nFiltered vector search: This type of search leverages the metadata index to filter for specific \nIt differs from a hybrid search in that you retrieve the data \nIn practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as \nPost-retrieval\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the \nThis is because the retrieved context can sometimes be too large or contain irrelevant information, \nTwo popular methods performed at the post-retrieval step are:\ninput and every retrieved chunk.\nthe data using a similarity distance between the embeddings and refine the retrieved ",
    "keywords": [
      "RAG Feature Pipeline",
      "RAG",
      "query",
      "data",
      "RAG Feature",
      "search",
      "vector search",
      "advanced RAG",
      "user",
      "retrieval",
      "LLM",
      "vector",
      "embedding",
      "retrieved",
      "RAG ingestion pipeline"
    ],
    "concepts": [
      "query",
      "queries",
      "retrieval",
      "retrieving",
      "retrieve",
      "rag",
      "data",
      "search",
      "searching",
      "searches"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 149-163)",
    "start_page": 149,
    "end_page": 163,
    "summary": "data you work with.\nRAG Feature Pipeline\nmodel, utilizing classic filtering DB operations, and removing noisy data.\nin mind, you can effectively optimize your RAG workflow for data processing and retrieval\nExploring the LLM Twin’s RAG feature pipeline \nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a \nwe get our raw data.\nRAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) \nfrom our MongoDB data warehouse.\na logical feature store using ZenML artifacts and a Qdrant vector DB.\nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and \nRAG Feature Pipeline\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and \nlogical feature store while processing the data accordingly.\nHaving the data in a feature store \nThe training pipeline will use the cleaned data from the feature store (stored \nThat is why we are designing a feature pipeline and not only a RAG ingestion \nIt clearly states that it takes raw data as input and then outputs features and optional \nbetween the data warehouse and the feature store goes into the feature pipeline namespace, con-\nin cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under \nthe feature pipeline umbrella as the artifacts are part of the logical feature store.\nwould be implementing a data validation pipeline on top of the raw data or computed features.\nAnother important observation to make is that text data stored as strings are not considered \nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse.\nThe data \nwarehouse is populated by the data collection ETL pipeline presented in Chapter 3.\nDesigning the architecture of the RAG feature pipeline\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM \nWe will use a batch design scheduled to poll data from the MongoDB data \nFigure 4.9: The architecture of the LLM Twin’s RAG feature pipeline\nRAG Feature Pipeline\nA batch pipeline in data systems refers to a data processing method where data is collected, pro-\nproach differs from real-time or streaming data processing, where data is processed continuously \nScheduled processing: Data processing is scheduled at regular intervals, for example, \nDuring this time, the collected data is processed in bulk.\nData loading: After processing, the data is loaded into the target system, such as a DB, data \nwarehouse, data lake, or feature store.\nThis processed data is then available for analysis, \nBatch pipelines are particularly useful when dealing with large volumes of data that do not require \nEfficiency: Batch processing can handle large volumes of data more efficiently than re-\nComplex processing: Batch pipelines can perform complex data transformations and \nBatch versus streaming pipelines\nWhen implementing feature pipelines, you have two main design choices: batch and streaming.\nbatch architecture over a streaming one for our LLM Twin use case.\nTable 4.1 compares batch and streaming pipelines based on multiple criteria such as processing \nProcesses data at regular \nProcesses data \ndata more efficiently, \ncomplex data transformations \nimmediate data processing \nand feature pipelines.\nTable 4.1: Batch versus streaming pipelines\nRAG Feature Pipeline\nBy implementing a streaming pipeline, you update \nactions as they occur, not after a few minutes or hours as a batch pipeline would process them.\ndations periodically, such as every night, based on historical user behavior data using a batch \nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load \ndata for different use cases.\nThe ETL design is widespread in data pipelines used to move data \nSome practical use cases include aggregating data for analytics, where \nyou have to extract data from multiple sources, aggregate it, and load it to a data warehouse \nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline \nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming \nTwin’s feature pipeline for the following reasons:\nDoes not require immediate data processing: Even if syncing the data warehouse and \ndata storages.\nThe whole data \nRAG Feature Pipeline\nbatch) and the quantity of data you have to process (small versus big data).\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\nIn the Change data capture: syncing the data warehouse and feature store section later in this chapter, \nMost of the RAG feature pipelines are composed of five core steps.\nRAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:\ngoDB data warehouse.\nAt the extraction step, you usually aggregate all the data you need \nThat is why you usually chunk a document based on your data \nData loading: The final step combines the embedding of a chunked document and its \npush data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB.\nRAG Feature Pipeline\nChange data capture: syncing the data warehouse and feature \nAs highlighted a few times in this chapter, data is constantly changing, which can result in DBs, \ndata lakes, data warehouses, and feature stores getting out of sync.\nChange data capture (CDC) \nhow to sync the data warehouse with the feature store to have data fresh enough for your par-\nWhat if we want to process only the new or updated items from the data warehouse and \nand transmits data modifications to target systems for processing.\ndata changes.\nin data propagation.\nsource DB, captures all data changes, and requires no schema modification.\nin our RAG feature pipeline to sync the data warehouse and feature store more optimally when \nthe data grows.\nin the source DB; it just pulls everything from the data warehouse.\nevents and a streaming pipeline to process them.\nFor more details on CDC, I recommend What is Change Data Capture?\nRAG Feature Pipeline\nWhy is the data stored in two snapshots?\nWe store two snapshots of our data in the logical feature store:\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the Mon-\nThe data from the warehouse is shared \nanother summarization use case where we must clean and preprocess the data differently.\ndata warehouse is generic and is modeled to specific applications only in downstream compo-\npipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, \nZenML will orchestrate the batch RAG feature pipeline.\nit after the ETL data collection pipeline finishes.\nImplementing the LLM Twin’s RAG feature pipeline\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied every-\nThe cleaning, chunking, and embedding logic for all our data categories",
    "keywords": [
      "RAG Feature Pipeline",
      "Feature Pipeline",
      "RAG Feature",
      "data",
      "data warehouse",
      "Feature",
      "feature store",
      "Pipeline",
      "LLM Twin",
      "RAG",
      "feature pipeline RAG",
      "pipeline RAG Feature",
      "batch pipeline",
      "batch RAG feature",
      "batch"
    ],
    "concepts": [
      "data",
      "pipeline",
      "pipelines",
      "processing",
      "processes",
      "process",
      "processed",
      "based",
      "useful",
      "rag"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 164-171)",
    "start_page": 164,
    "end_page": 171,
    "summary": "five core phases of RAG ingestion code: extracting raw documents, cleaning, chunking, embed-\ndef feature_engineering(author_full_names: list[str]) -> None:\nraw_documents = fe_steps.query_data_warehouse(author_full_names)\ncleaned_documents = fe_steps.clean_documents(raw_documents)\nlast_step_1 = fe_steps.load_to_vector_db(cleaned_documents)\nembedded_documents = fe_steps.chunk_and_embed(cleaned_documents)\nlast_step_2 = fe_steps.load_to_vector_db(embedded_documents)\nFigure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps \nThus, we can call the feature engineering pipeline calling the \nall the feature engineering pipeline steps is available on GitHub at \"steps/feature_engineering\".\nIt fetches all the raw data for the user from the data warehouse and extends the documents\nlist to include these user documents.\n) -> Annotated[list, \"raw_documents\"]:\ndocuments = []\nlogger.info(f\"Querying data warehouse for user: {author_full_\nuser_documents = [doc for query_result in results.values() for doc \nstep_context.add_output_metadata(output_name=\"raw_documents\", \nmetadata=_get_metadata(documents))\nThe _get_metadata() function takes the list of queried documents and authors and counts the \ndef _get_metadata(documents: list[Document]) -> dict:\nmetadata[collection][\"authors\"] = list()\nmetadata[collection][\"num_documents\"] = metadata[collection].\nmetadata[collection][\"authors\"].append(document.author_full_name)\nFor example, in Figure 4.13, we accessed the metadata tab of the query_data_warehouse()\nstep, where you can see that, within that particular run of the feature pipeline, we loaded 76 \ndocuments from three authors.\nFigure 4.13: Metadata of the “query the data warehouse” ZenML step\nCleaning the documents\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a \ndef clean_documents(\n) -> Annotated[list, \"cleaned_documents\"]:\ncleaned_documents = []\nstep_context.add_output_metadata(output_name=\"cleaned_documents\", \nmetadata=_get_metadata(cleaned_documents))\nreturn cleaned_documents\nThe computed metadata is similar to what we logged in the query_data_warehouse() step.\nChunk and embed the cleaned documents\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to \ncleaned_documents: Annotated[list, \"cleaned_documents\"],\n) -> Annotated[list, \"embedded_documents\"]:\nmetadata = {\"chunking\": {}, \"embedding\": {}, \"num_documents\": \nchunks = ChunkingDispatcher.dispatch(document)",
    "keywords": [
      "RAG Feature Pipeline",
      "Feature Pipeline",
      "RAG Feature",
      "documents",
      "Feature",
      "feature engineering pipeline",
      "Pipeline",
      "metadata",
      "step",
      "data",
      "list",
      "RAG",
      "ZenML",
      "query",
      "feature engineering"
    ],
    "concepts": [
      "pipeline",
      "pipelines",
      "metadata",
      "documents",
      "document",
      "steps",
      "step",
      "runs",
      "run",
      "different"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 172-179)",
    "start_page": 172,
    "end_page": 179,
    "summary": "step_context.add_output_metadata(output_name=\"embedded_documents\", \nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step.\nFigure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized \nIn Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details \nFigure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding \nLoading the documents to the vector DB\nAs each article, post, or code repository sits in a different collection inside the vector DB, we have \nto group all the documents based on their data category.\ngrouped_documents = VectorBaseDocument.group_by_class(documents)\nfor document_class, documents in grouped_documents.items():\nlogger.info(f\"Loading documents into {document_class.get_\nceeding, it’s important to understand the hierarchy of the domain classes we are working with.\nThe data category: Post, article, and repository\nThe state of the data: Cleaned, chunked, and embedded\nWe decided to create a base class for each state of the document, resulting in having the following \nbase abstract classes:\nclass CleanedDocument(VectorBaseDocument, ABC)\nclass Chunk(VectorBaseDocument, ABC)\nclass EmbeddedChunk(VectorBaseDocument, ABC)\nNote that all of them inherit the VectorBaseDocument class, which is our custom OVM implemen-\nmakes the class abstract.\nThat is why base classes are always marked as abstract.\nEach base abstract class from above (which models the state) will have a subclass that adds \nabstract classes.\nWe will implement a specific document class for each data category and state com-\nFigure 4.16: Domain entities class hierarchy and their interaction\nThus, structuring the classes after the state allows us to plug another data \ncategory by inheriting these base abstract classes.\ncleaned document will be saved within the metadata of the vector DB.\nwithin the vector DB, the data category of the entity, and whether to leverage the vector index \nclass CleanedDocument(VectorBaseDocument, ABC):\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embed-\nclass Chunk(VectorBaseDocument, ABC):\nclass EmbeddedChunk(VectorBaseDocument, ABC):\nVectorBaseDocument OVM class.\nOur OVM base class is called VectorBaseDocument.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\nif cls._has_class_attribute(\"embedding\"):\npayload[\"embedding\"] = point.vector or None\nvector = payload.pop(\"embedding\", {})",
    "keywords": [
      "RAG Feature Pipeline",
      "metadata",
      "documents",
      "vector",
      "str",
      "Feature Pipeline",
      "document",
      "embedding",
      "step",
      "domain",
      "RAG Feature",
      "data",
      "VectorBaseDocument",
      "ABC",
      "str class Config"
    ],
    "concepts": [
      "classes",
      "documents",
      "document",
      "vectors",
      "vector",
      "domain",
      "type",
      "typed",
      "types",
      "typing"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 180-188)",
    "start_page": 180,
    "end_page": 188,
    "summary": "For example, the from_record() method of the Chunk() class, \nThe from_record() method adapts a data point from Qdrant’s format to our internal structure \nThe bulk_insert() method maps each document to a point.\ndef bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \ncls._bulk_insert(documents)\nTrying to create the collection and reinsert the documents.\"\ncls._bulk_insert(documents)\nlogger.error(f\"Failed to insert documents in '{cls.get_\ndef _bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \nThe collection name is inferred from the Config class defined in the subclasses inheriting the OVM:\nThe function below scrolls the Qdrant vector DB, which returns a list of data \nthe point from which Qdrant starts returning records.\ndef bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ndocuments, next_offset = cls._bulk_find(limit=limit, **kwargs)\nlogger.error(f\"Failed to search documents in '{cls.get_\ndocuments, next_offset = [], None\nreturn documents, next_offset\ndef _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \nreturn documents, next_offset\ndef search(cls: Type[T], query_vector: list, limit: int = 10, \ndocuments = cls._search(query_vector=query_vector, \nlogger.error(f\"Failed to search documents in '{cls.get_\nreturn documents\ndef _search(cls: Type[T], query_vector: list, limit: int = 10, \nreturn documents\non to the dispatchers who clean, chunk, and embed the documents.\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, \nA handler can either clean, chunk, or embed a document.\nBased on its data category, it instantiates and calls a handler that \napplies the cleaning logic specific to that data point:\ndef dispatch(cls, data_model: NoSQLBaseDocument) -> \ndata_category = DataCategory(data_model.get_collection_name())\nhandler = cls.cleaning_factory.create_handler(data_category)\nclean_model = handler.clean(data_model)\n\"Data cleaned successfully.\",\nreturn clean_model\ncleaning handler based on the document’s data category:\ndef create_handler(data_category: DataCategory) -> \nYou have a single class that cleans any document, which \nan extra type, we must extend only the Factory class instead of multiple occurrences in the code.\ncorrect handler based on the data category of the input document.\nIn our case, these handlers implement the clean() method regardless of the \nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.\nInitially, we knew we wanted to clean the data, but as we knew the data category only at \nWhen we get a data point, we apply the abstract factory pattern and create the correct \ncleaning handler for its data type.\nimplement a new handler and modify the Factory class without touching any other part \nIn total, we will have nine Handler classes that follow the next \nclass CleaningDataHandler()\nFigure 4.17: Handler class hierarchy and their interaction\nThe cleaning handlers\ndef clean(self, data_model: DocumentT) -> CleanedDocumentT:\ndef clean(self, data_model: PostDocument) -> CleanedPostDocument:\ncontent=clean_text(\" #### \".join(data_model.content.\ndef clean(self, data_model: ArticleDocument) -> \ndef clean(self, data_model: RepositoryDocument) -> ",
    "keywords": [
      "data",
      "RAG Feature Pipeline",
      "collection",
      "documents",
      "handler",
      "data category",
      "RAG Feature",
      "Feature Pipeline",
      "type",
      "Qdrant",
      "category",
      "vector",
      "offset",
      "clean",
      "classmethod def"
    ],
    "concepts": [
      "classes",
      "document",
      "documents",
      "returns",
      "returning",
      "handlers",
      "handler",
      "clean",
      "cleaning",
      "cleaned"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 189-197)",
    "start_page": 189,
    "end_page": 197,
    "summary": "content=clean_text(\" #### \".join(data_model.content.\nThe chunking handlers\nchunking logic.\nThe handler takes cleaned documents as input and returns chunk entities.\n\"chunk_size\": 500,\ndef chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:\nThe handler’s chunk() method inputs cleaned article documents and returns a list of article chunk \nIt uses the chunk_text() function to split the cleaned content into chunks.\nThe chunking \nThe chunk_id \na list of chunk entities and return them.\ndef chunk(self, data_model: CleanedArticleDocument) -> \ndata_models_list = []\ncleaned_content = data_model.content\ndocument_id=data_model.id,\nauthor_id=data_model.author_id,\ndata_models_list.append(model)\nreturn data_models_list\ndef chunk_article(text: str, min_length: int, max_length: int) -> \nengineering/application/preprocessing/chunking_data_handlers.py, have a similar struc-\nThe chunk_text() function is a two-step process that has \nof the embedding model.\nAt this point, we also apply the chunk_overlap logic, as we want to do it only after we \ntokens_per_chunk=embedding_model.max_input_length,\nmodel_name=embedding_model.model_id,\nreturn chunks_by_tokens\nTo conclude, the function above returns a list of chunks that respect both the provided chunk \nparameters and the embedding model’s max input length.\ngathers their content into a list, passes them to the embedding model, and maps the results to an \nembedded chunk domain entity.\nembedding_model = EmbeddingModelSingleton()\nAbstract class for all embedding data handlers.\nAll data transformations logic for the embedding step is done here\ndef embed(self, data_model: ChunkT) -> EmbeddedChunkT:\nreturn self.embed_batch([data_model])[0]\ndef embed_batch(self, data_model: list[ChunkT]) -> \nembedding_model_input = [data_model.content for data_model in \ndata_model]\nembeddings = embedding_model(embedding_model_input, to_list=True)\nembedded_chunk = [\nself.map_model(data_model, cast(list[float], embedding))\nfor data_model, embedding in zip(data_model, embeddings, \nreturn embedded_chunk\ndef map_model(self, data_model: ChunkT, embedding: list[float]) -> \ntakes a chunk of input and computes the embeddings in batch mode.\ndef map_model(self, data_model: ArticleChunk, embedding: list[float]) \nid=data_model.id,\ncontent=data_model.content,\ndocument_id=data_model.document_id,\n\"max_input_length\": embedding_model.max_input_length,\nmodel.\nThat is why I am not insisting at all on what embedding model to use.\nclass, which can quickly be configured, you can experiment with multiple embedding models \nmodel_id: str = settings.TEXT_EMBEDDING_MODEL_ID,\nself._model = SentenceTransformer(\nself._model_id,\ndef model_id(self) -> str:\nreturn self._model_id\ndummy_embedding = self._model.encode(\"\")\nreturn self._model.max_seq_length\nreturn self._model.tokenizer\nembeddings = self._model.encode(input_text)\nlogger.error(f\"Error generating embeddings for {self._model_\nThe embedding model class implements the singleton pattern (https://refactoring.guru/",
    "keywords": [
      "chunk",
      "model",
      "RAG feature pipeline",
      "embedding",
      "data",
      "embedding model",
      "RAG feature",
      "RAG",
      "list",
      "length",
      "text",
      "feature pipeline",
      "chunks",
      "input",
      "size"
    ],
    "concepts": [
      "chunking",
      "chunks",
      "chunked",
      "embedding",
      "embedded",
      "embeddings",
      "model",
      "models",
      "list",
      "data"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 198-207)",
    "start_page": 198,
    "end_page": 207,
    "summary": "Kenton, J.D.M.W.C. and Toutanova, L.K., 2019, June.\nSFT refines the model’s capabilities using carefully curated pairs of instructions and correspond-\nthe model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.\nCreating a high-quality instruction dataset\nBy the end of this chapter, you will be able to create your own instruction datasets and efficiently \nCreating an instruction dataset\nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning \nMoreover, the quality of the data is also crucial.\nThis careful review helps ensure that the dataset is accurate and useful for training the model.\nIn this section, we will introduce a general framework to create your own instruction datasets, \nInstruction datasets are defined as pairs of instructions and answers.\ninputs of the model, used as context during fine-tuning.\nthe model.\nDuring fine-tuning, you can choose to train the model on the instructions and answers, \nIn this case, “inputs” contain the data the model \nof the model.\nTable 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\nThis example illustrates how the “system” field is used to define specific behaviors for the model, \nThe “instruction” field provides the necessary data (the concepts) and the task \nTo build an instruction dataset, we want to curate data that is representative of how the model will \nOnce we have gathered enough samples, our goal is to filter them to only keep high-quality \ndata.\nIn this context, high-quality data can be described through three main dimensions:\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering \nBy sampling data in a representative \nmanner, we allow models to develop robust instruction-following capabilities.\nIn the following sections, we will see techniques to filter and evaluate instruction samples ac-\nThe Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or \nlook for related open-source datasets to leverage for fine-tuning.\nit with high-quality data.\nCalculating an ideal number of samples is a difficult task, as both the quality of the data and the \nfor example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in \nquality of the data is a crucial factor, and a high number of samples is always desirable.\nTo provide additional numbers, we can look at the fine-tuned models developed by companies \naimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, \nGeneral-purpose models cover more topics, which requires additional samples.\nsource community, models like OpenHermes and Dolphin use around one million samples.\non the quality of these finetunes, we recommend an instruction dataset of at least one million \nsamples to create a good general-purpose instruct model.\nOn the other hand, models fine-tuned \nfor a specific purpose require fewer samples.\nHere, we differentiate task-specific models from \nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs.\nTask-specific models are designed to excel at a particular function, such as translation, summari-\nThe data required for task-specific fine-tuning is generally more manageable, \nThis makes task-specific fine-tuning an attractive option\nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge \nThese models \nThe data requirements for domain-specific fine-tuning can vary widely depending on the com-\nSome fields, like medicine or law, may require as much data \nor hospitality, might need fewer samples, more in line with task-specific fine-tuning.\nThe key factors determining the data needs for domain-specific models are the “size” of the \nthat domain in the model’s pre-training data.\ntraining data may require less fine-tuning, while those that are more specialized or underrep-\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and \ndomain-specific models.\nFor task-specific models, data curation often involves collecting examples \nDomain-specific data curation can be more challenging.\nrelevance of this data is crucial, as it directly impacts the model’s ability to understand and gen-\nerful models by providing a few examples of the desired task within the input prompt.\nnot a replacement for fine-tuning in all scenarios (e.g., when you want to learn a new domain), \nfew-shot prompting can be an efficient way to adapt models to new tasks without the need for \nIn practice, the line between task-specific and domain-specific models can sometimes blur.\ninstance, a model fine-tuned for medical diagnosis could be considered both task-specific (focused \nnext step consists of refining the quality of the samples through rule-based filtering, data dupli-\nRule-based filtering is a systematic approach to data quality control that relies on explicit, pre-\ndefined rules to evaluate and filter data samples.\nprimary goal of rule-based filtering is to maintain a high standard of data quality by removing \nphrases associated with low-quality or inappropriate content, and then filtering out any samples \nFormat checking is recommended for datasets that include structured data or follow specific \nticularly important for datasets containing code samples, JSON structures, or other formatted \nFor example, in a dataset of programming instructions and solutions, you might implement \nRule-based filtering offers significant advantages in preparing instruction datasets.\nfiltering reduces the need for manual intervention and enables continuous data quality monitoring.\nas data patterns and quality standards evolve, rules need regular review and updates to remain \nData deduplication\nDataset diversity is fundamental to training models that can generalize well to new, unseen data.\nOverfitting: Models may memorize specific examples rather than learning general patterns.\nBiased performance: Overrepresented data points may skew the model’s performance \ntion removes identical samples through a straightforward process involving data normalization, \nentire samples into vector representations using various natural language processing techniques.",
    "keywords": [
      "Data",
      "models",
      "model",
      "samples",
      "instruction",
      "instruction dataset",
      "Fine-Tuning",
      "dataset",
      "datasets",
      "domain-specific models",
      "RAG Feature Pipeline",
      "quality",
      "RAG Feature",
      "data quality",
      "rule-based filtering"
    ],
    "concepts": [
      "data",
      "models",
      "model",
      "specific",
      "samples",
      "sample",
      "sampling",
      "instructions",
      "instruct",
      "tasks"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 208-215)",
    "start_page": 208,
    "end_page": 215,
    "summary": "For large datasets, clustering techniques may be applied to group similar vectors.\nData decontamination\nData decontamination is the process of ensuring that the training dataset does not contain samples \nfor ensuring the quality of the model evaluation and preventing overfitting or memorization of \ntest data.\nData decontamination uses techniques from data deduplication.\nmethods to identify and remove training samples that are very similar to evaluation samples, \nAnother aspect of data decontamination is filtering out samples that may have been derived from \nthe same source as evaluation data.\nthe data they use) to identify and exclude data from specific sources that are known to be used \nA simple way to perform data decontamination is to add your evaluation set to the \ninstruction dataset during the data deduplication stage.\nensure that we only remove samples from the instruction dataset, which can be \nevaluation sets in the data deduplication stage to fully automate this process.\nData quality evaluation\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process \nTraditional methods of data quality assessment include human annotation, which generally \nas judges, reward models, and classifiers trained for quality prediction.\nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample.\nyou might want to use domain-specific models instead of better, general-purpose LLMs. Com-\nYou are a data quality evaluator.\nA score of 4 means that the answer is excellent and fully addresses the task.\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\nReward models are another way to re-purpose LLMs for data quality evaluation.\ncan be broadly defined as models that take an instruction and answer pair and return a score as \nThis allows for a more fine-grained approach to data quality evaluation.\ntypes of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set \nstruction data quality, it is a good resource for finding models capable of differentiating between \nClassifiers or encoder-only models can be trained to perform data quality evaluation.\nThis model was designed as a quality filter for pretraining data but a similar \napproach can be taken to evaluate instruction samples at scale.\ner-only models are still valuable to filter out outliers or as part of an automated data pipeline, \nData exploration\ntraining data.\ninconsistencies that automated processes might miss, including formatting issues, data entry \nFigure 5.4 shows an example with Argilla, a collaborative platform for manual data quality eval-\nFigure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\nIt is often associated with data visualization, with figures that show clusters \nLet’s consider the task of building an instruction dataset about various programming languages.\nidentify sub-topics like error handling, data structures, and web frameworks.\nData generation\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary.\nWhile data can be \nSynthetic data generation using LLMs offers a more efficient \ncan produce high-quality data at a much larger scale, effectively addressing the limitations of \nmanual data creation processes.\nThe process of synthetic data generation typically begins with the preparation of a set of carefully \nThe quality of synthetically generated data largely depends on the prompts and techniques \ncific instructions, examples, and constraints to ensure the generated data aligns with the desired \nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality.\nmodel or set of rules checks the generated pairs for accuracy, relevance, and adherence to spec-",
    "keywords": [
      "Data quality evaluation",
      "Data",
      "Data quality",
      "evaluation",
      "quality evaluation",
      "quality",
      "models",
      "Samples",
      "answer",
      "dataset",
      "LLMs",
      "instruction",
      "Data decontamination",
      "model",
      "process"
    ],
    "concepts": [
      "data",
      "model",
      "similarity",
      "similar",
      "generate",
      "generative",
      "generation",
      "generated",
      "generating",
      "answer"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 216-225)",
    "start_page": 216,
    "end_page": 225,
    "summary": "An important aspect of synthetic data generation is the ability to control various attributes of the \ngenerated data.\nThis includes factors such as the complexity of the instructions, the length of the \nFurthermore, synthetic data generation can be particularly useful for addressing biases and gaps \nHowever, synthetic data generation also comes with challenges.\npotential for the generated data to inherit biases or errors from the underlying language model \ngenerated data.\nAnother consideration is the need for the generated data to be sufficiently diverse and challeng-\nAdvanced techniques in synthetic data generation often focus on \ncreating varied and nuanced instruction-response pairs that can push the boundaries of what \nUnlike data generation, we use pre-existing instruction samples \nWhile it is possible to upsample pairs of instructions and answers, data \nA pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple \ninstructions into more qualitative ones.\nThe evolved instructions can then be used to generate \nIn-depth evolving focuses on enhancing the complexity of existing instructions.\nprecision to the instruction.\nIncreasing reasoning steps: It modifies instructions to explicitly request multiple-step \nComplicating input: This involves adding more complex data formats or structures to \nthe instruction, such as XML, JSON, or code snippets.\nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset.\nIt generates entirely new instructions inspired by existing ones, focusing on creating more rare \nYou simply need to provide the instruction you want \nto evolve as input, and a powerful model like GPT-4o will return a more complex version of the \noriginal instruction.\nYou are an Instruction Rewriter that rewrites the given #Instruction# into a more complex \nPlease follow the steps below to rewrite the given “#Instruction#” into a more complex \nStep 1: Please read the “#Instruction#” carefully and list all the possible methods \nto make this instruction more complex (to make it a bit harder for well-known AI \nchange the language of the instruction!\nStep 2: Please create a comprehensive plan based on the #Methods List# generated \nin Step 1 to make the #Instruction# more complex.\nStep 3: Please execute the plan step by step and provide the #Rewritten Instruction#.\nStep 4: Please carefully review the #Rewritten Instruction# and identify any \nEnsure that the #Rewritten Instruction# is only a more complex \nversion of the #Instruction#.\nJust provide the #Finally Rewritten Instruction# without \nStep 3 #Rewritten Instruction#:\nStep 4 #Finally Rewritten Instruction#:\n#Instruction#:\n{Instruction}\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language \nof instruction quality.\nUnlike Evol-Instruct, which evolves instructions, UltraFeedback uses a large pool of \ndiverse instructions and models to generate a wide range of responses.\nscores for these responses across multiple dimensions such as instruction-following, truthfulness, \ning and diverse instruction dataset.\nBy refining and evolving existing instructions and answers, \nthe resulting dataset can better train models to handle complex, multi-step tasks, and improve \nCreating our own instruction dataset\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter \n3. To create a high-quality instruction dataset, we need to address two main issues: the unstruc-\nof pairs of instructions and answers.\nBacktranslation refers to the process of providing the expected answer as output and generat-\ning its corresponding instruction.\nHowever, using a chunk of text like a paragraph as an answer \nThe number of articles we can retrieve is limited, which constrains the size of the instruction \ninto chunks and generate three instruction-answer pairs for each chunk.\nspecific templates or instructions, there’s no guarantee that the model will consistently adhere \nFigure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\nallow us to interact with a model to generate the instruction data, and datasets will format \nprogress during the data generation process.\nby extracting specific fields from each article: id, content, platform, author_id, author \ndef load_articles_from_json(file_path: str) -> Dataset:\ndata\"]],\ndata\"]],\nsubstack.com/p/\nsubstack.com/p/\nsubstack.com/p/\nof instructions and answers.\nThe extract_substrings function processes each article in the dataset by first cleaning the \nNext, we want to create instruction-answer pairs from the extracted chunks of text.\npairs = [(pair['instruction'], pair['answer'])\nfor pair in data['instruction_answer_pairs']]\nan LLM to transform them into pairs of instructions and answers.\nIn our example, we want to create instructions like “Write a paragraph about X topic” and \nalso choose to generate five instruction-answer pairs for each extract.\nof our function for instruction generation, including our prompt.\ndef generate_instruction_answer_pairs(\nprompt = f\"\"\"Based on the following extract, generate five \ninstruction-answer pairs.\nEach instruction \\\ninstructions.\nInstructions must never explicitly mention a context, a system, a \nInstructions must be self-contained and general.\nExample instruction: Explain the concept of an LLM Twin.\n\"instruction_answer_pairs\": [\n{{\"instruction\": \"...\", \"answer\": \"...\"}},",
    "keywords": [
      "instruction",
      "data",
      "Rewritten Instruction",
      "instruction dataset",
      "synthetic data generation",
      "data generation",
      "Finally Rewritten Instruction",
      "synthetic data",
      "dataset",
      "JSON",
      "pairs",
      "model",
      "text",
      "generation",
      "create"
    ],
    "concepts": [
      "instructions",
      "dataset",
      "important",
      "importance",
      "answers",
      "answer",
      "useful",
      "uses",
      "extracting",
      "extract"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 226-234)",
    "start_page": 226,
    "end_page": 234,
    "summary": "4o mini model in JSON mode and a maximum of 1,200 tokens in the answer.\nparsed using the InstructionAnswerSet class to return pairs of instructions and answers.\ngenerates instruction-answer pairs based on the given \ngenerate instruction-answer pairs for each extract.\ndef create_instruction_dataset(\n) -> Dataset:\ninstruction_answer_pairs = []\nfutures = [executor.submit(generate_instruction_answer_\ninstruction_answer_pairs.extend(future.result())\ninstructions, answers = zip(*instruction_answer_pairs)\n{\"instruction\": list(instructions), \"output\": list(answers)}\nWe can create our instruction dataset by calling this function.\ndata, creates the instruction dataset, splits it into training and testing sets, and pushes \ninstruction_dataset = create_instruction_dataset(raw_dataset, \nprint(\"Instruction dataset:\")\nprint(instruction_dataset.to_pandas())\nfiltered_dataset = instruction_dataset.train_test_split(test_\nDataset({\ndataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are \nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub\nAs seen in the previous section, we could refine this instruction dataset by increasing the diver-\nconciseness and simplicity, we will keep a straightforward approach for this instruction dataset \nand explore more advanced methods in Chapter 6 when we create a preference dataset.\nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instruc-\nThe goal of SFT is to turn a base model, which can only perform next-token \nprediction, into a useful assistant, capable of answering questions and following instructions.\nIn this section, we will discuss when to use fine-tuning and explore related concepts with storage \nIn most scenarios, it is recommended to start with prompt engineering instead of directly fine-tun-\nyour data”) and customizability (the fine-tuned model is unique).\nmodels in this book, several LLM providers offer automated fine-tuning services.\nleverages pre-existing knowledge in the base model’s weights and refocuses the parameters for \nEven worse, a study showed that fine-tuning a model on new knowledge could result in more \nInstruction dataset formats\nInstruction datasets are stored in a particular format to organize instructions and answers.\ncally, each sample in the dataset can be represented as a Python dictionary, where keys are prompt \nTable 5.5 – Examples of instruction data storage format\non raw text, this is a type of fine-tuning generally called “continual pre-training.”\nswers, which means it is limited to one instruction and one answer.\nconversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better \nOnce the instruction-answer pairs are parsed from the dataset format, we want to structure them \nChat templates offer a unified way to present the instructions and answers \nto the model.\nSince base models are not designed to follow instructions, they \nThis means that you can choose any template when you fine-tune \nIf you want to fine-tune an instruct model (not recommended), you need to use \nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and \ntemplate to the instruction-answer pair shown in Table 5.1:\ninput by the model during fine-tuning.\nsystem and user part as shown in Figure 5.6, and prompt the model to answer by adding <|im_\nBecause the model has been fine-tuned with this template, it understands that the next tokens \nshould be an answer relevant to the user instruction and guided by the system prompt.\nhow fine-tuned models acquire instruction-following capabilities.\nof such templates, including Alpaca, which is both the name of an instruction dataset format \n### Instruction: What is the capital of France?\n<start_of_turn>model\nParameter-efficient fine-tuning techniques\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every \nLike pre-training, SFT uses next-token prediction as its training \nmain difference between continual pre-training and full fine-tuning.\nmodel parameter.",
    "keywords": [
      "dataset",
      "instruction dataset",
      "instruction",
      "SFT",
      "capital of France",
      "Instruction dataset formats",
      "model",
      "Fine-Tuning",
      "France",
      "Hugging Face Hub",
      "end",
      "start",
      "prompt",
      "template",
      "raw"
    ],
    "concepts": [
      "instructions",
      "instruction",
      "instruct",
      "dataset",
      "model",
      "models",
      "format",
      "formats",
      "answer",
      "answers"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 235-244)",
    "start_page": 235,
    "end_page": 244,
    "summary": "to be kept in memory during the forward pass to compute gradients in the backward \nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning.\nModel \naccumulation enables larger effective batch sizes without proportional memory increase.\nwith model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the \nHowever, memory requirements remain substantial for large models even with \nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive \nare often preferred to full fine-tuning to create task and domain-specific models.\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the compu-\na cornerstone technique in LLM fine-tuning.\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced \nify the behavior of the model without changing its original parameters.\nDramatically reduced memory usage during training\nPreservation of pre-trained model weights (non-destructive)\nwith limited computational resources, effectively democratizing the process of LLM fine-tuning.\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently.\nFigure 5.10 – LoRA adds the two trainable matrices 𝐴𝐴 and 𝐵𝐵 and keeps the pre-trained weights \nDuring training, the original weights 𝑊𝑊 remain frozen, while only 𝐴𝐴 and 𝐵𝐵 are updated.\nmemory savings and faster training times.\nLoRA can be applied to various parts of the model architecture.\nLoRA’s application to other key components of the model.\nHowever, it’s important to note that increasing the number of LoRA-adapted modules also in-\ncreases the number of trainable parameters and, consequently, the memory requirements.\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-\nto full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable \nparameters, LoRA drastically reduces the number compared to full fine-tuning.\nLoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\nIn terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-\nMultiple sets of LoRA weights can be combined for different tasks or domains, allowing \nallows developers to fine-tune models on relatively small, widely available GPUs. The core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit \nof updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-\nrank matrices (adapters) to specific layers of the model.\ntraining, while the original model weights remain unchanged.\nAdditionally, it uses paged optimizers to manage memory spikes during training by leveraging \nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory \nFor example, for a 7B model, QLoRA reduces peak memory usage from 14 GB \nDuring fine-tuning, the memory savings increase \nthe cost of increased training time, with QLoRA being about 30% slower than LoRA.\nmodel performance, QLoRA shows only minor differences compared to LoRA.\nsuch as when working with very large models or on hardware with limited GPU memory.\nif training speed is crucial and sufficient memory is available, LoRA might be the preferred choice.\nproject, available hardware, and the need to balance memory usage, training speed, and model \nTraining parameters\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly \nA common starting point for transformer models is often around 1e-5.\nIt’s often beneficial to experiment with different learning rates to find the optimal value for your \nspecific task and model.\nlater stages to fine-tune the model more precisely.\nThe specific values and decay schedule depend on your model \nThe batch size determines the number of samples processed before the model’s weights are up-\nTypical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, \nFor instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while \nTo overcome memory constraints while still benefiting from larger batch sizes, a technique called \nupdate to the model’s parameters.\nmodels or limited GPU memory.\nand then update the model as if you had processed all 32 samples at once.\nchoosing the number of steps, consider the trade-off between training speed and memory usage.\nMore accumulation steps allow for larger effective batch sizes but increase the time required for \nThe maximum sequence length determines the longest input the model can process.\nThis parameter directly impacts batch size and memory usage; a batch \nthis parameter with your GPU capabilities and the nature of your training data to optimize per-\nFor LLM fine-tuning, the typical range is 1 to 10 epochs, \ntask complexity, dataset size, and model architecture.\nMore epochs allow the model to refine its \nFor example, a large model fine-\ntuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger \ntraining and implement early stopping if the model’s performance plateaus or degrades.\nOptimizers adjust the model’s parameters to minimize the loss function.\nmemory (but it doesn’t improve training speed).\nweight decay regularization, often leading to better training stability and model performance.\nIn situations involving extremely large models or limited GPU memory, paged \nfor the model to capture important patterns in the data.\nmodel architecture and dataset, so it’s generally a good practice to experiment with different \nGradient checkpointing is a technique that reduces memory consumption during training by stor-\nLet’s now fine-tune an open-source model on our custom dataset.\nThere are many efficient open-weight models we can leverage for task or domain-specific use \nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and \ntasks after fine-tuning.\nIn this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta.\nThere are specialized tools and libraries to fine-tune models.\ntraining (2-5x) and reduce memory use (up to 80% less memory).\nTo maximize efficiency, we will perform fine-tuning using the Unsloth library.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nLet’s now load the model to fine-tune and its corresponding tokenizer.\nWe’ll use LoRA in this example because of faster training and higher quality, but you can \nmodel, tokenizer = FastLanguageModel.from_pretrained(",
    "keywords": [
      "model",
      "memory",
      "LoRA",
      "training",
      "GPU memory",
      "Fine-Tuning",
      "memory usage",
      "Batch size",
      "batch",
      "models",
      "LLM fine-tuning",
      "parameters",
      "GPU",
      "learning rate",
      "limited GPU memory"
    ],
    "concepts": [
      "models",
      "model",
      "memory",
      "training",
      "train",
      "important",
      "parameter",
      "parameters",
      "gradients",
      "gradient"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 245-252)",
    "start_page": 245,
    "end_page": 252,
    "summary": "model,\nbecause the model might not correctly learn the chat template.\nthe 100,000 samples of this dataset, we will specify we only want 10,000 in the train split.\nall the instructions and answers to the Alpaca template.\ntence (EOS) token at the end of each message to ensure that the model learns to output \n8. Once the dataset is ready, we can divide it into training (95%) and test (5%) sets for val-\ndataset = dataset.train_test_split(test_size=0.05)\nThe model is now ready to be trained.\nfor our training.\nIn addition, we provide the model, tokenizer, LoRA configuration, and \ndatasets.\nWe train \nthis model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for \nmodel=model,\ntrain_dataset=dataset[\"train\"],\ntrainer.train()\nTraining this model on our concatenated dataset can take a few hours.\nthe fine-tuned model, but to make sure that there are no obvious errors related to the \nThis forces the model to answer the instruction instead of completing it.\nFastLanguageModel.for_inference(model)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\nHere is the answer provided by our model:\nSupervised fine-tuning is a method used to enhance a language model \nby providing it with a curated dataset of instructions and their \nThis process is designed to align the model's \nNow that our model has been successfully fine-tuned, we can save it locally and/or push \nmodel.save_pretrained_merged(\"model\", tokenizer, save_\nmodel.push_to_hub_merged(\"mlabonne/TwinLlama-3.1-8B\", tokenizer, \nCongratulations on fine-tuning a base model from scratch!\nML to monitor your training loss, validation loss, and many other metrics.\nTraining loss: It measures how well the model is performing on the task it’s being trained \nand continuous increases in the loss value are signs that the training is failing.\nValidation loss: It measures the loss using the validation set instead of the training set; \na well-fitted model typically shows both training and validation losses decreasing and \nis expected to exist as the model will always perform slightly better on the training data.\nIf the training loss continues to decrease while the validation loss starts to increase, it’s a \nby a divergence between training and validation losses.\ndecreasing gradient norm generally means that the model is converging toward a local \nIt is often interesting to try different learning rates and select the best model based on the minimal \namined the instruction data pipeline and how to create high-quality datasets, from curation \nSFT’s advantages and limitations, methods for storing and parsing instruction datasets with chat \ntemplates, and an overview of three primary SFT techniques: full fine-tuning, LoRA, and QLoRA.\na Llama 3.1 8 B model on our custom instruction dataset.\nIn the next chapter, we will use preference alignment techniques to create a new version of Twin-\ncalibrate the type of answers we expect from our model.\n“Automatic Instruction Evolving for Large Language Models.” arXiv pre-\n“Yi: Open Foundation Models by 01.AI.” arXiv preprint arXiv:2403.04652, March 2024.\n“LoRA: Low-Rank Adaptation of Large Language Models.” arXiv preprint \nFine-Tuning with Preference \nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI \nWe will build our own dataset to modify the writing style of our model, making \nalign the model trained in Chapter 5.\nUnderstanding preference datasets\nHow to create our own preference dataset\nImplementing DPO in practice to align our model",
    "keywords": [
      "model",
      "dataset",
      "training",
      "loss",
      "Fine-Tuning",
      "Supervised Fine-Tuning",
      "preference alignment",
      "preference",
      "arXiv",
      "instruction",
      "Large Language Models",
      "Alpaca",
      "datasets",
      "proj",
      "training loss"
    ],
    "concepts": [
      "model",
      "models",
      "dataset",
      "training",
      "train",
      "trained",
      "instructions",
      "instruction",
      "tokens",
      "token"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 253-265)",
    "start_page": 253,
    "end_page": 265,
    "summary": "By the end of this chapter, you will be able to create your own preference datasets and align \nUnderstanding preference datasets\nThe principles for creating high-quality preference datasets are the same as those discussed in \nChapter 5 for instruction datasets.\ndatasets: data generation and evaluation.\nPreference data\nPreference datasets lack the standardization of instruction datasets due to varying data require-\nPreference data comprises a collection of responses \nto a given instruction, ranked by humans or language models.\npaired with one preferred answer and one rejected answer.\ngenerate the preferred response rather than the rejected one.\nIn preference datasets, the rejected response is as important as the chosen one.\nrejected response, the dataset would be a simple instruction set.\nto use preference datasets in many contexts.\nHere is a list of examples where preference datasets \nA preference dataset \nSimple SFT might not capture the subtleties of what makes one response preferable over \nPreference datasets can help the model learn to dis-\nBy using preference datasets, models can learn to generate \nPreference datasets \nPreference datasets can capture human \nPreference \ndatasets can help models learn to produce translations that native speakers prefer, even \nIn all these scenarios, preference datasets enable a more refined training approach.\nsubjective quality assessments and human preferences that extend beyond simple correctness or \nThis method can produce models that generate output that is not only \ntechnically accurate but also better aligned with human judgment and preferences in complex, \nMost preference datasets follow a structure similar to that shown in Table 6.1, with columns for \nan instruction, a preferred answer, and a rejected answer.\nAs with instruction datasets, the required sample count depends on model size \nThis requires preference datasets with millions of samples.\ninvolving multiple rounds of preference alignment, and extensive use of synthetic data.\noften be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending \nence dataset, where the rejected answers are those claiming alternative origins, and the chosen \nanswers are responses where the model correctly states that it was trained by you.\nData generation and evaluation\nWhen creating preference datasets, data generation and evaluation are closely linked.\ncreate answers and then rate them to make the final dataset.\nGenerating preferences\nBefore making new preference data, it’s good to look at relevant open-source datasets.\nfewer of these compared to instruction datasets, but you can find high-quality preference data-\nWell-known preference datasets include the Anthropic HH-RLHF dataset, which has human \npreferences for helpful and harmless AI responses, and the OpenAI Summarize from Human \nHuman-generated, human-evaluated datasets: This method involves hiring people to \nboth create responses to prompts and evaluate the quality of these responses.\napproach can capture nuanced human preferences and is ideal for complex tasks, it’s \nHuman-generated, LLM-evaluated datasets: This method can be useful if you have \npotentially missing nuanced preferences during the LLM evaluation stage.\nLLM-generated, human-evaluated datasets: This method offers a good balance between \nLLMs generate multiple responses to prompts, and humans rank \nThis approach is often preferred because humans are generally better at \nresponses while still capturing human preferences effectively.\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both gener-\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of \nto scale, which is why large datasets benefit from LLM evaluation.\nFor instance, it is possible to use a high-quality model to generate preferred \noutputs and a lower-quality or intentionally flawed model to produce less preferred alternatives.\nThis creates a clear distinction in the preference dataset, allowing more effective training of AI \nAnother approach is to compare model-generated outputs with human-written responses, which \ncan provide insights into how well the model aligns with actual human preferences and highlight \nThe data generation is consistent between instruction and preference datasets.\ncapture the varied nature of human preferences.\nalign with human preferences.\ndatasets.\nUsing multiple LLMs to generate samples can be better than using just one model.\nopen-source datasets like argilla/Capybara-Preferences, combining GPT-4 with open-weight \nEvaluating preferences\nData evaluation can be performed by human raters or automated with LLMs. LLM evaluation\nlines to the LLM, and using the model to select preferred and rejected responses.\nLLM evaluation depends directly on the model’s performance and the provided guidelines.\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or \nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for \nLength bias: Similar to humans, LLM judges often show a preference for longer answers, \nFamily bias: LLM judges may favor responses that are generated by themselves or models \nTo mitigate these biases and enhance the quality of preference datasets, several solutions can \nIn the next section, we will create our own preference dataset.\nprocess to naturally create chosen (human-generated) and rejected (LLM-generated) answers.\nCreating our own preference dataset\nIn this section, we will create a preference dataset where the chosen answers are extracts from \nthe text, while rejected answers are generated by the model.\ncode created in Chapter 5, which was designed to generate instruction datasets.\nAs seen in the previous section, preference and instruction datasets rely on the same principles.\nFigure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\nWe are now ready to implement the preference data generation pipeline:\nclass is designed to handle triples of instructions, generated answers (rejected), and ex-\nfor triple in data['preference_triples']]\nThe generate_preference_triples function replaces the original generate_instruction_\ntype of instructions we’re interested in, how to extract answers from articles, and how \ndef generate_preference_triples(extract: str, client: OpenAI) -> \ninstruction-answer triples.\n2. A generated answer that attempts to answer the instruction based \n\"preference_triples\": [\n\"generated_answer\": \"...\",\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode.\ngenerates instruction-answer triples based on the given context.\nEach triple should include an instruction, a generated answer, and ",
    "keywords": [
      "preference datasets",
      "Preference",
      "datasets",
      "Preference Alignment",
      "LLM",
      "data",
      "answer",
      "instruction datasets",
      "human preferences",
      "model",
      "dataset",
      "instruction",
      "responses",
      "Preference data",
      "answers"
    ],
    "concepts": [
      "dataset",
      "preference",
      "preferable",
      "prefer",
      "preferences",
      "models",
      "model",
      "humans",
      "human",
      "data"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 266-274)",
    "start_page": 266,
    "end_page": 274,
    "summary": "8. Two new filtering functions are introduced for the preference data pipeline: filter_short_\ndef filter_short_answers(dataset: Dataset, min_length: int = 100) -> \nDataset:\ndef filter_answer_format(dataset: Dataset) -> Dataset:\nreturn dataset.filter(is_valid_format)\nThe create_preference_dataset function replaces the original create_instruction_\ndataset function.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_\nThe main function is updated to include the new filtering steps and to use the preference \nCreate preference dataset\ndataset = create_preference_dataset(raw_dataset, client)\nprint(\"Preference dataset:\")\ndataset = filter_short_answers(dataset)\ndataset = filter_answer_format(dataset)\nThe create_preference_dataset() function generated 2,970 samples.\nhuggingface.co/datasets/mlabonne/llmtwin-dpo.\nFigure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the Hugging \nprocess to generate your own preference datasets.\nFeedback (RLHF) and DPO.\nPreference alignment regroups techniques to fine-tune models on preference data.\nPreference Optimization (DPO).\n(RL) with human input to align models with human preferences and values.\nThe origins of RLHF can be traced back to the field of preference-based reinforcement learning\nin 2017 demonstrated the effectiveness of learning reward models from \nhuman preferences and using them to train RL agents.\nAt its core, RLHF works by iteratively improving both a reward model and a policy:\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a \nreward model from human feedback.\nThese preferences \nare used to train a reward model, often using a Bradley-Terry model or similar approaches \nPolicy optimization: With the learned reward model, standard RL algorithms can be \npredicted rewards from the learned model.\nevaluated by humans, leading to refinements in the reward model.\nideally resulting in a policy that aligns well with human preferences.\nA key innovation in RLHF is its approach to handling the high cost of human feedback.\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm \nHere, the reward model is used to \nscore the text that is generated by the trained model.\nWhile RLHF has proven effective for aligning AI systems with human preferences, it faces chal-\nModel is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\nDPO’s core innovation lies in its reformulation of the preference learning problem.\nwhich typically involves training a separate reward model and then using reinforcement learning \nalgorithms like PPO to fine-tune the language model, DPO takes a more direct approach.\nmathematical insight allows DPO to express the preference learning problem directly in terms of \nthe policy, eliminating the need for a separate reward model or complex reinforcement learning \nmodel to assign higher probability to preferred responses and lower probability to non-preferred \nwithout the need for sampling from the model during training or implementing complex RL \nFigure 6.5 – High-level view of the DPO algorithm for preference alignment\nDPO has several advantages over traditional RLHF methods.\nBy eliminating the need for a separate reward model and RL algorithms, DPO \nDespite its simplicity, DPO often matches the performance of more complex RLHF methods.\nadapt to new preferences, DPO offers a more straightforward path to achieving similar results.\nIn this section, we will DPO fine-tune the TwinLlama-3 1-8B model we created in Chapter 5.\nimport of DPOConfig and DPOTrainer from TRL, which are specific to DPO training.\nfrom datasets import load_dataset\nThis step loads our fine-tuned model from Chapter 5.",
    "keywords": [
      "DPO",
      "dataset",
      "preference",
      "model",
      "RLHF",
      "reward model",
      "Preference Alignment",
      "preference dataset dataset",
      "preference dataset",
      "human preferences",
      "reward",
      "Direct Preference Optimization",
      "answers",
      "Create preference dataset",
      "separate reward model"
    ],
    "concepts": [
      "dataset",
      "models",
      "model",
      "training",
      "train",
      "trained",
      "preference",
      "preferences",
      "prefer",
      "preferred"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 275-282)",
    "start_page": 275,
    "end_page": 282,
    "summary": "Let’s now prepare the model for PEFT with the LoRA configuration.\nmodel = FastLanguageModel.get_peft_model(\nmodel,\nWe load the llmtwin-dpo dataset (training split), which contains our prompts, chosen, \ndataset = load_dataset(\"mlabonne/llmtwin-dpo\", split=\"train\")\nThe model and data are now ready, so we can start fine-tuning.\nare a few new parameters, like ref_model and beta.\nwe don’t directly train the model but instead the adapters.\noriginal model (without adapters) as a reference, saving a lot of VRAM.\neter controls the importance of the reference model.\ndue to the fact that the trained model used formal language with lower values.\ncloser to the reference model helps to fix this issue.\nmodel=model,\nref_model=None,\nOnce the model is trained, we can run it for a quick sanity check.\nIt prepares the model for inference and generates a response to a prompt.\nFastLanguageModel.for_inference(model)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\nThe trained DPO model returns the following response:\nof pre-trained language models by utilizing labeled data.\ntechnique involves taking a pre-trained model and refining it on \nproviding the model with relevant data and guidance, it can learn to \nThis approach allows for the creation of more specialized models \nWe can compare it with the answer provided by the SFT model:\nSupervised fine-tuning is a method used to enhance a language model \nThis process is designed to align the model's \nThe DPO model provides an answer that is both more accurate and closer to the desired \nIt correctly identifies pre-training language models as source models for \nFinally, the last step consists of saving the trained model locally and pushing it to the \nHugging Face Hub. model.save_pretrained_merged(\"model\", tokenizer, save_\nWe have trained and exported our DPO model.\nSFT, DPO has a few additional metrics that need to be tracked during training.\nURL: https://www.comet.com/mlabonne/llm-twin-training/\nrapidly fall to zero, meaning that the model is no longer learning anything.\nOver time, we expect the model to choose the chosen answers and reject the rejected \nA well-trained model’s margin will quickly increase and then plateau.\nAccuracies: This metric represents the percentage of times the model correctly identifies \ndicates that the preference dataset might be too easy for the model.\ncess, involving a reference model.\nmodel, you can experiment with different ranks, beta parameters, learning rates, and number of \nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models \nwords in the text generated by different models (SFT and DPO) with our ground-truth dataset.\nIn this example, we expect the SFT model to output a lot of words that are overrepresented in \nThe distribution output by our DPO model should be a lot closer \nfine-tune our TwinLlama-3.1-8B model from Chapter 5.\ninstructions for training the model, as well as highlighting key differences from SFT.\nmodel is available on the Hugging Face Hub. In the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges \nthe concept of using larger models to evaluate smaller ones (LLM-as-a-judge).\n“Direct Preference Optimization: Your Language Model is Secretly a \nReward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n“Enhancing LLM-as-a-Judge with Grading Notes.” databricks.com, July 22, 2024, \n“Deep reinforcement learning from human preferences.” arXiv preprint \n“Training language models to follow instructions with human feedback.” ",
    "keywords": [
      "model",
      "DPO model",
      "Preference Alignment",
      "Preference",
      "SFT",
      "models",
      "DPO",
      "SFT model",
      "dataset",
      "learning",
      "chosen",
      "rejected",
      "language models",
      "prompt",
      "human"
    ],
    "concepts": [
      "model",
      "models",
      "preference",
      "preferences",
      "training",
      "train",
      "trained",
      "dataset",
      "evaluation",
      "evaluate"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 283-291)",
    "start_page": 283,
    "end_page": 291,
    "summary": "Evaluating LLMs\nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models.\nWhile general-purpose evaluations are the most popular ones, with benchmarks like Massive \nModel evaluation\nRAG evaluation\nModel evaluation\nIn model evaluation, the objective is to assess the capabilities of a single model without any \nEvaluating LLMs\nML and LLM evaluation to understand the main differences between these two fields.\nthen explore benchmarks for general-purpose, domain-specific, and task-specific models.\nComparing ML and LLM evaluation\nML evaluation is centered on assessing the performance of models designed for tasks like pre-\nhow well a model understands and generates language, ML evaluation is more concerned with \nThis difference comes from the nature of the tasks these models handle.\nbenchmarks, LLM evaluation requires a more nuanced approach and often incorporates qualita-\nevaluation process:\nNumerical metrics: Evaluating ML models typically involves measuring objective per-\nEvaluating \nWhile evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific \nGeneral-purpose LLM evaluations\nGeneral-purpose evaluations refer to metrics dedicated to base and general-purpose fine-tuned \nmodels.\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after \nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model.\npre-training evaluations:\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from \nEvaluating LLMs\nARC-C (reasoning): Evaluates models on grade-school-level multiple-choice science \nMany of these datasets are also used to evaluate general-purpose fine-tuned models.\ncase, we focus on the difference in a given score between the base and the fine-tuned model.\nexample, bad fine-tuning can degrade the knowledge of the model, measured by MMLU.\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks.\nconnected to the ability of fine-tuned models to understand and answer questions.\nAlpacaEval (instruction following): Automatic evaluation for fine-tuned models that is \nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their \nEven if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good \nway to compare different instruct models.\nple, public benchmarks can be gamed by training models on test data or samples that are very \nmultiple evaluations provide a similar answer, you can raise your confidence level about the real \nDomain-specific LLM evaluations\nDomain-specific LLMs don’t have the same scope as general-purpose models.\ncommon applications like a language-specific model or a code model, it is recommended to \nsearch for relevant evaluations and even benchmark suites.\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging \nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical ques-\nEvaluating LLMs\nBigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main \nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world \nThe evaluation focuses on specific \nincluding many general-purpose evaluations.\nthese benchmarks use machine translation, it is better to rely on human-translated evaluations \nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics.\nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language \nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs \nBoth general-purpose and domain-specific evaluations are designed with three main principles.\nTask-specific LLM evaluations\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, \nthey cannot provide insights into how well these models work for a given task.\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation data-\nto evaluate using traditional ML metrics.\nEvaluating LLMs\nby the model.\nThis benchmark can be inspired by general-purpose and domain-specific evaluation \nThere are two main ways of evaluating models with this scheme—text generation and log-like-\nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabil-\nmodels about a particular task, and even expand it to specific domains.\nChapter 5 can be used to evaluate the quality of the answers.\nIt is recommended to use large models for evaluation and to iteratively refine your prompt.",
    "keywords": [
      "model",
      "models",
      "evaluation",
      "LLMs",
      "LLM evaluation",
      "LLM",
      "benchmarks",
      "Evaluating LLMs",
      "Model evaluation",
      "General-purpose LLM evaluations",
      "MMLU",
      "general-purpose",
      "Leaderboard",
      "Evaluates",
      "LLM Leaderboard"
    ],
    "concepts": [
      "evaluating",
      "evaluations",
      "evaluate",
      "evaluates",
      "models",
      "model",
      "benchmark",
      "task",
      "tasks",
      "tasked"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 292-299)",
    "start_page": 292,
    "end_page": 299,
    "summary": "In order to easily parse answers, one can specify a structure in the instruction or use some kind \nYou are an evaluator who assesses the quality of an answer to an \n1. The answer is not relevant to the instruction.\n2. The answer is relevant but not helpful.\nPlease provide your evaluation as follows:\n##Evaluation##\nanswer)\nAnswer:\n{answer}\n##Evaluation##\nTable 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation\ncombine LLM evaluations with other metrics, use multiple judges, and carefully design prompts \nOnce a model has been properly evaluated and works as intended, it might be included within a \nRAG evaluation\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation \nrequires a more comprehensive approach that considers both the model’s generative abilities \nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing \nThe evaluation of RAG systems goes beyond assessing a standalone LLM.\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accura-\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected \nIn this section, we will cover two methods to evaluate how well RAG models incorporate external \nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to \nprovide developers with a comprehensive set of tools for RAG evaluation and optimization.\nAdditionally, Ragas can generate conversational samples that simulate chat-based question-and-\nFigure 7.1: Overview of the Ragas evaluation framework\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to \nFaithfulness: This metric measures the factual consistency of the generated answer against \nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the \nIt uses an innovative approach where an LLM is prompted to generate \nContext precision: This metric evaluates whether all the ground-truth relevant items \nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed \nto evaluate RAG systems.\ntion with fine-tuned classifiers to assess various aspects of RAG performance, including context \nThe ARES framework operates in three main stages: synthetic data generation, classifier training, \nand RAG evaluation.\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world sce-\nfrom the previous stage), test set for evaluation, label columns, and model choice.\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the \nRAG model’s performance.\nUsers provide evaluation datasets, few-shot examples for guiding the \nARES supports various evaluation metrics \nHTML, images, and so on), enabling comprehensive evaluation across different RAG system \nmetrics can be combined with ARES’s highly configurable evaluation process and classifier-based \nWhile Ragas may offer more nuanced evaluations based on LLM capabilities, ARES \nprovides consistent and potentially faster evaluations once its classifiers are trained.\nthem offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas \nEvaluating TwinLlama-3.1-8B\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and \nIn our evaluation framework, we will use the test split of our instruction dataset to get test in-\nWe will feed them to our models and generate answers.\nThese answers will then be \nevaluated by our judge LLM (GPT-4o-mini), based on a prompt that specifies our criteria.\nGenerating answers\nThe first step consists of efficiently generating answers for each instruction in our test set.\nWe import the relevant libraries, including vLLM for fast generation.\n2. We define a function called generate_answers that will process our dataset and generate \ndef generate_answers(model_id, dataset_name):\noutputs = llm.generate(dataset[\"prompt\"], sampling_params)\nThen, we run our generate_answers()",
    "keywords": [
      "RAG",
      "RAG evaluation",
      "evaluation",
      "RAG systems",
      "answer",
      "answers",
      "model",
      "Ragas",
      "LLM",
      "evaluation framework",
      "dataset",
      "models",
      "LLMs",
      "RAG pipeline",
      "instruction"
    ],
    "concepts": [
      "evaluating",
      "evaluator",
      "evaluation",
      "evaluations",
      "evaluated",
      "evaluate",
      "evaluates",
      "generation",
      "generative",
      "generate"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 300-308)",
    "start_page": 300,
    "end_page": 308,
    "summary": "generate_answers(model_id, \"mlabonne/llmtwin\")\nNow that we have the answer generation, we can move on to the evaluation process.\nEvaluating answers\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge.\nHere, we will score every generated answer from every model in \nterms of accuracy and style.\n2. We then define the evaluate_answer() function.\nprompt, which sets up the context for evaluating answers based on accuracy and style:\ndef evaluate_answer(\nPlease evaluate the \nExample of bad style: The Llama2 7B model constitutes a noteworthy \nforces that we are interested in answer evaluation based on accuracy and style:\nevaluates answers based on accuracy and style.\nwhy we create an evaluate_batch() function, which returns a list of parsed structured \n(i, evaluate_answer(instr, ans, client))\nWe can now orchestrate the previous code in the evaluate_answers() function.\ndef evaluate_answers(model_id: str, num_threads: int = 10, batch_\n8. We create batches of instruction-answer pairs from our dataset.\n(i, list(zip(dataset[\"instruction\"][i:i+batch_size], \ndataset[\"answers\"][i:i+batch_size])))\nWe perform parallel evaluation of batches of instruction-answer pairs using multiple \nWe use parallel processing to evaluate multiple batches simultaneously, speed-\nevaluate_batch().\nevaluations = [None] * len(dataset)\nfutures = [executor.submit(evaluate_batch, batch, start_\nfor index, evaluation in future.result():\nWe create a new column with the result of the evaluation process.\nthe raw JSON output of the judge model, including scores and explanations:\nif 'evaluation' in dataset.column_names:\ndataset = dataset.remove_columns(['evaluation'])\ndataset = dataset.add_column(\"evaluation\", evaluations)\nand style scores that should have been generated.\naccuracy_scores = []\nstyle_scores = []\nfor evaluation in dataset['evaluation']:\nWe add two new columns to store the accuracy and style scores for further analysis:\ndataset = dataset.add_column('accuracy', accuracy_scores)\nif 'style' in dataset.column_names:\ndataset = dataset.add_column('style', style_scores)\nLet’s push the final dataset with generated answers, evaluations, and scores to the Hug-\ndataset.push_to_hub(f\"mlabonne/{model_id.split('/')\nWe can now call the evaluate_answers() function with the three models we selected:\nevaluate_answers(model_id)\nThere are three elements we can review at the end of this evaluation process: model answers, \nexplanations from evaluations, and scores.\ninstruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\nDiscuss the concept of algorithm bias and its implications.\nmachine learning models that can affect their performance, accuracy, and fairness.\nAlgorithm bias \nFor example, a medical diagnosis algorithm that is biased towards certain demographics \nIn education, an algorithm that is biased towards certain learning \nbiases in machine learning models and take proactive steps to mitigate their effects.\nTable 7.4: Example of generated answers for a given instruction\nthe case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples.\nthe writing style but not the actual content of the answer.\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\nThe answer accurately defines algorithm bias \nto biased algorithms.\nThe answer accurately defines algorithm bias \nThe answer accurately defines algorithm bias \nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and ac-\nHowever, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-\n8B-Instruct, with a score of 2.\nperfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of \nalgorithm bias without becoming overly formal.”\nTwinLlama-3.1-8B - Style: 2.04\nTwinLlama-3.1-8B-DPO - Style: 2.12\nLlama-3.1-8B-Instruct - Accuracy: 2.62\nLlama-3.1-8B-Instruct - Style: 1.86\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct \na score of 2.12, successfully achieving a more accessible and less formal writing style without \nIn this chapter, we explored LLM evaluation with models and RAG systems.",
    "keywords": [
      "style",
      "accuracy",
      "algorithm bias",
      "model",
      "answer",
      "score",
      "evaluation",
      "Dataset",
      "answers",
      "bias",
      "algorithm",
      "batch",
      "scores",
      "evaluate",
      "models"
    ],
    "concepts": [
      "evaluating",
      "evaluation",
      "evaluate",
      "evaluates",
      "evaluations",
      "dataset",
      "style",
      "styles",
      "score",
      "scores"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 309-317)",
    "start_page": 309,
    "end_page": 317,
    "summary": "delve into optimization methods, model parallelism techniques and examine different quanti-\n“Instruction-Following Evaluation for Large Language Models.” arXiv \ntions in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n“RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv \nGeneration Systems.” arXiv preprint arXiv:2311.09476, November 2023.\ndocument generation, can be processed in batches overnight, others require low latency and fast \nmodels make predictions based on input data – is critical for many practical applications.\nincludes reducing the time it takes to generate the first token (latency), increasing the number \nof tokens generated per second (throughput), and minimizing the memory footprint of LLMs. Indeed, naive deployment approaches lead to poor hardware utilization and underwhelming \n(Text Generation Inference, vLLM, and TensorRT-LLM) and compare their features in terms of \nModel optimization strategies\nModel parallelism\nModel quantization\nmiliar with state-of-the-art optimization techniques, including model parallelism and weight \nModel optimization strategies\nThe decoder-only architecture is designed for text-generation tasks.\nto generate a context-rich representation, which the decoder then uses to produce the output \nand summarization, where understanding the input context and generating a relevant output \nFigure 8.1 – Inference process with decoder-only models.\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\nGenerating output tokens sequentially, one at a time, using the computed keys and values.\nmultiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs. The real challenge is that the token generation in Step 3 is inherently sequential – to generate \nthe next token, you need to have generated all previous tokens.\na (static) KV cache, continuous batching, speculative decoding, and optimized attention mech-\nWe saw that LLMs generate text token by token, which is slow because each new prediction \nthe model needs the context of tokens 1 through 99.\nInstead of recalculating these pairs for each new token, the model retrieves them \nWhen a new token is generated, only the key and value for that single token need to be computed \nthe model.\nThe size of the KV cache scales with the number of tokens (𝑛𝑛𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 ) and several model dimensions, \nTo configure a model to use a static KV cache with the transformers library, follow these steps:\nWe import the tokenizer and the model we want to optimize:\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nTo implement the static cache, we change the cache implementation in the model’s gen-\nmodel.generation_config.cache_implementation = \"static\"\nNow that our KV cache is static, we can compile the model using torch.compile:\nLet’s use the generate() method to get the model’s output and decode it with batch_\noutputs = model.generate(**inputs, do_sample=True, temperature=0.7, \nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\nBatching, or processing multiple inference requests simultaneously, is a standard approach to \nLarger batch sizes spread out the memory cost of model weights and \nHowever, decoder-only models pose a particular challenge due to the high variability in input \nText Generation Inference (TGI), vLLM, and NVIDIA TensorRT-LLM.\nAnother powerful optimization technique is speculative decoding, also called assisted generation.\nThe key insight is that even with continuous batching, the token-by-token generation process \nFeed these speculative completions into the full model to validate which predictions \nmatch what the large model would have generated.\nThe result is that, if the small model approximates the large model well, multiple tokens can be \nIt is crucial that both models use the same tokenizer.\nIf this is not the case, the tokens generated \nNote that, if you have enough VRAM, you can use much larger models like 14B, 32B, ",
    "keywords": [
      "model",
      "arXiv preprint arXiv",
      "inference optimization",
      "inference",
      "optimization",
      "cache",
      "arXiv",
      "generation",
      "models",
      "token",
      "tokens",
      "arXiv preprint",
      "Text Generation Inference",
      "preprint arXiv",
      "Large Language Models"
    ],
    "concepts": [
      "models",
      "model",
      "token",
      "generation",
      "generate",
      "generating",
      "optimizing",
      "optimized",
      "optimize",
      "optimal"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 318-325)",
    "start_page": 318,
    "end_page": 325,
    "summary": "We load the tokenizer and both models:\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_\noutputs = model.generate(**inputs, do_sample=True, assistant_\nprompt_lookup_num_tokens parameter in model.generate():\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=4)\nthese speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-\ntunes both the speculation heads and the large model.\nof a 7B parameter model on a range of tasks.\nblock-based approach naturally supports memory sharing across multiple output sequences \nbetween the GPU’s main memory and its processing units.\nimplementation parameter when loading a model.\nmodel = AutoModelForCausalLM.from_pretrained(\nThe techniques presented in this section focused on improving the model’s efficiency in processing \nmultiple GPUs. Model parallelism\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across \nmultiple GPUs. This enables the training and inference of models too large to fit on a single device, \nThere are three main approaches to model parallelism, each involving splitting the model weights \nand computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism.\nData parallelism (DP) is the simplest type of model parallelism.\nmodel and distributing these replicas across different GPUs (see Figure 8.4).\nFigure 8.4 – Illustration of data parallelism with four GPUs\nbetween GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient.\nthat this technique only works when the model is small enough to fit into a single GPU, leaving \nFor larger models or when memory is \nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for \nPipeline parallelism\nmultiple GPUs. Unlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism parti-\ntions the model’s layers across different GPUs. This approach allows each GPU to handle a specific \nportion of the model, thereby reducing the memory burden on individual GPUs. Figure 8.5 – Illustration of pipeline parallelism with four GPUs\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four \nThe first 25% of the model’s layers might \nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory \nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\nFigure 8.6 shows an example of pipeline parallelism with micro-batching.\nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, Deep-\npipeline parallelism.\n(TP) is another popular technique to distribute the computation of LLM layers across multiple \nIn contrast to pipeline parallelism, TP splits the weight matrices found in individual \nThis enables simultaneous computations, significantly reducing memory bottlenecks and \nFigure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset \nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism \nmodel to process large sequences more effectively.\nGPUs can compute these layers on different slices of the input sequence, avoiding replication of \nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined.\n8.8 illustrates how a given model can be split according to each approach:\nFigure 8.8 – Illustration of the different model parallelism techniques\nideal if the primary constraint fits the model in the GPU memory.\nIn practice, a model may be split depth-wise into a few pipeline stages, \non reducing the precision of the model’s weights and activations.",
    "keywords": [
      "model",
      "pipeline parallelism",
      "GPU",
      "parallelism",
      "GPUs",
      "pipeline",
      "Model parallelism",
      "memory",
      "tensor parallelism",
      "multiple GPUs",
      "large model",
      "Inference",
      "layers",
      "data parallelism",
      "large"
    ],
    "concepts": [
      "memory",
      "model",
      "models",
      "parallel",
      "parallelism",
      "parallelisms",
      "processing",
      "processes",
      "process",
      "processed"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 326-334)",
    "start_page": 326,
    "end_page": 334,
    "summary": "By default, weights are typically stored in a 16-bit or 32-bit floating-point format (FP16 or FP32), \nsmaller models (7B–13B LLMs) in terms of quality when quantized to 2- or 3-bit precision.\nIn this section, we will introduce the concepts of quantization, GGUF with llama.cpp, GPTQ, \nthis section, you can refer to AutoQuant (bit.ly/autoquant) to quantize their models using a \na pre-trained model are directly converted to a lower precision format without any retraining.\nperforms quantization during the training or fine-tuning stage, allowing the model to adapt to \nthe lower precision weights.\nFP32 uses 32 bits, providing high precision but also requiring more \nConversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a \n(8-bit integers), can be employed for quantization, further reducing the memory footprint.\nFigure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point \nquantization\nAbsmax quantization maps the original weights 𝐗𝐗 to the range [-127, 127] by dividing them by the \nIf we take the same example with a weight of 0.1, we get a scale of \nThe weight of 0.1 would be quantized to round(41.13 ⋅0.1 −5) = −1 , \nIn Python, zero-point quantization can be implemented as follows:\ncan significantly impact the quantization process, leading to reduced precision for other values.\nmixed-precision quantization scheme, where outlier features are processed using FP16, while the \nModels can be directly loaded in 8-bit precision with the transformer library, using LLM.int8(), \nTo load a model in NF4 (4-bit precision), you can use the load_in_4bit\nQuantization with GGUF and llama.cpp\nsigned to perform inference with various LLMs. It is the most popular quantization technique, \nwith many quantized models available on the Hugging Face Hub. Compared to other libraries that rely on hardware-specific closed-source libraries like CUDA, \nto 8-bit precision.\nIQ1_S and IQ1_M: 1-bit precision – very low quality\nIQ2_XXS/XS/S/M and Q2_K: 2-bit precision – generally low quality but IQ2 can be usable \nIQ3_XXS/XS/S/M and Q3_K_S/M/L: 3-bit precision – low quality but usable for large models\nIQ4_XS/NL and Q4_K_S/M, Q4_0/1: 4-bit precision – good quality and usable for most \nmodels\nQ5_K_S/M and Q5_0/1: 5-bit precision – high quality\nQ6_K: 6-bit precision –very high quality\nQ8_0: 8-bit precision – highest quality\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds \nand quantizing them based on the largest weight value in the block (w = q × block_scale ).\nvalues are also quantized in higher precision with 6 bits (w = q × block_scale(6bit) +  block_min(6bit) ).\nHere is a practical example of how to quantize a model in the GGUF format.\nFirst, we convert the model into FP16.\nfor every GGUF quantization type.\ncpp and are compatible with different models:\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile \nWe select a format (here, Q4_K_M) and start the quantization.\n!./llama.cpp/quantize {fp16} {qtype} {METHOD}\nOnce it’s done, your quantized model is ready.\nrepo_id = f\"{username}/{MODEL_NAME}-GGUF\",\nrepo_type=\"model\",\nrepo_id=f\"{username}/{MODEL_NAME}-GGUF\",\nGGUF models can be used with backends such as llama-cpp-python and frameworks like Lang-\nThis is useful if you want to integrate a quantized model into a broader system.\nalso directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, \nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two \nquantization formats dedicated to GPUs. This makes them both faster than llama.cpp during \nWhile GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with a highly customizable \nprecision that can mix different quantization levels.\neach linear layer, prioritizing more important weights with higher bit quantization.\nmodels to run on a single 24 GB GPU with 2.55-bit precision.\nEXL2 models.\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2.\n2. We download the model to quantize by cloning its repo from the Hugging Face Hub:\nMODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"",
    "keywords": [
      "model",
      "Quantization",
      "models",
      "precision",
      "GGUF",
      "Hugging Face Hub",
      "scale",
      "GGUF models",
      "Inference Optimization",
      "Inference",
      "llama.cpp",
      "weight",
      "GPTQ",
      "range",
      "GGUF quantization"
    ],
    "concepts": [
      "quantization",
      "models",
      "model",
      "bit",
      "bits",
      "precision",
      "precise",
      "weights",
      "weight",
      "value"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 335-343)",
    "start_page": 335,
    "end_page": 343,
    "summary": "GPTQ models are also supported in TensorRT-LLM.\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2.\nengines and integrated into TGI, vLLM, and TensorRT-LLM.\nlike EXL2, allow extreme quantization, the quality of the models often suffers significantly.\nter explored various optimization techniques, including optimized generation methods, model \nspeculative decoding, along with advanced attention mechanisms and model parallelism, users \nsystem by implementing the retrieval and generation components and integrating them into \nthe inference pipeline.\nHugging Face, Text Generation Inference, https://github.com/huggingface/text-\ngeneration-inference, 2022.\nZheng, C.H. Yu, J.E. Gonzalez, H.\nY. Leviathan, M.\nZheng, C.H. Yu, J.E. Gonzalez, H.\nHe, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at \nFirat, M.X. Chen, D.\nZ. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019.\nVerma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer \nAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024.\nRAG Inference Pipeline\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline \nfor retrieval, one to augment the prompt, and one to generate the answer.\nilar pattern by implementing a retrieval module to query the vector DB.\nwill implement advanced RAG techniques to optimize the search.\nHowever, we will write an inference service that inputs the user query and context, builds \nthe prompt, and calls the LLM to generate the answer.\nPython modules, one for retrieval and one for calling the LLM using the user’s input and context \nIn Chapters 5 and 6, we fine-tuned our LLM Twin model, and in Chapter 8, we learned how to \noptimize it for inference.\nRAG Inference Pipeline\nWe will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin model to AWS \ninto the advanced RAG retrieval module implementation.\nthe retrieval step because this is where the magic happens in an RAG system.\n(and not when calling the LLM), you write most of the RAG inference code.\nHence, most of the advanced RAG logic goes within the retrieval step.\nUnderstanding the LLM Twin’s RAG inference pipeline\nExploring the LLM Twin’s advanced RAG techniques\nImplementing the LLM Twin’s RAG inference pipeline\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, \naugment a prompt using the retrieved context, and call an LLM to generate the final answer.\nUltimately, you will know how to build a production-ready RAG inference pipeline end to end.\nUnderstanding the LLM Twin’s RAG inference \nBefore implementing the RAG inference pipeline, we want to discuss its software architecture \nand advanced RAG techniques.\nFigure 9.1 illustrates an overview of the RAG inference flow.\ninference pipeline starts with the input query, retrieves the context using the retrieval module \n(based on the query), and calls the LLM SageMaker service to generate the final answer.\nFigure 9.1: RAG inference pipeline architecture\nThe feature pipeline and the retrieval module, defined in Figure 9.1, are independent processes.\ntime, the retrieval module is called on demand, within the inference pipeline, on every user request.\nRAG Inference Pipeline\nThe input of the RAG retrieval module is the user’s query, based on which we \nTo fully understand the dynamics of the RAG inference pipeline, let’s go through the architecture \ninating redundant data points from the query vector space (making the search more \nchunk based on the relevance and importance relative to the initial user query.\nBuild the prompt and call the LLM: We map the final list of the most relevant K chunks \nthe retrieved context, and the user’s query.\nprompt, the RAG logic finishes by sending the generated response to the user.\nThat wraps up the overview of the RAG inference pipeline.\nExploring the LLM Twin’s advanced RAG techniques\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced \nRAG techniques we used in our retrieval module:\nPre-retrieval step: Query expansion and self-querying\nRetrieval step: Filtered vector search\nHandbook/blob/main/llm_engineering/application/rag/base.py.",
    "keywords": [
      "RAG Inference Pipeline",
      "RAG Inference",
      "inference pipeline",
      "RAG",
      "inference",
      "advanced RAG techniques",
      "RAG retrieval module",
      "LLM Twin model",
      "LLM Twin",
      "LLM",
      "advanced RAG",
      "RAG techniques",
      "Pipeline",
      "advanced RAG retrieval",
      "query"
    ],
    "concepts": [
      "inference",
      "llm",
      "rag",
      "query",
      "queries",
      "model",
      "models",
      "data",
      "generation",
      "generative"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 344-356)",
    "start_page": 344,
    "end_page": 356,
    "summary": "steps such as query expansion and self-querying.\nfrom llm_engineering.domain.queries import Query\ndef generate(self, query: Query, *args, **kwargs) -> Any:\nUltimately, we must understand how we modeled the Query domain entity to wrap the user’s \nNext, we define the Query entity class, which inherits from the VectorBaseDocument object-vector \nThus, each query can easily be saved or retrieved \nclass Query(VectorBaseDocument):\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a \nauthor_id: An optional UUID4 identifier extracted from the query used as a filter within \nauthor_full_name: An optional string used to query the author_id\nreturn Query(content=query.strip(\"\\n \"))\nAdditionally, there’s an instance method called replace_content() used to create a new Query\ninstance with updated content while retaining the original query’s id, author_id, author_full_\ndef replace_content(self, new_content: str) -> \"Query\":\nreturn Query(\nFollowing the Query class, \nclass EmbeddedQuery(Query):\nThe EmbeddedQuery class extends Query by adding the embedding field.\nAdvanced RAG pre-retrieval optimizations: query expansion \nand self-querying\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion \nand self-querying.\nfor query expansion and move to implementing self-querying.\nquery within the query expansion step and to extract the necessary metadata within the self-que-\nQuery expansion\nThe problem in a typical retrieval step is that you query your vector DB using a single vector rep-\nnuances of your query, the retrieved context may not be relevant.\nthat are semantically related but not near the query vector might be overlooked.\nLLM to generate multiple queries based on your initial question, you create various perspectives \nThese expanded queries, when embedded, target other \nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt \nto guide the LLM in generating these alternative queries.\nnumber of queries you generate to ensure the retrieval step meets your application requirements.\nfor query expansion:\nfrom llm_engineering.domain.queries import Query\nNext, we define the QueryExpansion class, which generates expanded query versions.\nHandbook/blob/main/llm_engineering/application/rag/query_expanison.py:\ndef generate(self, query: Query, expand_to_n: int) -> list[Query]:\nreturn [query for _ in range(expand_to_n)]\nquery_expansion_template = QueryExpansionTemplate()\nprompt = query_expansion_template.create_template(expand_to_n - 1)\nresponse = chain.invoke({\"question\": query})\nquery expansion.\nIt takes expand_to_n as an input parameter to define how many queries we wish to generate while \ngenerated queries.\nllm_engineering.application.rag.query_expansion command:\nquery_expander = QueryExpansion()\nlogger.info(expanded_query.content)\nNow, let’s move to the next pre-retrieval optimization method: self-querying.\nSelf-querying\nThe problem when embedding your query into a vector space is that you cannot guarantee that \nBy embedding the query prompt alone, you can never be sure that the tags \nThe solution is to use self-querying to extract the tags or other critical metadata within the query \nSelf-querying uses an LLM to extract various \nSelf-queries work hand-in-hand with filtered vector searches, which we will explain in the next \nfrom llm_engineering.domain.queries import Query\nHandbook/blob/main/llm_engineering/application/rag/self_query.py:\ndef generate(self, query: Query) -> Query:\nreturn query\nmodel instance (similar to the query expansion implementation).\nand the model into a chain and invoke it with the user’s query.\nresponse = chain.invoke({\"question\": query})\nreturn query\nIf the response is \"none\", it means no user name was found in the query, so we return the origi-\nFinally, we update the query object with the extracted author information and return it:\nquery.author_id = user.id\nquery.author_full_name = user.full_name\nreturn query\nThe updated query now contains the author_id and author_full_name values, which can be \nLet’s look at the SelfQueryTemplate class, which defines the prompt to extract user information:\nIn the SelfQueryTemplate class, we define a prompt instructing the AI model to extract the user \nBy implementing self-querying, we ensure that critical metadata required for our use case is ex-\ncode using the python -m llm_engineering.application.rag.self_query CLI command:\nself_query = SelfQuery()\nquery = self_query.generate(query)\nlogger.info(f\"Extracted author_id: {query.author_id}\")\nlogger.info(f\"Extracted author_full_name: {query.author_full_name}\")\nNow that we understand how self-querying works, let’s explore how it can be used together with \nfiltered vector search within the retrieval optimization step.\nAdvanced RAG retrieval optimization: filtered vector search\nVector search is pivotal in retrieving relevant information based on semantic similarity.\nAs the metadata used within the filtered vector search is often part of the user’s input, we have \nto extract it before querying the vector DB.\nThat’s precisely what we did during the self-query \nThus, as we processed the query within the self-query step, it went into the pre-retrieval \noptimization category, whereas when the filtered vector search optimized the query, it went into \nquery_vector=query_embedding,\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations \nbetween the query and chunk embeddings.\nN represents the number of searches after query expansion, while K is the number of chunks \nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which \nilarity between the query and each chunk more accurately than initial retrieval methods based ",
    "keywords": [
      "query",
      "RAG Inference Pipeline",
      "query expansion",
      "RAG",
      "RAG Inference",
      "advanced RAG",
      "original query",
      "vector search",
      "user",
      "vector",
      "advanced RAG methods",
      "model",
      "question",
      "prompt",
      "Inference Pipeline"
    ],
    "concepts": [
      "query",
      "queries",
      "querying",
      "classes",
      "importing",
      "search",
      "searches",
      "searching",
      "modeled",
      "models"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 357-366)",
    "start_page": 357,
    "end_page": 366,
    "summary": "Ultimately, we pick the top K most relevant chunks from the sorted list of N x K items based on \nReranking works well when combined with query expansion.\nunderstand how reranking works without query expansion:\nSearch for > K chunks: Retrieve more than K chunks to have a broader pool of potentially \nof each chunk relative to the query.\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\nNext, we define the Reranker class, which is responsible for reranking the retrieved documents \nbased on their relevance to the query:\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an \nrelevance of each document chunk with respect to the query.\ndef generate(self, query: Query, chunks: list[EmbeddedChunk], keep_\nscores = self._model(query_doc_tuples)\nscored_query_doc_tuples = list(zip(scores, chunks, strict=False))\nreranked_documents = scored_query_doc_tuples[:keep_top_k]\nThe generate() method takes a query, a list of chunks (document segments), and the number \nCreates pairs of the query content and each chunk’s content\nUses the cross-encoder model to score each pair, assessing how well the chunk matches \nthe query\nExtracts the chunks from the tuples and returns them as the reranked documents\nThe __call__ method allows us to pass in a list of text pairs (each consisting of the query and \nhow to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.\nlook at how to build the final prompt using the retrieved context and user query.\nry expansion, self-querying, reranking, and filtered vector search.\nFigure 9.2: Search logic of the RAG retrieval module\nglues together all the steps required to search results similar to the user’s query.\nthe extracted author details from the self-query step are used within the filtered vector search.\nof N searches), we want to retrieve a maximum of K results.\nwill have a list of ≤ K chunks.\ncategory or more returns < K / 3 items after applying the author filters due to missing chunks \nreturns ≤ K items, we will end up with ≤ N x K chunks that we aggregate into a single list.\ntheir reranking score, and pick the most relevant top K chunks we will use as context for RAG.\nself._query_expander = QueryExpansion(mock=mock)\nIn the search() method, we convert the user’s input string into a query object.\nquery: str,\nquery_model = Query.from_str(query)\nquery_model = self._metadata_extractor.generate(query_model)\nauthor_id=query_model.author_id,\nn_generated_queries = self._query_expander.generate(query_model, \n\"Successfully generated queries for search.\",\nWe then perform the search concurrently for all expanded queries using a thread pool.\nsearch_tasks = [executor.submit(self._search, _query_model, k) \nfor _query_model in n_generated_queries]\nAfter retrieving the documents, we rerank them based on their relevance to the original query \nk_documents = self.rerank(query, chunks=n_k_documents, keep_\ndef _search(self, query: Query, k: int = 3) -> list[EmbeddedChunk]:\nif embedded_query.author_id:\nquery_filter = None\nsame embedding model at ingestion and query time, which is critical for the retrieval step.\npost_chunks = _search_data_category(EmbeddedPostChunk, embedded_\nquery)\nembedded_query)\nFinally, the rerank() method takes the original query and the list of retrieved documents to \ndef rerank(self, query: str | Query, chunks: list[EmbeddedChunk], \nreranked_documents = self._reranker.generate(query=query, \nLeveraging the ContextRetriever class, we can retrieve context from any query with only a few \nfrom llm_engineering.application.rag.retriever import ContextRetriever\nquery = \"\"\"",
    "keywords": [
      "RAG Inference Pipeline",
      "query",
      "RAG Inference",
      "chunks",
      "RAG",
      "model",
      "Search",
      "documents",
      "Inference Pipeline",
      "list",
      "self.",
      "top",
      "LLM",
      "cross-encoder model",
      "pipeline"
    ],
    "concepts": [
      "query",
      "queries",
      "rag",
      "chunks",
      "chunk",
      "list",
      "search",
      "searches",
      "model",
      "models"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 367-374)",
    "start_page": 367,
    "end_page": 374,
    "summary": "documents = retriever.search(query, k=3)\nlogger.info(\"Retrieved documents:\")\nCalling the code from above using the following CLI command: poetry poe call-rag-retrieval-\n2024-09-18 19:01:50.588 | INFO - Retrieved documents:\n68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by \nPaul Iusztin Implement 4 advanced RAG retrieval techniques to optimize \nyour vector DB searches.\nIntegrate the RAG retrieval module into a \nproduction LLM system…\" platform='decodingml.substack.com' document_\nsubstack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'\n04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA \nDB for …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-\n256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-\nRAG Inference Pipeline\ncom/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-\nend-to-end RAG inference pipeline.\nTo fully implement the RAG flow, we still have to build the prompt using the context from the \nretrieval model and call the LLM to generate the answer.\nIt takes in a user’s query and an optional context, sets up the language model end-\ndef call_llm_service(query: str, context: str | None) -> str:\nanswer = InferenceExecutor(llm, query, context).execute()\nusing the user query and retrieved context:\ning relevant documents based on the query, mapping the documents to the context that will be \ndocuments = retriever.search(query, k=3)\nanswer = call_llm_service(query, context)\nindependently without context or use the retrieval module as a query engine on top of your vector \nthe LLM, along with the new user input and context, we also pass the conversation history from \nRAG Inference Pipeline\nAs for each search, we send three queries to the vector DB, one for each data category.\nsecond improvement is to add a router between the query and the search.\nmulti-category classifier that predicts the data categories we must retrieve for that specific query.\nexample, if the user wants to write a theoretical paragraph about RAG for an article, then most \nthe article class, which we can use to decide what collection we must query.\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG \nuse case, we can adapt the router algorithm to optimize the retrieval step.\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector \nsearch (based on embeddings) with a keyword search algorithm, such as BM25.\nused BM25 (or similar methods) to find similar items in a DB before vector search algorithms \nBy merging the methods, hybrid search retrieves results that match the exact \nterms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and \nvector search and BM25 algorithms.\nEach algorithm retrieves a set of relevant documents \nOne last improvement we can make to our RAG system is to use multi-index vector structures \nInstead of using the embeddings of a single field to do the vector search for a particular \nRAG Inference Pipeline\nFor example, in our LLM Twin use case, we used only the content field of our articles, posts, or \nSuperlinked, we defined a multi-index on the content and platform for our article collection in \nSuperlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, \nThis chapter taught us how to build an advanced RAG inference pipeline.\nods we used within the retrieval module, such as query expansion, self-querying, filtered vector \nglues all the advanced RAG components under a single interface, making searching for relevant \nretrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve \nas our RAG inference pipeline.\nsuperlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-\nRAG Inference Pipeline\n4 Advanced RAG Algorithms You Must Know | Decoding \nhttps://medium.com/decodingml/the-4-advanced-rag-algorithms-you-\nAdvanced Retrieval-Augmented Generation: From Theory \nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\nMulti-attribute search with vector embeddings | VectorHub by Superlinked.\nsuperlinked.com/vectorhub/articles/multi-attribute-semantic-search\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked.\nsuperlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-\nVisualize your RAG Data—Evaluate your Retrieval-Aug-\nvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-\nUsing LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications.",
    "keywords": [
      "RAG Inference Pipeline",
      "RAG",
      "RAG Inference",
      "LLM",
      "Advanced RAG",
      "Inference Pipeline",
      "Paul Iusztin",
      "search",
      "Advanced RAG Algorithms",
      "UUID",
      "query",
      "Inference",
      "advanced RAG inference",
      "vector search",
      "RAG system"
    ],
    "concepts": [
      "rag",
      "retriever",
      "retrieved",
      "retrieval",
      "retrieve",
      "retrieves",
      "query",
      "queries",
      "searches",
      "search"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 375-383)",
    "start_page": 375,
    "end_page": 383,
    "summary": "Deploying the inference pipeline for the large language model (LLM) Twin application is a critical \nsuch as latency, throughput, and costs.\nthe deployment processes, we must leverage MLOps best practices, such as model registries that \nTo understand how to design the deployment architecture of the LLM Twin, we will first look at \nthree deployment types we can choose from: online real-time inference, asynchronous inference, \nan architectural decision, such as latency, throughput, data, and infrastructure.\nUnderstanding inference deployment types\nExploring the LLM Twin’s inference pipeline deployment strategy\nWhen it comes to deploying ML models, the first step is to understand the four requirements \npresent in every ML application: throughput, latency, data, and infrastructure.\nFor example, should your model deployment be optimized for low latency or \nThroughput refers to the number of inference requests a system can process in a given period.\nML models when you expect to process many requests.\nHigh throughput often requires scalable and robust infrastructure, such as machines or clusters \nwith multiple high-end GPUs.Latency is the time it takes for a system to process a single inference \nis the average response time from when a user sends a request, and the service provides a result \nMeanwhile, the throughput is the average number of requests the API processes and \nLow-latency systems require optimized and often more costly infrastructure, such as faster pro-\nA lower latency translates to higher throughput when the service processes multiple queries in \nFor example, if the service takes 100 ms to process requests, this translates to \na throughput of 10 requests per second.\nIf the latency reaches 10 ms per request, the throughput \nprocess 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests \nIf you process 60 requests in 200 ms, the latency is 200 ms, while the throughput \nThus, even when batching requests at serving time, it’s essential \nof the processed data.\nData is the foundation of the inference process.\nThe type and size of the data directly impact latency and throughput, as more complex or exten-\nFor example, designing a model that takes input structured \nthat supports the deployment and operation of the ML models.\nnecessary resources for deploying, scaling, and maintaining ML models.\nFor high throughput, the systems require scalable infrastructure to manage large data \nvolumes and high request rates, possibly through parallel processing, distributed systems, \nInfrastructure must be optimized to reduce processing time to achieve low latency, such \nlow latency while batching your requests, you often have to sacrifice high throughput \nAs you process fewer requests per second, it results in idle computing, which \nincreases the overall cost of processing a request.\nIt is crucial to design infrastructure to meet specific data requirements.\nLet’s move on to the three deployment architectures we can leverage to serve our models.\nUnderstanding inference deployment types\nWhen selecting one design over the other, there is a trade-off between latency, throughput, and \nFor example, serving your model in \nFigure 10.1: The three fundamental architectures of inference deployment types\nIn real-time inference, we have a simple architecture based on a server that can be accessed \nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which \nimmediately processes the request and returns the result in the same response.\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML \nrecommendation engines in platforms like TikTok. The simplicity of real-time inference, with its direct client-server interaction, makes it an attrac-\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the \nrequest and places it in a queue for processing.\nInstead, the ML service processes the request asynchronously.\nquires a robust infrastructure that queues the messages to be processed by the ML service later on.\nIt doesn’t have to process all the requests \nFor example, let’s assume that on an e-shop site, we usually have 10 requests per second handled \nduces higher latency, making it less suitable for time-sensitive applications.\nFor example, this is a robust design where you don’t care too much about the latency of the infer-\nBut suppose you carefully design the autoscaling system to process the requests from \nIn a batch transform architecture, the ML service pulls data from a storage ",
    "keywords": [
      "Inference Pipeline Deployment",
      "LLM Twin",
      "Inference",
      "Deployment",
      "requests",
      "Inference Pipeline",
      "Pipeline Deployment",
      "LLM Twin service",
      "latency",
      "inference deployment types",
      "throughput",
      "deployment types",
      "data",
      "LLM",
      "model"
    ],
    "concepts": [
      "data",
      "latency",
      "inference",
      "infer",
      "model",
      "models",
      "deployment",
      "deploying",
      "deployed",
      "processes"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 384-392)",
    "start_page": 384,
    "end_page": 392,
    "summary": "Unlike the asynchronous inference architecture, a batch transform design is optimized for high \nMoreover, the batch transform architecture is the simplest way to serve a model, \nTaking this approach, the client never has to wait for the ML service to process its input, \nTo conclude, we examined the three most common architectures for serving ML models.\nstarted with online real-time inference, which serves clients when they request a prediction.\nMonolithic versus microservices architecture in \nIn the previous section, we saw three different methods of deploying the ML service.\nences in architecture were mainly based on the interaction between the client and the ML service, \nInference Pipeline Deployment\nBut another aspect to consider is the architecture of the ML service itself, which can be imple-\nFigure 10.2: Monolithic versus microservices architecture in model serving\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-pro-\ncessing steps) are bundled into a single service in a monolithic architecture.\nOne key challenge of a monolithic architecture is the difficulty of scaling components independent-\nThe LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound.\nMicroservices architecture\nA microservices architecture breaks down the inference pipeline into separate, independent ser-\nvices—typically splitting the LLM service and the business logic into distinct components.\nFor instance, since the LLM service might require more GPU resources \nThis optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus \nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service \nInference Pipeline Deployment\nFigure 10.3: Scaling microservices independently based on compute requirements\nNote that the proposed design for decoupling the ML model and business logic into two services \narchitecture for your application needs.\nThe choice between monolithic and microservices architectures for serving ML models largely \nif the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, \nmore complex systems where different components have varying scaling needs or require distinct \nas GPU-intensive LLM services.\nthe system for keeping these machines busy all the time or quickly scaling down when the GPU \nML and business logic into two different Python modules that don’t interact with each other.\nInference Pipeline Deployment\nAnother option is to write the ML and business logic as two different Python packages that you \nif you start with a monolith and down the line you want to move to a microservices architecture, \nExploring the LLM Twin’s inference pipeline \nstrategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to \nthe selection of an online real-time inference deployment architecture.\nOn the monolith versus microservice aspect, we will split the ML service between a REST API \nserver containing the business logic and an LLM microservice optimized for running the given \nAs the LLM requires a powerful machine to run the inference, and we can further optimize \nwith the microservice architecture.\nThe LLM microservice is strictly optimized for the RAG generation component.\nIn summary, our approach involves implementing an online real-time ML service using a micro-\nservice architecture, which effectively splits the LLM and business logic into two distinct services.\nFigure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\nInference Pipeline Deployment\nA fine-tuned LLM generated by the training pipeline, which is pulled from our model \nThe prompt is sent to the LLM microservice through an HTTP request.\nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast.\nThe LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s \nenable real-time predictions from deployed models.\nYou can deploy multiple models to an endpoint, \nOnce deployed, models are easily accessible via the \nTogether, these components create a robust infrastructure for deploying and managing ML models \ndeploy the inference pipeline.",
    "keywords": [
      "Inference Pipeline Deployment",
      "Inference Pipeline",
      "LLM",
      "LLM microservice",
      "inference",
      "business logic",
      "architecture",
      "model",
      "Pipeline",
      "service",
      "microservices architecture",
      "GPU",
      "Pipeline Deployment",
      "microservices",
      "business"
    ],
    "concepts": [
      "microservices",
      "microservice",
      "requirements",
      "required",
      "requires",
      "require",
      "inference",
      "service",
      "services",
      "llm"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 393-400)",
    "start_page": 393,
    "end_page": 400,
    "summary": "Inference Pipeline Deployment\nDeploying the LLM Twin service\nwe will deploy the LLM microservice using AWS SageMaker and the business microservice using \nDeploy our fined-tuned LLM Twin model to AWS SageMaker\nWrite an inference client to interact with the deployed model\nImplementing the LLM microservice using AWS SageMaker\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon \nSageMaker as an online real-time inference endpoint.\ninference container, known as the Hugging Face LLM DLC, to deploy our LLM.\nWhen it comes to serving models, the DLC is powered by the Text Generation Inference (TGI) \nInference Pipeline Deployment\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, \nwill be hosted on AWS SageMaker under inference endpoints that can be accessed through HTTP \nLLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.\nConfiguring SageMaker roles\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and \nroles to access and deploy the SageMaker infrastructure.\nthe deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3.\npoetry poe create-sagemaker-role\ndeploy everything related to SageMaker to ensure we modify only the resources associated \nwith SageMaker.\nThe last step is to take the new credentials from the JSON file and update the AWS_ACCESS_\nllm_engineering/infrastructure/aws/roles/create_sagemaker_role.py.\nWe will attach this role to the SageMaker deployment, \nIn our case, we will provide SageMaker access to AWS S3, CloudWatch, and ECR.\npoetry poe create-sagemaker-execution-role\nThis command will generate a JSON file called sagemaker_execution_role.json that \nvalue from the JSON file and update the AWS_ARN_ROLE variable from your .env file with \nLLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/\ndeployment.\nDeploying the LLM Twin model to AWS SageMaker\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which \nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command: poe \ndeploy-inference-endpoint.\nexcept for creating the SageMaker AWS IAMs we created and configured in the previous step.\nInference Pipeline Deployment\nmate the deployment process, starting with the create_endpoint() function.\ntest the CLI command and check the AWS console to see whether the deployment was successful.\nThe SageMaker deployment code is available at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.\nFigure 10.5: AWS SageMaker deployment steps\nmain function that deploys the LLM Twin model to AWS SageMaker.\nresource manager and deployment service:\ndeployment_service = DeploymentService(resource_manager=resource_\nconfig=hugging_face_deploy_config,\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,\nendpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,\n\"sagemaker\",\nInference Pipeline Deployment\nNext, we implement the endpoint_config_exists method, checking whether a specific Sage-\nself.sagemaker_client.describe_endpoint_\nSageMaker endpoint:\nself.sagemaker_client.describe_endpoint(EndpointName=endpoint_\nwhich will interface with AWS SageMaker and an instance of the ResourceManager class we \n\"sagemaker\",\nentire process of deploying a model to a SageMaker endpoint.\nif self.resource_manager.endpoint_config_exists(endpoint_config_\nself.prepare_and_deploy_model(",
    "keywords": [
      "LLM Twin model",
      "AWS",
      "AWS SageMaker",
      "endpoint",
      "LLM Twin",
      "LLM",
      "Inference Pipeline Deployment",
      "Hugging Face",
      "Inference Pipeline",
      "SageMaker",
      "Inference",
      "Hugging Face LLM",
      "Deployment",
      "config",
      "Face LLM DLC"
    ],
    "concepts": [
      "deployment",
      "deploying",
      "deploy",
      "deployed",
      "deployments",
      "deploys",
      "inference",
      "models",
      "endpoint",
      "endpoints"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 401-411)",
    "start_page": 401,
    "end_page": 411,
    "summary": "Inference Pipeline Deployment\nlogger.info(f\"Successfully deployed/updated model to endpoint \nlogger.error(f\"Failed to deploy model to SageMaker: {e}\")\nThe deploy method begins by checking whether the endpoint configuration already exists using \ndeploy_model() method, which is responsible for the actual deployment of the model to the \nspecified SageMaker endpoint.\nThe prepare_and_deploy_model() method is a static method within the DeploymentService\nThis method is focused on setting up and deploying the Hugging Face model to SageMaker:\ndef prepare_and_deploy_model(\nhuggingface_model.deploy(\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from \nwhat LLM to load from Hugging Face and its inference parameters, such as the maximum total \nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy \nparameter for multi-model endpoints and tags for tracking and categorization.\nThe class is initialized only with an instance of a deployment service, \ndef __init__(self, deployment_service):\nThis method orchestrates the deployment process, taking various parameters that define \nhow the Hugging Face model should be deployed to AWS SageMaker:\nInference Pipeline Deployment\nlogger.info(\"Starting deployment using Sagemaker Huggingface \nf\"Deployment parameters: nb of replicas: {settings.COPIES}, nb of \nThe parameters passed into the method are crucial to the deployment process:\nrole_arn: The AWS IAM role that provides permissions for the SageMaker deployment.\nconfig: A dictionary containing configuration settings for the model environment.\nresources: Optional resources dictionary used for multi-model endpoint deployments.\nendpoint_type: This can either be MODEL_BASED or INFERENCE_COMPONENT, determining \nwhether the endpoint includes an inference component.\nsources are leveraged when setting up multi-endpoint configurations that use multiple replicas \nthe deployed model.\ndeployed.\ndeployment.\nInference Pipeline Deployment\nwhen the model is deployed to a SageMaker endpoint.\ndeploying the endpoint, we suggest modifying them and seeing how the performance of the LLM \nwhich Hugging Face model to deploy.\nTo start the SageMaker deployment with the configuration shown above, call the \ncreate_endpoint(endpoint_type=EndpointType.MODEL_BASED)\npoetry poe deploy-inference-endpoint\nThat’s all you need to deploy an inference pipeline to AWS SageMaker.\nfrom your .env file and deploy the model with a different configuration without touching the \nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker \nFigure 10.6: AWS SageMaker Inference endpoints example\nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve gen-\nInference Pipeline Deployment\nFigure 10.7: AWS SageMaker twin inference endpoint example\nCalling the AWS SageMaker Inference endpoint\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service.\ninference endpoint through HTTP requests, and decode the results in a way the client can work \nAll the AWS SageMaker Inference code is available on GitHub at llm_engineering/model/\ntext = \"Write me a post about AWS SageMaker inference endpoints.\"\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE\nSageMaker endpoint:\nclass LLMInferenceSagemakerEndpoint(Inference):\nOne of the key features of the class is its ability to generate a default payload for inference requests.\nThe parameters section includes settings that influence the model’s behavior during \nThe class allows customization of the payload through the set_payload() method, which enables \nthe user to modify the inputs and parameters before sending an inference request:\ndef set_payload(self, inputs: str, parameters: Optional[Dict[str, Any]] = \nInference Pipeline Deployment\nAdditionally, it allows for modifying inference parameters if any are provided.\nUltimately, we leverage the inference() method to call the SageMaker endpoint with the cus-\ndef inference(self) -> Dict[str, Any]:\nif self.inference_component_name not in [\"None\", None]:\nlogger.exception(\"SageMaker inference failed.\")\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint.\nwe previously presented to send HTTP requests to the AWS SageMaker endpoint.\nThe llm parameter accepts any instance that implements the Inference interface, such \nllm: Inference,\nthe model from generating repetitive text, and the temperature setting that controls the ran-\nOnce the payload and parameters are set, the method calls the inference function from \nInference Pipeline Deployment\nself.llm.set_payload(\nanswer = self.llm.inference()[0][\"generated_text\"]\npoetry run python -m llm_engineering.model.inference.test\nclasses represent the request and response structure for the FastAPI endpoints:",
    "keywords": [
      "Inference Pipeline Deployment",
      "AWS SageMaker Inference",
      "Inference",
      "endpoint",
      "AWS SageMaker",
      "llm",
      "model",
      "Hugging Face model",
      "Deployment",
      "SageMaker Inference endpoint",
      "SageMaker",
      "Inference Pipeline",
      "SageMaker Inference",
      "SageMaker endpoint",
      "AWS"
    ],
    "concepts": [
      "setting",
      "settings",
      "sets",
      "inference",
      "infer",
      "deployment",
      "deploy",
      "deploying",
      "deploys",
      "deployments"
    ]
  },
  {
    "chapter_number": 45,
    "title": "Segment 45 (pages 412-421)",
    "start_page": 412,
    "end_page": 421,
    "summary": "let’s reiterate over the call_llm_service() and rag() functions we’ve presented in Chapter 9\nllm_service() function wraps the inference logic used to call the SageMaker LLM microservice:\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\nAlso, as the LLM inference logic is moved to a different microservice, the call_llm_service()\nanswer = call_llm_service(query, context)\nfunction processes the request by calling the rag function with the user’s query.\nanswer = rag(query=request.query)\nThis FastAPI application demonstrates how to effectively integrate an LLM hosted on AWS Sage-\npoetry poe run-inference-ml-service\nTo call the /rag endpoint, we can leverage the curl CLI command to make a POST HTTP request \npoetry poe call-inference-ml-service\nto deploy it to AWS Elastic Container Service (ECS), which is similar to AWS EKS but doesn’t \nSo far, the SageMaker LLM microservice has used a static number of replicas to serve our users, \nOnce you’re done testing your inference pipeline deployment, deleting all your AWS \nSageMaker resources used to deploy the LLM is essential.\nAs almost all AWS re-\nyou’re done testing your SageMaker infrastructure (or any AWS resource).\nwe have provided a script that deletes all the AWS SageMaker resources for you:\nFor example, let’s assume we requested four copies (replicas) with the following \nThe solution is to implement an autoscaling strategy that scales the number \nof replicas up and down dynamically based on various metrics, such as the number of requests.\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints \nscale in and out based on the number of requests.\nreplica so the server remains responsive to new user requests or even scales down to zero if the \nwe have to keep two replicas online, and when the number of requests spikes to 100 per second, \na request to each replica.\nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference \nSageMaker provides a feature called Application Auto Scaling that allows you to scale \nfectively leveraging this functionality: registering a scalable target and creating a scalable policy.\nThe first step in enabling autoscaling for your resources is to register a scalable target with the \nApplication Auto Scaling feature AWS provides.\nresource you intend to scale, as well as setting the boundaries within which the scaling should \nFor instance, when working with SageMaker Inference components, you’ll define the following:\nincluding the name of the SageMaker Inference component.\nScalable dimension: This specifies the resources to be scaled, such as the desired number \ning strategies, such as minimum and maximum limits of the number of replicas.\nBy registering a scalable target, you prepare your SageMaker Inference component for future \nOnce your scalable target is registered, the next step is defining how the scaling should occur.\nThis is where creating a scaling policy comes in.\nA scaling policy defines specific rules that trigger \nscaling events.\nIn the context of our SageMaker Inference component, the scalable policy might include the \nthe resource’s capacity to maintain a specific target value for a chosen metric.\nspecifying cooldown periods that control how quickly scaling actions can occur after \nThe scaling policy defines the rules for your scaling-in and scaling-out strategy.\ntarget value, it triggers actions to scale the number of inference component copies up or down, \nOnce defined, Application Auto Scaling creates and manages the necessary \nFor instance, consider an application running on SageMaker.\nGPU usage exceeds the target, the system scales out, adding resources to manage the increased \nConversely, when GPU usage drops below the target, the system scales in, reducing capacity \nOne significant advantage of setting up target tracking policies using Application Auto Scaling is \ndefine scaling adjustments manually.\nMinimum and maximum scaling limits\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish \nyour maximum and minimum scaling limits before creating your scaling policy.\nNext, configure the maximum value, which defines the upper limit of resources your model can \nscale up to.\nThus, you can scale up as much as your application needs within the \nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to \nof instances during scale-in requests and restricts the creation of new replicas during scale-out \nOnce you understand how to configure scaling policies for SageMaker, you can imme-\nAWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-\nIf your scaling policy or cooldown period is too sensitive, you may \nIn this chapter, we learned what design decisions to make before serving an ML model, whether \nan LLM or not, by walking you through the three fundamental deployment types for ML models: \nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Infer-\nconsists of all the RAG steps based on the retrieval module implemented in Chapter 9 and the LLM \nmicroservice deployed on AWS SageMaker.\nWe also reviewed a popular autoscaling strategy that scales in and out \nbased on a given set of metrics and saw how to implement it in AWS SageMaker.\nArchitect LLM & RAG inference pipelines | Decoding ML.\nllm-rag-inference-pipelines-73b94ef82a99",
    "keywords": [
      "Inference Pipeline Deployment",
      "AWS",
      "AWS SageMaker",
      "Application Auto Scaling",
      "inference",
      "AWS SageMaker resources",
      "SageMaker Inference",
      "Scaling",
      "llm",
      "AWS SageMaker Inference",
      "Inference Pipeline",
      "rag",
      "SageMaker",
      "SageMaker Inference endpoints",
      "SageMaker Inference component"
    ],
    "concepts": [
      "scales",
      "scaling",
      "scaled",
      "inference",
      "infer",
      "rag",
      "endpoint",
      "services",
      "service",
      "resources"
    ]
  },
  {
    "chapter_number": 46,
    "title": "Segment 46 (pages 422-429)",
    "start_page": 422,
    "end_page": 429,
    "summary": "MLOps and LLMOps\nmodels (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to \nBut MLOps is not just about these components; it takes an ML \napplication to the next level by automating data collection, training, testing, and deployment.\nThus, the end goal of MLOps is to automate as much as possible and let users focus on the most \nadopt these pre-trained foundational models for their use cases, focusing on LLMOps problems \nMLOps and LLMOps\ndeployment (CI/CD) pipeline to test the integrity of our code and automate the deployment \nprocess, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline \nstarting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging \ninto LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as \nThe path to LLMOps: Understanding its roots in DevOps and MLOps\nDevOps and MLOps\nThen, we will move to MLOps to understand how \nThus, DevOps was born to automate the process of shipping software at scale.\ncifically, DevOps is used in software development, where you want to completely automate your \nSuperior quality and security: DevOps ensures swift software development while main-\nMLOps and LLMOps\nDeployment environments: To thoroughly test your code before shipping it to produc-\nVersion control: Used to track, manage, and version every change made to the source code.\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field \nMLOps\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML.\napplication, such as the data, model, and, finally, the code.\nMLOps and LLMOps\nthe code, modifications in the data, or adjustments to the model.\nFigure 11.2: Relationship between data, model, and code changes\nIn that case, you must train (or fine-tune) a new model, resulting \na few examples that can trigger a change in the data and indirectly in the model:\nAfter deploying the ML model, its performance might decay as time passes, so we need \ndata or re-label it, which generates a new set of models.\nSo, what is MLOps?\nthat makes data and models their first-class citizen while preserving the DevOps methodology.\nLike DevOps, MLOps originates from the idea that isolating ML model development from its \ndeployment process (ML operations) diminishes the system’s overall quality, transparency, and \nMLOps core components\ner on the MLOps core components now that we better understand the field.\ncontrol and CI/CD, MLOps revolves around:\nModel registry: A centralized repository for storing trained ML models (tools: Comet \nFeature store: Preprocessing and storing input data as features for both model training \nML metadata store: This store tracks information related to model training, such as model \nconfigurations, training data, testing data, and performance metrics.\nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools: ZenML, \nMLOps and LLMOps\nmanual processes to automated pipelines through CT and CI/CD.\nretraining and deployment of ML models in response to triggers such as new data, per-\nautomation ensures that our ML systems are robust, scalable, and adaptable to changing \nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, \nCode is tracked using tools like Git, models are \nversioned through model registries, and data versioning can be managed using solutions \nExperiment tracking: As training ML models is an iterative and experimental process \nTesting: MLOps suggests that along with testing your code, you should also test your \ndata and models through unit, integration, acceptance, regression, and stress tests.\nMonitoring: This stage is vital for detecting performance degradation in served ML models \nmodel metrics and detecting drifts, we can maintain the health of ML systems in produc-",
    "keywords": [
      "MLOps",
      "DevOps",
      "LLM Twin",
      "model",
      "data",
      "LLMOps",
      "code",
      "models",
      "LLM",
      "software",
      "pipeline",
      "LLMs",
      "training",
      "Twin",
      "core"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "tool",
      "tools",
      "tooling",
      "operations",
      "operational",
      "operate",
      "tested"
    ]
  },
  {
    "chapter_number": 47,
    "title": "Segment 47 (pages 430-437)",
    "start_page": 430,
    "end_page": 437,
    "summary": "MLOps engineering\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description \nan MLOps engineer, you have a lot of work to do on the infrastructure side.\nseen in this section, an MLOps engineer still has to implement things such as experiment tracking, \nthese into the code and the MLOps engineer focus on making them work on their infrastructure.\nMLOps\ndata scientist/ML researcher, ML engineer, and MLOps engineer.\nMLOps and LLMOps\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of \nHowever, the MLOps engineer plays a pivotal role in this \nLLMOps encompasses the practices and processes essential for managing and running LLMs.\nassociated with LLMs. While MLOps addresses the principles and practices of managing various \nML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly \ncomplex training requirements, prompt management, and non-deterministic nature of generating \nWhen training LLMs from scratch, the data and model dimensions of an ML system grow sub-\nstantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns \nsive datasets required for training LLMs. It involves big data techniques for processing, \nThe massive size of LLMs directly impacts model training.\nWhen training an LLM from \noptimizing your processes and infrastructure to support data, model, or tensor parallelism.\nAt its core, LLMOps is MLOps at scale.\nIt uses the same MLOps principles but is applied to big data \nand huge models that require more computing power to train and run.\ntions now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or \nThus, for most LLM applications out there, your development steps will involve the selection of a \nfoundation model, which you further have to optimize by using prompt engineering, fine-tuning, \ndive into some popular components of LLMOps that can improve prompt engineering, fine-tun-\nMLOps and LLMOps\nyour LLM can produce harmful output or receive dangerous input, so you should monitor and \nyour system (model jailbreaking), and accepting violent or unethical prompts.\napplications don’t want to accept violent or unethical queries from users, such as asking \nOutput guardrails: At the output of an LLM response, you want to catch failed outputs \nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data \nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the \nthese tools, you usually want to track the user input, the prompt templates, the input variables, \nthe generated response, the number of tokens, and the latency.\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we \nThus, when it comes to tracking the latency of generating an answer, the final user experience \nTime per Output Token (TPOT): The time it takes to generate each output token\nMLOps and LLMOps\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting \nyour LLMs. Ultimately, you can compute metrics that validate your model’s performance for each input, \nhave multiple intermediate steps from the user query to the final general answer.\nand the final prompt sent to the model.\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated \nthe application can behave unexpectedly if the number of generated tokens suddenly fluctuates \nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong \nDeploying the LLM Twin’s pipelines to the cloud\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud.\nNote that the training and inference pipelines already work with AWS SageMaker.\nSageMaker training and inference components are more costly to run (which we \nMLOps and LLMOps\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, ",
    "keywords": [
      "LLM",
      "MLOps",
      "LLMs",
      "MLOps engineer",
      "training LLMs",
      "LLMOps",
      "data",
      "LLM Twin",
      "training",
      "output",
      "model",
      "LLM systems",
      "input",
      "prompt",
      "models"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "prompt",
      "prompts",
      "training",
      "trained",
      "train",
      "output",
      "outputting"
    ]
  },
  {
    "chapter_number": 48,
    "title": "Segment 48 (pages 438-445)",
    "start_page": 438,
    "end_page": 445,
    "summary": "Build a Docker image that contains all the system dependencies, the project dependencies, \nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS \nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2.\nBased on the pulled image, it creates a Docker container that executes the pipeline step.\nThe ZenML dashboard is a key tool, providing real-time updates \nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and \nthe ZenML cloud.\nWe will show you how to create and integrate a free MongoDB cluster into our projects.\nIn the left panel, go to Deployment | Database and click Build a Cluster.\nChoose AWS as your provider.\nWhat AWS cloud region should I choose?\nUnfortunately, MongoDB, Qdrant, or other services may change their UI or naming \nbut be careful to choose the same region for all future AWS services.\nTo test that your newly created MongoDB cluster works fine, we must connect to it from \ntool of your liking), which contains your username, password, and cluster URL, similar \nThe final step is to return to your project and open your .env file.\ncloud MongoDB cluster we just created.\nThus, to create a Qdrant cluster \nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\nIn the left panel, go to Clusters and click Create.\nChoose the Free version of the cluster.\nAccess the cluster in the Data Access Control section in the left panel.\nClick Create and choose your twin cluster to create a new access token.\nGo back to the Clusters section of Qdrant and open your newly created twin cluster.\nwill have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\nThe Qdrant cluster dashboard will now be empty, but after \nQDRANT_CLOUD_URL=<the endpoint URL found at step 7>\nQDRANT_APIKEY=<the access token created at step 5>\ncloud Qdrant cluster we just created.\nend data pipeline with the cloud version of MongoDB and Qdrant as follows:\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.\nSetting up the ZenML cloud\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process.\nup a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, \nwe will bundle our code in a Docker image to run it in AWS SageMaker.\nLet’s start with setting up the ZenML cloud:\nML cloud tenant, return to the project and run the zenml connect command provided \nTo ensure everything works fine, run a random pipeline from your code.\nGo to the Pipelines section in the left panel of the ZenML dashboard.\nTo ship the code to AWS, you must create a ZenML stack.\nworking locally, ZenML offers a default stack that allows you to quickly develop your code and \ninfrastructure environments, such as local and AWS runs, which we will showcase in this section.\nWith that in mind, let’s create an AWS stack for our project.\nThen, choose AWS as your cloud provider.\nNow ZenML will create a set of IAM roles to give permissions to all the other components \nEnsure that your ZenML server version matches your local ZenML version.\n8. Click the Deploy to AWS button.\nleverages CloudFormation (an infrastructure as code, or IaC, tool) to create all the AWS \nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML \nAWS resources or to connect ZenML with your current infrastructure.\nBefore moving to the next step, let’s have a quick recap of the AWS resources we just \nCloudFormation is a service that allows you to model and set up your AWS re-\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker \nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing.\nThe last step is to find a way to take our code and run it on top of this infrastructure.\npopular solution is Docker, a tool that allows us to create an isolated environment (a container) \nthat contains everything we need to run our application, such as system dependencies, Python \nPoetry to be installed is specified, and a few environment variables are set to ensure that package ",
    "keywords": [
      "AWS",
      "Qdrant",
      "ZenML",
      "ZenML cloud",
      "create",
      "Qdrant cluster",
      "Docker",
      "AWS infrastructure",
      "cloud",
      "cluster",
      "Docker image",
      "MongoDB",
      "step",
      "Choose",
      "Qdrant cluster dashboard"
    ],
    "concepts": [
      "mongodb",
      "zenml",
      "step",
      "steps",
      "creates",
      "create",
      "created",
      "creating",
      "cloud",
      "click"
    ]
  },
  {
    "chapter_number": 49,
    "title": "Segment 49 (pages 446-455)",
    "start_page": 446,
    "end_page": 455,
    "summary": "RUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\"\nRUN poetry config installer.max-workers 20\nWith the dependency files in place, the project’s dependencies are installed using Poetry.\npoetry install --no-root --no-interaction --no-cache --without dev && \nproject dependencies but mostly change your code, copying your project files in the last step makes \nIt allows the project to run smoothly in any environment that supports Docker.\nThe last step is to build the Docker image and push it to the ECR created by ZenML.\nDocker image from the root of the project, run the following:\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works \npoetry poe build-docker-image\nNow, let’s push the Docker image to ECR.\nFigure 11.7: AWS ECR example\nFor this to work, ensure that you have the AWS CLI installed \naws ecr get-login-password --region ${AWS_REGION}| docker login --username \ndocker tag llmtwin ${AWS_ECR_URL}:latest\nFinally, we push it to ECR by running:\ndocker push ${AWS_ECR_URL}:latest\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository.\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.\nRun the pipelines on AWS\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps.\nLet’s switch from the default ZenML stack to the AWS one we created in this chapter.\nroot of your project, run the following in the CLI:\nzenml stack set aws-stack\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9.\nthe settings.docker.parent_image attribute with your ECR URL, as shown below:\nWe’ve configured the pipeline to always use the latest Docker image available in ECR.\nThe last step is setting up to run the pipelines asynchronously so we don’t have to wait until they \nzenml orchestrator update aws-stack --synchronous=False\nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our \nRun the end-to-end-data-pipeline with the \npoetry poe run-end-to-end-data-pipeline\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run.\nthe ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10.\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\nFigure 11.11: ZenML step metadata example\nTo find even more details about the runs, you can go to AWS SageMaker.\nTo run other pipelines, you have to update the settings.docker.parent_image\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\na ZenML pipeline on SageMaker\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline \nZenML uses, by default, ml.t3.medium EC2 machines, which are part of the AWS freemium tier.\nIf you want to run the pipelines locally again, use the following CLI command:\nmanually building the Docker image and pushing it to ECR.\nprocess and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML.\nformatting, and gitleaks steps pass (also known as static analysis), we run the automated tests.\nNote that the static analysis steps run faster than the automated tests.",
    "keywords": [
      "AWS ECR",
      "AWS",
      "Docker image",
      "ECR",
      "AWS ECR dashboard",
      "Docker",
      "AWS ECR ZenML",
      "RUN",
      "ECR URL",
      "AWS ECR repository",
      "ZenML",
      "Poetry",
      "main AWS ECR",
      "image",
      "LLM Twin"
    ],
    "concepts": [
      "run",
      "running",
      "runs",
      "zenml",
      "pipeline",
      "pipelines",
      "poetry",
      "step",
      "steps",
      "install"
    ]
  },
  {
    "chapter_number": 50,
    "title": "Segment 50 (pages 456-464)",
    "start_page": 456,
    "end_page": 464,
    "summary": "Implementing a CI pipeline ensures that new features follow the repository’s standards and \nThe CD pipeline runs after the branch is merged.\ninto staging, the CD pipeline takes the code from the staging branch, builds a new Docker im-\nage, and pushes it to the AWS ECR Docker repository.\nthe staging environment, it will use the latest Docker image that was built by the CD pipeline.\nGitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their \nworkflows directly within a GitHub repository.\ncode directly from GitHub by defining workflows in YAML files.\nrepository’s .github/workflows directory.\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same \ncurrent GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\nuses: actions/setup-python@v3\nmight trigger a workflow every time code is pushed to a specific branch.\nhow GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.\nA test job that runs all our automatic tests using Pytest.\nGitHub Actions CI YAML file\nThe YAML file sits under .github/workflows/ci.yaml.\nHence, the CI workflow will automatically run \nThe concurrency section ensures that only one instance of this workflow runs for a given reference \ntrue line ensures that if a new workflow run is triggered before the previous one finishes, the \nThe workflow defines two separate jobs: qa and test.\nEach job runs on the latest version of Ubuntu, \nThe first job, named QA, is responsible for quality assurance tasks like code checks and format-\nWithin the qa job, the first step is to check out the repository’s code using the \nThis step is necessary to ensure that the job has access to the code \nsteps in the job will run in the correct Python environment.\nuses: actions/setup-python@v3\nThe workflow then installs Poetry using the abatilo/actions-poetry@v2 action, specifying the \nuses: abatilo/actions-poetry@v2\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the \nThe qa job then runs several quality checks on the code.\nrun: poetry poe gitleaks-check\nFollowing the gitleaks check, the workflow runs a linting process to enforce coding standards \nrun: poetry poe lint-check\nThe last step in the qa job is a format check, which ensures that the Python code is properly for-\nrun: poetry poe format-check\nThe second job defined in the workflow is the test job, which also runs on the latest version \nLike the qa job, it starts by checking out the code from the repository and installing \nFinally, the test job runs the project’s tests using the poetry poe test command.\n- name: Run tests\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting \nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository.\ncommit with the message feat: Add Docker image and CD pipeline and ran the two jobs de-\nFigure 11.15: GitHub Actions CI pipeline run example\nPush the Docker image to AWS ECR.\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under .github/workflows/\nThis workflow will automatically run when \nnew code is pushed to the main branch, usually when a PR is merged into the main branch.\nThe workflow then defines a single job named Build & Push Docker Image:\nname: Build & Push Docker Image\nThe first step within the job is to check out the repository’s code.\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends \nuses: docker/setup-buildx-action@v3\nrepository’s secrets to authenticate the workflow with AWS.\nnecessary permissions to push Docker images to the ECR repository.\nuses: aws-actions/configure-aws-credentials@v1\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR.\nuses: aws-actions/amazon-ecr-login@v1\nThe final step in the workflow involves building the Docker image and pushing it to the Ama-\nThis is accomplished using the docker/build-push-action@v6 action.\nuses: docker/build-push-action@v6",
    "keywords": [
      "Docker",
      "workflow",
      "Actions",
      "GitHub Actions",
      "Docker image",
      "Poetry",
      "code",
      "GitHub",
      "run",
      "AWS ECR Docker",
      "job",
      "pipeline",
      "Python",
      "Push Docker Image",
      "ECR"
    ],
    "concepts": [
      "actions",
      "action",
      "code",
      "coding",
      "jobs",
      "job",
      "runs",
      "running",
      "run",
      "workflows"
    ]
  },
  {
    "chapter_number": 51,
    "title": "Segment 51 (pages 465-475)",
    "start_page": 465,
    "end_page": 475,
    "summary": "To conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to \nIn Figure 11.16, you can observe how the CD pipeline looks after a PR is merged into the production \nFigure 11.16: GitHub Actions CD pipeline run example\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.\nTest out the CI/CD pipeline\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full \nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point \nThese secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\nAWS_ECR_NAME are the same ones used in the Deploying the LLM Twin’s pipelines to the cloud \nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create \nTo trigger the CD pipeline, merge the PR into the main branch.\nThe CT pipeline\nTo implement the CT pipeline, we will leverage ZenML.\nRemember the core difference between the CI/CD and CT pipelines.\nThe CI/CD pipeline takes care \nCT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, \nfor our pipelines and a way to monitor their execution.\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our \nFigure 11.19: CT pipeline\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines \nand how the pipelines are triggered by each other.\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline.\nREST API triggers: You can call a pipeline by an HTTP request.\nwhen integrating your ML pipelines with other components.\nZenML’s documentation: https://docs.zenml.io/v/docs/how-to/trigger-pipelines/\ntrigger-a-pipeline-from-rest-api.\nScheduled triggers: Another common approach is to schedule your pipeline to run con-\nfollowing example from ZenML, the pipeline is scheduled every hour:\nWhen it finds any, it generates a new config and triggers the pipelines through the REST API.\nother option is implementing the watcher as an additional pipeline and leveraging the schedule \nThe conclusion is that once you can manually trigger all your ML pipelines through a single \nTrigger downstream pipelines\ncollection pipeline has finished, it will trigger the feature pipeline.\nWhen the feature pipeline has \nbeen completed successfully, it triggers the dataset generation pipeline, and so on.\nthe logic more complex, like scheduling the generate instruct dataset pipeline to run daily, check-\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything \n@pipeline\nand deploy logic to the parent pipeline to implement an end-to-end flow.\nTo run the end-to-end pipeline, use the following poe command:\npoetry poe run-end-to-end-data-pipeline\npipeline (which we want to avoid), as illustrated in Figure 11.20.\npipeline isolated and use triggers to start downstream pipelines.\nFigure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\nwe have more, we avoided that limitation by compressing all the steps into a single pipeline.\nly trigger a pipeline from another pipeline, as you can see in the code snippet below where we \ntriggered the feature engineering pipeline after the data collection ETL:\nfrom zenml import pipeline, step\n@pipeline \ntrigger_feature_engineering_pipeline(user)\ndef trigger_feature_engineering_pipeline(user):\nClient().trigger_pipeline(\"feature_engineering\", run_configuration=run_\n@pipeline\ndef llm_chain(input_text: str) -> str:\ntags=[\"inference_pipeline\"],\nFigure 11.21: Inference pipeline serving architecture",
    "keywords": [
      "pipeline",
      "pipelines",
      "LLM",
      "LLM Twin",
      "data",
      "trigger",
      "str",
      "Docker image",
      "triggers",
      "AWS",
      "user",
      "ZenML",
      "GitHub Actions",
      "REST API triggers",
      "input"
    ],
    "concepts": [
      "pipeline",
      "pipelines",
      "zenml",
      "trigger",
      "triggered",
      "triggers",
      "user",
      "data",
      "llm",
      "steps"
    ]
  },
  {
    "chapter_number": 52,
    "title": "Segment 52 (pages 476-483)",
    "start_page": 476,
    "end_page": 483,
    "summary": "def call_llm_service(query: str, context: str | None) -> str:\nanswer = InferenceExecutor(llm, query, context).execute()\ndef rag(query: str) -> str:\nanswer = call_llm_service(query, context)\npost-processing steps, such as the ContextRetriever search function:\nMLOps and LLMOps\nquery_model = Query.from_str(query)\nquery_model = self._metadata_extractor.generate(query_model)\ndef generate(self, query: str) -> str:\ndef rag(query: str) -> str:\nanswer, prompt = call_llm_service(query, context)\nModel configuration: Here, we should consider both the LLM and other models used \nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such \nFor example, you can add a callback in your training pipeline to trigger \nfrom zenml import get_pipeline_context, pipeline\ndef training_pipeline(…):\nMLOps and LLMOps\nZenML and most orchestrators simplify implementing an alerter, as it’s a critical component \npipeline is, the three core dimensions of an ML application (code, data, model), and that, after \ndeployment, it is more critical than ever to implement a monitoring and alerting layer due to \nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud.\nThe final step was to add LLMOps to our LLM Twin project.\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an \nto any LLM-based application.\nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with \ndata collection and fine-tuning until deploying the LLM microservice and RAG service.\nMLOps: Continuous delivery and automation pipelines in machine learning.\nhttps://cloud.google.com/architecture/mlops-continuous-\nhttps://ml-ops.org/content/mlops-principles\nhttps://ml-ops.org/content/mlops-principles\n(2024c, July 5).\nhttps://ml-ops.org/content/motivation\nmadewithml.com/courses/mlops/monitoring/\nTesting Machine Learning Systems: Code, Data and Models.\nhttps://madewithml.com/courses/mlops/testing/\nMLOps and LLMOps\nUnderstanding LLMOps: Large Language Model Operations.\nGitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps over-\nview of ZenML pipelines from a Hugging Face model repository to a deployed AWS SageMaker \nGitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/\nBuilding robust and scalable ML systems requires more than creating powerful models.\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual \nan ML application.\nThe data scientist manually performs each pipeline step, such as data \npreparation and validation, model training, and testing.\nJupyter notebooks to train their models.\nthe data and train the models.\nContinuous training (CT): The next level involves automating model training.\nknown as continuous training, which triggers model retraining whenever required.\npoint, you often automate your data and model validation steps.\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable \nautomatic building, testing, and deployment of data, ML models, and training pipeline \nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can \nquickly move from a manual process to CI/CD/CT.\nOnce they improve the model by tinkering with how the data is processed \nor the model architecture, they push the code to the code repository, which triggers the CI/CD \npipeline to build, test, package, and deploy the new changes to the FTI pipelines.",
    "keywords": [
      "query",
      "llm",
      "model",
      "str",
      "data",
      "pipeline",
      "MLOps",
      "LLM Twin",
      "track def rag",
      "track def",
      "context",
      "models",
      "application",
      "rag",
      "LLMOps"
    ],
    "concepts": [
      "model",
      "models",
      "data",
      "pipelines",
      "monitoring",
      "monitor",
      "application",
      "applications",
      "zenml",
      "build"
    ]
  },
  {
    "chapter_number": 53,
    "title": "Segment 53 (pages 484-491)",
    "start_page": 484,
    "end_page": 491,
    "summary": "By now, we understand that the whole ML system changes if the code, model, or data changes.\nwe adopt to track the code, model, and data separately?\nmodels used within your system.\nmodel, such as what data it was trained on, its architecture, performance, latency, and \nVersioning the data isn’t as straightforward as versioning the code and model because it \nsolutions are based on Git-like systems, such as Data Version Control (DVC), that track \ncreating a new version for every change made to your data.\nTraining ML models is an entirely iterative and experimental process.\nmodel.\n4. Testing\nThe same trend is followed when testing ML systems.\nall three dimensions: the data, the model, and the code.\noverall user experience—for example, testing an entire ML pipeline, from data ingestion \nto model training and inference, ensuring the system produces the correct outputs for \nRegression tests: These tests check for previously identified errors to ensure that new \nWhat do we test?\nYou want to test that you get an expected output for a given \nTest examples\nWhen we talk about data tests, we mainly refer to data validity.\nThus, by writing integration or system tests for your feature pipeline, \nTesting data validity depends a lot on your application and data type.\nModel tests are the trickiest, as model training is the most non-deterministic process of an ML \nSome standard model test techniques involve checking:\nThe shapes of the input and model output tensors\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which \ntries to adopt the strategy from code testing and treats the model as a black box while looking \nsolely at the input data and expected outputs.\nThis makes the behavioral testing methods model \nA fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models \nquick overview, the paper proposes that you test your model against three types of tests.\nmodel(text=\"The advancements in AI are changing the world rapidly.\")\nmodel(text=\"The progress in AI is changing the world rapidly.\")\nexample where we know the outputs should change based on the provided inputs:\nfor example, below is a set of simple examples that we expect the model should always \nData, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/\ntesting/.\nmeans that our ML model will constantly be exposed to a level of degradation.\nbecause the data from production might differ from the data the model was trained on.\nIntuitively, monitoring detects the model’s performance degradation, which triggers an alarm that \nWhy retrain the model?\nAs the model performance degrades due to a drift in the training dataset \nand what it inputs from production, the only solution is to adapt or retrain the model on a new \ndifferent aspects of your application, such as the infrastructure, data, and model.\nModel metrics\nmodel.\nTherefore, moving on to the next layer of metrics that focus on the model’s performance \nwell as essential business metrics influenced by the model, such as ROI and click rate.\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance \nDrifts are proxy metrics that help us detect potential issues with the production model in time \nTable A.1: Relationship between data, model, and code changes\nData drift\nmodel cannot handle the changes in feature space, leading to potentially unreliable predictions.\nDrift can result from natural real-life changes or systemic problems like missing data, pipeline \nFigure A.3: Data drift examples\nWhen data begins to drift, the degradation in our model’s performance might not be immediately \nchance to consider retraining before the drift affects the model’s performance.\nIn addition to changes in input data (data drift), we might also encounter shifts in output dis-\nand model head to support the new schema of the output class.",
    "keywords": [
      "model",
      "data",
      "system",
      "data drift",
      "output",
      "drift",
      "code",
      "metrics",
      "System tests",
      "version",
      "performance",
      "outputs",
      "models",
      "systems",
      "pipeline"
    ],
    "concepts": [
      "data",
      "model",
      "models",
      "outputs",
      "output",
      "version",
      "metrics",
      "metric",
      "performance",
      "perform"
    ]
  },
  {
    "chapter_number": 54,
    "title": "Segment 54 (pages 492-500)",
    "start_page": 492,
    "end_page": 500,
    "summary": "Concept drift\nIn addition to changes in input and output data, their relationship can also shift.\nenon, known as concept drift, makes our model ineffective because the patterns it previously \nconcept drifts can manifest in various ways:\nFigure A.4: Concept drift examples\nFor example, this happens when using the model in a different geographic area.\nbuy smaller cars, creating a drift between the size feature of the car and the output probability of \nOf course, concept drifts can be more subtle than this example.\nHow to detect and measure drifts\nNow that we’ve recognized the various types of drift, it’s crucial to understand how to detect and \nA reference window: This is the collection of data points used as a baseline to compare \nagainst the production data distributions for drift identification.\nA test window: This collects data points gathered while the ML system is in production.\nIt is compared with the reference window to ascertain if drift has occurred.\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between \nFor example, you can use the Kolmogorov-Smirnov (KS) test to monitor a \nwindow distribution.\nWhen working with text data in an embedding representation, we have to model a multivariate \nthe test and reference windows, apply a dimensionality reduction algorithm, and apply an algo-\nmean of the embeddings of the two windows.\nMonitoring involves the collection and visualization of data, whereas observability provides in-\nTweaking the p-value of the statistical tests that check for drifts.\ntriggered the alarm, with what value, the time it happened, and anything else that makes sense \nand train the model on the newly shifted dataset to solve the drift.\nto train the model.\ntraining a model from scratch, all the weights are initially randomly initialized.\nyou use the same dataset and hyperparameters, you might end up with a model with a differ-\nrandom values or randomly remove data or labels.\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\nOther Books \nIf you enjoyed this book, you may be interested in these other books by Packt:\nOther Books You May Enjoy\nControl and build robust generative AI systems grounded in real-world data\nCombine text and image data for richer, more informative AI responses\nOther Books You May Enjoy\nOther Books You May Enjoy",
    "keywords": [
      "Concept drift",
      "drift",
      "data",
      "model",
      "books",
      "system",
      "drifts",
      "Concept",
      "LLM",
      "book",
      "reference window",
      "Enjoy",
      "reference",
      "alarm",
      "window"
    ],
    "concepts": [
      "data",
      "monitor",
      "monitoring",
      "randomly",
      "randomness",
      "llm",
      "drift",
      "drifts",
      "values",
      "value"
    ]
  },
  {
    "chapter_number": 55,
    "title": "Segment 55 (pages 501-509)",
    "start_page": 501,
    "end_page": 509,
    "summary": "advanced RAG post-retrieval optimization\nadvanced RAG retrieval optimization\nLLM Twin model, deploying to  375-385\nCI/CD pipeline  462\nCI pipeline, LLM Twin\ndeployment (CI/CD) pipeline  31, 402\nversus LLM Twin  4\ndata augmentation  193-196\ndata collection pipeline  19\ndata curation  182\ndata deduplication  184, 185\ndata drift  470\ndata evaluation  233\ndata exploration  189-191\ndata generation  191-233\npreference data, evaluating  235-237\npreference data, generating  233, 234\ndata indexing techniques  119\ndata quality evaluation  186-189\ndata quantity  180, 181\ndata tests  466\ndata  357\ndomain-specific LLM evaluations  265-267\ndownstream pipelines\nhuman-generated, LLM-evaluated \nLLM-generated, human-evaluated \nLLM-generated, LLM-evaluated datasets  234\ndata drift  470\nembedding handlers  169-173\nend-to-end RAG inference pipeline\nETL pipeline\nconnecting, to feature pipeline  60\nExtract, Transform, Load (ETL) pipeline  55\nfeature pipeline  14, 19, 20\nfeature pipeline  14\ninference pipeline  14\ntraining pipeline  14\nused, for building LLM system  462, 463\nFTI pipeline design\nLLM Twin architecture, designing  17\nFTI pipelines architecture\ninference pipeline  14\nembedding handlers  169-173\nhuman-generated, LLM-evaluated \ninference pipeline  22\nversus training pipeline  371, 372\ndata augmentation  193-196\ndata curation  182\ndata deduplication  184, 185\ndata exploration  189-191\ndata generation  191, 193\ndata quality evaluation  186-189\nlarge language model (LLM)  1, 99, 355, 401\nLLM evaluation  235\nversus, ML evaluation  262, 263\nLLM-generated, human-evaluated \nLLM-generated, LLM-evaluated datasets  234\nLLM system\nLLM Twin  2, 5, 6\nCD pipeline  442-444\nCI/CD pipeline flow  434, 435\nCI/CD pipeline, testing  445\nCI pipeline  438\nCT pipeline  446, 448\ninference pipeline deployment \nRAG feature pipeline architecture  127, 139\nLLM Twin architecture  23\ndata collection pipeline  19\ndesigning, with FTI pipeline design  17\nfeature pipeline  19, 20\ninference pipeline  22\ntraining pipeline  21, 22\nLLM Twin model\nLLM Twin RAG feature pipeline\nsetting  139\nZenML pipeline and steps  140, 141\nLLM Twin's data collection pipeline\nZenML pipeline and steps  61-65\nLLM Twin's pipelines, cloud deployment  415\npipelines, running on AWS  428-431\npipeline on SageMaker  432, 433\nvesus, LLM evaluation  262, 263\nML models\nCI/CD pipeline  462\nML pipeline orchestrator  407\nML pipeline automation\nML pipelines\nmodel evaluation  261\ndomain-specific LLM evaluations  265-267\nML, versus LLM evaluation  262, 263\ntask-specific LLM evaluations  267-271\nmodel optimization strategies  290\nmodel quantization  303, 304\nmodel tests  466\nsetting up  418, 419\nMongoDB, as data warehouse\nmonolithic batch pipeline architecture  10\nclasses  87-89\ndata evaluation  233\ndata generation  233\ndata indexing  119\nquery optimizing  119\ndata category  151\nsetting up  419, 420\nquery optimization  120\nRAG feature pipeline\ndata extraction  134\ndata loading  135\ndata warehouse and feature store, \nRAG feature pipeline architecture\nbatch pipelines  130\nbatch pipelines, versus streaming \npipelines  130-134\ninference pipeline  127\ningestion pipeline  127",
    "keywords": [
      "LLM Twin",
      "LLM",
      "pipeline",
      "data",
      "LLM Twin RAG",
      "LLM Twin architecture",
      "LLM Twin model",
      "RAG feature pipeline",
      "LLM evaluations",
      "Twin",
      "RAG",
      "LLM Twin pipelines",
      "LLM Twin service",
      "advanced RAG",
      "feature pipeline"
    ],
    "concepts": [
      "data",
      "pipeline",
      "pipelines",
      "llm",
      "models",
      "optimization",
      "optimizations",
      "optimized",
      "optimal",
      "optimizing"
    ]
  },
  {
    "chapter_number": 56,
    "title": "Segment 56 (pages 510-517)",
    "start_page": 510,
    "end_page": 517,
    "summary": "RAG inference pipeline\npolicy optimization  246\nreward model learning  246\nembeddings  107, 108\nembeddings, applications  114\nembeddings, creating  111-114\nretrieval-augmented generation (RAG) \npipeline  206, 261\nreward model learning  246\nSageMaker Orchestrator  423\nscalable policy\nscalable target\nSFT, techniques\ntechniques  211\nsystem tests  464\nsystem tests  464\ntest window  472\nTime between Tokens (TBT)  413\nTime per Output Token (TPOT)  413\nTime to First Token (TTFT)  413\ntraining pipeline  14, 21, 22\nversus inference pipeline  371, 372\ntriggers\nanswers, evaluating  278-283\nevaluating  275, 276\nalgorithms, for creating vector index  116\ntest window  472\nZenML pipeline  140-142\ncleaned documents, embedding  147-150\nDownload a free PDF copy of this book\nThanks for purchasing this book!\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\nhttps://packt.link/free-ebook/9781836200079\nWe’ll send your free PDF and other benefits to your email directly.",
    "keywords": [
      "reward model learning",
      "RAG",
      "REST API triggers",
      "Learning",
      "model",
      "inference",
      "REST API",
      "Understudy for Gisting",
      "triggers",
      "Retrieval-Augmented Generation",
      "model learning",
      "RAG inference pipeline",
      "Index",
      "reward model",
      "Generation"
    ],
    "concepts": [
      "model",
      "models",
      "triggers",
      "free",
      "book",
      "books",
      "evaluation",
      "evaluations",
      "evaluating",
      "window"
    ]
  },
  {
    "chapter_number": 57,
    "title": "Segment 57 (pages 518-522)",
    "start_page": 518,
    "end_page": 522,
    "summary": "Chapter 57: Segment 57 (pages 518-522)",
    "keywords": [],
    "concepts": []
  }
]