[
  {
    "chapter_number": 1,
    "title": "Segment 1 (pages 1-9)",
    "start_page": 1,
    "end_page": 9,
    "summary": "Software Architecture: The Hard Parts\nPraise for Software Architecture: The Hard Parts\n“Software Architecture: The Hard Parts provides the reader with valuable insight, practices, and real-world examples on pulling apart highly coupled systems and building them back up again.\n“This book will equip you with the theoretical background and with a practical framework to help answer the most difficult questions faced in modern software architecture.”\nSoftware Architecture: The Hard Parts Modern Trade-Off Analysis for Distributed Architectures\nSoftware Architecture: The Hard Parts by Neal Ford, Mark Richards, Pramod Sadalage, and Zhamak Dehghani\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Software Architecture: The Hard Parts, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n2 Giving Timeless Advice About Software Architecture 3 The Importance of Data in Architecture 4 Architectural Decision Records 5 Architecture Fitness Functions 6 Using Fitness Functions 7 Architecture Versus Design: Keeping Definitions Simple 13 Introducing the Sysops Squad Saga 15 Nonticketing Workflow 16 Ticketing Workflow 17 A Bad Scenario 17 Sysops Squad Architectural Components 18 Sysops Squad Data Model 19\n25 Architecture (Quantum | Quanta) 28 Independently Deployable 29 High Functional Cohesion 30 High Static Coupling 30 Dynamic Quantum Coupling 38 Sysops Squad Saga: Understanding Quanta 42\n81 Identify and Size Components Pattern 84 Pattern Description 84 Fitness Functions for Governance 87 Sysops Squad Saga: Sizing Components 90 Gather Common Domain Components Pattern 94 Pattern Description 94 Fitness Functions for Governance 95 Sysops Squad Saga: Gathering Common Components 97 Flatten Components Pattern 101 Pattern Description 102 Fitness Functions for Governance 107 Sysops Squad Saga: Flattening Components 107 Determine Component Dependencies Pattern 111 Pattern Description 112 Fitness Functions for Governance 117 Sysops Squad Saga: Identifying Component Dependencies 118 Create Component Domains Pattern 120 Pattern Description 121 Fitness Functions for Governance 122 Sysops Squad Saga: Creating Component Domains 123 Create Domain Services Pattern 126 Pattern Description 126 Table of Contents\n81 Identify and Size Components Pattern 84 Pattern Description 84 Fitness Functions for Governance 87 Sysops Squad Saga: Sizing Components 90 Gather Common Domain Components Pattern 94 Pattern Description 94 Fitness Functions for Governance 95 Sysops Squad Saga: Gathering Common Components 97 Flatten Components Pattern 101 Pattern Description 102 Fitness Functions for Governance 107 Sysops Squad Saga: Flattening Components 107 Determine Component Dependencies Pattern 111 Pattern Description 112 Fitness Functions for Governance 117 Sysops Squad Saga: Identifying Component Dependencies 118 Create Component Domains Pattern 120 Pattern Description 121 Fitness Functions for Governance 122 Sysops Squad Saga: Creating Component Domains 123 Create Domain Services Pattern 126 Pattern Description 126 Table of Contents\nFitness Functions for Governance 129 Sysops Squad Saga: Creating Domain Services 129 Summary 130\n131 Data Decomposition Drivers 132 Data Disintegrators 133 Data Integrators 146 Sysops Squad Saga: Justifying Database Decomposition 150 Decomposing Monolithic Data 151 Step 1: Analyze Database and Create Data Domains 156 Step 2: Assign Tables to Data Domains 156 Step 3: Separate Database Connections to Data Domains 158 Step 4: Move Schemas to Separate Database Servers 159 Step 5: Switch Over to Independent Database Servers 161 Selecting a Database Type 161 Relational Databases 163 Key-Value Databases 165 Document Databases 167 Column Family Databases 169 Graph Databases 171 NewSQL Databases 173 Cloud Native Databases 175 Time-Series Databases 177 Sysops Squad Saga: Polyglot Databases 179\n185 Granularity Disintegrators 188 Service Scope and Function 189 Code Volatility 191 Scalability and Throughput 192 Fault Tolerance 193 Security 195 Extensibility 196 Granularity Integrators 197 Database Transactions 198 Workflow and Choreography 200 Shared Code 203 Data Relationships 205 Finding the Right Balance 208 Sysops Squad Saga: Ticket Assignment Granularity 209 Sysops Squad Saga: Customer Registration Granularity 212 vii\n. 219 Code Replication 221 When to Use 223 Shared Library 223 Dependency Management and Change Control 224 Versioning Strategies 225 When To Use 227 Shared Service 228 Change Risk 229 Performance 231 Scalability 232 Fault Tolerance 232 When to Use 234 Sidecars and Service Mesh 234 When to Use 239 Sysops Squad Saga: Common Infrastructure Logic 239 Code Reuse: When Does It Add Value?\n. 249 Assigning Data Ownership 250 Single Ownership Scenario 251 Common Ownership Scenario 252 Joint Ownership Scenario 253 Table Split Technique 254 Data Domain Technique 256 Delegate Technique 258 Service Consolidation Technique 261 Data Ownership Summary 262 Distributed Transactions 263 Eventual Consistency Patterns 267 Background Synchronization Pattern 269 Orchestrated Request-Based Pattern 272 Event-Based Pattern 277 Sysops Squad Saga: Data Ownership for Ticket Processing 279",
    "keywords": [
      "Sysops Squad Saga",
      "Squad Saga",
      "Sysops Squad",
      "Pattern Description",
      "Fitness Functions",
      "Pattern",
      "Squad",
      "Saga",
      "Software Architecture",
      "Domain Components Pattern",
      "Component Domains Pattern",
      "Functions for Governance",
      "Components Pattern",
      "Hard Parts",
      "Sysops"
    ],
    "concepts": [
      "database",
      "databases",
      "patterns",
      "pattern",
      "architecture",
      "architectures",
      "architectural",
      "data",
      "saga",
      "components"
    ]
  },
  {
    "chapter_number": 2,
    "title": "Segment 2 (pages 10-18)",
    "start_page": 10,
    "end_page": 18,
    "summary": "When two of your authors, Neal and Mark, were writing the book Fundamentals of Software Architecture, we kept coming across complex examples in architecture that we wanted to cover but that were too difficult.\nWe set those examples aside into a pile we called “The Hard Parts.” Once that book was finished, we looked at the now gigantic pile of hard parts and tried to figure out: why are these problems so difficult to solve in modern architectures?\nWe took all the examples and worked through them like architects, applying trade-off analysis for each situation, but also paying attention to the process we used to arrive at the trade-offs.\nTo that end, we asked experts in those fields to join us, which allows this book to fully incorporate decision making from both angles: architecture to data and data to architecture.\nThe result is this book: a collection of difficult problems in modern software architec‐ ture, the trade-offs that make the decisions hard, and ultimately an illustrated guide to show you how to apply the same trade-off analysis to your own unique problems.\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\nIn general, if example code is offered with this book, you may use it in your programs and documentation.\nFor example, writing a program that uses several chunks of code from this book does not require permission.\nSelling or distributing examples from O’Reilly books does require permission.\nAnswering a question by citing this book and quoting example code does not require permission.\nIncorporating a significant amount of example code from this book into your product’s documentation does require permission.\nFor example: “Software Architecture: The Hard Parts by Neal Ford, Mark Richards, Pramod Sadalage, and Zhamak Deh‐ ghani (O’Reilly).\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\nFor news and information about our books and courses, visit http://oreilly.com.\nAcknowledgments Mark and Neal would like to thank all the people who attended our (almost exclu‐ sively online) classes, workshops, conference sessions, and user group meetings, as well as all the other people who listened to versions of this material and provided invaluable feedback.\nWe thank the publishing team at O’Reilly, who made this as painless an experience as writing a book can be.\nAcknowledgments from Mark Richards In addition to the preceding acknowledgments, I once again thank my lovely wife, Rebecca, for putting up with me through yet another book project.\nYour unending support and advice helped make this book happen, even when it meant taking time away from working on your own novel.\nLastly, I thank my wife, Candy, who continues to support this lifestyle that has me staring at things like book writing rather than our cats too much.\nWhy does a technologist like a software architect present at a conference or write a book?\nRegardless of the term, technologists write books when they have figured out a novel solution to a general problem and want to broadcast it to a wider audience.\nEntire classes of problems exist in software architecture that have no general good solutions, but rather present one messy set of trade-offs cast against an (almost) equally messy set.\nArchitects may have wondered why so few books exist about architecture compared to technical topics like frameworks, APIs, and so on.\nBecause virtually every problem presents novel challenges, the real job of an architect lies in their ability to objectively determine and assess the set of trade-offs on either side of a consequential decision to resolve it as well as possible.\nThe authors don’t talk about “best solutions” (in this book or in the real world) because “best” implies that an architect has managed to maximize all the possible competing factors within the design.\nWhich begs the question: “How can an architect find the least worst combination of trade-offs (and document them effectively)?” This book is primarily about decision making, enabling architects to make better decisions when confronted with novel situations.\nWhy did we call this book Software Architecture: The Hard Parts?",
    "keywords": [
      "Sysops Squad Saga",
      "Replicated Caching Pattern",
      "Ticket Assignment",
      "book",
      "Squad Saga",
      "Saga",
      "Data",
      "Caching Pattern",
      "Sysops Squad",
      "Replicated Caching",
      "Data Mesh",
      "Architecture",
      "Pattern",
      "Neal Ford",
      "Software Architecture"
    ],
    "concepts": [
      "saga",
      "sagas",
      "pattern",
      "patterns",
      "data",
      "architects",
      "architect",
      "book",
      "books",
      "architecture"
    ]
  },
  {
    "chapter_number": 3,
    "title": "Segment 3 (pages 19-26)",
    "start_page": 19,
    "end_page": 26,
    "summary": "The Importance of Data in Architecture\nFor many in architecture, data is everything.\nEvery enterprise building any system must deal with data, as it tends to live much longer than systems or architecture, requiring diligent thought and design.\nFor example, architects and DBAs must ensure that business data sur‐ vives the breaking apart of monolith systems and that the business can still derive value from its data regardless of architecture undulations.\nThis reliance on data means that all software architecture is in the service of data, ensuring the right data is available and usable by all parts of the enterprise.\nHowever, in microservices and the philosophical adherence to a bounded context from Domain-Driven Design, as a way of limiting the scope of implementation detail coupling, data has moved to an architectural concern, along with transactionality.\nWe will be leveraging ADRs as a way of documenting various architecture decisions made throughout the book.\nFortunately, modern engineering practices allow automating many common governance concerns by using architecture fitness functions.\nArchitecture Fitness Functions Once an architect has identified the relationship between components and codified that into a design, how can they make sure that the implementers will adhere to that design?\nThese questions fall under the heading of architecture governance, which applies to any organized oversight of one or more aspects of software development.\nAs this book primarily covers architecture structure, we cover how to automate design and quality principles via fitness functions in many places.\nFor example, if an architect chooses a particu‐ lar architecture style or communication medium, how can they make sure that a\nWhen done manually, architects perform code reviews or perhaps hold architecture review boards to assess the state of governance.\nUsing Fitness Functions In the 2017 book Building Evolutionary Architectures (O’Reilly), the authors (Neal Ford, Rebecca Parsons, and Patrick Kua) defined the concept of an architectural fit‐ ness function: any mechanism that performs an objective integrity assessment of some architecture characteristic or combination of architecture characteristics.\nArchitects can use a wide variety of tools to implement fitness functions; we will show numerous examples throughout the book.\nFor example, dedicated testing libraries exist to test architecture structure, architects can use monitors to test operational architecture characteristics such as performance or scalability, and chaos engineering frameworks test reliability and resiliency.\nFor example, an architect can’t specify that they want a “high performance” website; they must provide an object value that can be measured by a test, monitor, or other fitness function.\nIf architects strive toward measurable properties, it allows them to automate fitness function application.\nSome architecture characteristic or combination of architecture characteristics This characteristic describes the two scopes for fitness functions:\nThese fitness functions handle a single architecture characteristic in isola‐ tion.\nFor example, a fitness function that checks for component cycles within a codebase is atomic in scope.\nArchitecture Fitness Functions\nHolistic fitness functions validate a combination of architecture characteris‐ tics.\nHolistic fitness functions exercise a combination of interlocking architecture characteristics to ensure that the combined effect won’t negatively affect the architecture.\nAn architect implements fitness functions to build protections around unexpected change in architecture characteristics.\nIn the Agile software development world, developers implement unit, functional, and user acceptance tests to validate different dimensions of the domain design.\nFitness functions validate architecture characteristics, not domain criteria; unit tests are the opposite.\nThus, elasticity is an architectural concern and within the scope of a fitness function.\nArchitecture Fitness Functions\ntant rather than urgent practices of software development: it’s an important concern for architects, yet has little impact on day-to-day coding.\nHowever, architects may also want to validate the macro struc‐ ture of the architecture as well as the micro.\nWhen designing a layered architecture such as the one in Figure 1-2, the architect defines the layers to ensure separation of concerns.\nArchUnit allows architects to address this problem via a fitness function, shown in Example 1-2.",
    "keywords": [
      "architecture fitness functions",
      "architecture",
      "fitness functions",
      "architecture characteristics",
      "Data",
      "software architecture",
      "fitness",
      "software development",
      "architecture fitness",
      "architecture decision",
      "software",
      "functions",
      "architect",
      "Architectural Decision Records",
      "function"
    ],
    "concepts": [
      "architectures",
      "architectural",
      "architects",
      "architect",
      "development",
      "develop",
      "developer",
      "developers",
      "data",
      "function"
    ]
  },
  {
    "chapter_number": 4,
    "title": "Segment 4 (pages 27-35)",
    "start_page": 27,
    "end_page": 35,
    "summary": "In Example 1-2, the architect defines the desirable relationship between layers and writes a verification fitness function to govern it.\nThus, for these systems, architects design perfor‐ mance fitness functions that take into account the number of concurrent users.\nArchitecture Fitness Functions\nContinuity is important, as illustrated in this example of enterprise-level governance using fitness functions.\nBy codifying rules about code quality, structure, and other safeguards against decay into fitness functions that run continually, architects build a quality checklist that developers can’t skip.\nFitness func‐ tions represent a checklist of important principles defined by architects and run as part of the build to make sure developers don’t accidentally (or purposefully, because of external forces like schedule pressure) skip them.\nWe utilize fitness functions throughout the book when an opportunity arises to illus‐ trate governing an architectural solution as well as the initial design.\nTo that end, we need a problem to illustrate architecture con‐ cepts against—which leads us to the Sysops Squad.\nWe use the Sysops Squad saga within each chapter to illustrate the techniques and trade-offs described in this book.\nTherefore, our story starts with the existing Sysops Squad architecture highlighted here.\noccur, customer-facing technology experts (the Sysops Squad) come to the customer’s residence (or work office) to fix problems with the electronic device.\nThe four main users of the Sysops Squad ticketing application are as follows:\nThe adminis‐ trator also manages all of the billing processing for customers using the system, and maintains static reference data (such as supported products, name-value pairs in the system, and so on).\nThe customer registers for the Sysops Squad service and maintains their cus‐ tomer profile, support contracts, and billing information.\nCustomers enter prob‐ lem tickets into the system, and also fill out surveys after the work has been completed.\nSysops Squad expert\nExperts are assigned problem tickets and fix problems based on the ticket.\nThe manager keeps track of problem ticket operations and receives operational and analytical reports about the overall Sysops Squad problem ticket system.\nNonticketing Workflow The nonticketing workflows include those actions that administrators, managers, and customers perform that do not relate to a problem ticket.\n1. Sysops Squad experts are added and maintained in the system through an administrator, who enters in their locale, availability, and skills.\n2. Customers register with the Sysops Squad system and have multiple support plans based on the products they purchased.\nTicketing Workflow The ticketing workflow starts when a customer enters a problem ticket into the sys‐ tem, and ends when the customer completes the survey after the repair is done.\n1. Customers who have purchased the support plan enter a problem ticket by using the Sysops Squad website.\n2. Once a problem ticket is entered in the system, the system then determines which Sysops Squad expert would be the best fit for the job based on skills, cur‐ rent location, service area, and availability.\n3. Once assigned, the problem ticket is uploaded to a dedicated custom mobile app on the Sysops Squad expert’s mobile device.\nThe expert is also notified via a text message that they have a new problem ticket.\n4. The customer is notified through an SMS text message or email (based on their profile preference) that the expert is on their way.\n5. The expert uses the custom mobile application on their phone to retrieve the ticket information and location.\nThe Sysops Squad expert can also access a knowledge base through the mobile app to find out what has been done in the past to fix the problem.\n6. Once the expert fixes the problem, they mark the ticket as “complete.” The sysops squad expert can then add information about the problem and repair the knowl‐ edge base.\n7. After the system receives notification that the ticket is complete, it sends an email to the customer with a link to a survey, which the customer then fills out.\nA Bad Scenario Things have not been good with the Sysops Squad problem ticket application lately.\nCustomers have also been complaining that the system is not always available to enter new problem tickets.\nBecause of reliability issues, the Sysops Squad system frequently “freezes up,” or crashes, resulting in all application functionality not being available anywhere from five minutes to two hours while the problem is identified and the application restarted.\nIf something isn’t done soon, Penultimate Electronics will be forced to abandon the very lucrative support contract business line and lay off all the Sysops Squad adminis‐ trators, experts, managers, and IT development staff—including the architects.\nSysops Squad Architectural Components The monolithic system for the Sysops Squad application handles ticket management, operational reporting, customer registration, and billing, as well as general adminis‐ trative functions such as user maintenance, login, and expert skills and profile main‐ tenance.\nss.customer.profile\nss.ticket\nss.ticket.assign\nss.ticket.notify\nNotify customer that the expert is on their way\nSysops Squad Data Model The Sysops Squad application with its various components listed in Table 1-1 uses a single schema in the database to host all its tables and related database code.\nThe database is used to persist customers, users, contracts, billing, payments, knowledge base, and customer surveys; the tables are listed in Table 1-2, and the ER model is illustrated in Figure 1-4.",
    "keywords": [
      "Sysops Squad",
      "Sysops Squad expert",
      "Sysops Squad Saga",
      "Sysops Squad application",
      "Sysops Squad problem",
      "Sysops Squad system",
      "fitness functions",
      "existing Sysops Squad",
      "Squad",
      "Sysops",
      "Sysops Squad architecture",
      "Squad problem ticket",
      "Sysops Squad Data",
      "Squad expert",
      "Sysops Squad service"
    ],
    "concepts": [
      "customers",
      "customer",
      "custom",
      "architecture",
      "architectural",
      "architectures",
      "ticketing",
      "tickets",
      "ticket",
      "functions"
    ]
  },
  {
    "chapter_number": 5,
    "title": "Segment 5 (pages 36-45)",
    "start_page": 36,
    "end_page": 45,
    "summary": "To understand complex subjects (such as trade-offs in distributed architectures), an architect must figure out where to start untangling.\nStatic coupling refers to the way architectural parts (classes, components, services, and so on) are wired together: dependencies, coupling degree, connection points, and so on.\nAn architect can often measure static coupling at compile time as it represents the static depen‐ dencies within the architecture.\nDynamic coupling refers to how architecture parts call one another: what kind of communication, what information is passed, strictness of contracts, and so on.\nOur goal is to investigate how to do trade-off analysis in distributed architectures; to do that, we must pull the moving pieces apart so that we can discuss them in isolation to understand them fully before putting them back together.\nPart I primarily deals with architectural structure, how things are statically coupled together.\nIn Chapter 2, we tackle the problem of defining the scope of static and dynamic coupling in architectures, and present the entire picture that we must pull apart to understand.\nData and transactions have become increasingly important in architecture, driving many trade-off decisions by architects and DBAs. Chapter 6 addresses the architec‐ tural impacts of data, including how to reconcile service and data boundaries.\nFinally, Chapter 7 ties together architecture coupling with data concerns to define integrators and disintegrators—forces that encourage a larger or smaller service size and boundary.\nCHAPTER 2 Discerning Coupling in Software Architecture\nDistributed architectures like microservices are difficult, especially if architects cannot untangle all the forces at play.\nWhat we need is an approach or framework that helps us figure out the hard problems in our architecture.”\nOne of the most difficult tasks an architect will face is untangling the various forces and trade-offs at play in distibuted architectures.\nWhy have architects struggled with decisions in distributed architectures?\nBuilding services that model bounded contexts required a subtle but important change to the way architects designed distributed systems because now transactionality is a first-class architectural concern.\nWhile several frameworks have existed for decades (such as Architecture Trade-off Analysis Method, or ATAM), they lack focus on real problems architects face on a daily basis.\nChapter 2: Discerning Coupling in Software Architecture\nAs in many things in architecture, the advice is simple; the hard parts lie in the details, particularly how difficult parts become entan‐ gled, making it difficult to see and understand the individual parts, as illustrated in Figure 2-1.\nDiscerning Coupling in Software Architecture\nWe start with our first great untangling of forces in dis‐ tributed architectures: defining architecture quantum along with the two types of coupling, static and dynamic.\nAn architecture quantum measures several aspects of both topology and behavior in software architecture related to how parts connect and communicate with one another:\nArchitecture quantum\nAn architecture quantum is an independently deployable artifact with high func‐ tional cohesion, high static coupling, and synchronous dynamic coupling.\nA common example of an architecture quantum is a well-formed microservice within a workflow.\nRepresents how static dependencies resolve within the architecture via contracts.\nChapter 2: Discerning Coupling in Software Architecture\nFor example, in a microservices architecture, a service must contain dependent components such as a database, repre‐ senting static coupling—the service isn’t operational without the necessary data.\nWithin a distributed architecture such as microservices, developers tend toward the ability to deploy services independently, often in a highly automated way.\nThus, from an independently deployable standpoint, a service within a microservices architecture represents an architecture quantum (contingent on cou‐ pling—as discussed next).\nMaking each architecture quantum represent a deployable asset within the architec‐ ture serves several useful purposes.\nSecond, the architecture quantum represents one of the forces (static coupling) archi‐ tects must consider when striving for proper granularity of services within a dis‐ tributed architecture.\nOften, in microservices architectures, developers face the difficult question of what service granularity offers the optimum set of trade-offs.\nThird, independent deployability forces the architecture quantum to include common coupling points such as databases.\nThus, any system that uses a shared database fails the architecture quantum criteria for independent deployment unless the database deployment is in lockstep with the application.\nArchitects should also consider the second criteria for an architecture quantum, high functional cohesion, to limit the architecture quantum to a useful scope.\nIdeally, in a microservices architecture, each service models a single domain or work‐ flow, and therefore exhibits high functional cohesion.\nHigh Static Coupling High static coupling implies that the elements inside the architecture quantum are tightly wired together, which is really an aspect of contracts.\nThus, contracts are an architecture hard part; we cover coupling issues involving all types of contracts, including how to choose appropriate ones, in Chapter 13.\nChapter 2: Discerning Coupling in Software Architecture",
    "keywords": [
      "architecture",
      "architecture quantum",
      "Sysops Profile Profile",
      "payment Payment Payments",
      "coupling",
      "Software Architecture",
      "Sysops Squad",
      "existing Sysops Squad",
      "Static coupling",
      "Sysops Location Locations",
      "survey Billing Billing",
      "survey Survey",
      "Sysops support Customer",
      "Sysops",
      "quantum"
    ],
    "concepts": [
      "architecture",
      "architectures",
      "coupling",
      "services",
      "service",
      "architect",
      "architects",
      "deployed",
      "deploy",
      "deployment"
    ]
  },
  {
    "chapter_number": 6,
    "title": "Segment 6 (pages 46-53)",
    "start_page": 46,
    "end_page": 53,
    "summary": "For example, the following diagrams show the architecture styles featured in Fundamentals of Software Architecture, with the architecture quantum static coupling illustrated.\nThe architecture quantum measure of static coupling includes the database, and a system that relies on a single database cannot have more than a single quantum.\nThus, the static coupling measure of an architecture quantum helps identify coupling points in architecture, not just within\nMost monolithic architectures contain a single coupling point (typically, a database) that makes its quantum measure one.\nWhile this individual services model shows the isolation common in microservices, the architecture still utilizes a single relational database, rendering its architecture quantum score to one.\nSo far, the static coupling measurement of architecture quantum has evaluated all the topologies to one.\nFor example, the mediator style of event- driven architecture will always be evaluated to an single architecture quantum, as illustrated in Figure 2-4.\nEven though this style represents a distributed architecture, two coupling points push it toward a single architecture quantum: the database, as common with the previous monolithic architectures, but also the Request Orchestrator itself—any holistic coupling point necessary for the architecture to function forms an architecture quan‐ tum around it.\nThis broker-style event driven architecture (without a central mediator) is neverthe‐ less a single architecture quantum because all the services utilize a single relational database, which acts as a common coupling point.\nThe question answered by the static analysis for an architecture quantum is, “Is this dependent of the architecture necessary to bootstrap this service?” Even in the case of an event-driven architecture where some of the services don’t access the database, if they rely on services that do access the database, then they become part of the static coupling of the architecture quantum.\nEven a distributed architecture such as broker-style event-driven architecture can be a single quantum\nThus, the operating sys‐ tem, data store, message broker, container orchestration, and all other operational dependencies form the static coupling points of an architecture quantum, using the strictest possible contracts, operational dependencies (more about the role of con‐ tracts in architecture quanta in Chapter 13).\nArchitects in these architectures favor high degrees of decoupling and take care not to create coupling points between services, allowing each individual service to each form its own quanta, as shown in Figure 2-7.\nHowever, if the system is tightly coupled to a user interface, the architecture forms a single architecture quantum, as illustrated in Figure 2-8.\nA tightly coupled user interface can reduce a microservices architecture quantum to one\nAdditionally, it will be difficult for an architect to design different levels of opera‐ tional architecture characteristics (performance, scale, elasticity, reliability, and so on) for each service if they all must cooperate together in a single user interface (particu‐ larly in the case of synchronous calls, covered in “Dynamic Quantum Coupling” on page 38).\nIn a micro-frontend architecture, each service + user interface component forms an architecture quantum",
    "keywords": [
      "architecture",
      "architecture quantum",
      "single architecture quantum",
      "quantum",
      "coupling",
      "static coupling",
      "Software Architecture",
      "single architecture",
      "coupling points",
      "single",
      "services",
      "Dynamic Quantum Coupling",
      "Discerning Coupling",
      "database",
      "single quantum"
    ],
    "concepts": [
      "architectures",
      "architectural",
      "coupling",
      "coupled",
      "service",
      "services",
      "database",
      "interface",
      "interfaces",
      "styles"
    ]
  },
  {
    "chapter_number": 7,
    "title": "Segment 7 (pages 54-62)",
    "start_page": 54,
    "end_page": 62,
    "summary": "For example, transactionality is easier in synchronous architectures with mediation, whereas higher levels of scale are possible with eventually consistent asynchronous choreographed systems.\n“I’ve been reading about this architecture quantum stuff, and I just…don’t…get…it!”\nperspectives.”\n“Well,” said Addison, “the architecture quantum basically defines a DDD bounded context in archi- tectural terms.”\n“Why not just use bounded context, then?” asked Austen.\nBut the architecture quantum definition goes further by identifying types of cou- pling—that’s where the static and dynamic stuff comes in.”\nWhy make the distinction?”\n“It turns out that a bunch of different concerns revolve around the different types,” said Addison.\nWhat is all the wiring required to bootstrap that service?”\n“You’re missing a lot.” said Addison.\nWhat else?”\nWhen the service calls another service via the broker, we get into the dynamic side.”\n“OK, that makes sense,” said Austen.\n“If I think about what it would take to bootstrap it from scratch, that’s the static quantum coupling.”\nWe recently built a diagram of the static quan- tum coupling for each of our services defensively.”\nThey’re trying to do risk mitigation —if we change a service, they want to know what must be tested.”\nThey’re using that to build a call graph to see how things are connected.”\nWhat about dynamic coupling?”\n“Dynamic coupling concerns how quanta communicate with each other, particularly synchronous versus asynchronous calls and their impact on operational architecture characteristics—things like performance, scale, elasticity, reliability, and so on.\nIf on the other hand we make an asynchronous call, using the message queue as a buffer, we can allow the two services to execute operationally independently, allowing the caller to add messages to the queue and continue working, receiving notification when the workflow is complete.”\nBut I see now that, depending on the type of call you make, you might temporarily couple two services together.”\n“That’s right,” said Addison.\n“The architecture quanta can entangle one another temporarily, during the course of a call, if the nature of the call ties things like performance, responsiveness, scale, and a bunch of others.”\n“OK, I think I understand what an architecture quantum is, and how the coupling definitions work.\nBut I’m never going to get that quantum/quanta thing straight!”\n“Same for datum/data, but no one ever uses datum!” laughed Addison.\n“You’ll see a lot more of the impact of dynamic coupling on workflows and transactional sagas as you keep digging into our architecture.”\n“Without a working application,” they had said, “we cannot possibly continue to support this business line.”\n“That was a bad meeting,” said Addison.\n“Yeah, I know,” said Austen.\n“Neither can I,” said Addison.\n“Me too,” said Austen.\n“I still think breaking apart the application would solve most of these issues.”\n“I agree with you,” said Addison, “but how do we convince the business to spend more money and time to refactor the architecture?\n“You’re right,” Austen said.\n“But if we both agree that we need to break apart the application to keep it alive, how in the world are we going to convince the business and get the funding and time we need to completely restructure the Sysops Squad application?” asked Addison.\n“Beats me,” said Austen.\n“What makes you so sure that breaking apart the Sysops Squad application will solve all of the issues?” asked Logan.\n“Because,” said Austen, “we’ve tried patching the code over and over, and it doesn’t seem to be work- ing.\nWe still have way too many issues.”\n“Well,” said Austen, “actually, we don’t.”\n“Then how do you know breaking apart the application is the right approach?” asked Logan.\n“We already told you,” said Austen, “because nothing else we try seems to work!”\n“Sorry,” said Logan, “but you know as well as I do that’s not a reasonable justification for the business.\n“So, what would be a good business justification?” asked Addison.\n“Well,” said Logan, “to build a good business case for something of this magnitude, you first need to understand the benefits of architectural modularity, match those benefits to the issues you are fac- ing with the current system, and finally analyze and document the trade-offs involved with breaking apart the application.”",
    "keywords": [
      "Architecture",
      "Addison",
      "Coupling",
      "architecture quantum",
      "Austen",
      "Software Architecture",
      "static quantum coupling",
      "quantum coupling",
      "Sysops Squad Saga",
      "dynamic coupling",
      "service",
      "Asynchronous",
      "Quantum",
      "services",
      "application"
    ],
    "concepts": [
      "architecture",
      "architectures",
      "architectural",
      "saga",
      "sagas",
      "business",
      "businesses",
      "austen",
      "communicate",
      "service"
    ]
  },
  {
    "chapter_number": 8,
    "title": "Segment 8 (pages 63-71)",
    "start_page": 63,
    "end_page": 71,
    "summary": "One aspect of architectural modularity is breaking large monolithic applications into separate and smaller parts to provide more capacity for further scalability and growth, while at the same time facilitating constant and rapid change.\nThe water glass analogy is a great way of explaining architectural modularity (the breaking up of monolithic applications) to business stakeholders and C-level executives, who will inevitably be paying for the architecture-refactoring effort.\nThe primary business drivers for breaking applications into smaller parts include speed-to-market (sometimes called time-to-market) and achieving a level of competitive advantage in the marketplace.\nAs illustrated in Figure 3-3, the five key architectural characteristics to support agility, speed-to- market, and, ultimately, competitive advantage in today’s marketplace are availability (fault tolerance), scalability, deployability, testability, maintainability.\nMaintainability, testability, and deployability (defined in the following sections) can also be achieved through monolithic architectures such as a modular monolith or even a microkernel architecture (see Appendix B for a list of references providing more information about these architecture styles).\nBoth of these architec‐ ture styles offer a level of architectural modularity based on the way the components are structured.\nFor example, with a modular monolith, components are grouped into well-formed domains, providing for what is known as a domain partitioned architec‐ ture (see Fundamentals of Software Architecture, Chapter 8, page 103).\nWithin the context of architecture, we are defining a component as an architectural building block of the application that does some sort of business or infrastructure function, usually manifested through a package structure (Java), namespace (C#), or physical grouping of files (classes) within some sort of directory structure.\nLarge monolithic architectures generally have low levels of maintainability due to the technical partitioning of functionality into layers, the tight coupling between compo‐ nents, and weak component cohesion from a domain perspective.\nNotice in Figure 3-4 that the change scope of the new requirement is at an application level since the change is propagated to all of the layers within the application.\nWith monolithic layered architectures, change is at an application level\nDepending on the team structure, implementing this simple change to add an expira‐ tion date to wish list items in a monolithic layered architecture could possibly require the coordination of at least three teams:\nModular architectures, on the other hand, partition domains and subdomains into smaller, separately deployed units of software, thereby making it easier to modify a domain or subdomain.\nNotice that with a distributed service-based architecture, as shown in Figure 3-5, the change scope of the new requirement is at a domain level within a par‐ ticular domain service, making it easier to isolate the specific deployment unit requir‐ ing the change.\nMoving to even more architectural modularity such as a microservices architecture, as illustrated in Figure 3-6, places the new requirement at a function-level change\nWith service-based architectures, change is at a domain level\nWith microservices architectures, change is at a function level\nThese three progressions toward modularity demonstrate that as the level of architec‐ tural modularity increases, so does maintainability, making it easier to add, change, or remove functionality.\nLarge monolithic architecture styles like the layered architecture support relatively low levels of testability (and hence agility) due to the difficulty in achieving full and complete regression testing of all features within the large deploy‐ ment unit.\nArchitectural modularity—the breaking apart of applications into smaller deploy‐ ment units—significantly reduces the overall testing scope for changes made to a ser‐ vice, allowing for better completeness of testing as well as ease of testing.\nWhile architectural modularity generally improves testability, it can sometimes lead to the same problems that exist with monolithic, single-deployment applications.\nMonolithic architectures generally support low levels of deployability due to the amount of ceremony involved in deploying the application (such as code freezes, mock deployments, and so on), the increased risk that something else might break once new features or bug fixes are deployed, and a long time frame between deploy‐ ments (weeks to months).\nApplications having a certain level of architectural modu‐ larity in terms of separately deployed units of software have less deployment ceremony, less risk of deployment, and can be deployed more frequently than a large, single monolithic application.",
    "keywords": [
      "architectural modularity",
      "modularity",
      "architectural",
      "change",
      "Modularity Drivers",
      "application",
      "architecture",
      "level",
      "level Modularity Drivers",
      "Drivers",
      "component",
      "monolithic",
      "maintainability",
      "Architectural Modularity scope",
      "service"
    ],
    "concepts": [
      "architecture",
      "architectures",
      "deployment",
      "deployed",
      "deploy",
      "deploying",
      "deployments",
      "change",
      "changing",
      "changes"
    ]
  },
  {
    "chapter_number": 9,
    "title": "Segment 9 (pages 72-79)",
    "start_page": 72,
    "end_page": 79,
    "summary": "Although both scalability and elasticity improve with finer-grained services, elasticity is more a function of granularity (the size of a deployment unit), whereas scalability is more a function of modularity (the breaking apart of applications into separate deployment units).\nArmed with a better understanding of what is meant by architectural modularity and the corresponding drivers for breaking apart a system, Addison and Austen met to discuss the Sysops Squad issues and try to match them to modularity drivers in order to build a solid business justification to present to the business sponsors.\n“Let’s take each of the issues we are facing and see if we can match them to some of the modularity drivers,” said Addison.\n“That way, we can demonstrate to the business that breaking apart the application will in fact address the issues we are facing.”\n“And the developers are constantly complaining that the codebase is too large, and it’s difficult to find the right place to apply changes to new features or bug fixes,” said Addison.\n“OK,” said Austen, “so clearly, overall maintainability is a key issue here.”\n“Right,” said Addison.\n“So, by breaking apart the application, it would not only decouple the code, but it would isolate and partition the functionality into separately deployed services, making it easier for developers to apply changes.”\n“Testability is another key characteristic related to this problem, but we have that covered already because of all our automated unit tests,” said Austen.\nchanges made to the application, group relevant automated unit tests together, and get better completeness of testing—hence fewer bugs.”\n“I agree,” said Austen, “and besides, the mock deployments and code freezes we do for each release take up valuable time—time we don’t have.\n“I disagree,” said Addison.\n“I see what you mean,” said Austen, “and while I agree with you, I still maintain that at some point we will have to modify our current deployment pipeline as well.”\nSatisfied that breaking apart the Sysops Squad application and moving to a distributed architecture would address the change issues, Addison and Austen moved on to the other business sponsor concerns.\n“OK,” said Addison, “the other big thing the business sponsors complained about in the meeting was overall customer satisfaction.\n“Hold on,” said Austen.\n“Exactly,” said Austen.\n“So, we are in agreement then that overall availability through fault tolerance will address the application not always being available for the customers since they only interact with the ticketing portion of the system.”\n“It just so happens I asked Sydney from the Sysops Squad development team to run some analysis for me regarding exactly that issue,” said Austen.\n“So,” said Addison, “it appears we have both a scalability and a database load issue here.”\n“Exactly!” Austen said.\n“And get this—by breaking up the application and the monolithic database, we can segregate reporting into its own system and also provide the added scalability for the customer-facing ticketing functionality.”\nNow that Addison and Austen had the go-ahead to move to a distributed architecture and break apart the monolithic Sysops Squad application, they needed to determine the best approach for how to get started.\n“Well,” said Austen.\nSo let’s use the same principle with the Sysops Squad application,” said Austen.\n“That might be a good start,” said Addison, “but what about the data?\nThat should be easy to separate out as well,” said Addison.\nAddison and Austen met with Logan to discuss some of the approaches they were considering for how to break apart the application.\nAre there patterns we can use to break apart the application?” asked Addison.\n“You need to take a holistic view of the application and apply either tactical forking or component- based decomposition,” said Logan.\nWhereas architectural modularity describes the why for breaking apart a monolithic application, architectural decomposition describes the how.\nComponent-based decomposition and tactical forking are two common approaches for breaking apart monolithic applications.",
    "keywords": [
      "Sysops Squad application",
      "Sysops Squad",
      "Addison",
      "Austen",
      "Sysops Squad Saga",
      "application",
      "monolithic Sysops Squad",
      "system",
      "Squad application",
      "architecture",
      "Fault Tolerance",
      "architectural modularity",
      "business",
      "scalability",
      "Squad"
    ],
    "concepts": [
      "architecturally",
      "architecture",
      "architectures",
      "architectural",
      "said",
      "applications",
      "services",
      "service",
      "function",
      "functions"
    ]
  },
  {
    "chapter_number": 10,
    "title": "Segment 10 (pages 80-87)",
    "start_page": 80,
    "end_page": 87,
    "summary": "Generally, architects don’t spend much time cre‐ ating patterns for these kinds of systems; software architecture concerns internal structure, and these systems lack that defining feature.\nHowever, archi‐ tects do have tools to help determine macro characteristics of a codebase, particularly coupling metrics, to help evaluate internal structure.\nAbstractness and Instability Robert Martin, a well-known figure in the software architecture world, created some derived metrics for a C++ book in the late 1990s that are applicable to any object- oriented language.\nA component with an instability value near one is highly unstable, a value close to zero may be either stable or rigid: it is stable if the module or component contains mostly abstract elements, and rigid if it comprises mostly concrete elements.\nDistance from the Main Sequence One of the few holistic metrics architects have for architectural structure is distance from the main sequence, a derived metric based on instability and abstractness, shown in Equation 4-3.\nThe distance-from-the-main-sequence metric imagines an ideal relationship between abstractness and instability; components that fall near this idealized line exhibit a healthy mixture of these two competing concerns.\nFor example, graphing a particular component allows developers to calculate the distance-from-the-main-sequence met‐ ric, illustrated in Figure 4-3.\nComponents that fall too far into the upper-right corner enter into what architects call the zone of uselessness: code that is too abstract becomes difficult to use.\nWhat does the distance-from-the-main-sequence metric tell architects looking to restructure applications?\nIf an architect evaluates a codebase where many of the components fall into either the zones of uselessness or pain, perhaps it is not a good use of time to try to shore up the internal structure to the point where it can be repaired.\nFollowing the flowchart in Figure 4-1, once an architect decides that the codebase is decomposable, the next step is to determine what approach to take to decompose the application.\nThe following sections describe the two approaches for decomposing an application: component-based decomposition and tactical forking.\nComponent-Based Decomposition It has been our experience that most of the difficulty and complexity involved with migrating monolithic applications to highly distributed architecture like microservi‐ ces comes from poorly defined architectural components.\nThe directory structure of a codebase becomes the namespace of the component\nWhen breaking monolithic applications into distributed architec‐ tures, build services from components, not individual classes.\nThroughout many collective years of migrating monolithic applications to distributed architectures (such as microservices), we’ve developed a set of component-based decomposition patterns described in Chapter 5 that help prepare a monolithic appli‐ cation for migration.\nThese patterns involve the refactoring of source code to arrive at a set of well-defined components that can eventually become services, easing the effort needed to migrate applications to distributed architectures.\nComponent-Based Decomposition\nThese component-based decomposition patterns essentially enable the migration of a monolithic architecture to a service-based architecture, which is defined in Chapter 2 and described in more detail in Fundamentals of Software Architecture.\nService-based architecture does not require the database to be broken apart, therefore allowing architects to focus on the domain and functional partitioning prior to tackling database decomposition (discussed in detail in Chapter 6).\nWhen migrating monolithic applications to microservices, con‐ sider moving to a service-based architecture first as a stepping- stone to microservices.",
    "keywords": [
      "Codebase",
      "decomposition",
      "component-based decomposition",
      "architecture",
      "Codebase Decomposable",
      "component",
      "internal structure",
      "Architectural Decomposition",
      "Instability",
      "coupling",
      "structure",
      "service-based architecture",
      "components",
      "architects",
      "architect"
    ],
    "concepts": [
      "components",
      "architecture",
      "architectural",
      "architectures",
      "metrics",
      "metric",
      "architects",
      "architect",
      "coupling",
      "abstractness"
    ]
  },
  {
    "chapter_number": 11,
    "title": "Segment 11 (pages 88-95)",
    "start_page": 88,
    "end_page": 95,
    "summary": "For this decomposition approach, the system starts as a single monolithic application, as shown in Figure 4-8.\nThe first step in tactical forking involves cloning the entire monolith, and giving each team a copy of the entire codebase, as illustrated in Figure 4-9.\nEach team receives a copy of the entire codebase, and they start deleting (as illustrated previously in Figure 4-7) the code they don’t need rather than extract the desirable code.\nAt the completion of the tactical forking pattern, teams have split the original mono‐ lithic application into two parts, preserving the coarse-grained structure of the behav‐ ior in each part, as illustrated in Figure 4-11.\nTrade-Offs Tactical forking is a viable alternative to a more formal decomposition approach, most suited to codebases that have little or no internal structure.\nThe name of this pattern is apt (as all good pattern names should be)—it provides a tactical rather than strategic approach for restructuring architectures, allowing teams to quickly migrate important or critical systems to the next generation (albeit in an unstructured way).\n“I really like the tactical forking approach,” said Austen.\nThe thing I don’t like about the tactical forking approach is all the duplicate code and shared functionality within each service.\n“I’m not sure,” said Addison, “but I do know there’s quite a bit of shared code for the infrastructure stuff like logging and security, and I know a lot of the database calls are shared from the persistence layer of the application.”\nAddison and Austen came to an agreement that the component decomposition approach would be the appropriate one for the Sysops Squad application.\nAddison wrote an ADR for this decision, outlining the trade-offs and justification for the component-based decomposition approach.\nADR: Migration Using the Component-Based Decomposition Approach\nThe two approaches we considered for the migration to a distributed architecture were tactical forking and component-based decomposition.\nDecision We will use the component-based decomposition approach to migrate the existing mon- olithic Sysops Squad application to a distributed architecture.\nThe application has well-defined component boundaries, component-based decomposition approach.\nWith the tactical forking approach, we would have to define the service boundaries up front to know how many forked applications to create.\nWith the component-based decomposition approach, the service definitions will naturally emerge through compo- nent grouping.\nGiven the nature of the problems we are facing with the current application with regard to reliability, availability, scalability, and workflow, using the component-based decomposi- tion approach provides a safer and more controlled incremental migration than the tacti- cal forking approach does.\nConsequences The migration effort will likely take longer with the component-based decomposition approach than with tactical forking.\nAddison and Austen chose to use the component-based decomposition approach, but were unsure about the details of each decomposition pattern.\nYou talked earlier about component-based decomposition, and we chose that approach, but we aren’t able to find much about it on the internet.”",
    "keywords": [
      "component-based decomposition approach",
      "decomposition approach",
      "Tactical Forking",
      "tactical forking approach",
      "component-based decomposition",
      "approach",
      "Sysops Squad application",
      "decomposition",
      "tactical forking pattern",
      "Sysops Squad",
      "Forking",
      "Tactical",
      "forking approach",
      "code",
      "named by Fausto"
    ],
    "concepts": [
      "code",
      "approach",
      "approaches",
      "teams",
      "team",
      "services",
      "service",
      "said",
      "migrate",
      "migration"
    ]
  },
  {
    "chapter_number": 12,
    "title": "Segment 12 (pages 96-104)",
    "start_page": 96,
    "end_page": 104,
    "summary": "Component-based decomposition (introduced in Chapter 4) is a highly effective technique for breaking apart a monolithic application when the codebase is some‐ what structured and grouped by namespaces (or directories).\nThis chapter introduces a set of patterns, known as component-based decomposition patterns, that describe the refactoring of monolithic source code to arrive at a set of well-defined components that can eventually become services.\nFigure 5-1 shows the road map for the component-based decomposition patterns described in this chapter and how they are used together to break apart a monolithic application.\nThis pattern is used to identify, manage, and properly size components.\nIdentify and Size Components Pattern The first step in any monolithic migration is to apply the Identify and Size Compo‐ nents pattern.\nThe purpose of this pattern is to identify and catalog the architectural components (logical building blocks) of the application and then properly size the components.\nPattern Description Because services are built from components, it is critical to not only identify the com‐ ponents within an application, but to properly size them as well.\nOne metric we’ve found useful for component sizing is calculating the total number of statements within a given component (the sum of statements within all source files contained within a namespace or directory).\nHaving a relatively consistent component size within an application is important.\nGenerally speaking, the size of components in an application should fall between one to two standard deviations from the average (or mean) component size.\nWhile many static code analysis tools can show the number of statements within a source file, many of them don’t accumulate total statement by component.\nBecause of this, the architect usually must perform manual or automated post-processing to accumulate total statements by component and then calculate the percentage of code that component represents.\nComponent name\nFor example, the component Billing History shown in Table 5-1 is clearly a component that contains source code files used to manage a customer’s billing history.\nIdentify and Size Components Pattern\nFor example, the component namespace for source code files in the ss/customer/notification directory structure would have the namespace value ss.customer.notification.\nThe relative size of the component based on its percentage of the overall source code containing that component.\nThe percent metric is helpful in identifying components that appear too large or too small in the overall application.\nThis metric is calculated by taking the total number of statements within the source code files representing that component and dividing that number by the total number of statements in the entire codebase of the application.\nFor example, the percent value of 5 for the ss.billing.payment component in Table 5-1 means that this component constitutes 5% of the overall codebase.\nThe sum of the total number of source code statements in all source files con‐ tained within that component.\nThis metric is useful for determining not only the relative size of the components within an application, but also for determining the overall complexity of the component.\nThe total number of source code files (such as classes, interfaces, types, and so on) that are contained within the component.\nFor example, assume the Sysops Squad application has a Trouble Ticket component containing 22% of the codebase that is responsible for ticket creation, assignment, routing, and completion.\nFitness Functions for Governance Once this decomposition pattern has been applied and components have been identi‐ fied and sized correctly, it’s important to apply some sort of automated governance to identify new components and to ensure components don’t get too large during nor‐ mal application maintenance and create unwanted or unintended dependencies.\nIdentify and Size Components Pattern\nThis automated holistic fitness function, usually triggered on deployment through a CI/CD pipeline, identifies components that exceed a given threshold in terms of the percentage of overall source code represented by that component, and alerts the architect if any component exceeds that threshold.\nPseudocode for maintaining component size based on percent of code\n# Walk through the source code for each component, accumulating statements # and calculating the percentage of code each component represents.\nFitness function: No component shall exceed <some number of standard deviations> from the mean component size\nThis automated holistic fitness function, usually triggered on deployment through a CI/CD pipeline, identifies components that exceed a given threshold in terms of the number of standard deviations from the mean of all component sizes (based on the total number of statements in the component), and alerts the architect if any compo‐ nent exceeds that threshold.\nPseudocode for maintaining component size based on number of standard deviations\n# Walk through all of the source code to accumulate total statements and number # of statements per component SET total_statements TO 0 MAP component_size_map FOREACH component IN component_list { num_statements = accumulate_statements(component) ADD num_statements TO total_statements ADD component,num_statements TO component_size_map }\n# For each component calculate the number of standard deviations from the # mean.\nIdentify and Size Components Pattern\nAfter the discussion with Logan (the lead architect) about component-based decompo- sition patterns, Addison decided to apply the Identify and Size Components pattern to identify all of the components in the Sysops Squad ticketing application and calculate the size of each component based on the total number of statements in each component.\nComponent size analysis for the Sysops Squad application\nComponent name",
    "keywords": [
      "Size Components Pattern",
      "Component",
      "Components Pattern",
      "components",
      "component-based decomposition patterns",
      "decomposition patterns",
      "component size",
      "Pattern",
      "Size Components",
      "Size",
      "statements",
      "application",
      "Domain Components Pattern",
      "Component Domains Pattern",
      "list"
    ],
    "concepts": [
      "component",
      "components",
      "patterns",
      "pattern",
      "identify",
      "identifier",
      "identifiable",
      "identifying",
      "identified",
      "identifies"
    ]
  },
  {
    "chapter_number": 13,
    "title": "Segment 13 (pages 105-114)",
    "start_page": 105,
    "end_page": 114,
    "summary": "Addison noticed that most of the components listed in Table 5-2 are about the same size, with the exception of the Reporting component (ss.reporting) which consisted of 33% of the codebase.\nAfter doing some analysis, Addison found that the reporting component contained source code that implemented three categories of reports:\nSydney, one of the Sysops Squad developers assigned the architecture story, refactored the code to break apart the single Reporting component into four separate compo- nents—a Reporting Shared component containing the common code and three other components (Ticket Reports, Expert Reports, and Financial Reports), each representing a functional reporting area, as illustrated in Figure 5-3.\nss.reporting.shared\nss.reporting.tickets\nss.reporting.experts\nss.ticket\nss.ticket.notify\nAlthough the namespace still exists (ss.report ing), it is no longer considered a component, but rather a subdomain.\nThe refactored components listed in Table 5-3 will be used when applying the next decomposition pattern, Gather Common Domain Components.\nGather Common Domain Components Pattern When moving from a monolithic architecture to a distributed one, it is often benefi‐ cial to identify and consolidate common domain functionality to make common services easier to identify and create.\nOne hint that common domain processing exists in the application is the use of shared classes across components or a common inheritance structure used by multiple components.\nAnother way of identifying common domain functionality is through the name of a logical component or its corresponding namespace.\nNotice how each of these components (Ticket Auditing, Billing Auditing, and Survey Auditing) all have the same thing in common—writing the action performed and the user requesting the action to an audit table.\nThis common domain into a new component called penulti functionality can be consolidated mate.ss.shared.audit, resulting in less duplication of code and also fewer services in the resulting distributed architecture.\nFitness function: Find common names in leaf nodes of component namespace\nThis automated holistic fitness function can be triggered on deployment through a CI/CD pipeline to locate common names within the namespace of a component.\nGather Common Domain Components Pattern\nFitness function: Find common code across components\nLike the previous fitness function, an exclusion file is used to reduce the number of “false positives” for known common code that is not considered duplicate domain logic.\nPseudocode for finding common source files between components\n# Send an alert if any source files are used in multiple components IF common_source_file_list NOT EMPTY {\nHaving identified and sized the components in the Sysops Squad application, Addison applied the Gather Common Domain Components pattern to see if any common func- tionality existed between components.\nSysops Squad components with common domain functionality\nss.ticket.notify\nFigure 5-4 illustrates these common notification components within the Sysops Squad application.\nGather Common Domain Components Pattern\nAddison analyzed the incoming (afferent) coupling level for the existing Sysops Squad notification components and came up with the resulting coupling metrics listed in Table 5-5, with “CA” repre- senting the number of other components requiring that component (afferent coupling).\nAddison then found that if the customer notification functionality was consolidated into a single component, the coupling level for the resulting single component increased to an incoming cou- pling level of 5, as shown in Table 5-6.\nIn some cases, combining common domain functionality into a single consolidated component increased the incoming coupling level of that component, thus resulting in too many dependencies on a single shared component within the application.\nAddison wrote an architecture story to combine all of the notification functionality into a single namespace representing a common Notification component.\nSydney, assigned to the architecture story, refactored the source code, creating a single component for customer notification, as illustra- ted in Figure 5-5.\nGather Common Domain Components Pattern\nNotification functionality is consolidated into a new single component called Notification\nNotice that the Customer Notification component (ss.customer.notification), Ticket Notify component (ss.ticket.notify), and Survey Notify components (ss.survey.notify) were removed, and the source code moved to the new consolidated Notification component ( ss.notification).",
    "keywords": [
      "Common Domain Components",
      "Common Domain",
      "common domain functionality",
      "Domain Components Pattern",
      "component",
      "common",
      "Gather Common Domain",
      "domain functionality",
      "components",
      "Sysops Squad",
      "Domain",
      "Reporting component",
      "Components Pattern",
      "Size Components Pattern",
      "LIST"
    ],
    "concepts": [
      "common",
      "reports",
      "report",
      "list",
      "shared",
      "functional",
      "functionality",
      "functions",
      "function",
      "notification"
    ]
  },
  {
    "chapter_number": 14,
    "title": "Segment 14 (pages 115-125)",
    "start_page": 115,
    "end_page": 125,
    "summary": "ss.reporting.tickets\nss.ticket\nss.survey\nFlatten Components Pattern As mentioned previously, components—the building blocks of an application—are usually identified through namespaces, package structures, or directory structures and are implemented through class files (or source code files) contained within these structures.\nThe Flatten Components pattern is used to ensure that components are not built on top of one another, but rather flattened and represented as leaf nodes in a directory structure or namespace.\nTo illustrate this point, consider the customer survey functionality within the Sysops Squad application represented by two components: Survey (ss.survey) and Survey Templates (ss.survey.templates).\nNotice in Table 5-8 how the ss.survey name‐ space, which contains five class files used to manage and collect the surveys, is exten‐ ded with the ss.survey.templates namespace to include seven classes representing each survey type send out to customers.\nThe Survey component contains orphaned classes and should be flattened\nss.survey\nWith this definition, ss.survey.templates is a component, whereas ss.survey would be considered a subdomain, not a compo‐ nent.\nWe further define namespaces such as ss.survey as root namespaces because they are extended with other namespace nodes (in this case, .templates).\nNotice how the ss.survey root namespace in Table 5-8 contains five class files.\nRecall that a component is identified by a leaf node namespace containing source code.\nBecause the ss.survey namespace was extended to include .templates, ss.survey is no longer considered a component and therefore should not contain any class files.\nA collection of classes grouped within a leaf node namespace that performs some sort of specific functionality in the application (such as payment processing or customer survey functionality).\nFor example, given the namespaces ss.survey and ss.survey.templates, ss.sur vey would be considered a root namespace because it is extended by .templates.\nComponents, root namespaces, and orphaned classes (C box denotes source code)\nNotice that since both ss.survey and ss.ticket are extended through other name‐ space nodes, those namespaces are considered root namespaces, and the classes con‐ tained in those root namespaces are hence orphaned classes (belonging to no defined\nThus, the only components denoted in Figure 5-6 are ss.survey.tem plates, ss.login, ss.ticket.assign, and ss.ticket.route.\nThe Flatten Components decomposition pattern is used to move orphaned classes to create well-defined components that exist only as leaf nodes of a directory or name‐ space, creating well-defined subdomains (root namespaces) in the process.\nWe refer to the flattening of components as the breaking down (or building up) of namespaces within an application to remove orphaned classes.\nFor example, one way of flattening the ss.survey root namespace in Figure 5-6 and remove orphaned classes is to move the source code contained in the ss.survey.templates namespace down to the ss.survey namespace, thereby making ss.survey a single component (.survey is now the leaf node of that namespace).\nSurvey is flattened by moving the survey template code into the .survey namespace\nAlternatively, flattening could also be applied by taking the source code in ss.survey and applying functional decomposition or domain-driven design to identify separate functional areas within the root namespace, thus forming components from those functional areas.\nFor example, suppose the functionality within the ss.survey name‐ space creates and sends a survey to a customer, and then processes a completed sur‐ vey received from the customer.\nTwo components could be created from the ss.survey namespace: ss.survey.create, which creates and sends the survey, and ss.survey.process, which processes a survey received from a customer.\nSurvey is flattened by moving the orphaned classes to new leaf nodes ( components)\nRegardless of the direction of flattening, make sure source code files reside only in leaf node namespaces or directories so that source code can always be identified within a specific component.\nAnother common scenario where orphaned source code might reside in a root name‐ space is when code is shared by other components within that namespace.\nConsider the example in Figure 5-9 where customer survey functionality resides in three com‐ ponents (ss.survey.templates, ss.survey.create, and ss.survey.process), but common code (such as interfaces, abstract classes, common utilities) resides in the root namespace ss.survey.\nThe shared classes in ss.survey would still be considered orphaned classes, even though they represent shared code.\nApplying the Flatten Components pattern would move those shared orphaned classes to a new component called ss.survey.shared, therefore removing all orphaned classes from the ss.survey subdomain, as illustra‐ ted in Figure 5-10.\nShared survey code is moved into its own component\nAfter applying the “Gather Common Domain Components Pattern” on page 94, Addison analyzed the results in Table 5-7 and observed that the Survey and Ticket components contained orphaned classes.\nThe Survey and Ticket components contain orphaned classes and should be flattened\nSysops Squad Ticket and Survey components should be flattened\nss.ticket\nss.survey\nKnowing that flattening components meant getting rid of source code in nonleaf nodes, Addison had two choices: consolidate the code contained in the ticket assignment and ticket routing components into the ss.ticket component, or break up the 45 classes in the ss.ticket component into separate components, thus making ss.ticket a subdomain.\nAddison discussed these options with Sydney (one of the Sysops Squad developers), and based on the complexity and frequent changes in the ticket assignment function- ality, decided to keep those components separate and move the orphaned code from the ss.ticket root namespace into other namespaces, thus forming new components.\nWith help from Sydney, Addison found that the 45 orphaned classes contained in the ss.ticket namespace implemented the following ticketing functionality:\nSince ticket assignment and ticket routing functionality were already in their own components (ss.ticket.assign and ss.ticket.route, respectively), Addison created an architecture story to move the source code contained in the ss.ticket namespace to three new components, as shown in Table 5-10.\nss.ticket.shared\nss.ticket.maintenance\nWith this information, Addison created an architec- ture story to move the seven class files from ss.survey.templates into the ss.survey namespace and removed the ss.survey.template component, as shown in Table 5-11.\nss.survey\nAfter applying the Flatten Components pattern (illustrated in Figure 5-12), Addison observed that there were no “hills” (component upon component) or orphaned classes and that all of the compo- nents were contained only in the leaf nodes of the corresponding namespace.\nThe Survey component was flattened into a single component, whereas the Ticket component was raised up and flattened, creating a Ticket subdomain\nss.reporting.tickets\nss.ticket.shared\nss.ticket.maintenance\nss.survey",
    "keywords": [
      "Flatten Components Pattern",
      "components",
      "Components pattern",
      "Flatten Components",
      "Component",
      "Survey",
      "Ticket",
      "Components pattern Component",
      "Survey Templates",
      "Components decomposition pattern",
      "Namespace",
      "Flatten Components decomposition",
      "pattern Component Namespace",
      "ticket Ticket Route",
      "Component Namespace"
    ],
    "concepts": [
      "components",
      "component",
      "survey",
      "surveys",
      "ticket",
      "tickets",
      "functionality",
      "functional",
      "functions",
      "function"
    ]
  },
  {
    "chapter_number": 15,
    "title": "Segment 15 (pages 126-147)",
    "start_page": 126,
    "end_page": 147,
    "summary": "Pattern Description The purpose of the Determine Component Dependencies pattern is to analyze the incoming and outgoing dependencies (coupling) between components to determine what the resulting service dependency graph might look like after breaking up the monolithic application.\nWhile there are many factors in determining the right level of granularity for a service (see Chapter 7), each component in the monolithic applica‐ tion is potentially a service candidate (depending on the target distributed architec‐ ture style).\nNotice the dependency between the Survey and Notification components, because the CustomerNotification class used by the CustomerSurvey class resides outside the ss.survey namespace.\nNotice there is only a single dependency between the com‐ ponents in this diagram, making this application a good candidate for breaking apart since the components are functionally independent from one another.\nA monolithic application with minimal component dependencies takes less effort to break apart (golf ball sizing)\nA monolithic application with a high number of component dependencies takes more effort to break apart (basketball sizing)\nA monolithic application with too many component dependencies is not feasible to break apart (airliner sizing)\nIn essence these diagrams form a radar from which to determine where the enemy (high component coupling) is located, and also paint a picture of what the resulting service dependency matrix might look like if the monolithic application were to be broken into a highly distributed architecture.\nThis pattern is useful not only for identifying the overall level of component coupling in an application, but also for determining dependency refactoring opportunities prior to breaking apart the application.\nAn alert generated from this fitness function allows the architect to discuss any sort of increase in coupling with the development team, pos‐ sibly promoting action to break apart components to reduce coupling.\nExample 5-9 shows an exam‐ ple using ArchUnit for ensuring that the Ticket Maintenance component (ss.ticket.maintenance) does not have a dependency on the Expert Profile compo‐ nent (ss.expert.profile).\nAfter reading about the Determine Component Dependencies pattern, Addison won- dered what the Sysops Squad application dependency matrix looked like and whether it was feasible to even break the application apart.\nAddison used an IDE plug-in to gener- ate a component dependency diagram of the current Sysops Squad application.\nInitially, Addison felt a bit discouraged because Figure 5-16 showed a lot of dependencies between the Sysops Squad application components.\nComponent dependencies in the Sysops Squad application\nHowever, Addison also saw lots of dependencies within the Ticketing and Reporting components.\nReal- izing that both the ticketing and reporting shared code contains mostly compile-based class refer- ences and would likely be implemented as shared libraries rather than services, Addison filtered out these components to get a better view of the dependencies between the core functionality of the application, which is illustrated in Figure 5-17.\nComponent dependencies in the Sysops Squad application without shared library dependencies\nAddison showed these results to Austen, and they both agreed that most of the components were relatively self-contained and it appeared that the Sysops Squad application was a good candidate for breaking apart into a distributed architecture.\nCreate Component Domains Pattern While each component identified within a monolithic application can be considered a possible candidate for a separate service, in most cases the relationship between a service and components is a one-to-many relationship—that is, a single service may contain one or more components.\nThe purpose of the Create Component Domains pattern is to logically group components together so that more coarse-grained domain services can be created when breaking up an application.\nWhen breaking apart monolithic applications, consider first moving to service-based architecture as a stepping-stone to other distributed architectures.\nCreating component domains is an effective way of determining what will eventually become domain services in a service-based architecture.\nComponent domains are physically manifested in an application through component namespaces (or directories).\nThis technique is illustrated in Figure 5-18, where the second node in the namespace (.customer) refers to the domain, the third node represents a subdomain under the customer domain (.billing), and the leaf node (.payment) refers to the component.\nComponent domains are identified through the namespace nodes\nFor example, consider the components listed in Table 5-13 that make up the Customer domain within the Sysops Squad application.\nComponents related to the Customer domain before refactoring\nCreate Component Domains Pattern\nTo properly identify the Customer domain (manifested through the namespace ss.customer), the namespaces for the Billing Payment, Billing History, and Support Contract components would have to be modified to add the .customer node at the beginning of the namespace, as shown in Table 5-14.\nComponents related to the Customer domain after refactoring\nNotice in the prior table that all of the customer-related functionality (billing, profile maintenance, and support contract maintenance) is now grouped under .customer, aligning each component with that particular domain.\nFitness Functions for Governance Once refactored, it’s important to govern the component domains to ensure that namespace rules are enforced and that no code exists outside the context of a compo‐ nent domain or subdomain.\nThe following automated fitness function can be used to help govern component domains once they are established within the monolithic application.\nSysops Squad Saga: Creating Component Domains Thursday, November 18, 13:15\nAddison and Austen consulted with Parker, the Sysops Squad product owner, and together they identified five main domains within the application: a Ticketing domain (ss.ticket) containing all ticket-related functionality, including ticket processing, cus- tomer surveys, and knowledge base (KB) functionality; a Reporting domain (ss.report ing) containing all reporting (ss.customer) containing customer profile, billing, and support contracts; an Admin domain (ss.admin) containing maintenance of users and Sysops Squad experts; and finally, a Shared domain (ss.shared) containing login and notification functionality used by the other domains.\nThe exercise Addison did in diagramming and grouping the components was an important one as it validated the identified domain candidates and also demonstrated the need for collaboration with business stakeholders (such as the product owner or business application sponsor).\nSatisfied that all of the components fit nicely into these domains, Addison then looked at the various component namespaces in Table 5-12 after applying the “Flatten Components Pattern” on page 101 and identified the component domain refactoring that needed to take place.\nCreate Component Domains Pattern\nAddison started with the Ticket domain and saw that while the core ticket functionality started with the namespace ss.ticket, the survey and knowledge base components did not.\nTherefore, Addi- son wrote an architecture story to refactor the components listed in Table 5-15 to align with the ticketing domain.\nSysops Squad component refactoring for the Ticket domain\nNext Addison considered the customer-related components, and found that the billing and survey components needed to be refactored to include them under the Customer domain, creating a Bill- ing subdomain in the process.\nSysops Squad component refactoring for the Customer domain\nBy applying the “Identify and Size Components Pattern” on page 84, Addison found that the report- ing domain was already aligned, and no further action was needed with the reporting components listed in Table 5-17.\nSysops Squad Reporting components are already aligned with the Reporting domain\nCreate Component Domains Pattern\nAddison saw that both the Admin and Shared domains needed alignment as well, and decided to create a single architecture story for this refactoring effort and listed these components in Table 5-18.\nSysops Squad component refactoring for the Admin and Shared domains\nWith this pattern complete, Addison realized they were now prepared to structurally break apart the monolithic application and move to the first stage of a distributed architecture by applying the Cre- ate Domain Services pattern (described in the next section).\nCreate Domain Services Pattern Once components have been properly sized, flattened, and grouped into domains, those domains can then be moved to separately deployed domain services, creating what is known as a service-based architecture (see Appendix A).\nPattern Description The previous “Create Component Domains Pattern” on page 120 forms well-defined component domains within a monolithic application and manifests those domains through the component namespaces (or directory structures).\nThis pattern takes those well-defined component domains and extracts those component groups into separately deployed services, known as a domain services, thus creating a service- based architecture.\nIn its simplest form, service-based architecture consists of a user interface that remotely accesses coarse-grained domain services, all sharing a single monolithic database.\nAlthough there are many topologies within service-based architecture (such as breaking up the user interface, breaking up the database, adding an API gateway, and so on), the basic topology shown in Figure 5-20 is a good starting point for migrating a monolithic application.\nNotice in the diagram how the Reporting component domain defined in the “Create Component Domains Pattern” on page 120 is extracted from of the monolithic application, form‐ ing its own separately deployed Reporting service.\nComponent domains are moved to external domain services\nA word of advice, however: don’t apply this pattern until all of the component domains have been identified and refactored.\nThis helps reduce the amount of modi‐ fication needed to each domain service when moving components (and hence source code) around.\nFor example, suppose all of the ticketing and knowledge base function‐ ality in the Sysops Squad application was grouped and refactored into a Ticket domain, and a new Ticket service created from that domain.\nNow suppose that the customer survey component (identified through the ss.customer.survey name‐ space) was deemed part of the Ticket domain.\nSince the Ticket domain had already been migrated, the Ticket service would now have to be modified to include the Sur‐ vey component.\nBetter to align and refactor all of the components into component domains first, then start migrating those component domains to domain services.\nFitness Functions for Governance It is important to keep the components within each domain service aligned with the domain, particularly if the domain service will be broken into smaller microservices.\nThe following fitness function ensures that the namespace (and hence components) are kept consistent within a domain service.\nFitness function: All components in <some domain service> should start with the same namespace\nThis automated holistic fitness function can be triggered on deployment through a CI/CD pipeline to make sure the namespaces for components within a domain ser‐ vice remain consistent.\nFor example, all components within the Ticket domain ser‐ vice should start with ss.ticket.\nArchUnit code for governing components within the Ticket domain service\nAddison and Austen worked closely with the Sysops Squad development team to develop a migration plan to stage the migration from component domains to domain services.\nThey realized this effort not only required the code within each component domain to be extracted from the monolith and moved to a new project workspace, but also for the user interface to now remotely access the functionality within that domain.\nWorking from the component domains identified previously in Figure 5-19, the team migrated each component, one at a time, eventually arriving at a service-based architec- ture, as shown in Figure 5-22.\nSeparately deployed domain services result in a distributed Sysops Squad application\nApplying these component-based decomposition patterns provides a structured, controlled, and incremental approach for breaking apart monolithic architectures.\nOnce these patterns are applied, teams can now work to decompose monolithic data (see Chapter 6) and begin breaking apart domain services into more fine-grained microservices (see Chapter 7) as needed.\nNow that the Sysops Squad application was successfully broken into separately deployed domain services, Addison and Austen both realized that it was time to start thinking about breaking apart the monolithic Sysops Squad database.\nFor example, components translate to data domains, class files translate to database tables, and coupling points between classes translate to database artifacts such as foreign keys, views, triggers, or even stored procedures.",
    "keywords": [
      "Component Domains Pattern",
      "Sysops Squad application",
      "Sysops Squad",
      "Component Dependencies pattern",
      "Component Domains",
      "Component",
      "domain Component Domain",
      "Sysops Squad component",
      "Create Component Domains",
      "Domain Services pattern",
      "Determine Component Dependencies",
      "components",
      "domain",
      "domain services",
      "Component Dependencies"
    ],
    "concepts": [
      "component",
      "components",
      "domain",
      "domains",
      "function",
      "functionally",
      "functionality",
      "functions",
      "functional",
      "data"
    ]
  },
  {
    "chapter_number": 16,
    "title": "Segment 16 (pages 148-155)",
    "start_page": 148,
    "end_page": 155,
    "summary": "Services impacted by the database change must be deployed together with the database\nCoordinating changes to multiple distributed services for a shared database change is only half the story.\nThe real danger of changing a shared database in any distributed architecture is forgetting about services that access the table just changed.\nDatabase changes are isolated to only those services within the associated bounded context\nNotice in Figure 6-4 that Service C needs access to some of the data in Database D that is contained in a bounded context with Service D.\nSince Database D is in a differ‐ ent bounded context, Service C cannot directly access the data.\nOne important aspect of a bounded context related to the scenario between Service C needing data and Service D owning that data within its bounded context is that of database abstraction.\nThe advantage of the bounded context is that the data sent to Service C can be a dif‐ ferent contract than the schema for Database D.\nThis means that a breaking change to some table in Database D impacts only Service D and not necessarily the contract of the data sent to Service C.\nchange or column type change made in the database no longer impacts Service C because of the separate JSON contract.\nService D would have to be modified to accommodate this change because it is within the same bounded context as the database, but the corresponding contract would not have to change at the same time.\nThe bottom line is that Service C is abstracted from breaking changes made to Database D due to the bounded context.\nAs illustrated in Figure 6-6, when multiple services share the same database, the number of connections can quickly become saturated, particularly as the number of services or service instances increase.\nDatabase connections can quickly get saturated with multiple service instances\nTo illustrate the issues associated with database connections and distributed architec‐ ture, consider the following example: a monolithic application with 200 database con‐ nections is broken into a distributed architecture consisting of 50 services, each with 10 database connections in its connection pool.\nConnections per service\nNotice how the number of database connections within the same application context grew from 200 to 1,000, and the services haven’t even started scaling yet!\nAssuming half of the services scale to an average of 5 instances each, the number of database connections quickly grows to 1,700.\nOne effective approach is to assign each service a connection quota to govern the distribution of available database connec‐ tions across services.\nA connection quota specifies the maximum number of database connections a service is allowed to use or make available in its connection pool.\nBy specifying a connection quota, services are not allowed to create more database connections than are allocated to it.\nIf a service reaches the maximum number of database connections in its quota, it must wait for one of the connections it’s using to become available.\nThe advantage of this approach is that it optimizes the use of available database connections across distributed services, making sure those services that require more database connections have them available for use.\nNotice that Service A has only ever needed a maximum of 5 connections, Service C only 15 con‐ nections, and Service E only 14 connections, whereas Service B and Service D have reached their max connection quota and have experienced connection waits.\nMoving five database connections to Service B and five database connections to Service D yields the results shown in Table 6-2.\nAs illustrated in Figure 6-7, service scalability can put a tremendous strain on the database, not only in terms of database connections (as discussed in the prior sec‐ tion), but also on throughput and database capacity.",
    "keywords": [
      "service",
      "database",
      "services",
      "Database connections",
      "connections",
      "data",
      "connection quota",
      "Data Decomposition Drivers",
      "bounded context",
      "quota",
      "distributed services",
      "database change",
      "Change",
      "connection waits",
      "number"
    ],
    "concepts": [
      "services",
      "service",
      "connection",
      "connections",
      "data",
      "databases",
      "change",
      "changes",
      "changing",
      "changed"
    ]
  },
  {
    "chapter_number": 17,
    "title": "Segment 17 (pages 156-163)",
    "start_page": 156,
    "end_page": 163,
    "summary": "Scalability is another data disintegration driver to consider when thinking about breaking apart a database.\nWhen services scale by adding multiple instances, the picture changes dra‐ matically, as shown in Table 6-4, where the total number of database connections is 100.\nBreaking data into separate data domains or even a database-per-service, as illustra‐ ted in Figure 6-8, requires fewer connections to each database, hence providing better database scalability and performance as the services scale.\nFault tolerance is another driver for considering breaking apart data.\nNotice that since the data is now broken apart, if Database B goes down, only Service B and Service C are impacted and become nonoperational, whereas the other services continue to operate uninterrupted.\nThe architecture quantum helps provide guidance in terms of when to break apart a database, making it another data disintegration driver.\nThus, all five services, along with the database, form a single architectural quantum.\nBecause the database is included in the functional cohesion part of the architecture quantum definition, it is necessary to break apart the data so that each resulting part can be in its own quantum.\nNotice in Figure 6-12 that since the database is broken apart, Service A and Service B, along with the corresponding data, are now a separate quantum from the one formed with services C, D, and E.\nBreaking apart monolithic data allows the architect to move certain data to a more optimal database type.\nArtifacts like foreign keys, triggers, views, and stored procedures tie tables together, making it difficult to pull data apart; see Figure 6-13.\nImagine walking up to your DBA or data architect and telling them that since the database must be broken apart to support tightly formed bounded contexts within a microservices ecosystem, every foreign key and view in the database needs to be removed!\nThese artifacts are necessary in most relational databases to support data consistency and data integrity.\nHowever, as illustrated in Figure 6-14, these artifacts must be removed when moving data to another schema or database to form bounded contexts.\nNotice that the foreign key (FK) relationship between the tables in Service A can be preserved because the data is in the same bounded context, schema, or database.\nAnother data integrator is that of database transactions, something we discuss in detail in “Distributed Transactions” on page 263.\nAs shown in Figure 6-15, when a single service does multiple database write actions to separate tables in the same data‐ base or schema, those updates can be done within an Atomicity, Consistency, Isola‐ tion, Durability (ACID) transaction and either committed or rolled back as a single unit of work.\nHowever, when data is broken apart into either separate schemas or databases, as illustrated in Figure 6-16, a single transactional unit of work no longer exists because of the remote calls between services.\nWhile we dive into the details of distributed transaction management and transac‐ tional sagas in Chapter 12, the point here is to emphasize that database transactions are yet another data integration driver, and should be taken into account when con‐ sidering breaking apart a database.",
    "keywords": [
      "data",
      "database",
      "services",
      "Service",
      "Data Decomposition Drivers",
      "Operational Data",
      "Data Decomposition",
      "services scale",
      "data Data Decomposition",
      "Decomposition Drivers",
      "tables",
      "breaking",
      "Data Integrators",
      "Database transactions",
      "Database connections"
    ],
    "concepts": [
      "data",
      "database",
      "databases",
      "services",
      "service",
      "transactions",
      "transaction",
      "architecture",
      "connections",
      "connection"
    ]
  },
  {
    "chapter_number": 18,
    "title": "Segment 18 (pages 164-172)",
    "start_page": 164,
    "end_page": 172,
    "summary": "“Speaking of database connections,” said Devon, “look at this connection pool estimate as we start breaking apart the domain services.”\n“What Addison is saying,” added Devon, “is that by breaking apart the database, we can provide bet- ter fault tolerance by creating domain silos for the data.\nIn other words, if the survey database were to go down, ticketing functionality would still be available.”\n“Listen,” said Dana, “you’ve convinced me that there’s good reasons to break apart the Sysops Squad database, but explain to me how you can even think about doing that.\nThat’s where data domains and the five- step process come into play,” said Devon.\nAs illustrated in Figure 6-17, this evolutionary and iterative pro‐ cess leverages the concept of a data domain as a vehicle for methodically migrating data into separate schemas, and consequently different physical databases.\nA data domain is a collection of coupled database artifacts—tables, views, foreign keys, and triggers—that are all related to a particular domain and frequently used together within a limited functional scope.\nTo illustrate the concept of a data domain, consider the Sysops Squad tables introduced in Table 1-2 and the corresponding pro‐ posed data domain assignments shown in Table 6-5.\nExisting Sysops Squad database tables assigned to data domains\narticle_keyword\nTable 6-5 lists six data domains within the Sysops Squad application: Customer, Sur‐ vey, Payment, Profile, Knowledge base, and Ticketing.\nThe billing table belongs to the Payment data domain, ticket and ticket_type tables belong to the Ticketing data domain, and so on.\nDatabase objects in a hexagon belong in a data domain\nVisualizing the database this way allows the architect and database team to clearly see data domain boundaries and also the cross-domain dependencies (such as foreign keys, views, stored procedures, and so on) that need to be broken.\nFor example, in the diagram notice that solid lines represent dependencies that are self-contained to the data domain, while the dotted lines cross data domains and must be removed when the data domains are extracted into separate schemas.\nTables belonging to data domains, extracted out, and connections that need to be broken\nSince the customer table belongs to a different data domain than the v_customer_contract, the customer table must be removed from the view in the Payment domain.\nThe original view v_customer_contract prior to defining the data domain is defined in Example 6-1.\nDatabase view to get open tickets for customer with cross-domain joins\nDatabase view to get open tickets in ticket domain for a given customer\nOnce architects and database teams understand the concept of a data domain, they can apply the five-step process for decomposing a monolithic database.\nStep 1: Analyze Database and Create Data Domains As illustrated in Figure 6-20, all services have access to all data in the database.\nStep 2: Assign Tables to Data Domains The next step is to group tables along a specific bounded context, assigning tables that belong to a specific data domain into their own schema.\nAs illustrated in Figure 6-21, we have created schemas for each data domain and moved tables to the schemas to which they belong.\nServices use the primary schema according to their data domain needs\nWhen tables belonging to different data domains are tightly coupled and related to one another, data domains must necessarily be combined, creating a broader boun‐ ded context where multiple services own a specific data domain.\nData Domain Versus Database Schema A data domain is an architectural concept, whereas a schema is a database construct that holds the database objects belonging to a particular data domain.\nTo illustrate the assignment of tables to schemas, consider the Sysops Squad example where the billing table must be moved from its original schema to another data domain schema called payment:\nAlternatively, a database team can create synonyms for tables that do not belong in their schema.\nNext, create a synonym for the profile.sysops_user table in the ticketing schema:\nTo form proper data domains, these coupling points need to be broken apart at some later time, therefore moving the integration points from the database layer to the application layer.\nStep 3: Separate Database Connections to Data Domains In this step, the database connection logic within each service is refactored to ensure services connect to a specific schema and have read and write access to the tables belonging only to their data domain.\nNotice that the database configuration has been changed so that all data access is done strictly via services and their connected schemas.\nThere is no cross-schema access; all synonyms created in “Step 2: Assign Tables to Data Domains” on page 156 are removed.",
    "keywords": [
      "Justifying Database Decomposition",
      "Database Decomposition Monday",
      "data domain",
      "data",
      "Database",
      "Sysops Squad database",
      "Sysops Squad Saga",
      "Decomposition Monday",
      "domain",
      "Decomposing Monolithic Data",
      "Sysops Squad",
      "domains",
      "Monolithic Data",
      "Operational Data",
      "tables"
    ],
    "concepts": [
      "database",
      "databases",
      "data",
      "domain",
      "domains",
      "tables",
      "schemas",
      "schema",
      "services",
      "service"
    ]
  },
  {
    "chapter_number": 19,
    "title": "Segment 19 (pages 173-180)",
    "start_page": 173,
    "end_page": 180,
    "summary": "When data from other domains is needed, do not reach into their databases.\nUpon completion of this step, the database is in a state of data sovereignty per service, which occurs when each service owns its own data.\nStep 4: Move Schemas to Separate Database Servers Once database teams have created and separated data domains, and have isolated services so that they access their own data, they can now move the data domains to separate physical databases.\nWith this option, teams first back up each schema with data domains, then set up database servers for each data domain.\nUsing the replicate option, teams first set up database servers for each data domain.\nFigure 6-23 shows an example of the replication option, where the database team sets up multiple database servers so that there is one database server for each data domain.\nReplicate schemas (data domains) to their own database servers\nStep 5: Switch Over to Independent Database Servers Once the schemas are fully replicated, the service connections can be switched.\nThe last step in getting the data domains and services to act as their own independent deployable units is to remove the connection to the old database servers and remove the schemas from the old database servers as well.\nIndependent database servers for each data domain\nOnce the database team has separated the data domains, isolated the database con‐ nections, and finally moved the data domains to their own database servers, they can optimize the individual database servers for availability and scalability.\nTeams can also analyze the data to determine the most appropriate database type to use, intro‐ ducing polyglot database usage within the ecosystem.\nThis characteristic refers to the ease with which new developers, data architects, data modelers, operational DBAs, and other users of the databases can learn and adopt.\nThe higher the star rating, the better the database supports higher availability and/or better partition tolerance.\nThe higher the star rating, the more consistency the database supports.\nThe star ratings for relational databases appear in Figure 6-25.\nRelational databases rated for various adoption characteristics\nRelational databases have been around for many years.\nRelational databases allow for flexible data modeling.\nRelational databases organize data into tables and rows (similar to spreadsheets), something that is natural for most database modelers.\nSince relational databases have been around for many years, well-known design, implementation, and operational patterns can be applied to them, thus making them easy to adopt, develop, and integrate within an architecture.\nIn relational databases, the data model can be designed in such a way that either reads become more efficient or writes become more efficient.\nKey-Value Databases Key-value databases are similar to a hash table data structure, something like tables in an RDBMS with an ID column as the key and a blob column as the value, which can consequently store any type of data.\nUnlike relational databases, key-value databases should be picked based on needs.\nOther relational database constructs like joins, where, and order by are not supported, but rather the operations get, put, and delete.\nThe ratings for key-value databases appear in Figure 6-26.\nKey-value databases rated for various adoption characteristics\nKey-value databases are easy to understand.\nSince key-value databases are aggregate oriented, they can use memory structures like arrays, maps, or any other type of data, including big blob.\nThe data can be queried only by key or ID, which means the client should have access to the key outside of the database.",
    "keywords": [
      "database",
      "Relational Databases",
      "databases",
      "Database Servers",
      "data",
      "Key-Value Databases",
      "database type",
      "Databases Relational databases",
      "data domains",
      "Relational",
      "Databases Key-value databases",
      "Servers",
      "database supports",
      "Independent Database Servers",
      "Relational Databases Relational"
    ],
    "concepts": [
      "databases",
      "database",
      "data",
      "key",
      "keys",
      "relates",
      "relational",
      "related",
      "modelers",
      "modeling"
    ]
  },
  {
    "chapter_number": 20,
    "title": "Segment 20 (pages 181-194)",
    "start_page": 181,
    "end_page": 194,
    "summary": "Key-value databases have good programming language support, and many open source databases have an active community to help learn and understand them.\nSince key-value databases are aggregate oriented, access to data via a key or ID is geared toward read priority.\nSharding in Databases The concept of partitioning is well-known in relational databases: the table data is partitioned into sets based on a schema on the same database server.\nThe word shard means horizontal partition of data in a database.\nDocument data‐ bases are another type of NoSQL database, whose ratings appear in Figure 6-27.\nThese databases understand the structure of the data and can index multiple attributes of the documents, allowing for better query flexibility.\nSelecting a Database Type\nJust like key-value databases, data modeling involves modeling aggregates such as orders, tickets, and other domain objects.\nLike key-value databases, document databases can be configured for higher avail‐ ability.\nJust like key-value data‐ bases, document databases provide the ability to tune the read and write opera‐ tions using the quorum mechanism.\nCol‐ umn family databases are another type of NoSQL database that group related data that is accessed at the same time, and whose ratings appear in Figure 6-28.\nSelecting a Database Type\nData modeling with column family databases takes some getting used to.\nSome column family databases like Apache Cassandra have introduced a SQL-like query language known as Cassandra Query Language (CQL) that makes data modeling accessible.\nAll column family databases are highly scalable and suit use cases where high write or read throughput is needed.\nColumn family databases scale horizontally for read and write operations.\nSimilar to key-value and document databases, column family databases can tune writes and reads based on quorum needs.\nColumn family databases like Cassandra and Scylla have active communities, and the development of SQL-like interfaces has made the adoption of these databases easier.\nColumn family databases use the concepts of SSTables, commit logs, and memta‐ bles, and since the name-value pairs are populated when data is present, they can handle sparse data much better than relational databases.\nHaving aggregates improves read and write performance, and also allows for higher availabil‐ ity and partition tolerance when the databases are run as a cluster.\nGraph Databases Unlike relational databases, where relations are implied based on references, graph databases use nodes to store entities and their properties.\nSelecting a Database Type\nThe ratings for graph databases are illustrated in Figure 6-29.\nSince it’s difficult to split or shard graphs, write throughput is constrained with the type of graph database picked.\nSome of the graph databases that have high partition tolerance and availability are distributed.\nMany graph databases support ACID transactions.\nSome graph databases, such as Neo4j, support transactions, so that data is always consistent.\nIn graph databases, data storage is optimized for relationship traversal as opposed to relational databases, where we have to query the relationships and derive them at query time.\nGraph databases allow the same node to have various types of relationships.\nNewSQL Databases Matthew Aslett first used the term NewSQL to define new databases that aimed to provide the scalability of NoSQL databases while supporting the features of relational databases like ACID.\nNewSQL databases use different types of storage mechanisms, and all of them support SQL.\nNewSQL databases, whose ratings appear in Figure 6-31, improve upon relational databases by providing automated data partitioning or sharding, allowing for\nSelecting a Database Type\nSince NewSQL databases are just like relational databases (with SQL interface, added features of horizontal scaling, ACID compliant), the learning curve is much easier.\nSince NewSQL databases are like relational databases, data modeling is familiar to many and easier to pick up.\nNewSQL databases are designed to support horizontal scaling for distributed sys‐ tems, allowing for multiple active nodes, unlike relational databases that have only one active leader, and the rest of the nodes are followers.\nThe multiple active nodes allow NewSQL databases to be highly scalable and to have better throughput.\nThe data is always consistent, and this allows for relational database users to easily transition to NewSQL databases.\nNewSQL databases are used just like relational databases, with added benefits of indexing and distributing geographically either to improve read performance or write performance.\nSelecting a Database Type\nUnderstanding the type of modeling provided by the data‐ base is critical in selecting the database to use.\nThis trend has given rise to databases optimized for storing sequences of data points collected during a time window, enabling users to track changes over any duration of time.\nThe ratings for this database type appear in Figure 6-33.\nTime-series databases rated for various adoption characteristics\nThe underlying concept with time-series databases is to analyze changes in data over time.\nSelecting a Database Type\nSome databases like InfluxDB have better availability and partition tolerance options with configurations for meta and data nodes, along with replication factors.\nTime-series databases that use relational databases as their storage engine get ACID properties for consistency, while other databases can tune consistency using consistency-level of any, one, or quorum.\nSome of these databases, such as InfluxDB, provide a SQL-like query language known as InfluxQL.\nWhen using time-series databases, the database automatically attaches a timestamp to every datum creation, and the data contains tags or attributes of information.\nDatabase type Relational\nNow that the team had formed data domains from the monolithic Sysops Squad data- base, Devon noticed that the Survey data domain would be a great candidate for migrating from a traditional relational database to a document database using JSON.\n“Actually,” said Skyler, “if you had originally talked with us about this when the system was first being developed, you would understand that from a user interface perspective, it’s really hard to deal with relational data for something like a customer survey.\n“This is why we need to change it to a document database.”\nYou can’t just start adding different database types to the system,” said Dana.\n“Wait,” said Skyler, “didn’t we all agree that part of the problem of the current monolithic Sysops Squad application was that the development teams didn’t work close enough with the database teams?”\n“OK,” said Dana, “but what I’m going to need from you and Devon is a good solid justification for introducing another type of database into the mix.”\nDevon and Skyler knew that a document database would be a much better solution for the cus- tomer survey data, but they weren’t sure how to build the right justifications for Dana to agree to migrate the data.\nAddison agreed to help, and set up a meeting with Parker (the Sysops Squad product owner) to validate whether there was any business justification to migrating the customer survey tables to a document database.\n“Well,” said Devon, “it’s not too bad from the database side.\nThe existing relational database tables are illustrated in Figure 6-34.",
    "keywords": [
      "databases",
      "Column Family Databases",
      "database",
      "data",
      "Document Databases",
      "Graph Databases",
      "Database Type",
      "Family Databases",
      "relational databases",
      "database type Database",
      "NewSQL Databases",
      "Time-Series Databases",
      "Column Family",
      "type",
      "Key-value databases"
    ],
    "concepts": [
      "databases",
      "database",
      "data",
      "type",
      "types",
      "graph",
      "graphs",
      "read",
      "reads",
      "relational"
    ]
  },
  {
    "chapter_number": 21,
    "title": "Segment 21 (pages 195-202)",
    "start_page": 195,
    "end_page": 202,
    "summary": "“So, essentially we have two options for modeling the survey questions in a document database,” said Devon.\n“A single aggregate document or one that is split.”\n“How to we know which one to use?” asked Skyler, happy that the development teams were now finally working with the database teams to arrive at a unified solution.\n“I know,” said Addison, “let’s model both so we can visually see the trade-offs with each approach.”\nDevon showed the team that with the single aggregate option, as shown in Figure 6-36, with the corresponding source code listing in Example 6-3, both the survey data and all related question data were stored as one document.\nTherefore, the entire customer survey could be retrieved from the database by using a single get operation, making it easy for Skyler and others on the development team to work with the data.\n“Yeah,” said Devon, “but it would require additional work on the database side as questions would be replicated in each survey document.\nSkyler explained that another way to think about aggregates was to split the survey and question model so that the questions could be operated on in an independent fashion, as shown in Figure 6-37, with the corresponding source code listing in Example 6-4.\nMoving to a document database would not only provide better flexibility, but also better timeliness for changes needed to the customer surveys.\nConsequences Since we will be using a single aggregate, multiple documents would need to be changed when a common survey question is updated, added, or removed.\nCHAPTER 7 Service Granularity\nThe development team also had its own opinions, which made decision making for service granularity even more difficult.\n“I’m still not sure what to do with the core ticketing functionality,” said Addison.\n“Speaking of customer functionality, did you ever figure out if the customer login functionality is going to be a separate service?”\nAddison and Austen invited Taylen, the Sysops Squad tech lead, to the meeting with Logan so that all of them could be on the same page with regard to the service granularity issues they were facing.\n“I’m telling you,” said Taylen, “we need to break up the domain services into smaller services.\n“If that’s the case, then how do you determine what services should and shouldn’t be broken apart?” asked Taylen.\n“Let me ask you something, Taylen,” said Logan.\n“What is your reason for wanting to make all of the services so small?”\n“I know what the single-responsibility principle is,” said Logan.\nSo tell me everyone, one service or three services?”\nThat’s what micro- services is all about.”\n“The key to getting service granularity right,” said Logan, “is to remove opinion and gut feeling, and use granularity disintegrators and integrators to objectively analyze the trade-offs and form solid justifications for whether or not to break apart a service.”\n“What are granularity disintegrators and integrators?” asked Austen.\nChapter 7: Service Granularity\nGranularity dis‐ integrators address the question “When should I consider breaking apart a service into smaller parts?”, whereas Granularity integrators address the question “When should I consider putting services back together?” One common mistake many devel‐ opment teams make is focusing too much on granularity disintegrators while ignor‐ ing granularity integrators.\nService Granularity\nChapter 7: Service Granularity",
    "keywords": [
      "Service Granularity",
      "Service",
      "Survey",
      "Granularity",
      "Question",
      "document database",
      "customer survey",
      "Addison",
      "data",
      "customer",
      "document",
      "survey data",
      "granularity disintegrators",
      "database",
      "single aggregate"
    ],
    "concepts": [
      "service",
      "services",
      "survey",
      "surveys",
      "granularity",
      "question",
      "questions",
      "data",
      "aggregate",
      "aggregates"
    ]
  },
  {
    "chapter_number": 22,
    "title": "Segment 22 (pages 203-212)",
    "start_page": 203,
    "end_page": 212,
    "summary": "Service Scope and Function The service scope and function is the first and most common driver for breaking up a single service into smaller ones, particularly with regard to microservices.\nConsider a typical Notification Service that does three things: notifies a customer through SMS (Short Message Service), email, or a printed postal letter that is mailed to the customer.\nAlthough it is very tempting to break this service into three separate single-purpose services (one for SMS, one for email, and one for postal letters) as illustrated in Figure 7-2, this alone is not enough to justify breaking the service apart because it already has relatively strong cohesion—all of these functions relate to one thing, notifying the customer.\nAs a single service, any change to the postal letter code would require the developer to test and redeploy the entire service, including SMS and email functionality.\nHowever, by breaking this service into two separate services (Electronic Notification and Postal Letter Notification), as illustrated in Fig‐ ure 7-4, frequent changes are now isolated into a single, smaller service.\nThis in turn means that the testing scope is significantly reduced, deployment risk is lower, and SMS and email functionality is not disrupted during a deployment of postal letter changes.\nConsider once again the Notification Service example, where a single service notifies customers through SMS, email, and printed postal letter.\nAs a single service, email and postal letter functionality must unneces‐ sarily scale to meet the demands of SMS notifications, impacting cost and also elastic‐ ity in terms of mean time to startup (MTTS).\nBreaking the Notification Service into three separate services (SMS, Email, and Letter), as illustrated in Figure 7-5, allows each of these services to scale independently to meet their varying demands of throughput.\nConsider the same consolidated Notification Service example that notifies customers through SMS, email, and postal letter (Figure 7-6).\nIf the email functionality contin‐ ues to have problems with out-of-memory conditions and fatally crashes, the entire service comes down, including SMS and postal letter processing.\nSeparating this single consolidated Notification Service into three separate services provides a level of fault tolerance for the domain of customer notification.\nNow, a fatal error in the functionality of the email service doesn’t impact SMS or postal letters.\nNotice in this example that the Notification Service is split into three separate services (SMS, Email, and Postal Letter), even though email functionality is the only issue with regard to frequent crashes (the other two are very stable).\nSince email function‐ ality is the only issue, why not combine the SMS and postal letter functionality into a single service?\nservice into only two services made sense because Postal Letter was the offending functionality, but Email and SMS are related—they both have to do with electronically notifying the customer.\nFault tolerance and service availability are good disintegration drivers\nMoving the email functionality to a separate service disrupts the overall domain cohe‐ sion because the resulting cohesion between SMS and postal letter functionality is weak.\nEmail Service and…SMS-Letter Notification Service?\nThis naming problem relates back to the service scope and function granularity disintegrator—if a service is too hard to name because it’s doing multiple things, then consider breaking apart the service.\nNotification Service → Email Service, SMS-Letter Service (poor name)\nNotification Service → Email Service, SMS Service, Letter Service (good names)\nWhenever breaking apart a service, regardless of the disintegration driver, always check to see if strong cohesion can be formed with the “leftover” functionality.\nBy breaking this service into two separate services, access to the functionality used to maintain credit\nExtensibility Another primary driver for granularity disintegration is_ extensibility_—the ability to add additional functionality as the service context grows.\nTo understand how database transactions impact service granularity, consider the situa‐ tion illustrated in Figure 7-9 where customer functionality has been split into a Cus‐ tomer Profile Service that maintains customer profile information and a Password Service that maintains password and other security-related information and functionality.",
    "keywords": [
      "Service",
      "Notification Service",
      "postal letter",
      "Postal letter notification",
      "Service Granularity",
      "email service",
      "single service",
      "Notification",
      "services",
      "separate services",
      "SMS",
      "email",
      "postal letter functionality",
      "granularity",
      "SMS notification"
    ],
    "concepts": [
      "service",
      "services",
      "function",
      "functions",
      "functionality",
      "functional",
      "notification",
      "notifications",
      "payment",
      "payments"
    ]
  },
  {
    "chapter_number": 23,
    "title": "Segment 23 (pages 213-220)",
    "start_page": 213,
    "end_page": 220,
    "summary": "Overall performance and responsiveness is another driver for granularity integration (putting services back together).\nConsider the previous example, where 30% of the requests require a workflow between services to complete the request, and 70% are purely atomic.\nOverall reliability and data integrity are also impacted with increased service commu‐ nication.\nConsider the example in Figure 7-14: customer information is separated into five separate customer services.\nIf data integrity and data consistency are important or critical to an operation, it might be wise to consider putting those services back together.\nHowever, things can get complicated when dealing with shared code in a distributed architecture and can sometimes influence service granularity.\nWhile there may have been a good disintegrator driver for breaking apart these services, they all share a common codebase of domain functionality (as opposed to common utilities or infrastructure functionality).\nIn these cases, it might be wise to consolidate these five services into a single service to avoid multiple deployments, as well as having the service functionality be out of sync based on the use of different versions of a library.\nA change in shared code requires a coordinated change to all services\nBreaking up the collective functionality into separate services would mean that almost half of the source code is in a shared library used only by those three services.\nIn this example it might be wise to consider keeping the collective customer-related functionality in a single consolidated ser‐ vice along with the shared code (particularly if the shared code changes fre‐ quently, as discussed next).\nRegardless of the size of the shared library, frequent changes to shared function‐ ality require frequent coordinated changes to the services using that shared domain functionality.\nWhile versioning can sometimes be used to help mitigate coordinated changes, eventually services using that shared functionality will need to adopt the latest version.\nIf the shared code changes frequently, it might be wise to consider consolidating the services using that shared code to help mitigate the complex change coordination of multiple deployment units.\nData Relationships Another trade-off in the balance between granularity disintegrators and integrators is the relationship between the data that a single consolidated service uses as opposed to the data that separate services would use.\nThis integrator driver assumes that the data resulting from breaking apart a service is not shared, but rather formed into tight bounded contexts within each service to facilitate change control and support overall availability and reliability.\nConsider the example in Figure 7-16: a single consolidated service has three functions (A, B, and C) and corresponding data table relationships.\nAssume that based on some of the disintegration drivers outlined in the prior section, this service was broken into three separate services (one for each of the functions in the consolidated service); see Figure 7-17.\nHowever, breaking apart the single consoli‐ dated service into three separate services now requires the corresponding data tables to be associated with each service in a bounded context.\nNotice at the top of Figure 7-17 that Service A owns tables 1, 2, 4, and 6 as part of its bounded context; Service B owns table 3; and Service C owns table 5.\nHowever, notice in the diagram that every operation in Service B requires access to data in table 5 (owned by Service C), and every operation in Service C requires access to data in table 3 (owned by Service B).\nTo better understand the bounded context and why Service C cannot simply access table 3, say Service B (which owns table 3) decides to make a change to its business rules that requires a column to be removed from table 3.",
    "keywords": [
      "Service",
      "services",
      "Service Granularity",
      "Shared",
      "Shared Code",
      "Separate services",
      "Granularity",
      "data",
      "services back",
      "Code",
      "Profile Service",
      "Password Service",
      "consolidated service",
      "single service",
      "Profile Service inserts"
    ],
    "concepts": [
      "services",
      "service",
      "functionalities",
      "functionality",
      "functions",
      "function",
      "tables",
      "change",
      "changed",
      "changes"
    ]
  },
  {
    "chapter_number": 24,
    "title": "Segment 24 (pages 221-230)",
    "start_page": 221,
    "end_page": 230,
    "summary": "Project Sponsor: “Based on our business needs, I’d rather sacrifice a little bit slower time-to-market to have better data integrity and consistency, so let’s leave it as a single service for right now.”\nWhich is more important based on our business needs—better data consistency or better security?”\nIn this case, it’s more important to secure sensitive data, so let’s keep the services separate and work out how we can mitigate some of the issues with data consistency.”\nThe Sysops Squad development team was having trouble deciding whether these two components (assignment and routing) should be implemented as a single consolidated service or two separate services, as illustrated in Figure 7-18.\n“So you see,” said Taylen, “the ticket assignment algorithms are very complex, and therefore should be isolated from the ticket routing functionality.\n“But if we separated the assignment and routing functionality into two services, there would need to be constant communication between them,” said Skyler.\n“Furthermore, assignment and routing are really one function, not two.”\n“No,” said Taylen, “they are two separate functions.”\n“See,” said Skyler, “you cannot make a ticket assignment without routing it to the expert.\nSo the two functions are one.”\n“What happens in the current functionality if a ticket can’t be routed to the expert?” asked Addison.\n“OK, so think about it a minute, Taylen,” said Addison.\n“Yes, but they are still two separate functions, not one as Skyler is suggesting,” said Taylen.\nWhich is more important—isolating the assignment functionality for change control purposes, or combining assignment and routing into a single service for better performance, error handling, and workflow control?”\n“Well,” said Taylen, “when you put it that way, obviously the single service.\n“OK,” said Addison, “in that case, how about we make three distinct architectural components in the single service.\nLet’s go with a single service then.”\nDecision We will create a single consolidated ticket assignment service for the assignment and routing functions of the ticket.\nSome members of the development team insisted that this should be a single consoli- dated Customer Service containing all of the customer information, yet other members of the team disagreed and thought that there should be a separate service for each of these func- tions (a Profile service, Credit Card service, Password service, and a Supported Product service).\nSky- ler, having prior experience in PCI and PII data, thought that the credit card and password information should be a separate service from the rest, and hence only two services (a Profile service containing profile and product information and a separate Customer Secure service containing credit card and password information).\n“Well,” said Austen, “we are struggling with how many services to create for registering customers and maintaining customer-related information, You see, there are four main pieces of data we are dealing with here: profile info, credit card info, password info, and purchased product info.”\n“You know that credit card and password information must be secure, right?”\n“Of course we know it has to be secure,” said Austen.\n“What we’re struggling with is the fact that there’s a single customer registration API to the backend, so if we have separate services they all have to be coordinated together when registering a customer, which would require a distributed transaction.”\nEver.”\n“OK, but what about securing the credit card and password information?” asked Sam.\n“Seems to me, having separate services would allow much better security control access to that type of sensitive information.”\n“OK,” said Austen, “so it seems to me that what we really need to focus on here is controlling access to the password and credit card information separate from the other customer-related requests— you know, like getting and updating profile information, and so on.”\n“You’re telling me that if you separate all of this functionality into separate services, you can better secure access to sensitive data, but you cannot guarantee my all-or-nothing requirement.\nThat’s the trade-off,” said Austen.\n“OK, I’m good with a single service providing you use the Tortoise security framework.”\n“Me too, providing we can still have the all-or-nothing customer registration process,” said Parker.\n“Then I think we are all in agreement that the all-or-nothing customer registration is an absolute requirement and we will maintain multilevel security access using Tortoise,” said Austen.\nBased on the conversation with Parker and Sam, Austen made the decision that customer-related functionality would be managed through a single consolidated domain service (rather than sepa- rately deployed services) and wrote the following ADR for this decision:\nThis can be done through a single con- solidated customer service, a separate service for each of these functions, or a separate service for sensitive and nonsensitive data.\nDecision We will create a single consolidated customer service for profile, credit card, password, and products supported.",
    "keywords": [
      "Sysops Squad Saga",
      "Sysops Squad",
      "service",
      "service granularity",
      "Ticket Assignment",
      "Ticket Assignment Granularity",
      "single service",
      "Sysops Squad expert",
      "ticket assignment service",
      "services",
      "Assignment",
      "granularity",
      "Ticket",
      "separate services",
      "Squad Saga"
    ],
    "concepts": [
      "services",
      "said",
      "security",
      "secure",
      "securing",
      "functions",
      "functionality",
      "function",
      "customer",
      "custom"
    ]
  },
  {
    "chapter_number": 25,
    "title": "Segment 25 (pages 231-248)",
    "start_page": 231,
    "end_page": 248,
    "summary": "As the development team members worked on breaking apart the domain services, they started running into disagreements about what to do with all the shared code and shared functionality.\n“The authorization code has to be a shared service, you know——not in a shared library.\"”\n“Let’s go over the trade-offs of shared library granularity, and also go over the trade-offs between a shared library and a shared service to see if we can resolve these issues in a more reasonable and thoughtful manner.”\nIn this chapter, we introduce sev‐ eral techniques for managing code reuse within a distributed architecture, including replicating code, shared libraries, shared services, and sidecars within a service mesh.\nWhile it might sound crazy, this technique became popular in the early days of microservices when a lot of confusion and misunderstanding arose about the bounded context concept, hence the drive to create a “share nothing architecture.” In theory, code replication seemed like a good approach at that time to reduce code sharing, but in practice it quickly fell apart.\nIf this were a unique one-off class, it might be worth copying it into each service code repository rather than creating a shared library for it.\nA shared library is an external artifact (such as a JAR file, DLL, and so on) containing source code that is used by multiple services which is typically bound to the service at compile time (see Figure 8-3).\nWith the shared library technique, common code is consolidated and shared at compile time\nDependency Management and Change Control Similar to service granularity (discussed in Chapter 7), there are trade-offs associated with the granularity of a shared library.\nThe two opposing forces that form trade-offs with shared libraries are dependency management and change control.\nNote that while the dependency management is relatively straightforward (each service uses the sin‐ gle shared library), change control is not.\nIf a change occurs to any of the class files in the coarse-grained shared library, every service, whether it cares about the change or not, must eventually adopt the change because of a version deprecation of the shared library.\nThis forces unnecessary retesting and redeployment of all the services using that library, therefore significantly increasing the overall testing scope of a shared library change.\nChanges to coarse-grained shared libraries impact multiple services but keep dependencies low\nBreaking shared code into smaller functionality-based shared libraries (such as secu‐ rity, formatters, annotations, calculators, and so on) is better for change control and overall maintainability, but unfortunately creates a mess in terms of dependency management.\nAs shown in Figure 8-5, a change in shared class C7 impacts only Ser‐ vice D and Service E, but managing the dependency matrix between shared libraries and services quickly starts looking like a big ball of distributed mud (or what some people refer to as a distributed monolith).\nChanges to fine-grained shared libraries impact fewer services but increase dependencies\nThe choice of shared library granularity may not matter much with only a few serv‐ ices, but as the number of services increases, so do the issues associated with change control and dependency management.\nTo illustrate this point, consider a shared library containing common field validation rules called Validation.jar that is used by 10 services.\nWithout versioning, all 10 services would have to be tested and redeployed when making the shared library change, thereby increasing the amount of time and coordination for the shared library change (hence less agility).\nOne of the first complexities of shared library versioning is communicating a version change.\nIn a highly distributed architecture with multiple teams, it is often difficult to communicate a version change to a shared library.\nHowever, if the Calculators.jar shared library changes weekly, main‐ taining only two or three versions means that all services using that shared library will be incorporating a newer version on a monthly (or even weekly) basis—causing a lot of unnecessary frequent retesting and redeployment.\nRegardless of the deprecation strategy used, serious defects or breaking changes to shared code invalidate any sort of deprecation strategy, causing all services to adopt the latest version of a shared library at once (or within a very short period of time).\nWhile the shared library technique allows changes to be versioned (therefore provid‐ ing a good level of agility for shared code changes), dependency management can be difficult and messy.\nWhen To Use The shared library technique is a good approach for homogeneous environments where shared code change is low to moderate.\nThe ability to version (although some‐ times complex) allows for good levels of agility when making shared code changes.\nBecause shared libraries are usually bound to the service at compile time, operational characteristics such as performance, scalability, and fault tolerance are not impacted, and the risk of breaking other services with a change to common code is low because of versioning.\nShared Service The primary alternative to using a shared library for common functionality is to use a shared service instead.\nThe shared service technique, illustrated in Figure 8-6, avoids reuse by placing shared functionality in a separately deployed service.\nWith the shared service technique, common functionality is made available at runtime through separate services\nOne distinguishing factor about the shared service technique is that the shared code must be in the form of composition, not inheritance.\nInheritance: How to Choose” and Martin Fowler’s article “Designed Inheritance”), architecturally composition versus inheritance matters when choosing a code-reuse technique, particularly with the shared services technique.\nChanges to shared functionality no longer require redeployment of services; rather, since changes are isolated to a separate ser‐ vice, they can be deployed without redeploying other services needing the shared functionality.\nHowever, like everything in software architecture, many trade-offs are associated with using shared services, including change risk, performance, scalability, and fault tolerance.\nChange Risk Changing shared functionality using the shared service technique turns out to be a double-edged sword.\nAs illustrated in Figure 8-7, changing shared functionality is simply a matter of modifying the shared code contained in a separate service (such as a discount calculator), redeploying the service, and voila—the changes are now avail‐ able to all services, without having to retest and redeploy any other service needing that shared functionality.\nShared functionality changes are isolated to only the shared service\nChanges to a shared service can break other services at runtime\nShared Service\nIn the shared library technique, versioning is managed through compile-time bindings, significantly reducing risk associated with a change in a shared library.\nHowever, how does one version a simple shared service change?\nThe immediate response, of course, is to use API endpoint versioning—in other words, create a new endpoint containing each shared service change, as shown in Example 8-3.\nUsing this approach, each time a shared service changes, the team would create a new API endpoint containing a new version of the URI.\nThe bottom line is that with the shared service technique, changes to a shared service are generally runtime in nature, and therefore carry much more risk than with shared libraries.\nThis trade-off, shown in Figure 8-9, does not exist with the shared library technique when accessing shared code.\nShared Service\nHowever, as illustrated in Figure 8-10, the shared library technique does not have this issue because the shared functionality is contained within the service at compile time.\nThe shared library technique does not have this issue since the shared functionality is contained in the service at compile time, and therefore accessed through standard method or function calls.\nWhile the shared service technique preserves the bounded context and is good for shared code that changes frequently, operational characteristics such as performance, scalability, and availability suffer.\nTrade-offs for the shared service technique\nShared Service\nWhen to Use The shared service technique is good to use in highly polyglot environments (those with multiple heterogeneous languages and platforms), and also when shared func‐ tionality tends to change often.\nWhile changes in a shared service tend to be much more agile overall than with the shared library technique, be careful of runtime side- effects and risks to services needing the shared functionality.",
    "keywords": [
      "shared service",
      "shared library",
      "shared",
      "shared service technique",
      "shared library technique",
      "service",
      "shared code",
      "shared libraries",
      "shared functionality",
      "code",
      "services",
      "shared library change",
      "shared code changes",
      "shared service change",
      "library"
    ],
    "concepts": [
      "service",
      "services",
      "version",
      "versions",
      "share",
      "sharing",
      "change",
      "changes",
      "changed",
      "changing"
    ]
  },
  {
    "chapter_number": 26,
    "title": "Segment 26 (pages 249-256)",
    "start_page": 249,
    "end_page": 256,
    "summary": "Here, each service includes a split between operational concerns (the larger compo‐ nents toward the bottom of the service) and domain concerns, pictured in the boxes toward the top of the service labeled “domain.” If architects desire consistency in operational capabilities, the separable parts go into a sidecar component, metaphori‐ cally named for the sidecar that attaches to motorcycles, whose implementation is either a shared responsibility across teams or managed by a centralized infrastructure group.\nIf architects can assume that every service includes the sidecar, it forms a con‐ sistent operational interface across services, typically attached via a service plane, shown in Figure 8-14.\nIf architects and operations can safely assume that every service includes the sidecar component (governed by fitness functions), it forms a service mesh, illustrated in Fig‐ ure 8-15.\nThe Sidecar pattern allows governance groups like enterprise architects a reasonable restraint over too many polyglot environments: one of the advantages of microservi‐ ces is a reliance on integration rather than a common platform, allowing teams to choose the correct level of complexity and capabilities on a service-by-service basis.\nFor example, without a service mesh, if enterprise architects want to unify around a common monitoring solution, then teams must build a sidecar per platform that supports that solution.\nSidecars and Service Mesh\nThe Sidecar pattern represents not only a way to decouple operational capabilities from domains—it’s an orthogonal reuse pattern to address a specific kind of coupling (see “Orthogonal Coupling” on page 238).\nWhile the Sidecar pattern offers a nice abstraction, it has trade-offs like all other architectural approaches, shown in Table 8-4.\nTrade-offs for the Sidecar pattern / service mesh technique\nWhen to Use The Sidecar pattern and service mesh offer a clean way to spread some sort of cross- cutting concern across a distributed architecture, and can be used by more than just operational coupling (see Chapter 14).\nWhat about monitoring, service discovery, circuit breakers, even some of our utility functions, like the JSON toXML library that a few teams are sharing?\nThat’s why we’re in the process of implementing a service mesh with this common behavior in a sidecar component.”\nSydney said, “I’ve read about sidecars and service mesh—it’s a way to share things across a bunch of microservices, right?”\nThe intent of the service mesh and sidecar is to consolidate operational coupling, not domain coupling.\nFor example, just like in our case, we want consistency for logging and monitoring across all our services, but don’t want each team to have to worry about that.\nIf we consolidate logging code into the common sidecar that every service imple- ments, we can enforce consistency.”\nAddison replied, “We thought about that, but we have enough teams now; we’ve built a shared infrastructure team that is going to manage and maintain the sidecar component.\nThey have built the deployment pipeline to automatically test the sidecar once it’s been bound into the service with a set of fitness functions.”\nSydney said, “So if we need to share libraries between services, just ask them to put it in the sidecar?”\nAddison said, “Be careful—the sidecar isn’t meant to be used for just anything, only operational coupling.”\nBut you should never put domain shared compo- nents, like the Address or Customer class, in the sidecar.”\nIn most architectures, a single implementation of that service would be shared across the teams that need it.\n“Well, adding it to the sidecar makes it bigger, but not by much—it’s a small library.” said Sydney.\n“That’s the key trade-off for shared utility code—how many teams need it versus how much over- head does it add to every service, particularly ones that don’t need it.”\nContext Each service in our microservices architecture requires common and consistent opera- tional behavior; leaving that responsibility to each team introduces inconsistencies and coordination issues.\nDecision We will use a sidecar component in conjunction with a service mesh to consolidate shared operational coupling.\nThe shared infrastructure team will own and maintain the sidecar for service teams; service teams act as their customers.\nTeams work with the shared infrastructure team to place shared, operational libraries in the sidecar if enough teams require it.",
    "keywords": [
      "Sidecar",
      "service",
      "service mesh",
      "Sidecar pattern",
      "Reuse",
      "operational coupling",
      "teams",
      "coupling",
      "Reuse Patterns sidecar",
      "operational",
      "sidecar component",
      "Reuse Patterns",
      "Sidecar pattern leverages",
      "shared infrastructure team",
      "architects"
    ],
    "concepts": [
      "service",
      "services",
      "operational",
      "operations",
      "teams",
      "team",
      "architecture",
      "architectural",
      "architectures",
      "reuse"
    ]
  },
  {
    "chapter_number": 27,
    "title": "Segment 27 (pages 257-266)",
    "start_page": 257,
    "end_page": 266,
    "summary": "However, all three services used common database logic (queries and updates) and shared a set of database tables in the ticketing data domain.\nTaylen wanted to create a shared data service that would contain the common database logic, thus forming a database abstraction layer, as shown in Figure 8-18.\nOption using a shared Ticket Data service for common database logic for the Sysops Squad ticketing services\nOption using a shared library for common database logic for the Sysops Squad ticketing services\nShould the shared database logic be in a shared data service or a shared library?” asked Taylen.\nLet’s do a hypothesis-based approach and hypothesize that the most appropriate solution is to use the shared data service.”\n“First of all,” said Skyler, “all three services would need to make an interservice call to the shared data service for every database query or update.\nFurthermore, if the shared data service goes down, all three of those services become nonoperational.”\nDon’t forget, the Ticket Creation service is customer facing, and it would be using the same shared data service as the backend ticketing functionality.”\n“So far,” said Addison, “it looks like the trade-off for using the shared data service is performance and fault tolerance for the ticketing services.”\n“Let’s also not forget that any changes made to the shared data service are runtime changes.\nIn other words,” said Skyler, “if we make a change and deploy the shared data service, we could possi- bly break something.”\nEvery time we create more instances of the ticket creation service, we would have to make sure we create more instances of the shared data service as well.”\n“How about the positives of using a shared data service?”\n“OK,” said Addison, “let’s talk about the benefits of using a shared data service.”\nAll they would have to do is make a remote service call to the shared data service.”\n“Well,” said Taylen, “I was going to say centralized connection pooling, but we would need multiple instances anyway to support the customer ticket creation service.\nHowever, change control would be so much easier with a shared data service.\nWe wouldn’t have to redeploy any of the ticketing services for database logic changes.”\nWhile the database team worked on decomposing the monolithic Sysops Squad data- base, the Sysops Squad development team, along with Addison, the Sysops Squad archi- tect, started to work on forming bounded contexts between the services and the data, assigning table ownership to services in the process.\n“Why did you add the expert profile table to the bounded context of the Ticket Assign- ment service?” asked Addison.\nWhile this general rule of thumb works well for single ownership (only one service ever writes to a table), it gets messy when teams have joint ownership (multiple services do writes to the same table) or even worse, common ownership (most or all services write to the table).\nThe general rule of thumb for data ownership is that the service that performs write operations to a table is the owner of that table.\nIn this chapter, we unravel this complexity by discussing the three scenarios encoun‐ tered when assigning data ownership to services (single ownership, common owner‐ ship, and joint ownership), and exploring techniques for resolving these scenarios, using Figure 9-1 as a common reference point.\nCommon Ownership Scenario Common table ownership occurs when most (or all) of the services need to write to the same table.\nFor example, Figure 9-1 shows that all services (Wishlist, Catalog, and Inventory) need to write to the Audit table to record the action performed by the user.\nThe solution of simply putting the Audit table in a shared database or shared schema that is used by all services unfortunately reintroduces all of the data-sharing issues described at the beginning of Chapter 6, including change control, connection starva‐ tion, scalability, and fault tolerance.\nA popular technique for addressing common table ownership is to assign a dedicated single service as the primary (and only) owner of that data, meaning only one service is responsible for writing data to the table.\nComing back to the Audit table example, notice in Figure 9-3 that the architect cre‐ ated a new Audit Service and assigned it ownership of the Audit table, meaning it is the only service that performs read or write actions on the table.\nJoint Ownership Scenario One of the more common (and complex) scenarios involving data ownership is joint ownership, which occurs when multiple services perform write actions on the same table.\nThis scenario differs from the prior common ownership scenario in that with joint ownership, only a couple of services within the same domain write to the same table, whereas with common ownership, most or all of the services perform write\nFor example, notice in Figure 9-1 that all services per‐ form write operations on the Audit table (common ownership), whereas only the Catalog and Inventory services perform write operations on the Product table (joint ownership).\nJoint ownership occurs when multiple services within the same domain per‐ form write operations on the same table",
    "keywords": [
      "shared data service",
      "service",
      "data service",
      "shared data",
      "services",
      "Ticket Creation service",
      "Data Ownership",
      "data",
      "Ownership",
      "Shared",
      "Wishlist Service",
      "Audit table",
      "common database logic",
      "table ownership",
      "database logic"
    ],
    "concepts": [
      "services",
      "service",
      "tables",
      "said",
      "data",
      "databases",
      "ticketing",
      "ticket",
      "ownership",
      "figuring"
    ]
  },
  {
    "chapter_number": 28,
    "title": "Segment 28 (pages 267-278)",
    "start_page": 267,
    "end_page": 278,
    "summary": "Splitting the database table moves the joint ownership to a single table ownership sce‐ nario: the Catalog Service owns the data in the Product table, and the Inventory Ser‐ vice owns the data in the Inventory table.\nHowever, as shown in Figure 9-5, this technique requires communication between the Catalog Service and Inventory Ser‐ vice when products are created or removed to ensure the data remains consistent between the two tables.\nChoosing availability means that it’s more important that the Catalog Service always be able to add or remove products, even though a corresponding inventory record may not be created in the Inventory table.\nChoosing consistency means that it’s more important that the two tables always remain in sync with each other, which would cause a product creation or removal operation to fail if the Inventory Service is not available.\nIf no confirmation is required, the Catalog Service can use asynchronous fire-and-forget communication, providing better performance at the sacrifice of data consistency.\nThis is formed when data ownership is shared between the services, thus creating multiple owners for the table.\nWith this technique, the tables shared by the same services are put into the same schema or database, therefore forming a broader bounded context between the services and the data.\nNotice that Figure 9-6 looks close to the original diagram in Figure 9-4 with one noticeable difference—the data domain diagram has the Product table in a separate box outside the context of each owning service.\nWith joint ownership, services can share data by using the data domain tech‐ nique (shared schema)\nWhile data sharing is generally discouraged in distributed architectures (particularly with microservices), it does resolve some of the performance, availability, and data consistency issues found in other joint ownership techniques.\nBecause a broader bounded context is formed between the services and the data, changes to the shared table structures may require those changes to be coordinated among multiple services.\nAnother issue with the data domain technique with regard to data ownership is con‐ trolling which services have write responsibility to what data.\nTable 9-2 summarizes the trade-offs associated with the data domain technique for the joint ownership scenario.\nWith this technique, one service is assigned single ownership of the table and becomes the delegate, and the other service (or services) communicates with the delegate to perform updates on its behalf.\nThe second option, called operational characteristics priority, assigns table ownership to the service need‐ ing higher operational architecture characteristics, such as performance, scalability, availability, and throughput.\nTo illustrate these two options and the corresponding trade-offs associated with each, consider the Catalog Service and Inventory Service joint ownership scenario shown in Figure 9-4.\nAs illustrated in Figure 9-7, since the Catalog Service performs most of the CRUD operations on product information, the Catalog Service would be assigned as the single owner of the table.\nThis means that the Inventory service must communicate with the Catalog Service to retrieve or update inventory counts since it doesn’t own the table.\nTable ownership is assigned to the Catalog service because of domain priority\nLike the common ownership scenario described earlier, the delegate technique always forces interservice communication between the other services needing to update the data.\nWith synchronous communication, the Inventory Service must wait for the inventory to be updated by the Catalog Service, which impacts overall performance but ensures data consistency.\nUsing asynchronous communication to send inventory updates makes the Inventory Service perform much faster, but the data is only eventually con‐ sistent.\nFurthermore, with asynchronous communication, because an error can occur in the Catalog Service while trying to update inventory, the Inventory Service has no guarantee that the inventory was ever updated, impacting data integrity as well.\nIn this case, table ownership would be assigned to the Inventory Service, the jus‐ tification being that updating product inventory is a part of the frequent real-time transactional processing of purchasing products as opposed to the more infrequent administrative task of updating product information or adding and removing prod‐ ucts (see Figure 9-8).\nTable ownership is assigned to the Inventory Service because of operational characteristics priority\nRegardless of which service is assigned as the delegate (sole table owner), the delegate technique has some disadvantages, the biggest being service coupling and the need for interservice communication.\nThe service consolidation tech‐ nique resolves service dependency and addresses joint ownership by combining mul‐ tiple table owners (services) into a single consolidated service, thus moving joint ownership into a single ownership scenario (see Figure 9-9).\nTable ownership is resolved by combining services\nLike the data domain technique, this technique resolves issues associated with service dependencies and performance, while at the same time addressing the joint owner‐ ship problem.\nTable 9-4 summarizes the overall trade-offs of the service consolidation technique.\nJoint ownership service consolidation technique trade-offs\nFinally, for the more complex joint ownership scenario involving the product table with the Catalog Service and Inventory Service, we chose to use the delegate technique, assigning sin‐ gle ownership of the product table to the Catalog Service, with the Inventory Service sending update requests to the Catalog Service.\nOnce table ownership has been assigned to services, an architect must then validate the table ownership assignments by analyzing business workflows and their corre‐ sponding transaction requirements.\nFor example, during the course of an ACID transaction, when the customer profile information is inserted into the Customer Profile table, no other services outside of the ACID transaction scope can access the newly inserted information until the entire transaction is committed.\nEach service can perform its own commits and rollbacks to the tables it owns within the scope of the atomic business transaction.\nDistributed transactions occur when an atomic business request containing multiple database updates is performed by separately deployed remote services.\nAtomicity is not supported because each separately deployed service commits its own data and performs only one part of the overall atomic business request.\nConsistency is not supported because a failure in one service causes the data to be out of sync between the tables responsible for the business request.\nIsolation is not supported because once the Customer Profile Service inserts the pro‐ file data in the course of a distributed transaction to register a customer, that profile information is available to any other service or request, even though the customer registration process (the current transaction) hasn’t completed.",
    "keywords": [
      "Catalog Service",
      "Inventory Service",
      "Service",
      "inventory",
      "data",
      "data ownership",
      "ownership",
      "services",
      "joint ownership",
      "table ownership",
      "Service Consolidation Technique",
      "Catalog",
      "ACID transaction",
      "Distributed Transactions",
      "Inventory Service joint"
    ],
    "concepts": [
      "service",
      "services",
      "tables",
      "data",
      "transaction",
      "transactions",
      "consistent",
      "consistency",
      "inventory",
      "products"
    ]
  },
  {
    "chapter_number": 29,
    "title": "Segment 29 (pages 279-289)",
    "start_page": 279,
    "end_page": 289,
    "summary": "While asynchronous communication can help decouple services and address availability issues associated with the distributed transaction participants, it unfortunately impacts how long it will take the data to become consistent for the atomic business transaction (see eventual consistency later in this section).\nEventual consistency (the E part of BASE) means that given enough time, all parts of the distributed transaction will complete successfully and all of the data is in sync with one another.\nThe type of eventual consistency pattern used and the way errors are handled dictates how long it will take for all of the data sources involved in the distributed transaction to become consistent.\nWhile there are numerous ways to achieve eventual consistency between data sources and systems, the three main patterns in use today are the background synchronization pattern, orchestrated request-based pattern, and the event-based pattern.\nIn this example, three separate services are involved in the customer reg‐ istration process: a Customer Profile Service that maintains basic profile information, a Support Contract Service that maintains products covered under the Sysops Squad repair plan for each customer, and a Billing Payment Service that charges the cus‐ tomer for the support plan.\nAs shown in Figure 9-14, the Customer Profile Service receives this request from the user interface, removes the customer from the Profile table, and returns a confirmation to the customer that they are successfully unsubscribed and will no longer be billed.\nHowever, data for that customer still exists in the Contract table owned by the Support Contract Service and the Billing table owned by the Billing Payment Service.\nWe will use this scenario to describe each of the eventual consistency patterns for get‐ ting all of the data in sync for this atomic business request.\nBackground Synchronization Pattern The background synchronization pattern uses a separate external service or process to periodically check data sources and keep them in sync with one another.\nThe length of time for data sources to become eventually consistent using this pattern can vary based on whether the background process is implemented as a batch job running sometime in the middle of the night, or a service that wakes up periodically (say, every hour) to check the consistency of the data sources.\nThe Customer Profile Service receives the request, removes the data, and one second later (11:23:01) responds back to the customer that they have been successfully unsubscribed from the system.\nThe background synchronization pattern uses an external process to ensure data consistency\nThe biggest disadvantage of the background synchronization pattern is that it couples all of the data sources together, thus breaking every bounded context between the data and the services.\nThe background synchronization eventual consistency pattern is not suitable for dis‐ tributed architectures requiring tight bounded contexts (such as microservices) where the coupling between data ownership and functionality is a critical part of the architecture.\nTable 9-5 summarizes the trade-offs for the background synchronization pattern for eventual consistency.\nOrchestrated Request-Based Pattern A common approach for managing distributed transactions is to make sure all of the data sources are synchronized during the course of the business request (in other words, while the end user is waiting).\nUnlike the previous background synchronization pattern or the event-based pattern described in the next section, the orchestrated request-based pattern attempts to pro‐ cess the entire distributed transaction during the business request, and therefore requires some sort of orchestrator to manage the distributed transaction.\nThe approach we generally prefer when using the orchestrated request-based pattern is to use a dedicated orchestration service for the business request.\nThis approach, illustrated in Figure 9-18, frees up the Customer Profile Service from the responsibil‐ ity of managing the distributed transaction and places that responsibility on a sepa‐ rate orchestration service.\nWe will use this separate orchestration service approach to describe how this eventual consistency pattern works and the corresponding trade-offs with this pattern.\nThe request is received by the Unsubscribe Orchestrator Service, which then forwards the request synchronously to the Customer Profile Service to remove the customer from the Profile table.\nOne second later, the Customer Profile Service sends back an acknowledgment to the Unsubscribe Orchestrator Service, which then sends parallel requests (either through threads or some sort of asynchro‐ nous protocol) to both the Support Contract and Billing Payment Services.\nResponse time could be improved in Figure 9-18 by executing the Customer Profile request at the same time as the other services, but we chose to do that operation syn‐ chronously for error handling and consistency reasons.\nFor example, if the customer could not be deleted from the Profile table because of an outstanding billing charge, no other action is needed to reverse the operations in the Support Contract and Bill‐ ing Payment Services.\nWhile the orchestrated request-based pattern might seem straightforward, con‐ sider what happens when the customer is removed from the Profile table and Contract table, but an error occurs when trying to remove the billing information from the Billing table, as illustrated in Figure 9-19.\nSince the Profile and Support Contract Services individually committed their operations, the Unsubscribe Orches‐ trator Service must now decide what action to take while the customer is waiting for the request to be processed:\n2. Should the orchestrator perform a compensating transaction and have the Sup‐ port Contract and Customer Profile Services reverse their update operations?\nTable 9-6 summarizes the trade-offs for the orchestrated request-based pattern for eventual consistency.\nServ‐ ices are highly decoupled from one another with this pattern, and responsiveness is good because the service triggering the eventual consistency event doesn’t have to wait for the data synchronization to occur before returning information to the customer.\nThe Customer Profile Service receives the request, removes the cus‐ tomer from the Profile table, publishes a message to a message topic or event stream, and returns information one second later letting the customer know they were suc‐ cessfully unsubscribed.\nAt around the same time this happens, both the Support Con‐ tract and Billing Payment Services receive the unsubscribe event and perform whatever functionality is needed to unsubscribe the customer, making all the data sources eventually consistent.",
    "keywords": [
      "Customer Profile Service",
      "eventual consistency patterns",
      "customer profile",
      "Profile Service",
      "eventual consistency",
      "customer",
      "background synchronization pattern",
      "Service",
      "data",
      "pattern",
      "Billing Payment Service",
      "distributed transaction",
      "consistency patterns",
      "Support Contract Service",
      "services"
    ],
    "concepts": [
      "services",
      "service",
      "tables",
      "processed",
      "process",
      "processing",
      "pattern",
      "patterns",
      "transactions",
      "transaction"
    ]
  },
  {
    "chapter_number": 30,
    "title": "Segment 30 (pages 290-300)",
    "start_page": 290,
    "end_page": 300,
    "summary": "The advantages of the event-based pattern are responsiveness, timeliness of data con‐ sistency, and service decoupling.\nSo, from what Dana said, the service that performs write actions on the data table owns the table, regardless of what other services need to access the data in a read-only manner.\nIn that case, looks like the User Maintenance Service needs to own the data.”\nContext When forming bounded contexts between services and data, tables must be assigned ownership to a particular service or group of services.\nTherefore, for single table ownership scenarios, regardless of how many other services need to access the table, only one service is ever assigned an owner, and that owner is the service that maintains the data.\nConsequences Depending on the technique used, services requiring read-only access to a table in another bounded context may incur performance and fault-tolerance issues when access- ing data in a different bounded context.\nNow that Sydney and Addison better understood table ownership and how to form bounded con- texts between the service and the data, they started to work on the survey functionality.\n“Both the Ticket Completion Service and the Survey Service write to the Survey table.”\n“We can use a common data domain so that both services own the data, or we can use the delegate technique and assign only one service as the owner.”\nLet both services write to the table and share a common schema,” said Sydney.\n“Wait, I know, just add the survey tables to the ticketing data domain schema.”\n“That way, the Ticket Completion doesn’t need any access to the Survey table.”\nAddison and Sydney agreed that the Survey Service would own the Survey table, and would use the delegation technique to pass data when the table notifies the Survey Service to kick off the survey process as illustrated in Figure 9-21.\nSurvey Service owns the data using the delegation technique\nthe necessary ticket information can be passed along with that event, thus eliminating the need for the Ticket Completion Service to have any access to the Survey table.\nConsequences All of the necessary data that the Ticket Completion Service needs to insert into the Sur- vey table will need to be sent as part of the payload when triggering the customer survey process.\n“Now that we’ve assigned ownership of the expert profile table to the User Management Service,” said Sydney, “how should the Ticket Assignment Service get to the expert loca- tion and skills data?\nAddison and Sydney met with Taylen to discuss the data access issue and to see if Taylen could modify the expert assignment algorithms to reduce the nimber of database calls to the expert pro- file table.\n“But our only other option is to make remote calls to the User Management Service every time the assignment algorithm needs expert data,” said Addison.\nThat table simply needs to be in the same data domain as the ticketing tables.”\n“There’s got to be another solution to access data we no longer own,” said Addison.\nHowever, when data is broken into separate databases or schemas owned by distinct services, data access for read operations starts to become hard.\nThis chapter describes the various ways services can gain read access to data they don’t own—in other words, outside the bounded context of the services needing the data.\nThe four patterns of data access we discuss in this chapter include the Inter- service Communication pattern, Column Schema Replication pattern, Replicated Cache pattern, and the Data Domain pattern.\nHowever, the Wishlist Service does not have the item description in its table; that data is owned by the Catalog Service in a tightly formed bounded context provid‐ ing change control and data ownership.\nTherefore, the architect must use one of the data access patterns outlined in this chapter to ensure the Wishlist Service can obtain the product descriptions from the Catalog Service.\nWishlist Service needs item descriptions but doesn’t have access to the product table containing the data\nIf one service (or system) needs to read data that it cannot access directly, it simply asks the owning service or system for it by using some sort of remote access protocol.\nTable 10-1 summarizes the trade-offs associated with the interservice communication data access pattern.\nTrade-offs for the Interservice Communication data access pattern\nAs shown in Figure 10-3, the item_desc column is added to the Wishlist table, making that data available to the Wishlist Service without having to ask the Catalog Service for the data.\nWith the Column Schema Replication data access pattern, data is replicated to other tables\nBecause the data is replicated in tables belonging to other services, those services can update the data, even though they don’t officially own the data.\nEven though the services are still coupled because of data synchronization, the service requiring read access has immediate access to the data, and can do simple SQL joins or queries to its own table to get the data.\nWhile in general we caution against use of this data access pattern for scenarios such as the Wishlist Service and Catalog Service example, some situations where it might be a consideration are data aggregation, reporting, or situations where the other data access patterns are not a good fit because of large data volumes, high responsiveness requirements, or high-fault tolerance requirements.\nTable 10-2 summarizes the trade-offs associated with the Column Schema Replica‐ tion data access pattern.\nTrade-offs for the Column Schema Replication data access pattern\nThis pattern leverages replicated in-memory caching so that data needed by other services is made available to each service without them having to ask for it.",
    "keywords": [
      "Ticket Completion Service",
      "service",
      "Survey Service",
      "Wishlist Service",
      "data",
      "Catalog Service",
      "data access pattern",
      "Data Access",
      "Completion Service",
      "Distributed Data Access",
      "Data Ownership",
      "services",
      "survey table",
      "survey",
      "Ticket Completion"
    ],
    "concepts": [
      "data",
      "services",
      "service",
      "tables",
      "said",
      "pattern",
      "patterns",
      "access",
      "accessing",
      "accessed"
    ]
  },
  {
    "chapter_number": 31,
    "title": "Segment 31 (pages 301-316)",
    "start_page": 301,
    "end_page": 316,
    "summary": "With this caching model (illustrated in Figure 10-4), in-memory data is not synchronized between the caches, meaning each service has its own unique data specific to that service.\nWhile this caching model does help increase responsive‐ ness and scalability within each service, it’s not useful for sharing data between serv‐ ices because of the lack of cache synchronization between the services.\nWith a single in-memory cache, each service contains its own unique data\nAs illustrated in Figure 10-5, with this caching model, data is not held in-memory within each service, but rather held externally within a caching server.\nServices, using a pro‐ prietary protocol, make requests to the caching server to retrieve or update shared data.\nNote that unlike the single in-memory caching model, data can be shared among the services.\nThe distributed cache model is not an effective caching model to use for the replica‐ ted caching data access pattern for several reasons.\nRather than depending on a service to retrieve data, the dependency has merely shifted to the caching server.\nBecause the cache data is centralized and shared, the distributed cache model allows other services to update data, thereby breaking the bounded context regarding data ownership.\nLastly, since access to the centralized distributed cache is through a remote call, net‐ work latency adds additional retrieval time for the data, thus impacting overall responsiveness as compared to an in-memory replicated cache.\nWith replicated caching, each service has its own in-memory data that is kept in sync between the services, allowing the same data to be shared across multiple services.\nWith a replicated cache, each service contains the same in-memory data\nTo see how replicated caching can address distributed data access, we’ll return to our Wishlist Service and Catalog Service example.\nReplicated caching data access pattern\nWhen updates are made to the product description by the Catalog Service, the caching product will update the cache in the Wishlist Service to make the data consistent.\nBecause no explicit interservice communication is required between the services, data is readily available in-memory, providing the fastest possi‐ ble access to data a service doesn’t own.\nThe first trade-off with this pattern is a service dependency with regard to the cache data and startup timing.\nNotice that only the initial Wishlist Service instance is impacted by this startup dependency; if the Catalog Service is down, other Wishlist instances can be started up, with the cache data transferred from one of the other Wishlist instances.\nIt’s also important to note that once the Wishlist Service starts and has the data in the cache, it is not necessary for the Catalog Service to be available.\nIf the volume of data is too high (such as exceeding 500 MB), the feasibility of this pattern diminishes quickly, particularly with regard to multiple instances of services needing the data.\nArchitects must analyze both the size of the cache and the total number of services instances needing the cached data to determine the total memory requirements for the replicated cache.\nA third trade-off is that the replicated caching model usually cannot keep the data fully in sync between services if the rate of change of the data (update rate) is too high.\nTable 10-3 lists the trade-offs associated with the replicated cache data access pattern.\nTrade-offs associated with the replicated caching data access pattern\nData Domain Pattern In the previous chapter, we discussed the use of a data domain to resolve joint owner‐ ship, where multiple services both need to write data to the same table.\nThat same pattern can be used for data access as well.\nSuppose the Interservice Communication pattern is not a feasible solution because of reliability issues with the Catalog Service as well as the performance issues with network latency and the additional data retrieval.\nFinally, suppose that the Replicated Cache pattern isn’t an option because of the high data volumes.\nThe only other solution is to create a data domain, combining the Wishlist and Product tables in the same shared schema, accessible to both the Wishlist Service and the Catalog Service.\nFigure 10-8 illustrates the use of this data access pattern.\nResponsiveness is very good with this pattern because the data is available using a normal SQL call, removing the need to do additional data aggregations within the functionality of the service (as is required with the Replicated Cache pattern).\nSince multiple services access the same data tables, data does not need to be transferred, replicated, or synchronized.\nThe contracts used with the interservice communication pattern and the Replicated Cache pattern form an abstraction layer over the table schema, allowing changes to the table structures to remain within a tight bounded context and not impact other services.\nHowever, this pattern forms a broader boun‐ ded context, requiring multiple services to possibly change when the structure to any of the tables in the data domain changes.\nAnother disadvantage of this pattern is that it can possibly open up security issues associated with data access.\nFor example, in Figure 10-8 the Wishlist Service has com‐ plete access to all the data within the data domain.\nWhile this is OK in the Wishlist and Catalog Service example, there might be times when services accessing the data domain shouldn’t have access to certain data.\nTable 10-4 lists trade-offs associated with the data domain data access pattern.\nTrade-offs associated with the data domain data access pattern\n“Unless we start consolidating all of these services, I guess we are stuck with the fact that the Ticket Assignment needs to somehow get to the expert profile data, and fast,” said Taylen.\n“So service consolidation is out because these services are in entirely different domains, and the shared data domain option is out for the same reasons we talked about before— we cannot have the Ticket Assignment Service connecting to two different databases.”\nWhat data does the Ticket Assignment Service need from the expert profile table?”\n“Hmm, that isn’t much data to store in memory,” said Taylen.\n“Let’s not forget that if we used a replicated cache, we would have to take into account how many instances we would have for the User Management Service as well as the Ticket Assignment Ser- vice,” said Addison.\nI suggest that we should go with the in-memory replicated cache option to cache only the data necessary for the Ticket Assignment Service.\n“As long as the cache is populated, then the Ticket Assignment Service would be fine,” said Addison.\n“Wait, you mean to tell me that the data would be in-memory, even if the User Management Service is unavailable?” asked Taylen.\n“As long as the User Management Service starts before the Ticket Assignment Service, then yes,” said Addison.\n“But,” said Addison, “if we made remote calls to the User Management Service and it goes down, the Ticket Assignment Service becomes nonoperational.\nADR: Use of In-Memory Replicated Caching for Expert Profile Data\nAccess to the expert profile information can be done through interservice communication, in-memory replicated caching, or a common data domain.\nDecision We will use replicated caching between the User Management Service and the Ticket Assignment Service, with the User Management Service being the sole owner for write operations.\nBecause the Ticket Assignment Service already connects to the shared ticket data domain schema, it cannot connect to an additional schema.",
    "keywords": [
      "Ticket Assignment Service",
      "User Management Service",
      "Wishlist Service",
      "Catalog Service",
      "service",
      "data access pattern",
      "data",
      "data access",
      "Assignment Service",
      "distributed data access",
      "Management Service",
      "Data domain data",
      "Ticket Assignment",
      "caching data access",
      "services"
    ],
    "concepts": [
      "data",
      "service",
      "services",
      "said",
      "caching",
      "cache",
      "caches",
      "cached",
      "patterns",
      "accessible"
    ]
  },
  {
    "chapter_number": 32,
    "title": "Segment 32 (pages 317-326)",
    "start_page": 317,
    "end_page": 326,
    "summary": "As you can see, the workflow preceeds as normal until the Fulfillment Service notifies the orchestrator that the current item is out of stock, necessitating a back order.\nIn that case, the orchestrator must refund the payment (this is why many online services don’t charge until shipment, not at order time) and update the state of the Order Placement Service.\nOne interesting characteristic to note in Figure 11-6: even in the most elaborate error scenarios, the architect wasn’t required to add additional communication paths that weren’t already there to facilitate the normal workflow, which differs from the “Chor‐ eography Communication Style” on page 306.\nError handling is a major part of many domain workflows, assisted by having a state owner for the workflow.\nBecause an orchestrator monitors the state of the workflow, an architect may add logic to retry if one or more domain services suffers from a short-term outage.\nHaving an orchestrator makes the state of the workflow queriable, providing a place for other workflows and other transient states.\nWhile orchestration enhances recoverability for domain services, it creates a potential single point of failure for the workflow, which can be addressed with redundancy but adds more complexity.\nThis communication style doesn’t scale as well as choreography because it has more coordination points (the orchestrator), which cuts down on potential paral‐ lelism.\nThe orchestration communication style’s trade-offs appear in Table 11-1.\nFigure 11-4 described the orchestrated workflow for a customer purchasing electron‐ ics from Penultimate Electronics; the same workflow modeled in the choreography communication style appears in Figure 11-7.\nWhile the initial workflow in choreography illustrated in Figure 11-7 seemed simpler than Figure 11-4, the error case (and others) keeps adding more complexity to the choreographed solution.\nIn Figure 11-10, each error scenario forces domain services to interact with each other, adding communication links that weren’t necessary for the happy path.\nEvery workflow that architects need to model in software has a certain amount of semantic coupling—the inherent coupling that exists in the problem domain.\nThe semantic coupling of a workflow is mandated by the domain requirements of the solution and must be modeled somehow.\nIf the architect has organized their architecture the same as the domains, the implementation of the workflow should have similar complexity.\nThus, we can establish a relationship between the semantic coupling and the need for coordination—the more steps required by the workflow, the more potential error and other optional paths appear.\nWorkflow State Management Most workflows include transient state about the status of the workflow: what ele‐ ments have executed, which ones are left, ordering, error conditions, retries, and so on.\nFor orchestrated solutions, the obvious workflow state owner is the orchestrator (although some architectural solutions create stateless orchestrators for higher scale).\nHowever, for choreography, no obvious owner for workflow state exists.\nFirst, the Front Controller pattern places the responsibility for state on the first called service in the chain of responsibility, which in this case is Order Placement Service.\nIf that service contains information about both orders and the state of the workflow, some of the domain services must have a communication link to query and update the order state, as illustrated in Figure 11-13.\nIn choreography, a Front Controller is a domain service that owns workflow state in addition to domain behavior\nIn this scenario, some services must communicate back to the Order Placement Ser‐ vice to update the state of the order, as it is the state owner.\nWhile this simplifies the workflow, it increases communication overhead and makes the Order Placement Ser‐ vice more complex than one that handled only domain behavior.\nAdds additional workflow state to a domain service\nA second way for an architect to manage the transactional state is to keep no transi‐ ent workflow state at all, relying on querying the individual services to build a real- time snapshot.\nFor example, consider a workflow like the simple choreography happy path in Figure 11-7 with no extra state.\nIf a customer wants to know the state of their order, the architect must build a workflow that queries the state of each domain service to determine the most up-to-date order status.\nStateless choreography trades high performance for workflow control, as illustrated in Table 11-3.\nA third solution utilizes stamp coupling (described in more detail in “Stamp Coupling for Workflow Management” on page 378), storing extra workflow state in the mes‐ sage contract sent between services.\nEach domain service updates its part of the over‐ all state and passes that to the next in the chain of responsibility.\nThis is a partial solution, as it still does not provide a single place for users to query the state of the ongoing workflow.\nHowever, it does provide a way to pass the state between services as part of the workflow, providing each service with additional potentially useful context.\nAllows domain services to pass workflow state without additional queries to a state owner\nError handling becomes more difficult without an orchestrator because the domain services must have more workflow knowledge.",
    "keywords": [
      "Choreography Communication Style",
      "Communication Style",
      "workflow",
      "orchestration communication style",
      "Workflow State",
      "Order Placement Service",
      "Service",
      "Managing Distributed Workflows",
      "state",
      "communication",
      "Choreography Communication",
      "communication style include",
      "services",
      "domain services",
      "Style"
    ],
    "concepts": [
      "workflow",
      "workflows",
      "service",
      "services",
      "state",
      "states",
      "architecture",
      "architectural",
      "domain",
      "domains"
    ]
  },
  {
    "chapter_number": 33,
    "title": "Segment 33 (pages 327-334)",
    "start_page": 327,
    "end_page": 334,
    "summary": "Trade-Offs Between Orchestration and Choreography As with all things in software architecture, neither orchestration nor choreography represent the perfect solution for all possibilities.\nFor example, if an architect has a workflow that needs higher scale and typically has few error conditions, it might be worth trading the higher scale of choreography with the complexity of error handling.\nTrade-Offs Between Orchestration and Choreography\nAs the complexity of the workflow rises, orchestration becomes more useful\n“Sure,” said Logan.\nAre y’all ready to talk about workflow options for the primary ticket flow?”\n“Give me an overview of the workflow we’re looking at.”\n“It’s the primary ticket workflow,” said Addison.\n5. The Ticket Management Service communicates with the Survey Service to tell the customer to\n“Have you modeled both solutions?” asked Logan.\n“Yes. The drawing for choreography is in Figure 11-15.”\n“…and the model for orchestration is in Figure 11-16.”\nPrimary ticket workflow modeled as orchestration\n“OK, which handles that problem better—orchestration or choreography?”\n“Easier control of the workflow sounds like the orchestrator version is better—we can handle all the workflow issues there,” volunteered Austen.\nTrade-off between orchestration and choreography for ticket workflow\nThat implies we need an orchestrator so that we can query the state of the workflow.”\n“But you don’t have to have an orchestrator for that—we can query any given service to see if it has handled a particular part of the workflow, or use stamp coupling,” said Addison.\nUpdated trade-offs between orchestration and choreography for ticket workflow\nThat means orchestration?”\nComplex workflows must go somewhere, either in an orchestrator or scattered through services.\nFinal trade-offs between orchestration and choreography for ticket workflow\nAny more?”\nADR: Use Orchestration for Primary Ticket Workflow\nDecision We will use orchestration for the primary ticketing workflow.\nWe modeled orchestration and choreography and arrived at the trade-offs in Table 11-8.\nConsequences Ticketing workflow might have scalability issues around a single orchestrator, which should be reconsidered if current scalability requirements change.\nWhen I showed Addison my design for the Ticketing workflow, I was immedi- ately instructed to come to you and tell you I’ve created a horror story.”\nYou designed a workflow with asynchronous communication, atomic transactionality, and choreography, right?”\nThere are eight generic saga patterns we start from, so it’s good to know what they are, because each has a different balance of trade-offs.”",
    "keywords": [
      "ticket workflow Orchestration",
      "workflow Orchestration Choreography",
      "Orchestration Choreography Workflow",
      "primary ticket workflow",
      "ticket workflow",
      "workflow",
      "Managing Distributed Workflows",
      "ticket",
      "choreography",
      "Orchestration",
      "Saga",
      "Sysops Squad Saga",
      "Managing Workflows",
      "primary ticket",
      "Addison"
    ],
    "concepts": [
      "saga",
      "sagas",
      "workflow",
      "workflows",
      "patterns",
      "pattern",
      "orchestration",
      "orchestrator",
      "ticket",
      "tickets"
    ]
  },
  {
    "chapter_number": 34,
    "title": "Segment 34 (pages 335-344)",
    "start_page": 335,
    "end_page": 344,
    "summary": "Transactional Saga Patterns In Chapter 2, we introduced a matrix that juxtaposed each of the intersecting dimen‐ sions when architects must choose how to implement a transactional saga, repro‐ duced in Table 12-1.\nPattern name Epic Saga(sao) Phone Tag Saga(sac) Fairy Tale Saga(seo) Time Travel Saga(sec) Fantasy Fiction Saga(aao) Horror Story(aac) Parallel Saga(aeo) Anthology Saga(aec)\nFor example, the Epic Saga(sao) pattern indicates the values of synchronous, atomic, and orchestrated for com‐ munication, consistency, and coordination.\nEpic Saga(sao) Pattern This type of communication is the “traditional” saga pattern as many architects understand it, also called an Orchestrated Saga because of its coordination type.\nThis pattern utilizes synchronous communication, atomic consistency, and orchestra‐ ted coordination.\nTransactional Saga Patterns\nThe Epic Saga(sao) pattern’s dynamic coupling (communication, consistency, coordination) relationships\nThe isomorphic representation of the Epic Saga(sao) pattern appears in Figure 12-3.\nThe isomorphic communication illustration of the Epic Saga(sao) pattern\nConsider a common implementation of the Epic Saga(sao) pattern, utilizing compen‐ sating transactions.\nA compensating transaction pattern assigns a service to monitor the transactional completeness of a request, as shown in Figure 12-4.\nA successful orchestrated transactional Epic Saga using a compensating transaction\nTransactional Saga Patterns\nBecause the goal of the Epic Saga(sao) is atomic consistency, the mediator must utilize compensating transactions and request that the other two services undo the operation from before, returning the overall state to what it was before the transac‐ tion started.\nMany architects default to the Epic Saga(sao) pattern because it feels familiar to monolithic architectures, combined with a request (sometimes demand) from stakeholders that state changes must synchronize, regardless of technical con‐ straints.\nThe clear advantage of the Epic Saga(sao) is the transactional coordination that mimics monolithic systems, coupled with the clear workflow owner represented via an orchestrator.\nThe Epic Saga(sao) pattern features the following characteristics:\nThis pattern exhibits extremely high levels of coupling across all possible dimen‐ sions: synchronous communication, atomic consistency, and orchestrated coor‐ dination—it is in fact the most highly coupled pattern in the list.\nRatings for the Epic Saga(sao) Epic Saga(sao) pattern Communication\nTransactional Saga Patterns\nRefer to the “Sysops Squad Saga: Atomic Transactions and Compensating Updates” on page 358 for a concrete example of the Epic Saga(sao) and some of the complex challenges it presents (and how to address those challenges).\nPhone Tag Saga(sac) Pattern The Phone Tag Saga(sac) pattern changes one of the dimensions of the Epic Saga(sao), changing coordination from orchestrated to choreographed; this change is illustrated in Figure 12-6.\nThe Phone Tag Saga(sac) pattern features atomicity but also choreography, meaning that the architect designates no formal orchestrator.\nTransactional Saga Patterns\nGenerally, the Phone Tag Saga(sac) offers slightly better scale than the Epic Saga(sao) because of the lack of a mediator, which can sometimes become a limiting bottleneck.\nHowever, this pattern also features lower performance for error conditions and other workflow complexities—without a mediator, the workflow must be resolved via com‐ munication between services, which impacts performance.\nEven though this pattern utilizes synchronous requests, fewer wait conditions for happy path workflows exist, allowing for higher scale.\nIf error conditions are easy to resolve, or domain services can utilize idempotence and retries, then architects can build higher parallel scale using this pattern compared to an Epic Saga(sao).\nThis pattern relaxes one of the coupling dimensions of the Epic Saga(sao) pattern, utilizing a choreographed rather than orchestrated workflow.\nThus, this pattern is slightly less coupled, but with the same transactional requirement, meaning that the complexity of the workflow must be distributed between the domain services.\nThis pattern is significantly more complex than the Epic Saga(sao); complexity in this pattern rises linearly proportionally to the semantic complexity of the work‐ flow: the more complex the workflow, the more logic must appear in each service to compensate for lack of orchestrator.\nLess orchestration generally leads to better responsiveness, but error conditions in this pattern become more difficult to model without an orchestrator, requiring more coordination via callbacks and other time-consuming activities.\nThe Phone Tag Saga(sac) pattern is better for simple workflows that don’t have many common error conditions.\nWhile it offers a few better characteristics than the Epic Saga(sao), the complexity introduced by lack of an orchestrator offsets many of the advantages.\nFairy Tale Saga(seo) Pattern Typical fairy tales provide happy stories with easy-to-follow plots, thus the name Fairy Tale Saga(seo), which utilizes synchronous communication, eventual consistency, and orchestration, as shown in Figure 12-8.\nTransactional Saga Patterns",
    "keywords": [
      "Phone Tag Saga",
      "Epic Saga",
      "Transactional Saga Patterns",
      "saga",
      "Tag Saga",
      "Saga Patterns",
      "Transactional Saga",
      "Phone Tag",
      "Pattern",
      "transactional Epic Saga",
      "Epic",
      "Fairy Tale Saga",
      "Phone Tag pattern",
      "sagas",
      "transaction Transactional Saga"
    ],
    "concepts": [
      "patterns",
      "pattern",
      "transactions",
      "transaction",
      "saga",
      "sagas",
      "orchestrated",
      "orchestration",
      "coupling",
      "coupled"
    ]
  },
  {
    "chapter_number": 35,
    "title": "Segment 35 (pages 345-357)",
    "start_page": 345,
    "end_page": 357,
    "summary": "This communication pattern relaxes the difficult atomic requirement, providing many more options for architects to design systems.\nHaving a mediator makes managing workflows easier, synchronous communication is the easier of the two choices, and eventual consistency removes the most difficult coordination challenge, especially for error handling.\nThe Fairy Tale Saga(seo) features high coupling, with two of the three coupling drivers maximized in this pattern (synchronous communication and orchestra‐ ted coordination).\nComplexity for the Fairy Tale Saga(seo) is quite low; it includes the most conve‐ nient options (orchestrated, synchronicity) with the loosest restriction (eventual consistency).\nTransactional Saga Patterns\nTime Travel Saga(sec) Pattern The Time Travel Saga(sec) pattern features synchronous communication, and eventual consistency, but choreographed workflow.\nTransactional Saga Patterns\nIt is called Time Travel Saga(sec) because everything is decoupled from a time standpoint: each service owns its own transactional context, making workflow consistency temporally gradual—the state will become consistent over time based on the design of the interaction.\nThe lack of transactions in the Time Travel Saga(sec) pattern makes workflows easier to model; however, the lack of an orchestrator means that each domain service must include most workflow state and information.\nLack of coupling increases scalability with this pattern; only adding asynchronicity would make it more scalable (as in the Anthology Saga(aec) pattern).\nHowever, because this pattern lacks holistic transactional coordination, architects must take extra effort to synchronize data.\nThe coupling level falls in the medium range with the Time Travel Saga(sec), with the decreased coupling brought on by the absence of an orchestrator balanced by the still remaining coupling of synchronous communication.\nAs with all eventual consistency patterns, the absence of transactional coupling eases many data concerns.\nThe loss of transactionality provides a decrease in complexity for this pattern.\nBecause no orchestrator exists in this pattern, each domain ser‐ vice must handle the scenario to restore eventual consistency in the case of an error condition, which will cause a lot of overhead with synchronous calls, impacting responsiveness and performance.\nThis architecture pattern offers extremely good scale and elasticity; it could only be made better with asynchronicity (see the Anthology Saga(aec) pattern).\nThe ratings for the Time Travel Saga(sec) pattern appear in Table 12-5.\nThe Time Travel Saga(sec) pattern provides an on-ramp to the more complex but ulti‐ mately scalable Anthology Saga(aec) pattern.\nTransactional Saga Patterns\nFantasy Fiction Saga(aao) Pattern The Fantasy Fiction Saga(aao) uses atomic consistency, asynchronous communication, and orchestrated coordination, as shown in Figure 12-12.\nThis pattern resembles the Epic Saga(sao) in all aspects except for communication—this pattern uses asynchronous rather than synchronous communication.\nThe Fantasy Fiction Saga(aao) pattern is far-fetched because transaction coordination for asynchronous communication presents difficulties\nTransactional Saga Patterns\nThe coupling level is extremely high in this pattern, using an orchestrator and atomicity but with asynchronous communication, which makes coordination more difficult because architects and developers must deal with race conditions and other out-of-order problems imposed by asynchronous communication.\nBecause this pattern attempts transactional coordination across calls, responsive‐ ness will be impacted overall and be extremely bad if one or more of the services isn’t available.\nScale is much better in the similar pattern Parallel Saga(aeo), which switches atomic to eventual consistency.\nThe ratings for the Fantasy Fiction Saga(aao) pattern appear in Table 12-6.\nThis pattern is unfortunately more popular than it should be, mostly from the mis- guided attempt to improve the performance of Epic Saga(sao) while maintaining trans‐ actionality; a better option is usually Parallel Saga(aeo).\nHorror Story(aac) Pattern One of the patterns must be the worst possible combination; it is the aptly named Horror Story(aac) pattern, characterized by asynchronous communication, atomic con‐ sistency, and choreographed coordination, illustrated in Figure 12-14.\nTransactional Saga Patterns\nThis pattern requires a lot of interservice communication because of required transactionality and the lack of a mediator\nIn this pattern, no mediator exists to manage transactional consistency across multi‐ ple services—while using asynchronous communication.\nInstead, an architect would be better off choosing the Anthology Saga(aec) pattern, which removes holistic transactionality.\nWhile this pattern does attempt the worst kind of single coupling (transactionality), it relieves the other two, lacking both a media‐ tor and the coupling—increasing synchronous communication.\nJust as the name implies, the complexity of this pattern is truly horrific, the worst of any because it requires the most stringent requirement (transactionality) with the most difficult combination of other factors to achieve that (asynchronicity and choreography).\nResponsiveness is low for this pattern, similar to the other patterns that require holistic transactions: coordination for the workflow requires a large amount of interservice “chatter,” hurting performance and responsiveness.\nThe aptly named Horror Story(aac) pattern is often the result of a well-meaning archi‐ tect starting with an Epic Saga(sao) pattern, noticing slow performance because of complex workflows, and realizing that techniques to improve performance include asynchronous communication and choreography.\nTransactional Saga Patterns\nParallel Saga(aeo) Pattern The Parallel Saga(aeo) pattern is named after the “traditional” Epic Saga(sao) pattern with two key differences that ease restrictions and therefore make it an easier pattern to implement: asynchronous communication and eventual consistency.\nThe most difficult goals in the Epic Saga(sao) pattern revolve around transactions and synchronous communication, both of which cause bottlenecks and performance deg‐ radation.\nThis pattern uses a mediator, making it suitable for complex workflows.",
    "keywords": [
      "Fairy Tale Saga",
      "Time Travel Saga",
      "Transactional Saga Patterns",
      "Saga",
      "Travel Saga",
      "Tale Saga",
      "Fantasy Fiction Saga",
      "pattern",
      "Saga Patterns",
      "Transactional Sagas",
      "Fairy Tale",
      "Epic Saga",
      "Parallel Saga",
      "Time Travel",
      "Fiction Saga"
    ],
    "concepts": [
      "patterns",
      "transactions",
      "transaction",
      "coupling",
      "workflows",
      "workflow",
      "complexity",
      "complex",
      "saga",
      "sagas"
    ]
  },
  {
    "chapter_number": 36,
    "title": "Segment 36 (pages 358-366)",
    "start_page": 358,
    "end_page": 366,
    "summary": "Transactional Saga Patterns\nOverall, the Parallel Saga(aeo) pattern offers an attractive set of trade-offs for many scenarios, especially with complex workflows that need high scale.\nAnthology Saga(aec) Pattern The Anthology Saga(aec) pattern provides the exact opposite set of characteristics to the traditional Epic Saga(sao) pattern: it utilizes asynchronous communication, even‐ tual consistency, and choreographed coordination, providing the least coupled exem‐ plar among all these patterns.\nTransactional Saga Patterns\nWhile it may not seem possible without an orchestrator, stamp coupling (“Stamp Coupling for Workflow Manage‐ ment” on page 378) may be used to carry workflow state, as described in the similar Phone Tag Saga(sac) pattern.\nThe Anthology Saga(aec) pattern is well suited to extremely high throughput commu‐ nication with simple or infrequent error conditions.\nFor example, architects can manage transactional sagas through atomic transactions by using compensating updates or by managing transactional state with eventual con‐ sistency.\nState Management and Eventual Consistency State management and eventual consistency leverage finite state machines (see “Saga State Machines” on page 352) to always know the current state of the transactional saga, and to also eventually correct the error condition through retries or some sort of automated or manual corrective action.\nHowever, with this type of saga, rather than issue a compensating update, the state of the saga is changed to NO_SURVEY and a successful response is sent to the Sysops Expert (step 7 in the diagram).\nBy managing the state of the saga rather than issuing compensating updates, the end user (in this case, the Sysops Squad expert) doesn’t need to be concerned that the sur‐ vey was not sent to the customer—that responsibility is for the Ticket Orchestrator Service to worry about.\nSaga State Machines A state machine is a pattern that describes all of the possible paths that can exist within a distributed architecture.\nA state machine always starts with a beginning state that launches the transactional saga, then contains transition states and correspond‐ ing action that should occur when the transition state happens.\nTo illustrate how a saga state machine works, consider the following workflow of a new problem ticket created by a customer in the Sysops Squad system:\nThe various states that can exist within this transactional saga, as well as the corre‐ sponding transition actions, are illustrated in Figure 12-21.\nThe following items describe in more detail this transactional saga and the corre‐ sponding states and transition actions that happen within each state:\nThe transactional saga starts with a customer entering a new problem ticket into the system.\nOnce the ticket is inserted into the ticket table in the database, the transac‐ tional saga state moves to CREATED and the customer is notified that the ticket has been successfully created.\nThis is the only possible outcome for this state transition—any errors within this state prevent the saga from starting.\nIf no expert is available to service the ticket, it is held in a wait state until an expert is available.\nOnce an expert is assigned, the saga state moves to the ASSIGNED state.\nThis is the only outcome for this state transition, meaning the ticket is held in CREATED state until it can be assigned.\nIf the ticket cannot be routed because the expert cannot be located or is unavailable, the saga stays in this state until it can be routed.\nOnce this happens, the transactional saga state moves to ACCEPTED.\nThere are two possible states once a ticket has been accepted by a Sysops Squad expert: COMPLETED or REASSIGN.\nOnce the expert finishes the repair and marks the ticket as “complete,” the state of the saga moves to COMPLETED.\nHowever, if for some reason the ticket was wrongly assigned or the expert is not able to finish the repair, the expert notifies the system and the state moves to REASSIGN.\nOnce in this saga state, the system will reassign the ticket to a different expert.\nLike the CREATED state, if an expert is not available, the transactional saga will remain in the REASSIGN state until an expert is assigned.\nOnce a different expert is found and the ticket is once again assigned, the state moves into the ASSIGNED state, waiting to be accepted by the other expert.\nThis is the only pos‐ sible outcome for this state transition, and the saga remains in this state until an expert is assigned to the ticket.\nThe two possible states once an expert completes a ticket are CLOSED or NO_SURVEY.\nWhen the ticket is in this state, a survey is sent to the customer to rate the expert and the service, and the saga state is moved to CLOSED, thus end‐ ing the transaction saga.\nHowever, if the Survey Service is unavailable or an error occurs while sending the survey, the state moves to NO_SURVEY, indicating that the issue was fixed but no survey was sent to the customer.\nOnce successfully sent, the state moves to CLOSED, marking the end of the transactional saga.\nSaga state machine for a new problem ticket in the Sysops Squad system\nTransaction action Assign ticket to expert\nThe choice between using compensating updates or state management for distributed transaction workflows depends on the situation as well as trade-off analysis between responsiveness and consistency.",
    "keywords": [
      "Saga",
      "Saga State",
      "state",
      "Transactional Saga",
      "Anthology Saga",
      "Parallel Saga",
      "Transactional Saga Patterns",
      "ticket",
      "CLOSED Ticket saga",
      "saga state moves",
      "Saga State Machines",
      "Expert",
      "transactional saga state",
      "pattern",
      "Sysops Squad expert"
    ],
    "concepts": [
      "states",
      "stated",
      "state",
      "sagas",
      "transactions",
      "transaction",
      "patterns",
      "high",
      "highly",
      "service"
    ]
  },
  {
    "chapter_number": 37,
    "title": "Segment 37 (pages 367-376)",
    "start_page": 367,
    "end_page": 376,
    "summary": "Trade-offs associated with state management rather than atomic distributed transactions with compensating updates\nNotice that in both imple‐ mentations, the transactional sagas (NEW_TICKET, CANCEL_TICKET, and so on) are con‐ tained within the Transaction enum, providing a single place within the source code for listing and documenting the various sagas that exist within an application context.\nOnce defined, these annotations or attributes can be used to identify services that are involved in the transactional saga.\nFor example, the source code listing in Example 12-3 shows that the Survey Service (identified by the SurveyServiceAPI class as the service entry point) is involved in the NEW_TICKET saga, whereas the Ticket Service (identified by the TicketServiceAPI class as the service entry point) is involved in two sagas: the NEW_TICKET and the CANCEL_TICKET.\n@ServiceEntrypoint @Saga(Transaction.NEW_TICKET) public class SurveyServiceAPI { ...\n@ServiceEntrypoint @Saga({Transaction.NEW_TICKET,) Transaction.CANCEL_TICKET}) public class TicketServiceAPI { ...\nNotice how the NEW_TICKET saga includes the Survey Service and the Ticket Service.\nThis is valuable information to a developer because it helps them define the testing scope when making changes to a particular workflow or saga, and also lets them know what other services might be impacted by a change to one of the services within the transactional saga.\nFor example, using a simple custom code-walk tool, a developer, architect, or even a business analyst can query what services are involved for the NEW_TICKET saga:\nSysops Squad Saga: Atomic Transactions and Compensating Updates Tuesday, April 5, 09:44\nThe epic saga requires the ticket status to be updated and survey to be sent in one synchronous atomic operation\nOnce sent, the Ticket Service sends an acknowledgment to the Ticket Orchestrator Service that the update is complete.\nSysops Squad Saga: Atomic Transactions and Compensating Updates\n7. The Survey Service inserts data into a table with the survey information (customer, ticket info,\n9. Finally, the Ticket Orchestrator Service sends a response back to the Sysops Squad expert’s mobile device stating that the ticket completion processing is done.\nTo a sea of nods, Logan continued, “One of the first issues that occurs with compensating updates is that since there’s no transactional isolation within a distributed transaction (see “Distributed Transac- tions” on page 263), other services may have taken action on the data updated within the scope of the distributed transaction before the distributed transaction is complete.\nTo illustrate this issue, consider the same Epic Saga example appearing in Figure 12-23: the Sysops Squad expert marks a ticket as complete, but this time the Survey Service is not available.\nIn this case, a compensating update (step 7 in the diagram) is sent to the Ticket Service to reverse the update, changing the ticket state from completed back to in-progress (step 8 in the diagram).”\n“Notice also in Figure 12-23 that since this is an atomic distributed transaction, an error is then sent back to the Sysops Squad expert indicating that the action was not successful and to try again.\nNotice that as part of the original update to mark the ticket as complete, the Ticket Service asynchronously sent the ticket information to a queue (step 4 in the diagram) to be processed by the Analytics Service (step 5).\nHowever, when the compensating update is issued to the Ticket Service (step 7), the ticket informa- tion has already been processed by the Analytics Service in step 5.”\nBy reversing the transaction in the Ticket Service, actions performed by other services using data from the prior update may have already taken place and might not be able to be reversed.\nTo address this issue, the Ticket Service could send another request through the data pump to the Analytics Service, tell- ing that service to ignore the prior ticket information, but just imagine the amount of complex code and timing logic that would be required in the Analytics Service to address this compensating change.\nWith distributed architectures and distributed transactions, it really is sometimes turtles all the way down.”\nKeeping with the same Epic Saga example for completing a ticket, notice in Figure 12-24 that in step 7 a compensating update is issued to the Ticket Service to change the state from completed back to in- progress.\nHowever, in this case, the Ticket Service generates an error when trying to change the state of the ticket (step 8).”\nSysops Squad Saga: Atomic Transactions and Compensating Updates\n“Architects and developers tend to assume that compensating updates will always work,” Logan said.\nIf an error occurs, the end user must wait until all corrective action is taken (through compensating updates) before a response is sent telling the user about the error.”\n“Yes, while responsiveness can sometimes be resolved by asynchronously issuing compensating updates through eventual consistency (such as with the Parallel Saga and the Anthology Saga pat- tern), nevertheless most atomic distributed transactions have worse responsiveness when compen- sating updates are involved.”\nLet’s build a table to summarize some of the trade-offs associated with atomic distributed transactions and compensating updates.” (See Table 12-12.)\nTrade-offs associated with atomic distributed transactions and compensating updates\nAlternatively, the mediator could insist that other services don’t accept calls during the course of a workflow, which guarantees a valid transaction but destroys performance and scalability.”\nSysops Squad Saga: Atomic Transactions and Compensating Updates\nThese sagas rely on asynchronous eventual consistency and state management rather than atomic distributed transactions with compensating updates when errors occur.\nWith these types of sagas, the user is less impacted by errors that might occur within the distributed transaction, because the error is addressed behind the scenes, without end-user involvement.",
    "keywords": [
      "Ticket Orchestrator Service",
      "Ticket Service",
      "TICKET",
      "Sysops Squad Saga",
      "Service",
      "transactional sagas",
      "saga",
      "Ticket Service updates",
      "Survey Service",
      "Ticket Orchestrator",
      "Analytics Service",
      "TICKET saga",
      "atomic distributed transactions",
      "Orchestrator Service",
      "compensating updates"
    ],
    "concepts": [
      "sagas",
      "saga",
      "ticket",
      "ticketing",
      "services",
      "service",
      "transactions",
      "transaction",
      "logan",
      "updated"
    ]
  },
  {
    "chapter_number": 38,
    "title": "Segment 38 (pages 377-384)",
    "start_page": 377,
    "end_page": 384,
    "summary": "contract\nStrict Versus Loose Contracts Like many things in software architecture, contracts don’t exist within a binary but rather on a broad spectrum, from strict to loose.\nThe spectrum of contract types, from strict to loose\nHowever, architects aren’t forced to use strict contracts and should do so only when advantageous.\nExample 13-1 shows a strict JSON contract with schema information attached.\nStrict Versus Loose Contracts\nStrict JSON contract\nThis creates a strict contract, with required fields and types specified.\nAt the far end of the spectrum of contract coupling lie extremely loose contracts, often expressed as name-value pairs in formats like YAML or JSON, as illustrated in Example 13-4.\nUsing such loose contracts allows for extremely decoupled systems, often one of the goals in architectures, such as microservices.\nStrict Versus Loose Contracts\nTrade-Offs Between Strict and Loose Contracts When should an architect use strict contracts and when should they use looser ones?\nStrict contracts\nStrict contracts also have a few disadvantages:\nBy our general definition of coupling, strict contracts create tight coupling points.\nLoose contracts\nLoose contracts, such as name-value pairs, offer the least coupled integration points, but they too have trade-offs, as summarized in Table 13-2.\nThese are some advantages of loose contracts:\nMany architects have a stated goal for microservices architectures that includes high levels of decoupling, and loose contracts provide the most flexibility.\nLoose contracts also have a few disadvantages:\nLoose contracts by definition don’t have strict contract features, which may cause problems such as misspelled names, missing name-value pairs, and other defi‐ ciencies that schemas would fix.\nTo solve the contract issues just described, many teams use consumer-driven contracts as an architecture fitness function to make sure that loose contracts still contain sufficient information for the contract to function.\nStrict Versus Loose Contracts\nFor an example of the common trade-offs encountered by architects, consider the example of contracts in microservice architectures.\nContracts in Microservices Architects must constantly make decisions about how services interact with one another, what information to pass (the semantics), how to pass it (the implementa‐ tion), and how tightly to couple the services.\nThe architect could implement both services in the same technology stack and use a strictly typed contract, either a platform-specific remote procedure protocol (such as RMI) or an implementation-independent one like gRPC, and pass the customer information from one to another with high confidence of contract fidelity.\nWhen pass‐ ing information, the architect utilizes name-value pairs in JSON to pass the relevant information in a loose contract.\nSome protocols, such as JSON, include schema tools to allow architects to overlay loose contracts with more metadata.\nA common problem in microservices architectures is the seemingly contradictory goals of loose coupling yet contract fidelity.\nStrict Versus Loose Contracts",
    "keywords": [
      "Loose Contracts",
      "contracts",
      "Versus Loose Contracts",
      "contract",
      "Strict Versus Loose",
      "strict contracts",
      "Loose",
      "Strict",
      "strict JSON contract",
      "Versus Loose",
      "Profile",
      "contracts Loose contracts",
      "Loose contracts Loose",
      "Customer",
      "Strict Versus"
    ],
    "concepts": [
      "contracts",
      "contract",
      "types",
      "type",
      "coupling",
      "coupled",
      "couple",
      "architect",
      "architects",
      "information"
    ]
  },
  {
    "chapter_number": 39,
    "title": "Segment 39 (pages 385-393)",
    "start_page": 385,
    "end_page": 393,
    "summary": "The concept of a consumer-driven contract inverses that rela‐ tionship into a pull model; here, the consumer puts together a contract for the items they need from the provider, and passes the contract to the provider, who includes it in their build and keeps the contract test green at all times.\nThe contract encapsulates the information the consumer needs from the provider.\nConsumer-driven contracts allow the provider and consumers to stay in sync via automated architectural governance\nEach consumer creates a contract specify‐ ing required information and passes it to the provider, who includes their tests as part of a continuous integration or deployment pipeline.\nThis allows each team to specify the contract as strictly or loosely as needed while guaranteeing contract fidelity as part of the build process.\nConsumer-driven contracts are quite common in microservices architecture because they allow architects to solve the dual problems of loose coupling and governed inte‐ gration.\nTrade-offs of consumer-driven contracts are shown in Table 13-3.\nAdvantages of consumer-driven contracts are as follows:\nAllow loose contract coupling between services\nThese are disadvantages of consumer-driven contracts:\nFor example, if all teams run continuous integration that includes contract tests, then fitness functions provide a good verification mechanism.\nThus, many architects use the combination of name-value pairs and consumer-driven contracts to validate contracts.\nThe architect’s best solution for this trade-off comes down to team maturity and decoupling with loose contracts versus complexity plus certainty with stricter contracts.\nTrade-offs for consumer-driven contracts\nAllows loose contract coupling between services\nStamp Coupling A common pattern and sometimes anti-pattern in distributed architectures is stamp coupling, which describes passing a large data structure between services, but each service interacts with only a small part of the data structure.\nStamp coupling, however, is often an accidental anti-pattern, where an architect has over-specified the details in a contract that aren’t needed or accidentally consumes far too much bandwidth for mundane calls.\nOver-Coupling via Stamp Coupling Going back to our Wishlist and Profile Services, consider tying the two together with a strict contract combined with stamp coupling, as illustrated in Figure 13-7.\nIn this example, even though the Wishlist Service needs only the name (accessed via a unique ID), the architect has coupled Profile’s entire data structure as the contract, perhaps in a misguided effort for future proofing.\nOver-specifying details in contracts is generally an anti-pattern but easy to fall into when also using stamp coupling for legitimate concerns, including uses such as work‐ flow management (see “Stamp Coupling for Workflow Management” on page 378).\nArchitects can use stamp coupling to manage the workflow state between services, passing both domain knowledge and workflow state as part of the contract, as illus‐ trated in Figure 13-8.\nIn this example, an architect designs the contract to include workflow information: status of the workflow, transactional state, and so on.\nUsing stamp coupling to manage workflow does create higher coupling between serv‐ ices than nominal, but the semantic coupling must go somewhere—remember, an architect cannot reduce semantic coupling via implementation.\nSysops Squad Saga: Managing Ticketing Contracts Tuesday, May 10, 10:10\n“The contracts between the orchestrator and the two ticket services, Ticket Manage- ment and Ticket Assignment, are tight; that information is highly semantically coupled and likely to change together,” Addison said.\nTypes of contracts between collaborators in the ticket management workflow\nADR: Loose Contract for Sysops Squad Expert Mobile Application\nDecision We will use a loose, name-value pair contract to pass information to and from the orches- trator and the mobile application.\nThe separation of application and data processing allowed better transactional management, coordination, and numerous other benefits, including the ability to start utilizing historical data for new purposes, such as analytics.",
    "keywords": [
      "Stamp Coupling",
      "data",
      "contract",
      "contracts",
      "coupling",
      "loose contract coupling",
      "Consumer-driven contracts",
      "Analytical Data",
      "data warehouse",
      "Stamp",
      "contract coupling",
      "services",
      "loose contract",
      "Managing Ticketing Contracts",
      "Managing Analytical Data"
    ],
    "concepts": [
      "data",
      "allows",
      "allowing",
      "allowed",
      "services",
      "service",
      "coupling",
      "coupled",
      "architectural",
      "architecture"
    ]
  },
  {
    "chapter_number": 40,
    "title": "Segment 40 (pages 394-416)",
    "start_page": 394,
    "end_page": 416,
    "summary": "For example, an operational system needs to structure schemas and behavior around transactions, whereas an analytical system is rarely OLTP data (see Chapter 1) but typically deals with large amounts of data, for reporting, aggregations, and so on.\nThus, most data warehouses utilized a Star Schema to implement dimensional modelling, transforming data from operational systems in differing formats into the warehouse schema.\nBecause the operational data resides in individual systems, the warehouse must build mechanisms to regularly extract the data, transform it, and place it in the warehouse.\nBecause the data “lives” in the warehouse, all analysis is done there.\nThe data warehouse utilized data analysts, whose job included building reports and other business intelligence assets.\nHowever, building useful reports requires domain understanding, meaning that domain expertise must reside in both the operational data system and the analytical systems, where query designers must use the same data in a transformed schema to build meaningful reports and busi‐ ness intelligence.\nThe output of the data warehouse included business intelligence reports, dash‐ boards that provide analytical data, reports, and any other information to allow the company to make better decisions.\nThe Data Warehouse pattern provides a good example of technical partitioning in software architecture: warehouse designers transform the data into a schema that facilitates queries and analysis but loses any domain partitioning, which must be re- created in queries where required.\nHowever, the major failings of the Data Warehouse pattern included integration brit‐ tleness, extreme partitioning of domain knowledge, complexity, and limited function‐ ality for intended purpose:\nThe requirement built into this pattern to transform the data during the injection phase creates crippling brittleness in systems.\nA database schema for a particular problem domain is highly coupled to the semantics of that problem; changes to the domain require schema changes, which in turn require data import logic changes.\nBuilding an alternate schema to allow advanced analytics adds complexity to the system, along with the ongoing mechanisms required to injest and transform data.\nA data warehouse is a separate project outside the normal operational systems for an organization, so must be maintained as a wholly separate ecosys‐ tem, yet highly coupled to the domains embedded inside the operational systems.\nThe need in a data warehouse to synchronize data across a wide variety of opera‐ tional systems creates both operational and organizational bottlenecks—a loca‐ tion where multiple and otherwise independent data streams must converge.\nTable 14-1 shows the trade-offs for the data warehouse pattern.\nTrade-offs for the Data Warehouse pattern\n“We looked at creating a data warehouse, but realized that it fit better with older, mono- lithic kinds of architectures than modern distributed ones,” said Logan.\n“What about the data lake idea I’ve been hearing about?” asked Dana.\n“I read a blog post on Martin Fowler’s site.1 It seems like it addresses a bunch of the issues with the data warehouse, and it is more suitable for ML use cases.”\nThe data lake was one of the early answers, mostly as a counter to the data warehouse, which definitely won’t work in something like microservices.”\nRather than do the immense work of transformation, the philosophy of the Data Lake pattern holds that, rather than do useless transformations that may never be used, do no transformations, allowing business users access to analytical data in its natural format, which typically required transformation and massaging for their purpose.\nThe basic observation that many architects made was that the prebuilt schemas in data warehouses were frequently not suited to the type of report or inquiry required by users, requiring extra work to understand the warehouse schema enough to craft a solution.\nCharacteristics of the Data Lake pattern are as follows:\nOperational data is still extracted in this pattern, but less transformation into another schema takes place—rather, the data is often stored in its “raw,” or native, form.\nThe Data Lake pattern, while an improvement in many ways to the Data Warehouse pattern, still suffered many limitations.\nWhile the Data Lake pattern avoided the transformation-induced problems from the Data Warehouse pattern, it also either didn’t address or created new problems.\ndiscovery process, domain experts have the knowledge necessary to avoid acci‐ dental exposures, forcing them to reanalyze data in the lake.\nThe current trend in software architecture shifts focus from partitioning a system based on technical capabilities into ones based on domains, whereas both the Data Warehouse and Data Lake patterns focus on technical partitioning.\nFor example, the microservices architecture attempts to separate services by domain rather than technical capabilities, encapsulating domain knowledge, including data, inside the service boundary.\nHowever, both the Data Warehouse and Data Lake patterns try to separate data as a separate entity, losing or obscuring important domain perspectives (such as PII data) in the process.\nThe last point is critical—increasingly, architects design around domain rather than technical partitioning in architecture, and both previous approaches exemplify sepa‐ rating data from its context.\nWhat architects and data scientists need is a technique that preserves the appropriate kind of macro-level partitioning, yet supports a clean separation of analytical from operational data.\nTable 14-2 lists the trade-offs for the Data Lake pattern.\nTrade-offs for the Data Lake pattern\nAlthough they do less transformation in the Data Lake pattern, it is still common, as well as data cleansing.\nThe Data Lake pattern pushes data integrity testing, data quality, and other quality issues to downstream lake pipelines, which can create some of the same operational bottlenecks that manifest in the Data Warehouse pattern.\nWithout careful coordination, architects either ignore the changes in upstream systems, resulting in stale data, or allow the coupled pipelines to break.\n“OK, so we can’t use the data lake either!” exclaimed Dana.\n“Fortunately, some recent research has found a way to solve the problem of analytical data with distributed architectures like microservices,” replied Logan.\n“It adheres to the domain boundaries we’re trying to achieve, but also allows us to project analytical data in a way that the data scientists can use.\nThe Data Mesh Observing other trends in distributed architectures, Zhamak Dehghani and several other innovators derived the core idea of the Data Mesh pattern from domain- oriented decoupling of microservices, service mesh, and sidecars (see “Sidecars and Service Mesh” on page 234), and applied it to analytical data, with modifications.\nAs we mentioned in Chapter 8, the Sidecar Pattern provides a nonentangling way to organize orthogonal coupling (see “Orthogonal Coupling” on page 238); the separa‐ tion between operational and analytical data is another excellent example of just such a coupling, but with more complexity than simple operational coupling.\nDomain ownership of data\nThis architecture allows for distributed sharing and accessing the data from multiple domains and in a peer-to-peer fashion without\nThe Data Mesh\nany intermediary and centralized lake or warehouse, and without a dedicated data team.\nData as a product\nThis principle leads to the introduction of a new architectural quantum called data product quantum, to maintain and serve discoverable, understanda‐ ble, timely, secure, and high-quality data to the consumers.\nThis chapter introdu‐ ces the architectural aspect of the data product quantum.\nTo empower the domain teams to build and maintain their data products, data mesh introduces a new set of self-serve platform capabilities.\nData mesh introduces a federated decision- making model composed of domain data product owners.\nThe architectural implication of this approach to governance is a platform- supplied embedded sidecar in each data product quantum to store and execute the policies at the point of access: data read or write.\nIn this chapter, we focus on the core architectural element, the data product quantum.\nData Product Quantum The core tenet of the data mesh overlays modern distributed architectures such as microservices.\nJust as in the service mesh, teams build a data product quantum (DPQ) adjacent but coupled to their service, as illustrated in Figure 14-1.\nThe domain includes a data product quantum, which also contains code and data, and which acts as an interface to the overall analytical and reporting por‐ tion of the system.\nThe DPQ acts as an operationally independent but highly coupled set of behaviors and data.\nProvides analytical data on behalf of the collaborating architecture quantum, typ‐ ically a microservice, acting as a cooperative quantum.\nThe Data Mesh\nThe data product quantum acts as a separate but highly coupled adjunct to a service\nThe data\nproduct quantum also likely has behavior as well as data for the purposes of analytics and business intelligence.\nAn operationally separate quantum that communicates with its cooperator via asynchronous communication and eventual consistency, yet features tight con‐ tract coupling with its cooperator and generally looser contract coupling to the analytics quantum, the service responsible for reports, analysis, business intelli‐ gence, and so on.\nTo operate, this analytical quantum has static quantum coupling to the individual data product quanta it needs for information.\nData Mesh, Coupling, and Architecture Quantum Because analytical reporting is probably a required feature of a solution, the DPQ and its communication implementation belong to the static coupling of an architecture quantum.\nHowever, like the Sidecar pattern in a service mesh, the DPQ should be orthogonal to implementation changes within the service, and maintain a separate contract with the data plane.\nFrom a dynamic quantum coupling standpoint, the data sidecar should always imple‐ ment one of the communication patterns that features both eventual consistency and asynchronicity: either the “Parallel Saga(aeo) Pattern” on page 346 or “Anthology Saga(aec) Pattern” on page 349.\nIn other words, a data sidecar should never include a transactional requirement to keep operational and analytical data in sync, which would defeat the purpose of using a DPQ for orthogonal decoupling.\nSimilarly, com‐ munication to the data plane should genearlly be asynchronous, so as to have mini‐ mal impact on the operational architecture characteristics of the domain service.\nWhen to Use Data Mesh Like all things in architecture, this pattern has trade-offs associated with it, as shown in Table 14-3.\nThe Data Mesh\nTrade-offs for the Data Mesh pattern\nRequires contract coordination with data product quantum\nAllows excellent decoupling between analytical and operational data\nIt is more difficult in architectures where analytical and operational data must stay in sync at all times, which presents a daunting challenge in distributed architectures.\nData mesh is an outstanding example of the constant incremental evolution that occurs in the software development ecosystem; new capabilities create new perspec‐ tives, which in turn help address some persistent headaches from the past, such as the artificial separation of domain from data, both operational and analytical.\nSysops Squad Saga: Data Mesh Friday, June 10, 09:55\n“I just returned from a meeting with our data scientists, and they are trying to figure out a way we can solve a long-term problem for us—we need to become data-driven in expert supply planning, for skill sets demand for different geographical locations at dif- ferent points in time.\n“I haven’t been involved in much of the data mesh implementation—how far along are we?” asked Addison.\nLogan said, “Tickets DPQ is its own architecture quantum, and acts as an aggregation point for a couple of different ticket views that other systems care about.”\n“The data mesh platform team is supplying the data users and data product developers with a set of self-serve capabilities.\nThat allows any team that wants to build a new analytical use case to search and find the data products of choice within existing architecture quanta, directly connect to them, and start using them.\nThe platform also supports domains that want to create new data products.\nThe platform continuously monitors the mesh for any data prod- uct downtimes, or incompatibility with the governance policies and informs the domain teams to take actions.”\nLogan said, “The domain data product owners in collaboration with security, legal, risk, and compli- ance SMEs, as well as the platform product owners, have formed a global federated governance group, which decides on aspects of the DPQs that must be standardized, such as their data-sharing contracts, modes of asynchronous transport of data, access control, and so on.\nSysops Squad Saga: Data Mesh\n“What data do we need in order to supply the information for the expert supply problem?”\nIts first product can be called supply recommendations, which uses an ML model trained using data aggregated from DPQs in sur- veys, tickets, and maintenance domains.\nThe Experts Supply DPQ will provide daily recommenda- tions data, as new data becomes available about tickets, surveys and expert profiles.\n“That’s correct—no data for a time period is better than incomplete data, which makes it seem like there was less traffic than there was,” Dana said.\n“Yes, I certainly do—an ADR that specifies complete information or none, and a fitness function to make sure we get complete data.”\nDecision We will ensure that each data source for the Expert Supply DPQ receives complete snap- shots for daily trends or no data for that day, allowing data scientists to exempt that day.\nSysops Squad Saga: Data Mesh\nFor example, for our architecture quantum dynamic coupling analysis, we chose coupling, complexity, responsiveness/availability, and scale/elasticity as our primary trade-off concerns, in addition to analyzing the three forces of communication, consistency, and coordina‐ tion, as shown in the ratings table for the “Parallel Saga(aeo) Pattern” on page 346, appearing again in Table 15-1.",
    "keywords": [
      "data",
      "Data Lake pattern",
      "Data Warehouse pattern",
      "data warehouse",
      "Data Mesh",
      "data product quantum",
      "data lake",
      "Managing Analytical Data",
      "analytical data",
      "Data Mesh Data",
      "data product",
      "Data Mesh pattern",
      "Expert Supply DPQ",
      "domain data product",
      "operational data"
    ],
    "concepts": [
      "data",
      "coupled",
      "coupling",
      "couple",
      "architecture",
      "architectures",
      "architectural",
      "domains",
      "requirements",
      "required"
    ]
  },
  {
    "chapter_number": 41,
    "title": "Segment 41 (pages 417-424)",
    "start_page": 417,
    "end_page": 424,
    "summary": "First, finding the best context for a decision allows the architect to consider fewer options, greatly simplifying the decision process.\nFinding the correct narrow context for decisions allows architects to think about less, in many cases sim‐ plifying design.\nModel Relevant Domain Cases Architects shouldn’t make decisions in a vacuum, without relevant drivers that add value to the specific solution.\nAdding those domain drivers back to the decision pro‐ cess can help the architect filter the available options and focus on the really impor‐ tant trade-offs.\nFor example, consider this decision by an architect as to whether to create a single payment service or a separate service for each payment type, as illustrated in Figure 15-5.\nHowever, those forces are generic—an architect may add more nuance to the decision by modeling some likely scenarios.\nFor example, consider the first scenario, illustrated in Figure 15-6, to update a credit card processing service.\nIn the second scenario, the architect models what happens when the system adds a new payment type, as shown in Figure 15-7.\nThe architect adds a reward points payment type to see what impact it has on the architecture characteristics of interest, highlighting extensibility as a benefit of sepa‐ rate services.\nIn this scenario, the architect starts gaining insight into the real trade-offs involved in this decision.\nHaving modeled these three scenarios, the architect realizes that the real trade-off analysis comes down to which is more important: performance and data consistency (a single payment service) or extensibility and agility (separate services).\nAs architecture generally evades generic solutions, it is important for architects to build their skills in modeling relevant domain scenarios to home in on better trade-off analysis and decisions.\nPrefer Bottom Line over Overwhelming Evidence It’s easy for architects to build up an enormous amount of information in pursuit of learning all the facets of a particular trade-off analysis.\nRather than show all the information they have gathered, an architect should reduce the trade-off analysis to a few key points, which are sometimes aggregates of individ‐ ual trade-offs.\nConsider the common problem an architect might face in a microservices architec‐ ture about the choice of synchronous or asynchronous communication, illustrated in Figure 15-9.\nAfter considering the generic factors that point to one versus the other, the architect next thinks about specific domain scenarios of interest to nontechnical stakeholders.\nTrade-offs between synchronous and asynchronous communication for credit card processing\nAfter modeling these scenarios, the architect can create a bottom-line decision for the stakeholders: which is more important, a guarantee that the credit approval process starts immediately or responsiveness and fault-tolerance?\nThis architect has likely worked on problems in the past where extensibility was a key driving architecture characteristic and believes that capability will always drive the decision process.\nWhile experience is useful, scenario analysis is one of an architect’s most powerful tools to allow iterative design without building whole systems.\nBy modeling likely scenarios, an architect can discover if a particular solution will, in fact, work well.\nThe architect’s goal is to add bid history to the workflow—should the team keep the existing publish-and-subscribe approach or move to point-to-point messaging for each consumer?\nTo discover the trade-offs for this specific problem, the architect should model likely domain scenarios using the two topologies.",
    "keywords": [
      "architect",
      "Trade-Off Analysis",
      "decision",
      "Trade-Off",
      "architects",
      "trade-offs",
      "Trade-Off Techniques",
      "separate services",
      "services",
      "architecture",
      "Analysis",
      "scenario",
      "single payment service",
      "illustrated in Figure",
      "Build"
    ],
    "concepts": [
      "decision",
      "decisions",
      "solution",
      "architectures",
      "architecture",
      "architect",
      "architects",
      "trade",
      "service",
      "services"
    ]
  },
  {
    "chapter_number": 42,
    "title": "Segment 42 (pages 425-432)",
    "start_page": 425,
    "end_page": 432,
    "summary": "Table 8-1, “Trade-offs for the code replication technique” on page 223\nTable 8-2, “Trade-offs for the shared library technique” on page 227\nTable 8-3, “Trade-offs for the shared service technique” on page 233\nTable 8-4, “Trade-offs for the Sidecar pattern / service mesh technique” on page 239\nTable 9-2, “Joint ownership data-domain technique trade-offs” on page 258\nTable 9-4, “Joint ownership service consolidation technique trade-offs” on page 262\nTable 9-6, “Orchestrated request-based pattern trade-offs” on page 277\nTable 9-7, “Event-based pattern trade-offs” on page 279\nTable 10-1, “Trade-offs for the Interservice Communication data access pattern” on page 286\nTable 10-2, “Trade-offs for the Column Schema Replication data access pattern” on page 288\nTable 10-3, “Trade-offs associated with the replicated caching data access pattern” on page 293\nTable 10-4, “Trade-offs associated with the data domain data access pattern” on page 295\nTable 11-1, “Trade-offs for orchestration” on page 306\nTable 11-2, “Trade-offs for the Front Controller pattern” on page 312\nTable 11-3, “Stateless choreography trade-offs” on page 313\nTable 11-4, “Stamp coupling trade-offs” on page 314\nTable 11-5, “Trade-offs for the choreography communication style” on page 315\nTable 11-7, “Updated trade-offs between orchestration and choreography for ticket workflow” on page 320\nTable 11-8, “Final trade-offs between orchestration and choreography for ticket workflow” on page 320\nTable 12-11, “Trade-offs associated with state management rather than atomic dis‐ tributed transactions with compensating updates” on page 356\nTable 12-12, “Trade-offs associated with atomic distributed transactions and compen‐ sating updates” on page 363\nTable 13-1, “Trade-offs for strict contracts” on page 371\nTable 13-2, “Trade-offs for loose contracts” on page 372\nTable 13-4, “Trade-offs for stamp coupling” on page 379\nTable 14-1, “Trade-offs for the Data Warehouse pattern” on page 385\nTable 14-2, “Trade-offs for the Data Lake pattern” on page 388\nTable 14-3, “Trade-offs for the Data Mesh pattern” on page 394",
    "keywords": [
      "trade-offs",
      "trade-off",
      "trade-off analysis",
      "trade-off tables",
      "ADR",
      "Trade-Off References",
      "technique trade-offs",
      "databases",
      "Data",
      "pattern trade-offs",
      "Sysops Squad",
      "databases rated",
      "architecture",
      "pattern",
      "adoption characteristics"
    ],
    "concepts": [
      "database",
      "databases",
      "tables",
      "architecture",
      "architectural",
      "coupling",
      "coupled",
      "patterns",
      "pattern",
      "data"
    ]
  },
  {
    "chapter_number": 43,
    "title": "Segment 43 (pages 433-447)",
    "start_page": 433,
    "end_page": 447,
    "summary": "about, 132 about decomposing monolithic, 151 assign tables to data domains, 156-158 create data domains, 156 drivers of decomposition, 132-149 Refactoring Databases (Ambler and\ndata warehouses, 384 strict contracts creating, 367 composition versus inheritance, 228 data importance, 4 (see also data) definitions of terms, 14\narchitecture quantum about, 28, 144 coupling, static versus dynamic, 28\ndata disintegration driver, 144 separate databases, 159 Sysops Squad saga, 151 data product quantum, 390-393 high functional cohesion, 30\nabout complexity of, 341, 345 definition, 14 delegate technique of data ownership, 259 distributed data access, 287 fault tolerance, 59 performance issues, 231\nbasic availability of BASE transactions, 267 coupling relationship, 403 database types\narchitecture quanta, 35 architecture quantum versus, 42 breaking database changes controlled,\ndata domains, 155\ncombining data domains, 157\ndata requirement in some architectures, 132 data sovereignty per service, 159 granularity and, 206 high functional cohesion, 30 ownership of data (see ownership of data)\nbrittleness in architecture, 243 data warehouses, 384 strict contracts creating, 367\nC caching for distributed data access, 288-293 Chain of Responsibility design pattern, 338 The Checklist Manifesto (Gawande), 13 choreographed coordination, 306-311\ndefinition, 15 service granularity and, 200-203 stamp coupling for management, 378 workflow state management, 311-315 Front Controller pattern, 311 stamp coupling, 313 stateless choreography, 313\nclient/server applications, 382 cloud native databases, 175 Cockburn, Alistair, 235 CockroachDB NewSQL database, 174 code replication as reuse pattern, 221-223 code reuse patterns about, 220 code replication, 221-223 granularity and shared code, 203-205, 224 orthogonal reuse pattern, 238 shared libraries about, 223 granularity and, 224 versioning strategies, 225-227\ndatabase architecture quantum, 145\ndatabase connection pool, 138 Sysops Squad saga, 150 distributed data access, 287 dynamic coupling as, 23, 29\ntions, 275, 327 state management instead, 352, 355 Sysops Squad saga, 358-364\nabout, 71, 82 architecture stories, 84 create component domains, 120-122 create domain services, 126-129 determine component dependencies,\n120-122 fitness functions, 122 definition, 14, 51, 71, 103 dependencies determination, 112 Identify and Size Components pattern,\nsize and granularity, 189 composite architecture characteristics, 7 composition versus inheritance in design, 228 connection management\ndatabase per service, 143 Sysops Squad saga, 150\ndefinition, 15 distributed data access\nabout orchestration versus, 300, 331 definition, 15 stamp coupling for management, 378 workflow state management, 311 dynamic coupling relationship matrix, 40,\nabout choreographed versus, 300, 331 definition, 14 workflow state management, 311 stamp coupling for management, 378 Sysops Squad saga, 299, 317-321\nabout static versus dynamic, 29 dynamic coupling, 23, 29 static coupling, 23, 28 Sysops Squad saga, 42 architectural decomposition afferent coupling, 66, 68 common domain functionality consoli‐\ndynamic quantum coupling, 38-41 high static coupling, 30-37 independently deployable, 29 Sysops Squad saga, 42 user interface, 36\navailability relationship, 403 data relationships, 147 definition, 14, 27 interservice communication pattern, 286 loose contracts for decoupling, 369, 371 consumer-driven contracts, 373 microservices, 372 microservices and contract fidelity, 373 strict contract tight coupling, 370\nmaintainability and, 51 operational coupling, 234-239 Sysops Squad saga, 239\nfitness functions, 122 Sysops Squad saga, 123-126, 129 Create Domain Services pattern, 126-129\nfitness functions, 129 Sysops Squad saga, 129\nabout, 4, 381 data lakes, 386-389 data meshes, 389-394 data meshes coupled to reporting, 393 data warehouses, 382-385 definition, 5 domain over technical partitioning, 388 Sysops Squad saga, 381, 386, 389,\nrelationships among data, 147 data domains (see data domains) database-per-service, 143\nabout, 132 about decomposing monolithic, 151 assign tables to data domains, 156-158 create data domains, 156 data access in other domains, 158 database-per-service requiring, 147 dependencies, 153-155 drivers of, 132-149 Refactoring Databases (Ambler and\nabout, 133 architecture quantum, 144 connection management, 138-141 database change control, 134-138 database type optimization, 146 fault tolerance, 143 scalability, 141\ngranularity and data relationships, 205-207 integration drivers about, 146 database transactions, 148\nabout, 249 assigning, 250 common ownership, 252 data mesh domain ownership, 389 data sovereignty per service, 159 distributed data access, 287, 289 joint ownership, 253-260 service consolidation technique, 261 single ownership, 251 summary, 262 Sysops Squad saga, 249, 279-282\nSysops Squad saga, 151 Sysops Squad data model, 19\ndata domains, 152\ndata domain technique of joint ownership,\n256-258 data domains about, 152 about decomposing monolithic data, 151 assign tables to data domains, 156-158 bounded context rules, 155 create data domains, 156 schemas to separate servers, 159 separate connections to data domains,\nservice consolidation, 261 tables tightly coupled, 157 data access in other domains, 158 data schemas versus, 157\nsynonyms for tables, 157 distributed data access, 293-295 joint ownership of data, 256-258 soccer ball visualization, 153 Sysops Squad data model, 152\nabout, 389 Data Mesh (Dehghani), 390 data product quantum, 390-393 Sysops Squad saga, 394-397 when to use, 393\nNewSQL databases, 174 database refactoring book, 154 database structure Sysops Squad saga, 180-184 database transactions and granularity, 198 database types, 178\nabout characteristics rated, 161 aggregate orientation, 165 cloud native databases, 175 column family databases, 169 database type optimization, 146 document databases, 167 graph databases, 171-173 key-value databases, 165-167 NewSQL databases, 173 NoSQL databases, 165 relational databases, 163-164 schema-less databases, 169 Sysops Squad saga, 179 time series databases, 177-178\ndatabase-per-service, 143\nmonolithic data domains, 153-155\n111-118 fitness functions, 117 Sysops Squad saga, 118-120\ndata sovereignty per service, 159 database connection pool, 138\nconnection management, 139 Sysops Squad saga, 150\nsharing data, 257 side effects, 361 state machines, 352-355 static coupling, 32\nColumn Schema Replication pattern, 287 Data Domain pattern, 293-295 Interservice Communication pattern, 285 Replicated Caching pattern, 288-293 Sysops Squad saga, 283, 295-298\nBASE transactions, 267 compensating updates, 275, 327 Sysops Squad saga, 358-364\n(see also transactional saga patterns)\nservice granularity and workflow, 200-203 stamp coupling for management, 378\nSysops Squad saga, 299, 317-321 transactional saga patterns, 323\n(see also transactional saga patterns)\ndata domains, 152 data lakes losing relationships, 387 data mesh domain ownership, 389 granularity and shared domain functional‐\nCreate Domain Services pattern, 126-129 fitness function for namespace, 129 domain cohesion and granularity, 194 Front Controller in choreography, 311 service-based architecture definition, 72,\nabout dimensions of, 300, 401 analytical data communication pattern, 393 communication, 38, 43 consistency, 40 coordination, 40 relationships among factors, 40, 324\nEpic Saga pattern, 325-330 Equifax data breach, 12 Evans, Erik, 165 event-based eventual consistency, 277-279 publish-and-subscribe messaging, 277\nchoreography, 314 data disintegration driver, 143 definition, 58, 143, 193 granularity disintegration driver, 193 granularity integration driver, 200-203 modularity driver, 49, 58 orchestration, 305\nshared libraries, 232 shared services, 232 single point of failure, 143 Sysops Squad saga, 151\nloose coupling requiring, 371 patterns for component-based decomposi‐\nfitness functions, 107 metrics for shared components, 106 Sysops Squad saga, 107-110\ntern, 94-96 fitness functions, 95 Sysops Squad saga, 97-99\nabout, 187, 197 data relationships, 205-207 database transactions, 198 shared code, 203-205 workflow and choreography, 200-203\nmetrics, 187 microservice single responsibility, 186, 190 modularity versus, 186 shared data and, 257 shared libraries, 224 Sysops Squad saga, 185, 209-216\ndatabase architecture quantum, 145\nfitness functions, 87-89 Sysops Squad saga, 81 implementation coupling, 309 independent deployability, 29 InfluxDB time series database, 177 infrastructure versus domain shared function‐\nabout, 253 data domain technique, 256-258 delegate technique, 258-261 service consolidation technique, 261 table split technique, 254-256\narchitecture quantum, 28, 29 decoupled services, 35 high functional cohesion, 30 user interface coupling, 36\narchitecture quanta, 35 architecture quantum versus, 42 breaking database changes controlled,\ndata domains, 155 data domains combined, 157 data requirement, 132 database abstraction, 136-138 granularity and, 206 high functional cohesion, 30 ownership of data (see ownership of\ndeployability, 56 scalability and elasticity, 57 operational coupling, 234-239 orchestrator per workflow, 301 saga pattern, 323 service-based architecture as stepping-\ngranularity versus, 186 monolithic architectures about modularity, 50 about need for modularity, 47 deployability, 55 fault tolerance, 58 maintainability, 51 modular monoliths, 50 scalability and elasticity, 57 Sysops Squad saga, 45, 59-62 testability, 54 water glass analogy, 47\ndata decomposition about, 151 assign tables to data domains, 156-158 create data domains, 156 Refactoring Databases (Ambler and\ndata importance, 4 database connection pool, 138 database type optimization, 146 Epic Saga mimicking, 325-330 modular architecture\nabout need for, 47 deployability, 55 fault tolerance, 58 maintainability, 51 modular monoliths, 50 scalability and elasticity, 57 Sysops Squad saga, 45, 59-62 testability, 54 water glass analogy, 47\nabout, 71, 82 architecture stories, 84 create component domains, 120-122 create domain services, 126-129 determine component dependencies,\nsingle architecture quantum, 29 functional cohesion, 30 static coupling, 31\nO online resources (see resources online) Online Transactional Processing (OLTP), 4 operational coupling, 234-239 Sysops Squad saga, 239 operational data definition, 4 Oracle Coherence replicated caching, 290 Oracle relational database, 164 orchestrated coordination, 301-306\nOrchestrated Saga pattern, 325-330 orphaned classes definition, 103 orthogonal code reuse pattern, 238 orthogonal coupling, 238 out-of-context trap, 405-407 ownership of data about, 249 assigning, 250 common ownership, 252 data mesh domain ownership, 389 data sovereignty per service, 159 distributed data access, 287, 289 joint ownership about, 253 data domain technique, 256-258 delegate technique, 258-261 table split technique, 254-256 service consolidation technique, 261 single ownership, 251 summary, 262 Sysops Squad saga, 249, 279-282\nP Page-Jones, Meilir, 23 Parallel Saga pattern, 346-348 PCI (Payment Card Industry) data security and granularity, 195\nPersonally Identifiable Information (PII), 387 Phone Tag Saga pattern, 330-333 PII (Personally Identifiable Information), 387 Pipes and Filters design pattern, 338, 351 platforms for code reuse, 244 Ports and Adaptors Pattern, 235 PostgreSQL relational database, 164 publish-and-subscribe messaging, 277\nSaga Pattern (Richardson), 323 stamp coupling information, 376 static code analysis tools, 85\nabout, 323 definition, 15 dynamic coupling matrix, 41, 324, 402 microservice saga pattern, 323 state machines, 352-355 Sysops Squad saga background, 15 (see also Sysops Squad sagas)\ntechniques for managing, 356 transactional saga patterns\nchoreography, 314, 331 connection management, 141 database-per-service, 143 Sysops Squad saga, 150 coupling relationship, 332, 403 data disintegration driver, 141 database types\ncross-schema access resolved, 158 data domain versus data schema, 157 databases separated physically, 159 schema-less databases, 169 synonyms for tables, 157\ngranularity disintegration driver, 195 PCI data Sysops Squad saga, 212-216 security latency, 285 semantic coupling, 309\nSysops Squad saga, 239 service-based architecture about, 32, 72, 126 data access in other domains, 158 data shared by services, 132 changes to database, 134 connection management, 138-141 single point of failure, 143\n121 Create Domain Services pattern, 126-129\nabout, 71, 82 architecture stories, 84 create component domains, 120-122 create domain services, 126-129 determine component dependencies,\ndata access in other domains, 158 data shared by services, 132 changes to database, 134 connection management, 138-141 single point of failure, 143 database connection pool, 138 Sysops Squad saga, 150\ndatabase-per-service, 143 definition, 14 distributed data access\nData Domain pattern, 293-295 Interservice Communication Pattern,\nReplicated Caching pattern, 288-293 Sysops Squad saga, 283, 295-298\norchestrated coordination definition, 14 ownership of data assigning, 250 common ownership, 252 data sovereignty per service, 159 distributed data access, 287, 289 joint ownership, 253-260 service consolidation technique, 261 single ownership, 251 summary, 262 Sysops Squad saga, 249, 279-282\nshared code and granularity, 203-205 (see also code reuse patterns) shared libraries, 224 shared domain functionality granularity and, 204 shared infrastructure functionality versus,\nchange risk, 229 fault tolerance, 232 performance, 231 scalability, 232 Sysops Squad saga, 244 versioning via API endpoints, 230\nSMS information link, 189 Snowflake cloud native database, 175 soft state of BASE transactions, 267 software architecture (see architecture) SOLID principles (Martin), 190 SonarQube code hygiene tool, 10 speed-to-market as modularity driver, 49 stamp coupling about, 376 bandwidth, 377 choreography state management, 313, 332 over-coupling via, 376 workflow management, 378 standard deviation calculation, 88 Star Schema of data warehouses, 383, 384 Starbucks Does Not Use Two-Phase Commit\nhigh static coupling, 30-37 independently deployable, 29 Sysops Squad saga, 42\nasynchronous versus, 38, 43 definition, 14 delegate technique of data ownership, 259 fault tolerance, 59 granularity and, 200-203 scalability and elasticity versus, 57\narchitectural components, 18 architecture quantum, 42 contracts, 365, 379 data model, 19\nACID transaction example, 264 data domains, 152\nData Domain pattern, 293-295 Interservice Communication Pattern,\n6. create domain services, 127 business case for, 59-62 code reuse patterns, 219, 239, 244 create component domains, 123-126 create domain services, 129 data pulled apart, 131, 150 database structure, 180-184 database types, 179 decomposition method, 63, 78 decomposition pattern, 81 determine component dependencies, 97-99\ngranularity, 185, 209-216 identify and size components, 90-93 ownership of data, 249, 279-282\ntime series databases, 177-178 Time Travel Saga pattern, 336-339 TimeScale time series database, 177 tools\nabout importance of, 116 afferent and efferent coupling, 66 data domains via soccer ball, 153 dependencies, 66, 113, 116 distance from main sequence, 69 domain diagramming exercise, 123 graph databases, 171-173 isomorphic diagrams, 325 static coupling diagram, 401\nbuilding your own, 400-403 consistency and availability, 178 coordination, 315 coupling, 26-28, 401-403 coupling points, 402 dynamic quantum coupling factors, 41 static coupling diagram, 401 documenting versus governing, 5 granularity, 208 iterative nature of, 403 loose contracts, 371 strict contracts, 370 Sysops Squad saga, 399, 416 tactical forking, 77 techniques of",
    "keywords": [
      "Sysops Squad saga",
      "Sysops Squad",
      "Squad saga",
      "Sysops Squad data",
      "234-239 Sysops Squad",
      "data Sysops Squad",
      "Index Sysops Squad",
      "create component domains",
      "create domain services",
      "distributed data access",
      "288-293 Sysops Squad",
      "create data domains",
      "common domain components",
      "data domains",
      "Data Domain pattern"
    ],
    "concepts": [
      "databases",
      "database",
      "data",
      "saga",
      "sagas",
      "pattern",
      "patterns",
      "architectural",
      "architecture",
      "architectures"
    ]
  },
  {
    "chapter_number": 44,
    "title": "Segment 44 (pages 448-455)",
    "start_page": 448,
    "end_page": 455,
    "summary": "transactional saga patterns\ndistributed architecture challenge, 40 security versus, 212-216\ndata integration driver, 148 database transactions and granularity, 198 distributed transactions about, 265, 327 ACID transactions, 263-267 BASE transactions, 267 compensating updates, 275, 327 eventual consistency, 267\nNeal Ford is a director, software architect, and meme wrangler at Thoughtworks, a software company and a community of passionate, purpose-led individuals who think disruptively to deliver technology that addresses the toughest challenges, all while seeking to revolutionize the IT industry and create positive social change.\nHe’s an internationally recognized expert on software development and delivery, especially in the intersection of Agile engineering techniques and software architecture.\nNeal has authored seven books (and counting), a number of magazine articles, and dozens of video presentations and spoken at hundreds of developers conferences worldwide.\nHis topics include software architecture, continuous delivery, functional program‐ ming, cutting-edge software innovations, and a business-focused book and video on improving technical presentations.\nMark Richards is an experienced, hands-on software architect involved in the archi‐ tecture, design, and implementation of microservices architectures, service-oriented architectures, and distributed systems in a variety of technologies.\nHe has been in the software industry since 1983 and has significant experience and expertise in applica‐ tion, integration, and enterprise architecture.\nMark is the author of numerous techni‐ cal books and videos, including the Fundamentals of Software Architecture, the “Software Architecture Fundamentals” video series, and several books and videos on microservices as well as enterprise messaging.\nHis expertise includes application development, Agile database development, evolutionary data‐ base design, algorithm design, and database administration.\nThe animal on the cover of Software Architecture: The Hard Parts is a black-rumped golden flameback woodpecker (Dinopium benghalense), a striking species of wood‐ pecker found throughout the plains, foothills, forests, and urban areas of the Indian subcontinent.\nLike other common, small-billed woodpeckers, the black-rumped golden flameback has a straight pointed bill, a stiff tail to provide support against tree trunks, and four-toed feet—two toes pointing forward and two backward.\nAs if its markings weren’t distinc‐ tive enough, the black-rumped golden flameback woodpecker is often detected by its call of “ki-ki-ki-ki-ki,” which steadily increases in pace.",
    "keywords": [
      "Sysops Squad saga",
      "Fairy Tale Saga",
      "Fantasy Fiction Saga",
      "Phone Tag Saga",
      "Time Travel Saga",
      "349-351 Epic Saga",
      "343-345 Parallel Saga",
      "330-333 Sysops Squad",
      "358-364 Time Travel",
      "transactional saga patterns",
      "Squad saga",
      "340-342 Horror Story",
      "325-330 Fairy Tale",
      "333-336 Fantasy Fiction",
      "346-348 Phone Tag"
    ],
    "concepts": [
      "sagas",
      "software",
      "architecture",
      "architectures",
      "database",
      "databases",
      "base",
      "based",
      "development",
      "developers"
    ]
  },
  {
    "chapter_number": 45,
    "title": "Segment 45 (pages 456-462)",
    "start_page": 456,
    "end_page": 462,
    "summary": "Chapter 45: Segment 45 (pages 456-462)",
    "keywords": [],
    "concepts": []
  }
]