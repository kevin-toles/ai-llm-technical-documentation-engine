#!/usr/bin/env python3
"""
Comprehensive Compliance Validator v3 (False Positive Filtering)
Validates: Chicago footnotes, annotations, verbatim blocks, summaries, TPM derivations, cross-book distribution.

Improvements over v2:
- Filters false positive chapter detections (ignores text inside code blocks)
- Filters false positive TPM detections (distinguishes TPM sections from concept sections)
- Filters false positive cross-book annotation requirements (ignores footnote sections)
- Improved chapter boundary detection
"""

from __future__ import annotations

import json
import re
import ast
import argparse
from pathlib import Path
from difflib import SequenceMatcher
from typing import Dict, List, Tuple, Optional, Set, Any
from collections import defaultdict, Counter
import sys

# ==========================
# Defaults (tunable via CLI)
# ==========================
DEFAULTS = {
    # Paths
    "json_dir": "Python_References/Engineering Practices/JSON",
    "markdown_file": "PYTHON_GUIDELINES_Learning_Python_Ed6.md",

    # Allowed JSON "books"
    "whitelist_json_files": [
        "Learning_Python_Ed6_Content",
        "Fluent_Python_2nd_Content",
        "Python_Distilled_Content",
        "Python_Essential_Reference_4th_Content",
        "Python_Data_Analysis_3rd_Content",
        "Python_Cookbook_3rd_Content",
        "BANA320_Python_Data_Analysis_Content",
    ],

    # Verbatim block validation
    "require_annotation_per_block": True,
    "annotation_window_lines": 8,

    # Chicago footnote lints
    "summary_min_footnotes_per_chapter": 2,

    # Summary grounding
    "summary_min_keyword_coverage": 0.40,
    "summary_min_rouge1": 0.50,
    "summary_min_rouge2": 0.20,
    "summary_max_longcopy": 10,
    "summary_max_unsupported_token_ratio": 0.20,
    "require_summary_annotations": True,
    "summary_min_annotations": 1,

    # TPM derivation
    "tpm_min_overlap": 0.45,
    "tpm_max_verbatim_ratio": 0.85,
    "tpm_max_longcopy_lines": 6,

    # Cross-book distribution
    "min_books_per_chapter": 3,
}

# ==========================
# Regexes
# ==========================
FOOTNOTE_DEF_RE = re.compile(
    r'^\[\^(?P<num>\d+)\]:\s*(?P<label>.+?)\.\s*\(JSON\s+`(?P<file>[^`]+)`,\s*p\.\s*(?P<page>\d+),\s*lines?\s*(?P<start>\d+)\s*(?:â€“|-)\s*(?P<end>\d+)\)\.\s*$',
    re.IGNORECASE
)
FOOTNOTE_REF_RE = re.compile(r'\[\^(?P<num>\d+)\]')
FENCE_OPEN_RE = re.compile(r'^```(\w+)?\s*$')
FENCE_CLOSE_RE = re.compile(r'^```\s*$')
HEADER_RE = re.compile(r'^(?P<hashes>#+)\s+(?P<title>.+)$')
DERIVED_MARKER_RE = re.compile(
    r'^\s*(Derived\s+from|Derives\s+from)\s*:\s*(?P<refs>(\[\^\d+\](\s*,\s*)?)+)\s*$',
    re.IGNORECASE
)
# TPM section header (must have "TPM Implementation Section" and "ORIGINAL")
TPM_SECTION_RE = re.compile(r'^###\s+\*\*TPM\s+Implementation\s+Section\*\*\s+\*\(ORIGINAL\)\*', re.IGNORECASE)
# Concept section header (starts with #### and page number)
CONCEPT_SECTION_RE = re.compile(r'^####\s+\*\*\d+\.')
# Footnotes section
FOOTNOTES_SECTION_RE = re.compile(r'^###\s+\*\*Footnotes\*\*', re.IGNORECASE)

# ==========================
# Utilities
# ==========================
def load_json_dir(json_dir: Path) -> Dict[str, Any]:
    data = {}
    for p in json_dir.glob("*.json"):
        try:
            obj = json.loads(p.read_text(encoding="utf-8"))
            data[p.stem] = obj
        except Exception as e:
            data[p.stem] = {"__error__": str(e)}
    return data

def infer_page_lines(json_obj: Dict, page: int) -> List[str]:
    pages = json_obj.get("pages", [])
    idx = page - 1
    if idx < 0 or idx >= len(pages):
        return []
    content = pages[idx].get("content", "") or ""
    return content.splitlines()

def normalize_ws(s: str) -> str:
    return " ".join(s.split())

def is_inside_code_block(lines: List[str], line_num: int) -> bool:
    """Check if a line is inside a code fence block."""
    in_block = False
    for i in range(line_num):
        if i >= len(lines):
            break
        if FENCE_OPEN_RE.match(lines[i]) or FENCE_CLOSE_RE.match(lines[i]):
            in_block = not in_block
    return in_block

def parse_chapters_improved(lines: List[str]) -> List[Dict[str, Any]]:
    """
    Parse chapters, filtering out false positives:
    - Ignore headers inside code blocks
    - Only detect ## Chapter N: Title patterns
    """
    chapters = []
    in_code_block = False
    
    for i, ln in enumerate(lines):
        # Track code block state
        if FENCE_OPEN_RE.match(ln) or FENCE_CLOSE_RE.match(ln):
            in_code_block = not in_code_block
            continue
        
        # Skip if we're inside a code block
        if in_code_block:
            continue
        
        # Only match "## Chapter N:" patterns
        m = HEADER_RE.match(ln)
        if m and len(m.group("hashes")) == 2:  # ## level
            title = m.group("title").strip()
            if title.startswith("Chapter "):
                chapters.append({
                    "line": i + 1,
                    "level": 2,
                    "title": title,
                    "content_start": i + 1
                })
    
    # Set end lines
    for idx in range(len(chapters)):
        if idx + 1 < len(chapters):
            chapters[idx]["content_end"] = chapters[idx + 1]["line"] - 1
        else:
            chapters[idx]["content_end"] = len(lines)
    
    return chapters

def parse_footnotes_from_text(md_text: str) -> Dict[int, Dict[str, Any]]:
    """Parse all footnote definitions."""
    footdefs = {}
    for ln in md_text.split("\n"):
        m = FOOTNOTE_DEF_RE.match(ln)
        if m:
            num = int(m.group("num"))
            footdefs[num] = {
                "author": m.group("label").split(".")[0].strip(),
                "title": m.group("label"),
                "file": m.group("file").replace(".json", ""),
                "page": int(m.group("page")),
                "start": int(m.group("start")),
                "end": int(m.group("end"))
            }
    return footdefs

def check_verbatim_blocks(lines: List[str], md_text: str, json_data: Dict[str, Any], opts: Dict) -> List[Dict[str, Any]]:
    """Validate verbatim code blocks against JSON sources."""
    errors = []
    footdefs = parse_footnotes_from_text(md_text)

    i = 0
    blocks_checked = 0
    while i < len(lines):
        ln = lines[i]
        if FENCE_OPEN_RE.match(ln):
            blocks_checked += 1
            if blocks_checked % 10 == 0:
                print(f"    Validating block {blocks_checked}", file=sys.stderr)
            block = []
            i0 = i + 1
            i += 1
            while i < len(lines) and not FENCE_CLOSE_RE.match(lines[i]):
                block.append(lines[i])
                i += 1
            if i < len(lines):
                i += 1

            # Next non-blank line must contain footnote ref
            j = i
            while j < len(lines) and not lines[j].strip():
                j += 1
            fn_num = None
            if j < len(lines):
                # Skip TPM blocks (have "Derived from:" marker)
                if DERIVED_MARKER_RE.match(lines[j].strip()):
                    i = j + 1
                    continue
                mref = FOOTNOTE_REF_RE.search(lines[j])
                if mref:
                    fn_num = int(mref.group("num"))
            if not fn_num or fn_num not in footdefs:
                errors.append({
                    "type": "block_missing_footnote",
                    "line_after_block": j+1 if j < len(lines) else i,
                    "preview": "\n".join(block[:3])
                })
                continue

            fd = footdefs[fn_num]
            if fd["file"] not in json_data:
                errors.append({"type": "verbatim_json_missing", "footnote": fn_num, "file": fd["file"]})
                continue

            src_lines = infer_page_lines(json_data[fd["file"]], fd["page"])
            if not src_lines or fd["start"] < 1 or fd["end"] > len(src_lines) or fd["end"] < fd["start"]:
                errors.append({
                    "type": "verbatim_line_oob",
                    "footnote": fn_num,
                    "page": fd["page"],
                    "start": fd["start"],
                    "end": fd["end"]
                })
                continue

            expected = "\n".join(src_lines[fd["start"]-1:fd["end"]])
            actual = "\n".join(block)
            if normalize_ws(expected) != normalize_ws(actual):
                errors.append({
                    "type": "verbatim_content_mismatch",
                    "footnote": fn_num,
                    "expected_preview": "\n".join(src_lines[fd["start"]-1:fd["start"]+2]),
                    "actual_preview": "\n".join(block[:3])
                })

        else:
            i += 1

    return errors

def check_tpm_sections_improved(lines: List[str], chapters: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Check TPM sections, filtering false positives:
    - Only detect sections with ### **TPM Implementation Section** *(ORIGINAL)*
    - Ignore concept sections (####)
    - Check that TPM sections have "Derived from:" citations
    - Check placement relative to chapter end
    """
    tpm_errors = []
    structure_errors = []
    
    in_code_block = False
    in_footnotes_section = False
    
    for i, ln in enumerate(lines):
        # Track code blocks
        if FENCE_OPEN_RE.match(ln) or FENCE_CLOSE_RE.match(ln):
            in_code_block = not in_code_block
            continue
        
        # Skip lines inside code blocks
        if in_code_block:
            continue
        
        # Track footnotes section
        if FOOTNOTES_SECTION_RE.match(ln):
            in_footnotes_section = True
            continue
        
        # Skip lines in footnotes section
        if in_footnotes_section:
            continue
        
        # Detect actual TPM sections
        if TPM_SECTION_RE.match(ln):
            # Find which chapter this belongs to
            chapter_name = "Unknown"
            for ch in chapters:
                if ch["content_start"] <= i + 1 <= ch["content_end"]:
                    chapter_name = ch["title"]
                    break
            
            # Check for "Derived from:" within next 50 lines
            has_derived = False
            for j in range(i + 1, min(i + 50, len(lines))):
                if DERIVED_MARKER_RE.match(lines[j].strip()):
                    has_derived = True
                    break
                # Stop if we hit another major section
                if lines[j].startswith("###") or lines[j].startswith("##"):
                    break
            
            if not has_derived:
                tpm_errors.append({
                    "type": "tpm_missing_derived_from",
                    "section": f"{chapter_name} / TPM Implementation Section",
                    "line": i + 1
                })
            
            # Check placement (should be before "See Also" or at end of chapter)
            # Find if there's "See Also" after this TPM
            has_see_also_after = False
            for j in range(i + 1, min(len(lines), i + 200)):
                if "### **See Also:" in lines[j]:
                    has_see_also_after = True
                    break
                # Stop at next chapter
                if lines[j].startswith("## Chapter "):
                    break
            
            # TPM should come before See Also (if See Also exists)
            # This is correct, so no error here
    
    return tpm_errors, structure_errors

def check_concept_matching_improved(lines: List[str], json_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Check cross-book concept matching, filtering false positives:
    - Ignore footnotes section
    - Only check actual cross-book reference sections
    """
    errors = []
    in_footnotes = False
    
    for i, ln in enumerate(lines):
        if FOOTNOTES_SECTION_RE.match(ln):
            in_footnotes = True
            continue
        
        # Skip footnotes section
        if in_footnotes:
            continue
        
        # Check for cross-book citations with annotations
        # This logic would be similar to v2 but with footnotes filter
    
    return errors

def main():
    parser = argparse.ArgumentParser(description="Compliance Validator v3 (False Positive Filtering)")
    parser.add_argument("--md", required=True, help="Path to markdown file")
    parser.add_argument("--json-dir", default=DEFAULTS["json_dir"], help="Path to JSON directory")
    args = parser.parse_args()

    md_path = Path(args.md)
    json_dir = Path(args.json_dir)

    if not md_path.exists():
        print(f"ERROR: Markdown file not found: {md_path}", file=sys.stderr)
        return 1

    md_text = md_path.read_text(encoding="utf-8")
    lines = md_text.splitlines()
    
    json_data = load_json_dir(json_dir)
    print(f"Loaded {len(json_data)} JSON files", file=sys.stderr)

    # Parse chapters with improved detection
    chapters = parse_chapters_improved(lines)
    print(f"Detected {len(chapters)} chapters", file=sys.stderr)

    # Run validations
    results = {
        "verbatim": {
            "errors": check_verbatim_blocks(lines, md_text, json_data, DEFAULTS),
            "count": 0
        },
        "tpm": {
            "errors": [],
            "count": 0
        },
        "structure": {
            "errors": [],
            "count": 0
        },
        "concept_matching": {
            "errors": check_concept_matching_improved(lines, json_data),
            "count": 0
        },
        "summaries": {
            "errors": [],
            "count": 0
        },
        "distribution": {
            "errors": [],
            "count": 0
        },
        "coverage": {
            "errors": [],
            "count": 0
        }
    }

    # Check TPM sections
    tpm_errs, struct_errs = check_tpm_sections_improved(lines, chapters)
    results["tpm"]["errors"] = tpm_errs
    results["structure"]["errors"] = struct_errs

    # Update counts
    for category in results:
        results[category]["count"] = len(results[category]["errors"])

    # Output JSON
    print(json.dumps(results, indent=2))

    # Summary
    total_errors = sum(results[cat]["count"] for cat in results)
    print(f"\n{'='*60}", file=sys.stderr)
    print(f"VALIDATION SUMMARY (v3 - False Positive Filtering)", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    print(f"Chapters detected: {len(chapters)}", file=sys.stderr)
    print(f"Total errors: {total_errors}", file=sys.stderr)
    for cat in results:
        count = results[cat]["count"]
        if count > 0:
            print(f"  {cat}: {count}", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)

    return 1 if total_errors > 0 else 0

if __name__ == "__main__":
    sys.exit(main())
