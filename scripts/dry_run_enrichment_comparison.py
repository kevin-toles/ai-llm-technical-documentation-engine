#!/usr/bin/env python3
"""
Dry Run Enrichment Comparison

Compares current enriched metadata outputs with new outputs generated by
the updated Tab 4 enrichment pipeline (with BERTopic + Sentence Transformers).

Usage:
    python scripts/dry_run_enrichment_comparison.py --book "Architecture Patterns with Python"
    python scripts/dry_run_enrichment_comparison.py --all --limit 3

Output:
    - Side-by-side comparison of key metrics
    - Quality analysis of topic_id assignments
    - Similarity score comparisons
    
Reference:
    - BERTOPIC_SENTENCE_TRANSFORMERS_DESIGN.md: Option C Architecture
"""

import argparse
import json
import sys
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


@dataclass
class ComparisonMetrics:
    """Metrics for comparing two enriched outputs."""
    book_name: str
    original_chapters: int = 0
    new_chapters: int = 0
    
    # Topic clustering metrics (NEW)
    topic_ids_added: int = 0
    unique_topics: int = 0
    topic_coverage: float = 0.0  # % of chapters with topic_id
    
    # Related chapters metrics
    orig_avg_related: float = 0.0
    new_avg_related: float = 0.0
    related_chapters_diff: float = 0.0
    
    # Keywords metrics
    orig_avg_keywords: float = 0.0
    new_avg_keywords: float = 0.0
    keyword_overlap: float = 0.0
    
    # Concepts metrics  
    orig_avg_concepts: float = 0.0
    new_avg_concepts: float = 0.0
    concept_overlap: float = 0.0
    
    # Quality indicators
    enrichment_method: str = ""
    libraries_used: Dict[str, str] = field(default_factory=dict)
    processing_time_seconds: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "book_name": self.book_name,
            "chapters": {"original": self.original_chapters, "new": self.new_chapters},
            "topic_clustering": {
                "topic_ids_added": self.topic_ids_added,
                "unique_topics": self.unique_topics,
                "coverage_pct": round(self.topic_coverage * 100, 1)
            },
            "related_chapters": {
                "original_avg": round(self.orig_avg_related, 2),
                "new_avg": round(self.new_avg_related, 2),
                "diff": round(self.related_chapters_diff, 2)
            },
            "keywords": {
                "original_avg": round(self.orig_avg_keywords, 2),
                "new_avg": round(self.new_avg_keywords, 2),
                "overlap_pct": round(self.keyword_overlap * 100, 1)
            },
            "concepts": {
                "original_avg": round(self.orig_avg_concepts, 2),
                "new_avg": round(self.new_avg_concepts, 2),
                "overlap_pct": round(self.concept_overlap * 100, 1)
            },
            "enrichment_method": self.enrichment_method,
            "libraries": self.libraries_used,
            "processing_time_seconds": round(self.processing_time_seconds, 2)
        }


def load_enriched_file(path: Path) -> Optional[Dict[str, Any]]:
    """Load enriched metadata JSON file."""
    if not path.exists():
        print(f"  ‚ö†Ô∏è  File not found: {path}")
        return None
    
    with open(path, encoding='utf-8') as f:
        return json.load(f)


def calculate_overlap(list1: List[Any], list2: List[Any]) -> float:
    """Calculate Jaccard similarity between two lists."""
    if not list1 and not list2:
        return 1.0  # Both empty = perfect match
    if not list1 or not list2:
        return 0.0
    
    # Normalize strings for comparison
    def normalize(item):
        if isinstance(item, dict):
            return item.get("term", str(item)).lower()
        return str(item).lower()
    
    # S7494: Use set comprehension instead of set() constructor
    set1 = {normalize(x) for x in list1}
    set2 = {normalize(x) for x in list2}
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union if union > 0 else 0.0


def analyze_topic_distribution(chapters: List[Dict[str, Any]]) -> Tuple[int, int, float]:
    """Analyze topic_id distribution across chapters."""
    topic_ids = [ch.get("topic_id") for ch in chapters if ch.get("topic_id") is not None]
    
    count = len(topic_ids)
    unique = len(set(topic_ids)) if topic_ids else 0
    coverage = count / len(chapters) if chapters else 0.0
    
    return count, unique, coverage


def compare_enriched_outputs(
    original: Dict[str, Any],
    new_output: Dict[str, Any],
    book_name: str,
    processing_time: float
) -> ComparisonMetrics:
    """Compare original and new enriched outputs."""
    metrics = ComparisonMetrics(book_name=book_name)
    
    orig_chapters = original.get("chapters", [])
    new_chapters = new_output.get("chapters", [])
    
    metrics.original_chapters = len(orig_chapters)
    metrics.new_chapters = len(new_chapters)
    
    # Topic clustering (NEW feature)
    topic_count, unique_topics, coverage = analyze_topic_distribution(new_chapters)
    metrics.topic_ids_added = topic_count
    metrics.unique_topics = unique_topics
    metrics.topic_coverage = coverage
    
    # Related chapters
    orig_related_counts = [len(ch.get("related_chapters", [])) for ch in orig_chapters]
    new_related_counts = [len(ch.get("related_chapters", [])) for ch in new_chapters]
    
    metrics.orig_avg_related = sum(orig_related_counts) / len(orig_related_counts) if orig_related_counts else 0
    metrics.new_avg_related = sum(new_related_counts) / len(new_related_counts) if new_related_counts else 0
    metrics.related_chapters_diff = metrics.new_avg_related - metrics.orig_avg_related
    
    # Keywords enriched
    orig_kw_counts = [len(ch.get("keywords_enriched", [])) for ch in orig_chapters]
    new_kw_counts = [len(ch.get("keywords_enriched", [])) for ch in new_chapters]
    
    metrics.orig_avg_keywords = sum(orig_kw_counts) / len(orig_kw_counts) if orig_kw_counts else 0
    metrics.new_avg_keywords = sum(new_kw_counts) / len(new_kw_counts) if new_kw_counts else 0
    
    # Calculate keyword overlap per chapter
    kw_overlaps = []
    for i in range(min(len(orig_chapters), len(new_chapters))):
        orig_kw = orig_chapters[i].get("keywords_enriched", [])
        new_kw = new_chapters[i].get("keywords_enriched", [])
        kw_overlaps.append(calculate_overlap(orig_kw, new_kw))
    metrics.keyword_overlap = sum(kw_overlaps) / len(kw_overlaps) if kw_overlaps else 0
    
    # Concepts enriched
    orig_concept_counts = [len(ch.get("concepts_enriched", [])) for ch in orig_chapters]
    new_concept_counts = [len(ch.get("concepts_enriched", [])) for ch in new_chapters]
    
    metrics.orig_avg_concepts = sum(orig_concept_counts) / len(orig_concept_counts) if orig_concept_counts else 0
    metrics.new_avg_concepts = sum(new_concept_counts) / len(new_concept_counts) if new_concept_counts else 0
    
    # Calculate concept overlap per chapter
    concept_overlaps = []
    for i in range(min(len(orig_chapters), len(new_chapters))):
        orig_c = orig_chapters[i].get("concepts_enriched", [])
        new_c = new_chapters[i].get("concepts_enriched", [])
        concept_overlaps.append(calculate_overlap(orig_c, new_c))
    metrics.concept_overlap = sum(concept_overlaps) / len(concept_overlaps) if concept_overlaps else 0
    
    # Enrichment metadata
    new_meta = new_output.get("enrichment_metadata", {})
    metrics.enrichment_method = new_meta.get("method", "unknown")
    metrics.libraries_used = new_meta.get("libraries", {})
    metrics.processing_time_seconds = processing_time
    
    return metrics


def run_enrichment(book_name: str, temp_output: Path) -> Tuple[Optional[Dict[str, Any]], float]:
    """Run the enrichment script and return results."""
    import time
    
    # Find input files
    metadata_dir = PROJECT_ROOT / "workflows" / "metadata_extraction" / "output"
    taxonomy_dir = PROJECT_ROOT / "workflows" / "taxonomy_setup" / "output"
    
    input_path = metadata_dir / f"{book_name}_metadata.json"
    
    if not input_path.exists():
        print(f"  ‚ö†Ô∏è  Metadata not found: {input_path}")
        return None, 0.0
    
    # Find taxonomy file (comprehensive or specific)
    taxonomy_path = find_taxonomy_file(taxonomy_dir)
    if not taxonomy_path:
        print(f"  ‚ö†Ô∏è  No taxonomy file found in: {taxonomy_dir}")
        return None, 0.0
    
    print(f"  Using taxonomy: {taxonomy_path.name}")
    
    # Import and run enrichment
    try:
        from workflows.metadata_enrichment.scripts.enrich_metadata_per_book import enrich_metadata
        
        start_time = time.time()
        enrich_metadata(input_path, taxonomy_path, temp_output)
        elapsed = time.time() - start_time
        
        if temp_output.exists():
            with open(temp_output, encoding='utf-8') as f:
                return json.load(f), elapsed
        
        return None, elapsed
        
    except Exception as e:
        print(f"  ‚ùå Enrichment failed: {e}")
        import traceback
        traceback.print_exc()
        return None, 0.0


def print_comparison_report(metrics: ComparisonMetrics) -> None:
    """Print human-readable comparison report."""
    print(f"\n{'='*60}")
    print(f"üìä COMPARISON: {metrics.book_name}")
    print(f"{'='*60}")
    
    print(f"\nüìö Chapters: {metrics.original_chapters} ‚Üí {metrics.new_chapters}")
    
    print("\nüè∑Ô∏è  TOPIC CLUSTERING (NEW FEATURE)")
    print(f"   ‚Ä¢ Topics assigned: {metrics.topic_ids_added} chapters")
    print(f"   ‚Ä¢ Unique topics: {metrics.unique_topics}")
    print(f"   ‚Ä¢ Coverage: {metrics.topic_coverage*100:.1f}%")
    
    print("\nüîó RELATED CHAPTERS")
    print(f"   ‚Ä¢ Original avg: {metrics.orig_avg_related:.2f}")
    print(f"   ‚Ä¢ New avg: {metrics.new_avg_related:.2f}")
    print(f"   ‚Ä¢ Change: {'+' if metrics.related_chapters_diff > 0 else ''}{metrics.related_chapters_diff:.2f}")
    
    print("\nüîë KEYWORDS ENRICHED")
    print(f"   ‚Ä¢ Original avg: {metrics.orig_avg_keywords:.2f}")
    print(f"   ‚Ä¢ New avg: {metrics.new_avg_keywords:.2f}")
    print(f"   ‚Ä¢ Overlap: {metrics.keyword_overlap*100:.1f}%")
    
    print("\nüí° CONCEPTS ENRICHED")
    print(f"   ‚Ä¢ Original avg: {metrics.orig_avg_concepts:.2f}")
    print(f"   ‚Ä¢ New avg: {metrics.new_avg_concepts:.2f}")
    print(f"   ‚Ä¢ Overlap: {metrics.concept_overlap*100:.1f}%")
    
    print("\n‚öôÔ∏è  ENRICHMENT DETAILS")
    print(f"   ‚Ä¢ Method: {metrics.enrichment_method}")
    print(f"   ‚Ä¢ Libraries: {', '.join(f'{k}={v}' for k, v in metrics.libraries_used.items())}")
    print(f"   ‚Ä¢ Processing time: {metrics.processing_time_seconds:.2f}s")


def find_taxonomy_file(taxonomy_dir: Path) -> Optional[Path]:
    """Find the most recent comprehensive taxonomy file."""
    # Try comprehensive taxonomy first
    comprehensive = list(taxonomy_dir.glob("comprehensive_taxonomy_*.json"))
    if comprehensive:
        return sorted(comprehensive)[-1]  # Most recent
    
    # Try any taxonomy file
    any_taxonomy = list(taxonomy_dir.glob("*_taxonomy_*.json"))
    if any_taxonomy:
        return sorted(any_taxonomy)[-1]
    
    return None


def get_available_books() -> List[str]:
    """Get list of books that have both metadata and enriched outputs."""
    enriched_dir = PROJECT_ROOT / "workflows" / "metadata_enrichment" / "output"
    metadata_dir = PROJECT_ROOT / "workflows" / "metadata_extraction" / "output"
    
    books = []
    for enriched_file in enriched_dir.glob("*_enriched.json"):
        book_name = enriched_file.stem.replace("_enriched", "")
        metadata_file = metadata_dir / f"{book_name}_metadata.json"
        
        if metadata_file.exists():
            books.append(book_name)
    
    return sorted(books)


# =============================================================================
# S3776 Helper Functions - Extract Method Pattern
# Reference: CODING_PATTERNS_ANALYSIS.md Category 2
# =============================================================================


def _parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments. Extracted from main() for S3776."""
    parser = argparse.ArgumentParser(
        description="Compare current vs new enrichment outputs"
    )
    parser.add_argument(
        "--book",
        type=str,
        help="Specific book to analyze (e.g., 'Architecture Patterns with Python')"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Analyze all available books"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=5,
        help="Maximum number of books to analyze when using --all (default: 5)"
    )
    parser.add_argument(
        "--output-report",
        type=str,
        help="Path to save JSON comparison report"
    )
    parser.add_argument(
        "--list-books",
        action="store_true",
        help="List available books and exit"
    )
    return parser.parse_args()


def _list_available_books() -> None:
    """List available books and exit. Extracted from main() for S3776."""
    books = get_available_books()
    print(f"\nüìö Available books for comparison ({len(books)}):")
    for book in books:
        print(f"  ‚Ä¢ {book}")


def _process_book(book_name: str, enriched_dir: Path) -> Optional[Dict[str, Any]]:
    """
    Process a single book comparison.
    Extracted from main() for S3776.
    
    Returns metrics dict or None if processing failed.
    """
    print(f"\n{'‚îÄ'*60}")
    print(f"üìñ Processing: {book_name}")
    print(f"{'‚îÄ'*60}")
    
    # Load original enriched output
    original_path = enriched_dir / f"{book_name}_enriched.json"
    original = load_enriched_file(original_path)
    
    if not original:
        return None
    
    # Run new enrichment to temp file
    with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
        temp_output = Path(tmp.name)
    
    try:
        new_output, processing_time = run_enrichment(book_name, temp_output)
        
        if new_output:
            metrics = compare_enriched_outputs(
                original, new_output, book_name, processing_time
            )
            print_comparison_report(metrics)
            return metrics.to_dict()
        return None
    finally:
        if temp_output.exists():
            temp_output.unlink()


def _print_summary(all_metrics: List[Dict[str, Any]]) -> None:
    """Print summary of all book comparisons. Extracted from main() for S3776."""
    if len(all_metrics) <= 1:
        return
    
    print(f"\n{'='*60}")
    print("üìä SUMMARY")
    print(f"{'='*60}")
    
    total_topics = sum(m["topic_clustering"]["topic_ids_added"] for m in all_metrics)
    avg_coverage = sum(m["topic_clustering"]["coverage_pct"] for m in all_metrics) / len(all_metrics)
    avg_kw_overlap = sum(m["keywords"]["overlap_pct"] for m in all_metrics) / len(all_metrics)
    avg_concept_overlap = sum(m["concepts"]["overlap_pct"] for m in all_metrics) / len(all_metrics)
    
    print(f"\n‚úÖ Books analyzed: {len(all_metrics)}")
    print(f"‚úÖ Total topics assigned: {total_topics}")
    print(f"‚úÖ Average topic coverage: {avg_coverage:.1f}%")
    print(f"‚úÖ Average keyword overlap: {avg_kw_overlap:.1f}%")
    print(f"‚úÖ Average concept overlap: {avg_concept_overlap:.1f}%")


def _save_report(output_path: str, all_metrics: List[Dict[str, Any]]) -> None:
    """Save comparison report to JSON. Extracted from main() for S3776."""
    report_path = Path(output_path)
    report = {
        "generated": datetime.now().isoformat(),
        "description": "Dry run comparison: Current vs BERTopic/Sentence Transformers enrichment",
        "books_analyzed": len(all_metrics),
        "comparisons": all_metrics
    }
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2)
    print(f"\nüìÅ Report saved to: {report_path}")


def main():
    """Main entry point - refactored per S3776 to use helper functions."""
    args = _parse_arguments()
    
    if args.list_books:
        _list_available_books()
        return
    
    if not args.book and not args.all:
        print("Error: Either --book or --all must be specified")
        return
    
    print("\n" + "="*60)
    print("üî¨ DRY RUN: Enrichment Comparison")
    print("   Comparing current outputs vs new BERTopic/Sentence Transformers")
    print("="*60)
    
    # Determine which books to analyze
    if args.all:
        books = get_available_books()[:args.limit]
        print(f"\nüìö Analyzing {len(books)} books...")
    else:
        books = [args.book]
    
    enriched_dir = PROJECT_ROOT / "workflows" / "metadata_enrichment" / "output"
    all_metrics = []
    
    for book_name in books:
        metrics = _process_book(book_name, enriched_dir)
        if metrics:
            all_metrics.append(metrics)
    
    # Summary and report
    _print_summary(all_metrics)
    
    if args.output_report:
        _save_report(args.output_report, all_metrics)


if __name__ == "__main__":
    main()
