# LLM Document Enhancer - Application

## Overview

The LLM Document Enhancer is an **application** (not a microservice) that transforms raw technical documentation into cross-referenced guidelines. It consumes the LLM Gateway, AI Agents, Semantic Search, and Code-Orchestrator microservices to perform its work.

## Architecture Type

**Application** - A batch processing application that runs on-demand or scheduled. It is a **consumer** of the microservices infrastructure, not a service itself.

---

## Kitchen Brigade Role: CUSTOMER

In the Kitchen Brigade architecture, **llm-document-enhancer** is a **Customer** - it places orders and receives the final dish:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ‘¤ CUSTOMER - ORDER PLACER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  WHAT IT DOES:                                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                               â”‚
â”‚  âœ“ Places orders (calls services via HTTP)                                  â”‚
â”‚  âœ“ Receives results (enriched metadata, cross-references)                   â”‚
â”‚  âœ“ Writes output files ({book}_enriched.json)                               â”‚
â”‚  âœ“ Orchestrates the 6-step workflow pipeline                                â”‚
â”‚                                                                              â”‚
â”‚  SIMILARITY COMPUTATION (IMPLEMENTED 2025-12-14):                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  âœ“ SemanticSimilarityEngine with SBERT (all-MiniLM-L6-v2)                   â”‚
â”‚  âœ“ Auto-fallback to TF-IDF when sentence-transformers unavailable           â”‚
â”‚  âœ“ 47 books, 1922 chapters, 9614 similar_chapters links                     â”‚
â”‚  âœ“ Method tracked: "sentence_transformers" or "tfidf"                       â”‚
â”‚                                                                              â”‚
â”‚  FUTURE ENHANCEMENT (Code-Orchestrator Integration):                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚  For CODE-SPECIFIC content, call Code-Orchestrator-Service:                 â”‚
â”‚  â€¢ CodeBERT: Codeâ†”NL search                                                 â”‚
â”‚  â€¢ GraphCodeBERT: Code structure understanding                              â”‚
â”‚  â€¢ CodeT5+: Code generation                                                 â”‚
â”‚                                                                              â”‚
â”‚  Note: Textbook content uses SBERT (text similarity).                       â”‚
â”‚        Code examples/repositories use Code-Orchestrator.                    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Current vs Target Data Flow

```
CURRENT STATE (Broken - Zero Cross-Book References):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
enrich_metadata_per_book.py
    â”œâ”€â†’ TfidfVectorizer (sklearn, local)
    â”œâ”€â†’ cosine_similarity with threshold=0.7
    â”œâ”€â†’ find_related_chapters() returns ZERO cross-book
    â””â”€â†’ Only within-book references (max TF-IDF ~0.50 cross-book)

TARGET STATE (Semantic Cross-References):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
enrich_metadata_per_book.py
    â”‚
    â”‚ --use-orchestrator flag
    â–¼
Code-Orchestrator-Service (Port 8083)
    â”‚
    â”œâ”€â†’ CodeT5+ extracts semantic terms from chapter text
    â”œâ”€â†’ GraphCodeBERT validates terms (filters generic)
    â”œâ”€â†’ CodeBERT ranks by embedding similarity
    â”‚
    â–¼
Semantic Search Service (Port 8081)
    â”‚
    â”œâ”€â†’ Queries Qdrant with validated terms
    â”œâ”€â†’ Returns ALL matches
    â”‚
    â–¼
Code-Orchestrator-Service (Curation)
    â”‚
    â”œâ”€â†’ Filters domain mismatches (C++ chunks vs LLM chunks)
    â”œâ”€â†’ Ranks by relevance to original query
    â”œâ”€â†’ Returns curated cross-book references
    â”‚
    â–¼
enrich_metadata_per_book.py
    â”‚
    â””â”€â†’ Writes {book}_enriched.json with REAL cross-references
```

---

## Folder Structure

```
llm-document-enhancer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                  # Typer CLI entry point
â”‚   â”‚   â”œâ”€â”€ enhance.py               # enhance command
â”‚   â”‚   â”œâ”€â”€ index.py                 # index command (build indices)
â”‚   â”‚   â””â”€â”€ evaluate.py              # evaluate command
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                # Pydantic settings
â”‚   â”‚   â””â”€â”€ exceptions.py
â”‚   â”‚
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gateway_client.py        # HTTP client for llm-gateway
â”‚   â”‚   â”œâ”€â”€ search_client.py         # HTTP client for semantic-search
â”‚   â”‚   â””â”€â”€ agents_client.py         # HTTP client for ai-agents (optional)
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py              # Main ingestion orchestrator
â”‚   â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf.py               # PDF text extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ markdown.py          # Markdown parsing
â”‚   â”‚   â”‚   â””â”€â”€ json.py              # JSON/YAML metadata
â”‚   â”‚   â”œâ”€â”€ structure/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ detector.py          # Chapter/section detection
â”‚   â”‚   â”‚   â””â”€â”€ chunker.py           # Semantic chunking
â”‚   â”‚   â””â”€â”€ metadata/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ extractor.py         # Metadata extraction
â”‚   â”‚       â””â”€â”€ tier_assigner.py     # Tier assignment logic
â”‚   â”‚
â”‚   â”œâ”€â”€ precompute/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py              # Pre-compute orchestrator
â”‚   â”‚   â”œâ”€â”€ keywords/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ yake_extractor.py    # YAKE keyword extraction
â”‚   â”‚   â”‚   â””â”€â”€ tfidf.py             # TF-IDF vectors
â”‚   â”‚   â”œâ”€â”€ similarity/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ matcher.py           # Pre-compute similar chunks
â”‚   â”‚   â””â”€â”€ topics/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ assigner.py          # Topic assignment via service
â”‚   â”‚
â”‚   â”œâ”€â”€ enhancement/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py              # 3-step enhancement orchestrator
â”‚   â”‚   â”œâ”€â”€ steps/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ step1_hierarchy.py   # Step 1: Document Hierarchy Review
â”‚   â”‚   â”‚   â”œâ”€â”€ step2_crossref.py    # Step 2: Cross-Referencing
â”‚   â”‚   â”‚   â””â”€â”€ step3_conflict.py    # Step 3: Conflict Resolution
â”‚   â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hierarchy.py
â”‚   â”‚   â”‚   â”œâ”€â”€ crossref.py
â”‚   â”‚   â”‚   â””â”€â”€ conflict.py
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ definitions.py       # Tool definitions for LLM
â”‚   â”‚
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ writer.py                # Write enhanced guidelines
â”‚   â”‚   â”œâ”€â”€ crossref_index.py        # Generate cross-reference index
â”‚   â”‚   â””â”€â”€ evaluation.py            # Generate evaluation report
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document.py              # Document, Chunk, Section models
â”‚   â”‚   â”œâ”€â”€ match.py                 # Match, Conflict models
â”‚   â”‚   â””â”€â”€ output.py                # Output models
â”‚   â”‚
â”‚   â””â”€â”€ main.py                      # Alternative entry point
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_ingestion/
â”‚   â”‚   â”œâ”€â”€ test_precompute/
â”‚   â”‚   â””â”€â”€ test_enhancement/
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_full_pipeline.py
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py                  # Default settings
â”‚   â”œâ”€â”€ chapter_patterns.json        # Chapter detection patterns
â”‚   â”œâ”€â”€ extraction_profiles.json     # Per-doc-type extraction rules
â”‚   â”œâ”€â”€ metadata_keywords.json       # Domain keyword lists
â”‚   â”œâ”€â”€ tier_taxonomy.json           # Tier assignments
â”‚   â””â”€â”€ validation_rules.json        # Output validation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus/                      # Input documents (gitignored)
â”‚   â”œâ”€â”€ guidelines/                  # Baseline guidelines
â”‚   â””â”€â”€ cache/                       # Pre-compute cache (gitignored)
â”‚
â”œâ”€â”€ outputs/                         # Generated outputs (gitignored)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md              # This file
â”‚   â”œâ”€â”€ WORKFLOW.md                  # Enhancement workflow details
â”‚   â”œâ”€â”€ CONFIG.md                    # Configuration guide
â”‚   â””â”€â”€ EVALUATION.md                # Evaluation methodology
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_enhance.sh
â”‚   â””â”€â”€ build_indices.sh
â”‚
â”œâ”€â”€ Dockerfile                       # For containerized runs
â”œâ”€â”€ docker-compose.yml               # Local dev with services
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## System Context

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLM DOCUMENT ENHANCER (Application)                      â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                           CLI Interface                                  â”‚ â”‚
â”‚  â”‚  $ doc-enhancer index --corpus ./data/corpus                            â”‚ â”‚
â”‚  â”‚  $ doc-enhancer enhance --guidelines ./data/guidelines/baseline.md      â”‚ â”‚
â”‚  â”‚  $ doc-enhancer evaluate --output ./outputs/enhanced.md                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                        PROCESSING PIPELINE                               â”‚ â”‚
â”‚  â”‚                                                                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚  Ingestion   â”‚â”€â–¶â”‚  Pre-Compute â”‚â”€â–¶â”‚ Enhancement  â”‚â”€â–¶â”‚   Output   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  (3-Step)    â”‚  â”‚            â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ PDF parse  â”‚  â”‚ â€¢ YAKE       â”‚  â”‚ 1. Hierarchy â”‚  â”‚ â€¢ Markdown â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Structure  â”‚  â”‚ â€¢ TF-IDF     â”‚  â”‚ 2. Cross-Ref â”‚  â”‚ â€¢ JSON idx â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Chunking   â”‚  â”‚ â€¢ Similarity â”‚  â”‚ 3. Conflict  â”‚  â”‚ â€¢ Report   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Metadata   â”‚  â”‚ â€¢ Topics     â”‚  â”‚              â”‚  â”‚            â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚                           â”‚                 â”‚                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                 â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                      â–¼                            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ semantic-search  â”‚   â”‚ llm-gateway      â”‚         â”‚ ai-agents        â”‚
   â”‚ microservice     â”‚   â”‚ microservice     â”‚         â”‚ microservice     â”‚
   â”‚ (Port 8081)      â”‚   â”‚ (Port 8080)      â”‚         â”‚ (Port 8082)      â”‚
   â”‚                  â”‚   â”‚                  â”‚         â”‚ (optional)       â”‚
   â”‚ â€¢ Embed chunks   â”‚   â”‚ â€¢ LLM calls      â”‚         â”‚                  â”‚
   â”‚ â€¢ Search similar â”‚   â”‚ â€¢ Tool execution â”‚         â”‚                  â”‚
   â”‚ â€¢ Topic inferenceâ”‚   â”‚ â€¢ Sessions       â”‚         â”‚                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CLI Commands

| Command | Description |
|---------|-------------|
| `doc-enhancer index` | Ingest documents and build indices |
| `doc-enhancer precompute` | Run pre-compute pipeline (similarity, topics) |
| `doc-enhancer enhance` | Run 3-step enhancement on guidelines |
| `doc-enhancer evaluate` | Evaluate enhanced output quality |
| `doc-enhancer full` | Run complete pipeline (index â†’ precompute â†’ enhance â†’ evaluate) |

---

## Processing Pipeline

### 1. Ingestion
- Parse PDFs, Markdown, JSON sources
- Detect document structure (chapters, sections)
- Chunk into semantic units
- Extract metadata, assign tiers

### 2. Pre-Compute
- YAKE keyword extraction (local)
- TF-IDF vector generation (local)
- Similarity matching via semantic-search microservice
- Topic assignment via semantic-search microservice

### 3. Enhancement (3-Step Process)
All steps use the **llm-gateway microservice** for LLM inference:

**Step 1: Document Hierarchy Review**
- Validate tier assignments
- Verify chapter/section extraction
- Identify key concepts

**Step 2: Guideline Concept Review & Cross-Referencing**
- For each guideline section, provide pre-computed matches
- LLM validates matches and builds cross-references
- LLM can use tools to retrieve additional context

**Step 3: Conflict Identification & Resolution**
- Present potential conflicts from pre-compute
- LLM applies tier-based priority rules
- LLM synthesizes resolutions with citations

### 4. Output
- Write enhanced guidelines (Markdown)
- Generate cross-reference index (JSON)
- Generate evaluation report (JSON)

---

## CME-1.0: Configurable Metadata Extraction (NEW)

**Feature**: Configurable Metadata Extraction  
**Status**: âœ… COMPLETE (December 2025)  
**Architecture Document**: [CME_ARCHITECTURE.md](../../../textbooks/pending/platform/CME_ARCHITECTURE.md)

### Overview

CME-1.0 adds a configurable toggle to switch between two metadata extraction implementations:

| Mode | Flag Value | Description |
|------|------------|-------------|
| **Local** | `use_orchestrator=False` | StatisticalExtractor (YAKE + Summa). Fast, offline, may include noise. |
| **Orchestrator** | `use_orchestrator=True` | Code-Orchestrator-Service API. TF-IDF + noise filtering, higher quality. |

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  llm-document-enhancer                                                      â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ MetadataExtractionClient    â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Code-Orchestrator-Service       â”‚   â”‚
â”‚  â”‚ (httpx async client)        â”‚      â”‚ :8083                           â”‚   â”‚
â”‚  â”‚ â”œâ”€â”€ extract_metadata()      â”‚ HTTP â”‚                                 â”‚   â”‚
â”‚  â”‚ â””â”€â”€ health_check()          â”‚â—„â”€â”€â”€â”€â”€â”‚ POST /api/v1/metadata/extract   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”œâ”€â”€ TF-IDF keywords             â”‚   â”‚
â”‚                                       â”‚ â”œâ”€â”€ Concept extraction          â”‚   â”‚
â”‚         OR (fallback)                 â”‚ â”œâ”€â”€ Noise validation            â”‚   â”‚
â”‚                 â”‚                     â”‚ â””â”€â”€ Quality scoring             â”‚   â”‚
â”‚                 â–¼                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ StatisticalExtractor        â”‚                                            â”‚
â”‚  â”‚ (fallback if service down)  â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Toggle Methods

```bash
# Default: Local StatisticalExtractor
python3 generate_metadata_universal.py --input book.json

# Orchestrator via CLI flag (takes precedence)
python3 generate_metadata_universal.py --input book.json --use-orchestrator

# Orchestrator via env var
EXTRACTION_USE_ORCHESTRATOR_EXTRACTION=true python3 generate_metadata_universal.py --input book.json

# With fallback disabled (strict mode)
python3 generate_metadata_universal.py --input book.json --use-orchestrator --no-fallback
```

### Configuration

```python
# config/extraction_settings.py
class ExtractionSettings(BaseSettings):
    use_orchestrator_extraction: bool = False  # Default: local
    orchestrator_url: str = "http://localhost:8083"
    fallback_on_error: bool = True
    
    model_config = SettingsConfigDict(env_prefix="EXTRACTION_")
```

### Files

| File | Description |
|------|-------------|
| `config/extraction_settings.py` | Pydantic Settings with EXTRACTION_* prefix |
| `workflows/shared/clients/metadata_client.py` | MetadataExtractionClient + FakeClient |
| `workflows/metadata_extraction/scripts/generate_metadata_universal.py` | Generator with --use-orchestrator flag |

---

## Dependencies

| Dependency | Type | Purpose |
|------------|------|---------|
| llm-gateway | Microservice | LLM inference for enhancement steps |
| semantic-search-service | Microservice | Embedding, search, topics |
| ai-agents | Microservice (optional) | Code review for code examples in docs |

---

## Local Development

```yaml
# docker-compose.yml
services:
  # The microservices this application depends on
  semantic-search:
    image: semantic-search-service:latest
    ports:
      - "8081:8081"

  llm-gateway:
    image: llm-gateway:latest
    ports:
      - "8080:8080"
    depends_on:
      - semantic-search

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

```bash
# Run the application (not as a service)
$ doc-enhancer enhance \
    --corpus ./data/corpus \
    --guidelines ./data/guidelines/baseline.md \
    --output ./outputs/enhanced.md \
    --gateway-url http://localhost:8080 \
    --search-url http://localhost:8081
```

---

## Configuration

```python
# src/core/config.py
class Settings(BaseSettings):
    # Microservice URLs
    llm_gateway_url: str = "http://localhost:8080"
    semantic_search_url: str = "http://localhost:8081"
    ai_agents_url: str = "http://localhost:8082"
    
    # Paths
    corpus_path: str = "./data/corpus"
    guidelines_path: str = "./data/guidelines"
    output_path: str = "./outputs"
    cache_path: str = "./data/cache"
    
    # Pre-compute settings
    chunk_size: int = 512
    chunk_overlap: int = 64
    yake_num_keywords: int = 20
    similarity_top_k: int = 10
    similarity_threshold: float = 0.7
    
    # Enhancement settings
    default_model: str = "claude-3-sonnet-20240229"
    max_tool_calls_per_section: int = 5
    
    class Config:
        env_prefix = "DOC_ENHANCER_"
```

---

## Not a Microservice

This application:
- âœ… Runs on-demand or scheduled (batch job)
- âœ… Consumes microservices via HTTP
- âœ… Has no REST API of its own
- âœ… Does not need to be "up" 24/7
- âœ… Can run locally, in CI/CD, or as a K8s Job
