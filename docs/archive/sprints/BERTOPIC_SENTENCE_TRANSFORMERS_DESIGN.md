# BERTopic + Sentence Transformers Integration Design

## Overview

This document describes Option C implementation: BERTopic runs in Tab 4, outputs `topic_id` per chapter, Tab 5 consumes it for enhanced cross-referencing.

## Architecture

```
Tab 4 (Enrichment)                    Tab 5 (Base Guideline)
─────────────────                     ────────────────────
1. BERTopic clusters all chapters     1. Reads topic_id from enriched JSON
2. Stores topic_id in output          2. _find_cross_references() uses:
3. Sentence Transformers similarity      - related_chapters (has topic boost)
4. find_related_chapters() uses:         - Keyword overlap
   - Semantic similarity                 - Topic_id for grouping
   - Same-topic boost
        ↓                                     ↓
*_metadata_enriched.json              Uses pre-computed topic info
```

## Enhanced JSON Schema

### Tab 4 Output: `{book}_metadata_enriched.json`

```json
{
  "book": "architecture_patterns",
  "enrichment_metadata": {
    "generated": "2025-01-15T10:30:00",
    "method": "statistical_semantic",
    "libraries": {
      "yake": "0.4.8",
      "summa": "1.2.0",
      "scikit-learn": "1.3.2",
      "sentence-transformers": "2.2.2",
      "bertopic": "0.16.0"
    },
    "corpus_size": 5,
    "total_chapters_analyzed": 120,
    "embedding_model": "all-MiniLM-L6-v2",
    "topic_count": 15
  },
  "topics": [
    {
      "topic_id": 0,
      "name": "Repository Pattern",
      "keywords": ["repository", "persistence", "aggregate", "domain"],
      "representative_chapters": [
        {"book": "architecture_patterns.json", "chapter": 2, "title": "Repository Pattern"},
        {"book": "python_patterns.json", "chapter": 8, "title": "Data Mappers"}
      ]
    },
    {
      "topic_id": 1,
      "name": "Unit Testing",
      "keywords": ["test", "mock", "fixture", "pytest"],
      "representative_chapters": [...]
    }
  ],
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Domain Modeling",
      "tier": "Core",
      "summary": "...",
      "keywords": ["domain", "model", "entity"],
      "concepts": ["Domain-Driven Design", "Entity"],
      
      "topic_id": 0,
      "topic_name": "Repository Pattern",
      "topic_confidence": 0.87,
      
      "related_chapters": [
        {
          "book": "python_patterns.json",
          "chapter": 8,
          "title": "Data Mappers",
          "relevance_score": 0.85,
          "semantic_score": 0.82,
          "method": "sentence_transformers",
          "same_topic": true,
          "topic_id": 0
        },
        {
          "book": "clean_code.json",
          "chapter": 12,
          "title": "Clean Architecture",
          "relevance_score": 0.73,
          "semantic_score": 0.73,
          "method": "sentence_transformers",
          "same_topic": false,
          "topic_id": 3
        }
      ],
      "keywords_enriched": [...],
      "concepts_enriched": [...]
    }
  ]
}
```

### Key Schema Additions

1. **Chapter-Level**:
   - `topic_id`: Integer ID from BERTopic clustering
   - `topic_name`: Human-readable topic name (auto-generated by BERTopic)
   - `topic_confidence`: Probability that chapter belongs to this topic (0.0-1.0)

2. **Related Chapters**:
   - `semantic_score`: Raw Sentence Transformer cosine similarity
   - `same_topic`: Boolean - whether related chapter shares topic_id
   - `topic_id`: Topic ID of the related chapter

3. **Enrichment Metadata**:
   - `embedding_model`: Model used for semantic similarity
   - `topic_count`: Total number of topics discovered

4. **New Top-Level Section**:
   - `topics`: Array of topic metadata for reference

## New Modules

### 1. `topic_clusterer.py`

```python
class TopicClusterer:
    """BERTopic-based topic clustering for chapters."""
    
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        ...
    
    def cluster_chapters(self, corpus: List[str], index: List[Dict]) -> TopicResults:
        """Cluster all chapters and return topic assignments."""
        ...
    
    def get_topic_for_chapter(self, chapter_idx: int) -> TopicInfo:
        """Get topic_id, name, and confidence for a chapter."""
        ...
```

### 2. `semantic_similarity_engine.py`

```python
class SemanticSimilarityEngine:
    """Sentence Transformers-based semantic similarity (replaces TF-IDF)."""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        ...
    
    def compute_embeddings(self, corpus: List[str]) -> np.ndarray:
        """Compute embeddings for all chapters."""
        ...
    
    def compute_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:
        """Compute pairwise cosine similarity from embeddings."""
        ...
    
    def find_similar(self, query_embedding, embeddings, top_k: int = 5) -> List[SimilarityResult]:
        """Find top-k similar chapters for a query."""
        ...
```

## Integration Points

### Tab 4: `enrich_metadata_per_book.py`

1. Initialize `TopicClusterer` and `SemanticSimilarityEngine` at module load
2. In `enrich_metadata()`:
   - Call `topic_clusterer.cluster_chapters()` after building corpus
   - Replace `compute_similarity_matrix()` with `semantic_engine.compute_similarity_matrix()`
3. In `find_related_chapters()`:
   - Use semantic similarity instead of TF-IDF cosine
   - Add topic boost: +0.1 if `same_topic=True`
   - Include `topic_id`, `same_topic`, `semantic_score` in output
4. In `_enrich_single_chapter()`:
   - Add `topic_id`, `topic_name`, `topic_confidence` to chapter output

### Tab 5: `chapter_generator_all_text.py`

1. In `_find_cross_references()`:
   - Read `topic_id` from enriched metadata
   - Group cross-references by topic for better organization
   - Prioritize same-topic references

## Dependencies

Add to `requirements.txt`:
```
sentence-transformers>=2.2.2
bertopic>=0.16.0
hdbscan>=0.8.29  # Required by BERTopic
umap-learn>=0.5.3  # Required by BERTopic
```

## Model Size Considerations

- `all-MiniLM-L6-v2`: ~80MB download, fast inference
- BERTopic uses same model by default
- First run will download model (cached after)

## Fallback Strategy

If Sentence Transformers or BERTopic unavailable:
1. Fall back to TF-IDF (current implementation)
2. Set `topic_id = -1` (no topic)
3. Log warning but continue processing

## Test Plan

### Unit Tests (TDD RED→GREEN)

1. `test_topic_clusterer.py`:
   - Test topic assignment for known chapter types
   - Test confidence thresholds
   - Test empty corpus handling
   - Test single-chapter edge case

2. `test_semantic_similarity_engine.py`:
   - Test embedding generation
   - Test similarity scores for known pairs
   - Test top-k retrieval
   - Test batch processing

### Integration Tests

1. Test full enrichment pipeline with new modules
2. Verify JSON schema compatibility with Tab 5
3. Test fallback to TF-IDF when imports fail

## Performance

- Embedding computation: ~100ms per chapter (batch)
- BERTopic clustering: ~2-5 seconds for 100 chapters
- Memory: ~500MB for model + embeddings
- Caching: Embeddings can be cached for re-runs
