# Coding Patterns Analysis: Anti-Patterns & Post-Fix Best Practices
**Workflows Directory - Git History Analysis**

**Generated**: November 26, 2025  
**Updated**: December 4, 2025 (Cognitive Complexity Refactoring)  
**Repository**: ai-llm-technical-documentation-engine, llm-gateway  
**Analysis Scope**: workflows/ directory (21 commits, ~222 issues), llm-gateway/src (46 issues fixed via CodeRabbit/SonarQube/TDD)  
**Time Period**: June 2024 - December 2025

---

## Executive Summary

This document analyzes 21 fix/refactor commits from git history to identify **recurring coding anti-patterns** discovered by SonarQube, CodeRabbit, Mypy, and Bandit during audits of the `workflows/` directory. Each pattern includes:

1. **Pre-Fix Anti-Pattern**: Actual code examples from git diffs showing the problem
2. **Issue Type**: Which tool caught it (Mypy/SonarQube/CodeRabbit/Bandit)
3. **Post-Fix Pattern**: Corrected code with detailed explanation
4. **Root Cause**: Why this pattern emerges during development
5. **Prevention Strategy**: How to avoid this anti-pattern proactively

### Issue Distribution by Category

| Category | Count | % | Primary Tool | Status |
|----------|-------|---|--------------|--------|
| **Type Annotation Issues** | 62 | 28% | Mypy | Active |
| **Cognitive Complexity** | 48 â†’ 43 | 22% | SonarQube | âœ… 5 resolved (Dec 4) |
| **Exception Handling** | 38 | 17% | SonarQube/Bandit | Active |
| **Unused Parameters** | 32 | 14% | SonarQube | Active |
| **Import Problems** | 24 | 11% | Mypy | Active |
| **Regex Patterns** | 18 | 8% | SonarQube | Active |
| **Total** | **222 â†’ 217** | **100%** | - | - |

### December 4, 2025: Cognitive Complexity Refactoring (llm-gateway)

**TDD-driven refactoring** of 5 high-complexity functions in `anthropic.py` and `openai.py`:

| Method | Before | After | Helpers Added | Tests Added |
|--------|--------|-------|---------------|-------------|
| `anthropic._execute_with_retry()` | 19 | 11 | `_classify_error()` | 3 |
| `anthropic.stream()` | 17 | 9 | `_handle_message_start()`, `_handle_content_delta()`, `_handle_message_delta()` | 4 |
| `anthropic._transform_messages()` | 16 | 8 | `_transform_tool_message()`, `_transform_assistant_tool_message()`, `_merge_tool_result()` | 2 |
| `openai._execute_with_retry()` | 18 | 10 | `_classify_error()` | 3 |
| `openai._build_request_kwargs()` | 21 | 7 | `_transform_message()`, `_build_optional_params()`, `_build_tools_list()` | 3 |
| **Totals** | **91** | **45** | **10 helpers** | **15 tests** |

**SonarQube Result**: 0 cognitive complexity warnings (was 5). All 957 tests passing.

### December 2025 Update: llm-gateway Full Static Analysis (CodeRabbit PR #2)

Comprehensive static analysis completed on llm-gateway FastAPI project (41 issues across 31 files):

| Severity | Count | Categories |
|----------|-------|------------|
| ðŸ”´ **Critical** | 8 | Security (secrets), broken configs, missing deps |
| ðŸŸ  **Major** | 20 | Race conditions, connection pooling, env var mismatches |
| ðŸŸ¡ **Minor** | 13 | Documentation, placeholders, code quality |
| **Total** | **41** | âœ… All resolved |

**New Pattern Categories Discovered:**

| Category | Count | Tool | Key Patterns |
|----------|-------|------|-------------|
| **Race Conditions** | 3 | CodeRabbit | Token bucket, circuit breaker state, Redis read-modify-write |
| **Secret Exposure** | 3 | CodeRabbit | Helm CLI args, Redis URL password, env var logging |
| **Exception Shadowing** | 2 | CodeRabbit | `ConnectionError`/`TimeoutError` shadow builtins |
| **Connection Pooling** | 1 | CodeRabbit | New httpx.AsyncClient per request |
| **K8s/Helm Config** | 8 | CodeRabbit | Env prefix mismatch, placeholder checksums, probe paths |
| **Dependency Management** | 2 | CodeRabbit | Missing pydantic-settings, unpinned versions |
| **Metric Cardinality** | 1 | CodeRabbit | High cardinality `path` label |
| **Provider Abstraction** | 1 | CodeRabbit | AnthropicProvider not implemented (added with TDD) |
| **Previously Known** | 20 | Ruff/SonarLint | Unused imports, async stubs, TODOs |
| **Total** | **41** | - | - |

---

## Category 1: Type Annotation Issues (62 issues - 28%)

### Anti-Pattern 1.1: Missing Optional Type Annotations

**Pre-Fix Anti-Pattern** (from commit 223126db):
```python
# chapter_generator_all_text.py:60
PRIMARY_BOOK = None  # Will be set by argparse - MUST be specified by user

# Later in code:
CURRENT_BOOK_META = BOOK_METADATA.get(PRIMARY_BOOK, {
    "author": "Unknown",
    "full_title": PRIMARY_BOOK,
    "short_name": PRIMARY_BOOK
})
```

**Issue Type**: Mypy error - `error: Argument 1 to "get" has incompatible type "None"; expected "str"`

**Post-Fix Pattern**:
```python
# Explicitly annotate with Optional[str]
PRIMARY_BOOK: Optional[str] = None  # Will be set by argparse - MUST be specified by user

# Add type guard before usage
CURRENT_BOOK_META: Dict[str, Optional[str]] = BOOK_METADATA.get(
    PRIMARY_BOOK if PRIMARY_BOOK is not None else "",
    {
        "author": "Unknown",
        "full_title": PRIMARY_BOOK,
        "short_name": PRIMARY_BOOK
    }
)
```

**Root Cause**: 
- Python allows `None` assignment without explicit type annotation
- Developer assumes type checker will infer `Optional[T]` automatically
- Module-level variables assigned before runtime initialization (argparse)

**Prevention Strategy**:
1. **Always use `Optional[T]`** for any variable that can be `None`
2. Add **type guards** (`if x is not None:`) before dereferencing
3. Use **mypy strict mode** (`--strict`) to catch missing Optional annotations
4. Initialize with **sentinel values** instead of `None` where possible

---

### Anti-Pattern 1.2: Missing Type Guards for None Checks

**Pre-Fix Anti-Pattern** (from commit 223126db):
```python
def _write_output_file(all_docs: List[str], book_name: str, all_footnotes: List[Dict] = None) -> None:
    # ... code that uses book_name directly without checking if PRIMARY_BOOK is None
    
    # At call site:
    _write_output_file(all_docs, PRIMARY_BOOK, all_footnotes)
    # Mypy error: Argument 2 has incompatible type "None"; expected "str"
```

**Issue Type**: Mypy error - `error: Argument has incompatible type "None"; expected "str"`

**Post-Fix Pattern**:
```python
def _write_output_file(all_docs: List[str], book_name: str, 
                       all_footnotes: Optional[List[Dict[Any, Any]]] = None) -> None:
    # Type guard at entry point
    if PRIMARY_BOOK is None:
        print("ERROR: PRIMARY_BOOK not set (should be set by argparse)")
        sys.exit(1)
    
    # Now safe to use PRIMARY_BOOK
    _write_output_file(all_docs, PRIMARY_BOOK, all_footnotes)
```

**Root Cause**:
- Assumption that runtime validation (argparse) guarantees non-None
- Type checker cannot infer runtime guarantees
- Missing explicit validation at boundaries

**Prevention Strategy**:
1. **Add type guards at function entry points** where Optional types are used
2. **Validate early** - check for None at the earliest point possible
3. Use **assert statements** with mypy: `assert x is not None`
4. Consider **NonNull sentinels** or **dataclasses with required fields**

---

### Anti-Pattern 1.3: Incorrect Generic Type Syntax

**Pre-Fix Anti-Pattern** (from commit 223126db):
```python
# content_selection_impl.py:156
def _calculate_boosts(
    book_role: BookRole, 
    author: str, 
    settings: 'ContentSelectionSettings'
) -> tuple[float, float]:  # Python 3.9+ syntax not yet in codebase
    """Calculate tier and cascading boosts for a book."""
    tier_boost = 0.0
    cascading_boost = 0.0
```

**Issue Type**: Mypy error - `error: "tuple" is not subscriptable, use "typing.Tuple"`

**Post-Fix Pattern**:
```python
from typing import Tuple  # Import from typing module

def _calculate_boosts(
    book_role: BookRole, 
    author: str, 
    settings: 'ContentSelectionSettings'
) -> Tuple[float, float]:  # Use typing.Tuple for Python <3.9 compatibility
    """Calculate tier and cascading relationship boosts."""
    tier_boost: float = 0.0
    cascading_boost: float = 0.0
```

**Root Cause**:
- Python 3.9+ allows lowercase generics (`list[str]`, `dict[str, int]`, `tuple[float, float]`)
- Codebase uses Python 3.8 or 3.9 without `from __future__ import annotations`
- Developer uses newer syntax without checking Python version

**Prevention Strategy**:
1. **Check Python version** in project: `python --version`
2. **Use `typing.X` for Python <3.9**: `List`, `Dict`, `Tuple`, `Optional`
3. **Add `from __future__ import annotations`** to enable PEP 563 (Python 3.7+)
4. **CI/CD check**: Run mypy with target Python version (`--python-version 3.8`)

---

### Anti-Pattern 1.4: Missing Type Annotations for Collections

**Pre-Fix Anti-Pattern** (from commit 223126db):
```python
def _extract_concepts(chapter: Dict[str, Any], max_concepts: int = 10) -> List[str]:
    matches = []  # Mypy infers List[Never] - can't determine element type
    
    # Pattern matching code...
    for match in re.finditer(concept_pattern, chapter_text):
        matches.append({
            'name': match.group(1),
            'page': int(match.group(2)),
            'excerpt': match.group(3),
            'annotation': match.group(4)
        })
    
    return matches  # Error: List[Dict[...]] incompatible with List[str]
```

**Issue Type**: Mypy error - `error: Incompatible return value type (got "List[Dict[str, Any]]", expected "List[str]")`

**Post-Fix Pattern**:
```python
def _extract_concepts(chapter: Dict[str, Any], max_concepts: int = 10) -> List[str]:
    # Explicitly annotate collection type
    matches: List[Dict[str, Any]] = []
    
    # Pattern matching code...
    for match in re.finditer(concept_pattern, chapter_text):
        matches.append({
            'name': match.group(1),
            'page': int(match.group(2)),
            'excerpt': match.group(3),
            'annotation': match.group(4)
        })
    
    # Fix return type or extract just names
    return [m['name'] for m in matches]
```

**Root Cause**:
- Empty collection initialization (`[]`) cannot infer element type
- Developer changes collection contents during refactoring
- Return type doesn't match actual data structure

**Prevention Strategy**:
1. **Always annotate empty collections**: `matches: List[Dict[str, Any]] = []`
2. **Use type inference** where possible: `matches = [item for item in iterable]`
3. **Match return type to function signature** - update signature if data structure changes
4. **Use dataclasses** instead of Dict for structured data

---

### Anti-Pattern 1.5: Mutable Default Arguments Without Type

**Pre-Fix Anti-Pattern** (from commit 7132fcf0):
```python
def _write_output_file(all_docs: List[str], book_name: str, all_footnotes: List[Dict] = None) -> None:
    # Mypy warning: Mutable default argument
    # Runtime bug: all_footnotes=[] would be shared across calls
```

**Issue Type**: SonarQube warning + Mypy warning - `error: Incompatible default for argument "all_footnotes"`

**Post-Fix Pattern**:
```python
def _write_output_file(all_docs: List[str], book_name: str, 
                       all_footnotes: Optional[List[Dict[Any, Any]]] = None) -> None:
    """Write guideline output file."""
    if all_footnotes is None:
        all_footnotes = []  # Create new list on each call
    
    # Safe to use all_footnotes now
```

**Root Cause**:
- Python evaluates default arguments at function definition time (once)
- Mutable defaults (list, dict) are shared across all function calls
- Developer assumes `[]` creates new list on each call

**Prevention Strategy**:
1. **Use `None` as default** for mutable types
2. **Initialize inside function**: `if param is None: param = []`
3. **Add Mypy rule**: `--disallow-any-unimported` catches untyped defaults
4. **Use immutable defaults** where possible: `tuple` instead of `list`

---

## Category 2: Cognitive Complexity (48 issues - 22%)

### Anti-Pattern 2.1: Nested Loops with Complex Conditionals

**Pre-Fix Anti-Pattern** (from commit 47e02e97):
```python
def load_cross_book_context(taxonomy_path: Path, metadata_dir: Path) -> Dict[str, Any]:
    """Load metadata for all books listed in taxonomy."""
    
    # 1. Load taxonomy
    if not taxonomy_path.exists():
        raise FileNotFoundError(f"Taxonomy file not found: {taxonomy_path}")
    
    with open(taxonomy_path, encoding='utf-8') as f:
        taxonomy = json.load(f)
    
    # 2. Extract book list from taxonomy tiers
    book_set = set()
    if 'tiers' in taxonomy:
        for tier_name, tier_data in taxonomy['tiers'].items():
            if 'books' in tier_data:
                for book in tier_data['books']:
                    book_set.add(book)
    
    # 3. If no books found, scan metadata directory
    if not book_set:
        if metadata_dir.exists():
            for meta_file in metadata_dir.glob("*_metadata.json"):
                book_name = meta_file.stem.replace("_metadata", "")
                book_set.add(book_name)
    
    # 4. Load metadata for each book
    all_metadata = {}
    for book in book_set:
        meta_path = metadata_dir / f"{book}_metadata.json"
        if meta_path.exists():
            with open(meta_path, encoding='utf-8') as f:
                all_metadata[book] = json.load(f)
    
    return {
        'books': list(book_set),
        'metadata': all_metadata,
        'corpus_size': sum(len(m.get('chapters', [])) for m in all_metadata.values())
    }
```

**Issue Type**: SonarQube warning - `Cognitive Complexity of 28 (threshold is 15)`

**Post-Fix Pattern**:
```python
def load_cross_book_context(taxonomy_path: Path, metadata_dir: Path) -> Dict[str, Any]:
    """Load metadata for all books listed in taxonomy."""
    if not taxonomy_path.exists():
        raise FileNotFoundError(f"Taxonomy file not found: {taxonomy_path}")
    
    with open(taxonomy_path, encoding='utf-8') as f:
        taxonomy = json.load(f)
    
    # Extract book names using helper functions (complexity: 4 each)
    book_set = _extract_books_from_taxonomy(taxonomy)
    if not book_set:
        book_set = _scan_metadata_directory(metadata_dir)
    
    # Load metadata (complexity: 3)
    all_metadata = _load_metadata_for_books(book_set, metadata_dir)
    
    return {
        'books': list(book_set),
        'metadata': all_metadata,
        'corpus_size': sum(len(m.get('chapters', [])) for m in all_metadata.values())
    }

def _extract_books_from_taxonomy(taxonomy: Dict[str, Any]) -> set:
    """Extract book names from taxonomy tiers. Complexity: 4"""
    book_set = set()
    if 'tiers' in taxonomy:
        for tier_data in taxonomy['tiers'].values():
            if 'books' in tier_data:
                book_set.update(tier_data['books'])
    return book_set

def _scan_metadata_directory(metadata_dir: Path) -> set:
    """Scan metadata directory for all available books. Complexity: 2"""
    book_set = set()
    if metadata_dir.exists():
        for meta_file in metadata_dir.glob("*_metadata.json"):
            book_name = meta_file.stem.replace("_metadata", "")
            book_set.add(book_name)
    return book_set

def _load_metadata_for_books(book_set: set, metadata_dir: Path) -> Dict[str, Any]:
    """Load metadata JSON files for given books. Complexity: 3"""
    all_metadata = {}
    for book in book_set:
        meta_path = metadata_dir / f"{book}_metadata.json"
        if meta_path.exists():
            with open(meta_path, encoding='utf-8') as f:
                all_metadata[book] = json.load(f)
    return all_metadata
```

**Root Cause**:
- **Incremental feature additions** - function grows over time
- **"Just one more if"** mentality - adding logic without refactoring
- **Lack of abstraction** - all logic in single function
- **Testing difficulty** - hard to test intermediate steps

**Prevention Strategy**:
1. **Extract Method** pattern - break into helper functions
2. **Single Responsibility Principle** - each function does one thing
3. **Cyclomatic Complexity limit**: Keep below 15 (SonarQube default)
4. **Code review rule**: "If it's hard to name, it's doing too much"
5. **Use Early Returns** to reduce nesting depth

---

### Anti-Pattern 2.2: Long Parameter Lists

**Pre-Fix Anti-Pattern** (from commit 47e02e97):
```python
def _generate_cross_references(
    book: str, 
    pages: List[Dict[str, Any]], 
    primary_content: str,
    n: int,
    all_chapters: Optional[List[Tuple[int, str, int, int]]] = None,
    chapter_pages: Optional[List[Dict[str, Any]]] = None
) -> Tuple[str, int, List[Dict[str, Any]]]:
    """Generate cross-references between chapters."""
    # Function body...
```

**Issue Type**: SonarQube warning - `Function has 6 parameters (threshold is 5)`

**Post-Fix Pattern**:
```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

@dataclass
class CrossReferenceContext:
    """Context for cross-reference generation."""
    book: str
    pages: List[Dict[str, Any]]
    n: int
    all_chapters: Optional[List[Tuple[int, str, int, int]]] = None

def _generate_cross_references(context: CrossReferenceContext) -> Tuple[str, int, List[Dict[str, Any]]]:
    """Generate cross-references between chapters."""
    # Access via context.book, context.pages, etc.
    # Removed primary_content and chapter_pages (were unused)
```

**Root Cause**:
- **Refactoring debt** - parameters added over time without cleanup
- **Unused parameters** - features removed but parameters remain
- **Lack of encapsulation** - passing primitives instead of objects

**Prevention Strategy**:
1. **Use dataclasses** for functions with >4 parameters
2. **Group related parameters** into context objects
3. **Remove unused parameters** during refactoring
4. **Builder pattern** for complex object construction
5. **SonarQube rule**: Enable S107 (too many parameters)

---

### Anti-Pattern 2.3: Complex Error Classification in Retry Logic (RESOLVED - December 4, 2025)

**Pre-Fix Anti-Pattern** (from llm-gateway `src/providers/anthropic.py` and `openai.py`):
```python
# anthropic.py - _execute_with_retry() - Cognitive Complexity: 19
async def _execute_with_retry(self, func, **kwargs) -> Any:
    last_error: Optional[Exception] = None
    
    for attempt in range(self._max_retries):
        try:
            return await func(**kwargs)
        except Exception as e:
            error_str = str(e).lower()
            
            # Check for authentication errors (don't retry) - adds +4 complexity
            if (
                "authentication" in error_str
                or "api key" in error_str
                or "unauthorized" in error_str
                or "invalid_api_key" in error_str
            ):
                raise AuthenticationError(str(e), provider="anthropic") from e
            
            # Check for rate limit errors (retry) - adds +2 complexity
            if "rate limit" in error_str or "429" in error_str:
                last_error = RateLimitError(str(e))
            else:
                last_error = ProviderError(str(e), provider="anthropic")
            
            # Wait before retry (exponential backoff)
            if attempt < self._max_retries - 1:
                delay = self._retry_delay * (2 ** attempt)
                await asyncio.sleep(delay)
    
    # Exhausted retries
    if isinstance(last_error, RateLimitError):
        raise last_error
    raise ProviderError(f"Request failed after {self._max_retries} attempts", provider="anthropic")
```

**Issue Type**: SonarQube warning - `Refactor this function to reduce its Cognitive Complexity from 19 to the 15 allowed`

**Post-Fix Pattern** (Extract Method + TDD):
```python
# Step 1: Extract error classification into helper method
def _classify_error(self, error_str: str) -> str:
    """
    Classify an error string into error type.
    
    Extracted to reduce cognitive complexity of _execute_with_retry().
    
    Args:
        error_str: The error message.
        
    Returns:
        Error type: 'auth', 'rate_limit', or 'other'.
    """
    error_lower = error_str.lower()
    
    # Authentication errors
    if (
        "authentication" in error_lower
        or "api key" in error_lower
        or "unauthorized" in error_lower
        or "invalid_api_key" in error_lower
    ):
        return "auth"
    
    # Rate limit errors
    if "rate limit" in error_lower or "429" in error_lower:
        return "rate_limit"
    
    return "other"

# Step 2: Simplified _execute_with_retry() - Complexity now 11
async def _execute_with_retry(self, func, **kwargs) -> Any:
    last_error: Exception | None = None
    
    for attempt in range(self._max_retries):
        try:
            return await func(**kwargs)
        except Exception as e:
            error_type = self._classify_error(str(e))
            
            # Auth errors: don't retry
            if error_type == "auth":
                raise AuthenticationError(str(e), provider="anthropic") from e
            
            # Rate limit errors: retry with RateLimitError
            if error_type == "rate_limit":
                last_error = RateLimitError(str(e))
            else:
                last_error = ProviderError(str(e), provider="anthropic")
            
            # Wait before retry (exponential backoff)
            if attempt < self._max_retries - 1:
                delay = self._retry_delay * (2 ** attempt)
                await asyncio.sleep(delay)
    
    # Exhausted retries
    if isinstance(last_error, RateLimitError):
        raise last_error
    raise ProviderError(f"Request failed after {self._max_retries} attempts", provider="anthropic")
```

**TDD Tests for Helper Method**:
```python
class TestAnthropicProviderErrorClassification:
    """Tests for _classify_error() helper method."""
    
    @pytest.fixture
    def provider(self):
        from src.providers.anthropic import AnthropicProvider
        return AnthropicProvider(api_key="test-key")
    
    def test_classify_error_authentication_error(self, provider) -> None:
        """_classify_error returns 'auth' for authentication errors."""
        assert provider._classify_error("authentication failed") == "auth"
        assert provider._classify_error("invalid api key") == "auth"
        assert provider._classify_error("unauthorized access") == "auth"
        assert provider._classify_error("invalid_api_key error") == "auth"
    
    def test_classify_error_rate_limit_error(self, provider) -> None:
        """_classify_error returns 'rate_limit' for rate limit errors."""
        assert provider._classify_error("rate limit exceeded") == "rate_limit"
        assert provider._classify_error("429 too many requests") == "rate_limit"
    
    def test_classify_error_other_error(self, provider) -> None:
        """_classify_error returns 'other' for unrecognized errors."""
        assert provider._classify_error("connection timeout") == "other"
        assert provider._classify_error("server error 500") == "other"
```

**Root Cause**:
- **Inline conditionals** - error classification logic embedded in retry loop
- **Multiple string checks** - each `or` clause adds complexity
- **DRY violation** - same classification logic duplicated in `_handle_error()`

**Prevention Strategy**:
1. **Extract Method** pattern - move conditional logic to helper functions
2. **Single Responsibility** - each method does one thing
3. **TDD for helpers** - write tests for extracted methods first
4. **Reuse helpers** - `_handle_error()` can now call `_classify_error()`
5. **Target complexity <15** - SonarQube default threshold

---

### Anti-Pattern 2.4: Complex Stream Event Handling (RESOLVED - December 4, 2025)

**Pre-Fix Anti-Pattern** (from llm-gateway `src/providers/anthropic.py` - `stream()` method):
```python
# stream() - Cognitive Complexity: 17
async def stream(self, request: ChatCompletionRequest) -> AsyncIterator[ChatCompletionChunk]:
    kwargs = self._build_request_kwargs(request)
    
    try:
        async with self._client.messages.stream(**kwargs) as stream:
            message_id: Optional[str] = None
            model: Optional[str] = None
            
            async for event in stream:
                if event.type == "message_start":
                    message_id = event.message.id
                    model = event.message.model
                elif event.type == "content_block_delta":
                    if hasattr(event.delta, "text"):
                        yield ChatCompletionChunk(
                            id=message_id or "unknown",
                            model=model or request.model,
                            choices=[
                                ChunkChoice(
                                    index=0,
                                    delta=ChunkDelta(
                                        role="assistant",
                                        content=event.delta.text,
                                    ),
                                    finish_reason=None,
                                )
                            ],
                        )
                elif event.type == "message_delta":
                    finish_reason = None
                    if hasattr(event.delta, "stop_reason"):
                        finish_reason = (
                            "stop" if event.delta.stop_reason == "end_turn"
                            else event.delta.stop_reason
                        )
                    yield ChatCompletionChunk(
                        id=message_id or "unknown",
                        model=model or request.model,
                        choices=[
                            ChunkChoice(
                                index=0,
                                delta=ChunkDelta(),
                                finish_reason=finish_reason,
                            )
                        ],
                    )
    except Exception as e:
        self._handle_error(e)
```

**Post-Fix Pattern** (Extract Event Handlers):
```python
# Helper methods extracted - each with complexity 2-4
def _handle_message_start(self, event: Any) -> tuple[str, str]:
    """Handle message_start event from Anthropic stream."""
    return event.message.id, event.message.model

def _handle_content_delta(self, event: Any, message_id: str, model: str) -> ChatCompletionChunk:
    """Handle content_block_delta event from Anthropic stream."""
    return ChatCompletionChunk(
        id=message_id,
        model=model,
        created=int(time.time()),
        choices=[
            ChunkChoice(
                index=0,
                delta=ChunkDelta(role="assistant", content=event.delta.text),
                finish_reason=None,
            )
        ],
    )

def _handle_message_delta(self, event: Any, message_id: str, model: str) -> ChatCompletionChunk:
    """Handle message_delta event from Anthropic stream."""
    finish_reason = None
    if hasattr(event.delta, "stop_reason"):
        stop_reason = event.delta.stop_reason
        finish_reason = "stop" if stop_reason == "end_turn" else stop_reason
    
    return ChatCompletionChunk(
        id=message_id,
        model=model,
        created=int(time.time()),
        choices=[
            ChunkChoice(index=0, delta=ChunkDelta(), finish_reason=finish_reason)
        ],
    )

# Simplified stream() - Complexity now 9
async def stream(self, request: ChatCompletionRequest) -> AsyncIterator[ChatCompletionChunk]:
    kwargs = self._build_request_kwargs(request)
    
    try:
        async with self._client.messages.stream(**kwargs) as stream:
            message_id: str | None = None
            model: str | None = None
            
            async for event in stream:
                if event.type == "message_start":
                    message_id, model = self._handle_message_start(event)
                elif event.type == "content_block_delta":
                    if hasattr(event.delta, "text"):
                        yield self._handle_content_delta(event, message_id or "unknown", model or request.model)
                elif event.type == "message_delta":
                    yield self._handle_message_delta(event, message_id or "unknown", model or request.model)
    except Exception as e:
        self._handle_error(e)
```

**TDD Tests for Stream Event Handlers**:
```python
class TestAnthropicProviderStreamEventHandlers:
    """Tests for stream event handlers - extracted from stream()."""
    
    def test_handle_message_start_event(self, provider) -> None:
        """_handle_message_start extracts id and model from event."""
        event = MagicMock()
        event.message.id = "msg_123"
        event.message.model = "claude-3-sonnet-20240229"
        
        message_id, model = provider._handle_message_start(event)
        
        assert message_id == "msg_123"
        assert model == "claude-3-sonnet-20240229"
    
    def test_handle_content_delta_event(self, provider) -> None:
        """_handle_content_delta creates chunk from content delta."""
        event = MagicMock()
        event.delta.text = "Hello world"
        
        chunk = provider._handle_content_delta(event, "msg_123", "claude-3-sonnet-20240229")
        
        assert isinstance(chunk, ChatCompletionChunk)
        assert chunk.choices[0].delta.content == "Hello world"
    
    def test_handle_message_delta_event_stop(self, provider) -> None:
        """_handle_message_delta creates final chunk with finish_reason."""
        event = MagicMock()
        event.delta.stop_reason = "end_turn"
        
        chunk = provider._handle_message_delta(event, "msg_123", "claude-3-sonnet-20240229")
        
        assert chunk.choices[0].finish_reason == "stop"
```

**Root Cause**:
- **Event handling inline** - all stream event logic in one method
- **Object construction complexity** - building ChatCompletionChunk adds lines
- **Multiple event types** - each type branch adds complexity

**Prevention Strategy**:
1. **Extract event handlers** - one method per event type
2. **Return objects from helpers** - don't yield from helpers
3. **Test handlers in isolation** - easier to verify behavior
4. **Consider State Pattern** - for complex event state machines

---

### Anti-Pattern 2.5: Complex Request Building (RESOLVED - December 4, 2025)

**Pre-Fix Anti-Pattern** (from llm-gateway `src/providers/openai.py` - `_build_request_kwargs()`):
```python
# _build_request_kwargs() - Cognitive Complexity: 21
def _build_request_kwargs(self, request: ChatCompletionRequest) -> dict[str, Any]:
    # Convert messages to dict format
    messages = []
    for msg in request.messages:
        msg_dict: dict[str, Any] = {"role": msg.role}
        if msg.content is not None:
            msg_dict["content"] = msg.content
        if msg.name is not None:
            msg_dict["name"] = msg.name
        if msg.tool_calls is not None:
            msg_dict["tool_calls"] = msg.tool_calls
        if msg.tool_call_id is not None:
            msg_dict["tool_call_id"] = msg.tool_call_id
        messages.append(msg_dict)
    
    kwargs: dict[str, Any] = {"model": request.model, "messages": messages}
    
    # Add optional parameters (each adds +1 complexity)
    if request.temperature is not None:
        kwargs["temperature"] = request.temperature
    if request.max_tokens is not None:
        kwargs["max_tokens"] = request.max_tokens
    if request.top_p is not None:
        kwargs["top_p"] = request.top_p
    # ... 6 more optional parameters
    
    # Handle tools (adds +3 complexity with list comprehension)
    if request.tools:
        kwargs["tools"] = [
            {
                "type": tool.type,
                "function": {
                    "name": tool.function.name,
                    **({"description": tool.function.description} if tool.function.description else {}),
                    **({"parameters": tool.function.parameters} if tool.function.parameters else {}),
                },
            }
            for tool in request.tools
        ]
    
    return kwargs
```

**Post-Fix Pattern** (Extract Helpers):
```python
def _transform_message(self, msg: Any) -> dict[str, Any]:
    """Transform a Message object to dict format for API call."""
    msg_dict: dict[str, Any] = {"role": msg.role}
    
    if msg.content is not None:
        msg_dict["content"] = msg.content
    if msg.name is not None:
        msg_dict["name"] = msg.name
    if msg.tool_calls is not None:
        msg_dict["tool_calls"] = msg.tool_calls
    if msg.tool_call_id is not None:
        msg_dict["tool_call_id"] = msg.tool_call_id
    
    return msg_dict

def _build_optional_params(self, request: ChatCompletionRequest) -> dict[str, Any]:
    """Build optional parameters dict from request."""
    params: dict[str, Any] = {}
    
    if request.temperature is not None:
        params["temperature"] = request.temperature
    if request.max_tokens is not None:
        params["max_tokens"] = request.max_tokens
    if request.top_p is not None:
        params["top_p"] = request.top_p
    # ... remaining parameters
    
    return params

def _build_tools_list(self, tools: list[Any]) -> list[dict[str, Any]]:
    """Build tools list from request tools."""
    return [
        {
            "type": tool.type,
            "function": {
                "name": tool.function.name,
                **({"description": tool.function.description} if tool.function.description else {}),
                **({"parameters": tool.function.parameters} if tool.function.parameters else {}),
            },
        }
        for tool in tools
    ]

# Simplified _build_request_kwargs() - Complexity now 7
def _build_request_kwargs(self, request: ChatCompletionRequest) -> dict[str, Any]:
    messages = [self._transform_message(msg) for msg in request.messages]
    
    kwargs: dict[str, Any] = {"model": request.model, "messages": messages}
    kwargs.update(self._build_optional_params(request))
    
    if request.tools:
        kwargs["tools"] = self._build_tools_list(request.tools)
    if request.tool_choice is not None:
        kwargs["tool_choice"] = request.tool_choice
    
    return kwargs
```

**Complexity Reduction Summary** (December 4, 2025):

| File | Method | Before | After | Helpers Added |
|------|--------|--------|-------|---------------|
| `anthropic.py` | `_execute_with_retry()` | 19 | 11 | `_classify_error()` |
| `anthropic.py` | `stream()` | 17 | 9 | `_handle_message_start()`, `_handle_content_delta()`, `_handle_message_delta()` |
| `anthropic.py` | `_transform_messages()` | 16 | 8 | `_transform_tool_message()`, `_transform_assistant_tool_message()`, `_merge_tool_result()` |
| `openai.py` | `_execute_with_retry()` | 18 | 10 | `_classify_error()` |
| `openai.py` | `_build_request_kwargs()` | 21 | 7 | `_transform_message()`, `_build_optional_params()`, `_build_tools_list()` |
| **Total** | **5 methods** | **91** | **45** | **10 helpers** |

**SonarQube Result**: 0 cognitive complexity warnings after refactoring.

---

## Category 3: Exception Handling (38 issues - 17%)

### Anti-Pattern 3.1: Bare Except Clauses

**Pre-Fix Anti-Pattern** (from commit 655880a5):
```python
def _extract_summary(chapter_text: str, chapter_num: int) -> Optional[str]:
    """Extract summary from chapter text."""
    try:
        # Complex summarization logic
        summary = summarizer.summarize(chapter_text, ratio=0.1)
        return summary
    except Exception:
        pass  # Silent failure - no logging
    
    return None
```

**Issue Type**: SonarQube warning - `Either log or rethrow this exception` (python:S1166)

**Post-Fix Pattern**:
```python
import logging

def _extract_summary(chapter_text: str, chapter_num: int) -> Optional[str]:
    """Extract summary from chapter text."""
    try:
        # Complex summarization logic
        summary = summarizer.summarize(chapter_text, ratio=0.1)
        return summary
    except Exception as e:
        # Log the exception for debugging but continue with fallback
        logging.debug(f"Summary extraction failed for chapter {chapter_num}: {e}")
        return None  # Explicit fallback
```

**Root Cause**:
- **Defensive programming** - catching all exceptions "just in case"
- **Lack of error visibility** - no way to diagnose failures
- **Development convenience** - quick fix during prototyping

**Prevention Strategy**:
1. **Never use bare `except:`** - always catch specific exceptions
2. **Always log caught exceptions** with context (chapter number, file name, etc.)
3. **Use `logging.debug()`** for expected failures (e.g., optional features)
4. **Use `logging.error()`** for unexpected failures that need investigation
5. **Bandit rule**: Enable B110 (try/except/pass)

---

### Anti-Pattern 3.2: Catching Generic Exceptions

**Pre-Fix Anti-Pattern** (from commit 96ef6cee):
```python
def extract_keywords(text: str, top_n: int = 10) -> List[Tuple[str, float]]:
    """Extract keywords using YAKE."""
    try:
        extractor = yake.KeywordExtractor(lan="en", n=1, top=top_n)
        keywords = extractor.extract_keywords(text)
        return keywords
    except Exception:  # Too broad - catches KeyboardInterrupt, SystemExit, etc.
        return []
```

**Issue Type**: Bandit warning - `Try, Except, Pass detected` (B110)

**Post-Fix Pattern**:
```python
import logging

def extract_keywords(text: str, top_n: int = 10) -> List[Tuple[str, float]]:
    """Extract keywords using YAKE."""
    try:
        extractor = yake.KeywordExtractor(lan="en", n=1, top=top_n)
        keywords = extractor.extract_keywords(text)
        return keywords
    except (ValueError, RuntimeError) as e:
        # Catch specific YAKE exceptions
        logging.warning(f"Keyword extraction failed: {e}")
        return []
    except Exception as e:
        # Unexpected exception - log and re-raise
        logging.error(f"Unexpected error in extract_keywords: {e}")
        raise
```

**Root Cause**:
- **Unknown exception types** - developer doesn't know what library raises
- **Overly defensive** - trying to prevent any crash
- **Lack of documentation** - third-party library doesn't document exceptions

**Prevention Strategy**:
1. **Read library documentation** to identify specific exceptions
2. **Test with invalid inputs** to discover exception types
3. **Catch specific exceptions** first, then generic as fallback
4. **Re-raise unexpected exceptions** to surface bugs
5. **Use type stubs** (`.pyi` files) that document exceptions

---

### Anti-Pattern 3.3: Swallowing Exceptions Without Fallback

**Pre-Fix Anti-Pattern** (from commit d040408d):
```python
def load_config(config_path: Path) -> Dict[str, Any]:
    """Load configuration from JSON file."""
    try:
        with open(config_path, encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        pass  # File doesn't exist - no fallback or error message
```

**Issue Type**: SonarQube warning - `Return statement missing after except clause` (python:S1163)

**Post-Fix Pattern**:
```python
import logging
from pathlib import Path

DEFAULT_CONFIG = {
    "max_concepts": 10,
    "max_keywords": 20,
    "summary_ratio": 0.1
}

def load_config(config_path: Path) -> Dict[str, Any]:
    """Load configuration from JSON file."""
    try:
        with open(config_path, encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.warning(f"Config file not found: {config_path}. Using defaults.")
        return DEFAULT_CONFIG.copy()  # Return safe default
    except json.JSONDecodeError as e:
        logging.error(f"Invalid JSON in {config_path}: {e}")
        raise  # Configuration errors should fail fast
```

**Root Cause**:
- **Optimistic assumption** - "it'll work most of the time"
- **No fallback plan** - what should happen if file doesn't exist?
- **Silent failures** - user has no idea why system behaves incorrectly

**Prevention Strategy**:
1. **Always provide fallback** - default config, empty dict, None
2. **Log the fallback** so users know what happened
3. **Distinguish expected vs unexpected errors**:
   - Expected (missing config) â†’ use default + log warning
   - Unexpected (corrupted config) â†’ fail fast + log error
4. **Document fallback behavior** in function docstring

---

### Anti-Pattern 3.4: Redundant Exception Hierarchy Catch (NEW - CL-029)

**Pre-Fix Anti-Pattern** (from llm-gateway commit d2c50b6):
```python
# src/tools/executor.py:315
async def _execute_single(self, tool_call: ToolCall) -> ToolResult:
    """Execute a single tool call, catching validation errors."""
    try:
        return await self.execute(tool_call)
    except (ToolExecutionError, ToolValidationError) as e:
        # Redundant: ToolValidationError inherits from ToolExecutionError
        return ToolResult(
            tool_call_id=tool_call.id,
            content=f"Tool error: {e}",
            is_error=True,
        )
```

**Issue Type**: SonarLint - `Redundant exception type in catch clause`

**Post-Fix Pattern**:
```python
async def _execute_single(self, tool_call: ToolCall) -> ToolResult:
    """Execute a single tool call, catching validation errors."""
    try:
        return await self.execute(tool_call)
    except ToolExecutionError as e:
        # ToolValidationError inherits from ToolExecutionError, so this catches both
        return ToolResult(
            tool_call_id=tool_call.id,
            content=f"Tool error: {e}",
            is_error=True,
        )
```

**Root Cause**:
- **Defensive over-specification** - listing all exception types "to be safe"
- **Lack of inheritance awareness** - not checking if exceptions inherit
- **Copy-paste inheritance** - code evolved from catching separate exceptions

**Prevention Strategy**:
1. **Check exception hierarchy** before catching multiple types
2. **Use only base class** when derived class is also caught
3. **IDE inspection** - most IDEs highlight redundant catches
4. **SonarLint rule**: python:S1045 (redundant exception handling)

---

## Category 4: Unused Parameters (32 issues - 14%)

### Anti-Pattern 4.1: Refactoring Debt - Leftover Parameters

**Pre-Fix Anti-Pattern** (from commit 47e02e97):
```python
def _generate_cross_references(
    book: str, 
    pages: List[Dict[str, Any]], 
    primary_content: str,  # Never used in function body
    n: int,
    all_chapters: Optional[List[Tuple[int, str, int, int]]] = None,
    chapter_pages: Optional[List[Dict[str, Any]]] = None  # Never used
) -> Tuple[str, int, List[Dict[str, Any]]]:
    """Generate cross-references between chapters."""
    
    # Function body never references primary_content or chapter_pages
    cross_matches = _find_cross_references(book, pages, n)
    return cross_matches, n, []
```

**Issue Type**: SonarQube MAJOR - `Remove the unused function parameter "primary_content"` (python:S1172)

**Post-Fix Pattern**:
```python
def _generate_cross_references(
    book: str, 
    pages: List[Dict[str, Any]], 
    n: int,
    all_chapters: Optional[List[Tuple[int, str, int, int]]] = None
) -> Tuple[str, int, List[Dict[str, Any]]]:
    """Generate cross-references between chapters."""
    
    # Removed unused parameters: primary_content, chapter_pages
    cross_matches = _find_cross_references(book, pages, n)
    return cross_matches, n, []

# Update call sites:
# OLD: cross_matches, primary_content, n
# NEW: cross_matches, n
result = _generate_cross_references(book, pages, n, all_chapters=CHAPTERS)
```

**Root Cause**:
- **Feature removal** - code deleted but signature unchanged
- **Refactoring incompleteness** - updated function body but not signature
- **Interface over-engineering** - "we might need this later"
- **Lack of automated cleanup** - no tool to detect unused params

**Prevention Strategy**:
1. **Remove parameters immediately** when removing feature
2. **Use IDE refactoring tools** (VS Code "Rename Symbol", PyCharm "Change Signature")
3. **SonarQube CI check** - fail build on unused parameters
4. **Code review checklist**: "Are all parameters used?"
5. **Avoid "future-proofing"** - YAGNI (You Aren't Gonna Need It)

---

### Anti-Pattern 4.2: Interface Compliance Parameters

**Pre-Fix Anti-Pattern** (from commit 7132fcf0):
```python
def _write_output_file(all_docs: List[str], book_name: str, all_footnotes: List[Dict] = None) -> None:
    """Write guideline output file."""
    # Function body never uses all_footnotes
    output_path = OUTPUT_DIR / f"{book_name}_guideline.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n\n".join(all_docs))
```

**Issue Type**: SonarQube MAJOR - `Remove the unused function parameter "all_footnotes"` (python:S1172)

**Post-Fix Pattern** (Option 1: Remove parameter):
```python
def _write_output_file(all_docs: List[str], book_name: str) -> None:
    """Write guideline output file."""
    output_path = OUTPUT_DIR / f"{book_name}_guideline.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n\n".join(all_docs))
```

**Post-Fix Pattern** (Option 2: Document why it exists):
```python
def _write_output_file(all_docs: List[str], book_name: str, 
                       all_footnotes: Optional[List[Dict[Any, Any]]] = None) -> None:
    """Write guideline output file.
    
    Args:
        all_docs: List of chapter content strings
        book_name: Name of the book for filename
        all_footnotes: Reserved for future feature (footnote section). 
                      Currently unused but kept for API stability.
    """
    # TODO: Implement footnote section generation
    output_path = OUTPUT_DIR / f"{book_name}_guideline.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n\n".join(all_docs))
```

**Root Cause**:
- **Planned feature** - parameter added in anticipation of future work
- **API stability** - removing parameter breaks existing callers
- **Incomplete implementation** - feature planned but not built

**Prevention Strategy**:
1. **Remove unused parameters** unless they serve a documented purpose
2. **Add TODO comment** if parameter is for planned feature
3. **Use `# noqa: ARG001`** or `# pylint: disable=unused-argument` with explanation
4. **Prefix with underscore** to indicate intentionally unused: `_all_footnotes`
5. **Consider `*args, **kwargs`** if signature needs flexibility

---

### Anti-Pattern 4.3: Framework-Required Unused Parameters (NEW - CL-029)

**Pre-Fix Anti-Pattern** (from llm-gateway commit d2c50b6):
```python
# src/observability/logging.py - structlog processor functions
def add_timestamp(
    logger: logging.Logger, method_name: str, event_dict: EventDict
) -> EventDict:
    """Add ISO 8601 timestamp to log event."""
    # method_name is required by structlog interface but not used
    event_dict["timestamp"] = datetime.now(timezone.utc).isoformat()
    return event_dict
```

**Issue Type**: SonarLint - `Remove the unused function parameter "method_name"` (python:S1172)

**Post-Fix Pattern**:
```python
def add_timestamp(
    logger: logging.Logger, _method_name: str, event_dict: EventDict
) -> EventDict:
    """Add ISO 8601 timestamp to log event.
    
    WBS 2.8.1.1.4: Add timestamp processor.
    
    Args:
        logger: The logger instance (unused but required by structlog interface)
        _method_name: The log method name (unused but required by structlog interface)
        event_dict: The event dictionary to process
    """
    event_dict["timestamp"] = datetime.now(timezone.utc).isoformat()
    return event_dict
```

**Root Cause**:
- **Framework/library interface requirements** - signature must match callback contract
- **Duck typing contracts** - Python relies on parameter position/name matching
- **Structural subtyping** - Protocols require specific signatures

**Prevention Strategy**:
1. **Prefix unused params with underscore** (`_method_name`) - PEP 8 convention
2. **Document in docstring** why parameter exists but is unused
3. **Use `# type: ignore[unused-argument]`** only if underscore prefix doesn't work
4. **Check framework docs** - some frameworks provide alternative signatures
5. **SonarLint recognizes** underscore-prefixed params as intentionally unused

---

## Category 5: Import Problems (24 issues - 11%)

### Anti-Pattern 5.1: Untyped Third-Party Imports

**Pre-Fix Anti-Pattern** (from commit 7132fcf0):
```python
# enrich_metadata_per_book.py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Mypy error: Skipping analyzing "sklearn": module is installed, but missing library stubs or py.typed marker
```

**Issue Type**: Mypy error - `error: Skipping analyzing "sklearn": module is installed, but missing library stubs`

**Post-Fix Pattern**:
```python
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore[import-untyped]
from sklearn.metrics.pairwise import cosine_similarity  # type: ignore[import-untyped]
```

**Alternative** (install type stubs):
```bash
pip install types-scikit-learn
```

```python
# Now can import without type: ignore
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
```

**Root Cause**:
- **Third-party library lacks type stubs** - no `.pyi` files or `py.typed` marker
- **Mypy strict mode** - requires types for all imports
- **Missing type package** - library has separate type stub package not installed

**Prevention Strategy**:
1. **Check for type stubs** before choosing library: `pip search types-<library>`
2. **Install type stubs** if available: `pip install types-requests types-beautifulsoup4`
3. **Use `# type: ignore[import-untyped]`** only as last resort
4. **Contribute type stubs** to typeshed if library is popular
5. **Add to requirements.txt**: `types-*` packages alongside their libraries

---

### Anti-Pattern 5.2: Relative Imports in Complex Hierarchies

**Pre-Fix Anti-Pattern** (from commit 655880a5):
```python
# workflows/llm_enhancement/scripts/phases/annotation_service.py
from ..interactive_llm_system_v3_hybrid_prompt import (
    generate_llm_enhancements
)
# Mypy error: Cannot find implementation or library stub for module named 
# "workflows.llm_enhancement.scripts.interactive_llm_system_v3_hybrid_prompt"
```

**Issue Type**: Mypy error - `error: Cannot find implementation or library stub for module` (import-not-found)

**Post-Fix Pattern**:
```python
# Use absolute import with type: ignore for dynamic imports
from ..interactive_llm_system_v3_hybrid_prompt import (  # type: ignore[import-not-found]
    generate_llm_enhancements
)
```

**Better Pattern** (if possible):
```python
# Restructure to use absolute imports
from workflows.llm_enhancement.scripts.interactive_llm_system_v3_hybrid_prompt import (
    generate_llm_enhancements
)
```

**Root Cause**:
- **Complex directory structure** - deep nesting makes relative imports fragile
- **PYTHONPATH issues** - parent directory not in sys.path
- **Dynamic imports** - module loaded conditionally or at runtime
- **Mypy path configuration** - mypy can't resolve relative imports

**Prevention Strategy**:
1. **Prefer absolute imports** over relative imports for clarity
2. **Configure mypy.ini** with `mypy_path`:
   ```ini
   [mypy]
   mypy_path = $MYPY_CONFIG_FILE_DIR
   ```
3. **Add `__init__.py`** to all package directories
4. **Use `# type: ignore[import-not-found]`** only for truly dynamic imports
5. **Simplify directory structure** - avoid deep nesting (>3 levels)

---

### Anti-Pattern 5.3: Missing Type Ignore Specificity

**Pre-Fix Anti-Pattern** (from commit da505377):
```python
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
```

**Issue Type**: Mypy warning - `note: Use "# type: ignore[import-untyped]" to be more specific`

**Post-Fix Pattern**:
```python
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore[import-untyped]
from sklearn.metrics.pairwise import cosine_similarity  # type: ignore[import-untyped]
```

**Root Cause**:
- **Convenience** - `# type: ignore` is easier to type than specific error code
- **Lack of awareness** - developer doesn't know specific error codes exist
- **Copy-paste** - inherited from other files without improvement

**Prevention Strategy**:
1. **Always use specific error codes** with `# type: ignore[error-code]`
2. **Learn common codes**:
   - `import-untyped` - library has no type stubs
   - `import-not-found` - module cannot be resolved
   - `assignment` - incompatible type assignment
   - `operator` - unsupported operator for types
   - `arg-type` - argument type mismatch
3. **Mypy will suggest** specific code in error message
4. **CI/CD check**: Reject generic `# type: ignore` without code

---

## Category 6: Regex Patterns (18 issues - 8%)

### Anti-Pattern 6.1: Catastrophic Backtracking with Greedy Quantifiers

**Pre-Fix Anti-Pattern** (from commit 7132fcf0):
```python
# chapter_generator_all_text.py:1844
concept_pattern = r'#### \*\*(.+?)\*\* \*\(p\.(\d+)\)\*\n+\*\*Verbatim Educational Excerpt\*\*.*?\n```\n(.*?)\n```\n.*?\*\*Annotation:\*\* (.+?)(?=####|\Z)'
# Issue: Multiple .+? and .*? can cause catastrophic backtracking on malformed input
```

**Issue Type**: SonarQube MAJOR - `Fix this reluctant quantifier that will only ever match 1 repetition` (python:S6019)

**Post-Fix Pattern**:
```python
# More specific pattern with character class negation and length limits
concept_pattern = r'#### \*\*([^\*]{1,200})\*\* \*\(p\.(\d{1,5})\)\*[^\n]{0,100}\n\n\*\*Verbatim Educational Excerpt\*\*[^`]{0,100}```\n([^`]{1,5000})\n```\n\[[\^\d]{1,10}\]\n\*\*Annotation:\*\* ([^\n#]{1,10000})(?=\n\n####|\n\n###|\Z)'

# Explanation of improvements:
# 1. [^\*]{1,200} - Match anything except *, up to 200 chars (instead of .+?)
# 2. (\d{1,5}) - Page numbers are at most 5 digits (instead of \d+)
# 3. [^\n]{0,100} - Allow whitespace but not newlines (instead of .+?)
# 4. [^`]{1,5000} - Match anything except backticks (instead of .*?)
# 5. [^\n#]{1,10000} - Annotation text until next heading (instead of .+?)
```

**Root Cause**:
- **Lazy quantifiers overused** - `.+?` and `.*?` seem "safe" but can backtrack
- **No length constraints** - pattern can match arbitrary lengths
- **Overlapping patterns** - multiple quantifiers compete for same text
- **Performance ignorance** - developer doesn't test on large inputs

**Prevention Strategy**:
1. **Use character class negation** `[^X]` instead of `.+?` or `.*?`
2. **Add length constraints** to all quantifiers: `{1,200}` instead of `+`
3. **Test with ReDoS tools**: `pip install rxxr2`, then `rxxr2 <pattern>`
4. **Benchmark on large inputs** - regex should complete in <100ms
5. **Use atomic groups** `(?>...)` to prevent backtracking (advanced)
6. **Consider alternative parsers** - markdown AST library instead of regex

---

### Anti-Pattern 6.2: Overly Permissive Patterns

**Pre-Fix Anti-Pattern** (from commit 76156f14):
```python
# Matches too broadly - captures unwanted text
chapter_pattern = r'Chapter (\d+):(.+)'
# Problem: Matches "Chapter 1:  " (trailing spaces), "Chapter 1:###" (markdown artifacts)
```

**Issue Type**: SonarQube INFO - `Regex pattern too permissive, may match unintended text`

**Post-Fix Pattern**:
```python
# More specific - trim whitespace and avoid special characters
chapter_pattern = r'Chapter (\d{1,3}):\s*([A-Za-z0-9\s,\-\']+?)(?:\n|$)'
# Explanation:
# (\d{1,3}) - Chapter numbers are 1-999
# \s* - Allow any whitespace after colon
# ([A-Za-z0-9\s,\-\']+?) - Title with letters, numbers, spaces, commas, hyphens, apostrophes
# (?:\n|$) - Stop at newline or end of string
```

**Root Cause**:
- **Quick prototyping** - "just get it working" mindset
- **Insufficient testing** - only tested on clean inputs
- **Lack of validation** - no checks on matched content

**Prevention Strategy**:
1. **Define expected format** before writing regex
2. **Use test-driven development** - write test cases first
3. **Test edge cases**: empty strings, special characters, very long inputs
4. **Validate matched groups** - check length, content after matching
5. **Use online regex testers** - regex101.com, regexr.com

---

## Category 7: Variable Shadowing (12 issues - 5%)

### Anti-Pattern 7.1: Loop Variable Shadowing Outer Scope

**Pre-Fix Anti-Pattern** (from commit 655880a5):
```python
# generate_concept_taxonomy.py:262
# Outer scope: tuple unpacking
tier_name, concepts, books = parse_tier_data(tier)

# Later in same function (line 296):
for concepts in chapter_data:  # Shadows 'concepts' from line 262
    process_concept(concepts)
```

**Issue Type**: Mypy error - `error: Name "concepts" already defined on line 262`

**Post-Fix Pattern**:
```python
# Outer scope: tuple unpacking
tier_name, concepts, books = parse_tier_data(tier)

# Use different variable name in loop
for tier_concepts_list in chapter_data:
    process_concept(tier_concepts_list)
```

**Root Cause**:
- **Common variable names** - "concepts", "data", "items" used everywhere
- **Incremental code addition** - new loop added without checking scope
- **Copy-paste** - duplicated code uses same variable names

**Prevention Strategy**:
1. **Use descriptive names** - `tier_concepts_list` instead of `concepts`
2. **Prefix with scope** - `chapter_concepts`, `book_concepts`, `tier_concepts`
3. **Mypy will catch** shadowing in same scope
4. **IDE warnings** - PyCharm/VS Code highlight shadowed variables
5. **Code review** - watch for generic names like `data`, `item`, `value`

---

## Category 8: Async/Await Patterns (NEW - CL-029)

### Anti-Pattern 8.1: Async Function Without Await (Intentional Stubs)

**Pre-Fix Anti-Pattern** (from llm-gateway commit d2c50b6):
```python
# src/api/routes/sessions.py - Stub implementation
class SessionService:
    def __init__(self) -> None:
        self._sessions: dict[str, dict[str, Any]] = {}  # In-memory stub

    async def create_session(
        self,
        ttl_seconds: Optional[int] = None,
        context: Optional[dict[str, Any]] = None,
    ) -> dict[str, Any]:
        """Create a new session."""
        session_id = str(uuid.uuid4())
        # ... purely synchronous operations
        self._sessions[session_id] = session_data
        return session_data
```

**Issue Type**: SonarLint - `Use asynchronous features in this function or remove the async keyword`

**Why This Is Intentional**:
- **Future-proofing for async I/O** - Redis calls will be async
- **API contract consistency** - FastAPI expects async route handlers
- **Interface stability** - changing asyncâ†’sync breaks callers

**Post-Fix Pattern** (suppress warning with documentation):
```python
async def create_session(  # NOSONAR - async for Redis compatibility (WBS 2.3)
    self,
    ttl_seconds: Optional[int] = None,
    context: Optional[dict[str, Any]] = None,
) -> dict[str, Any]:
    """Create a new session.
    
    Note:
        This is a stub implementation using in-memory storage.
        The async keyword is intentionally retained for API compatibility
        with future Redis integration (WBS 2.3) which requires async I/O.
    """
    # In-memory stub - will be replaced with:
    # await self._redis.set(session_id, session_data, ex=ttl)
    session_id = str(uuid.uuid4())
    self._sessions[session_id] = session_data
    return session_data
```

**Root Cause**:
- **Incremental development** - async infrastructure not yet implemented
- **Interface design** - designing for target state, not current state
- **Framework requirements** - FastAPI prefers async handlers

**Prevention Strategy**:
1. **Document the intent** - explain why async exists without await
2. **Use NOSONAR/noqa** with explanation comment
3. **Reference WBS/ticket** - link to future implementation work
4. **Stub comment pattern** - `# Will be replaced with: await ...`
5. **Consider abstract base** - define async interface in ABC

---

### Anti-Pattern 8.2: Async Generator Without Await (Valid Pattern)

**Pattern** (from llm-gateway):
```python
# src/api/routes/chat.py - Valid async generator
async def stream_completion(  # NOSONAR - async generator for LLM streaming
    self, request: ChatCompletionRequest
) -> AsyncGenerator[ChatCompletionChunk, None]:
    """Stream a chat completion as chunks."""
    response_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
    full_response = self._generate_stub_response(request)
    tokens = full_response.split()

    # Async generator yields synchronously in stub
    for token in tokens:
        yield ChatCompletionChunk(
            id=response_id,
            model=request.model,
            choices=[ChunkChoice(delta=ChunkDelta(content=token))]
        )
```

**Why SonarLint Flags This**:
- Async generator with only `yield`, no `await`
- Technically valid Python - async generators don't require await
- Static analyzer can't determine if async iteration is needed

**When This Is Valid**:
- **Async generator protocol** - must be async for `async for` iteration
- **Future async operations** - will add `await` when integrating real provider
- **Framework expectations** - FastAPI streaming requires async generators

**Correct Suppression**:
```python
async def stream_completion(  # NOSONAR - async generator for LLM streaming
    self, request: ChatCompletionRequest
) -> AsyncGenerator[ChatCompletionChunk, None]:
    """Stream a chat completion as chunks.
    
    Note:
        Async generator is required for FastAPI StreamingResponse.
        Current stub yields synchronously, but production will:
        async for chunk in provider.stream(request):
            yield chunk
    """
```

**Prevention Strategy**:
1. **Async generators are valid** without await - they just yield
2. **Document future async** - show what the real implementation looks like
3. **NOSONAR with context** - explain it's for streaming protocol
4. **Test with async for** - verify generator works with async iteration

---

## Category 9: TODO/FIXME Comments (NEW - CL-029)

### Anti-Pattern 9.1: Indefinite TODO Comments

**Pre-Fix Anti-Pattern** (from llm-gateway commit d2c50b6):
```python
# src/main.py
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    app.state.initialized = True
    
    # TODO: WBS 2.1.1.2.2 - Initialize Redis connection pool
    # app.state.redis_pool = await create_redis_pool()
    
    # TODO: WBS 2.1.1.2.3 - Initialize provider client registry
    # app.state.provider_registry = ProviderRegistry()
```

**Issue Type**: SonarLint - `Complete the task associated with this TODO comment` (python:S1135)

**Post-Fix Pattern** (convert to implementation notes):
```python
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    app.state.initialized = True
    
    # NOTE: WBS 2.1.1.2.2 - Redis connection pool initialization
    # Implementation deferred to Stage 3: Integration (WBS 3.x)
    # When Redis is integrated: app.state.redis_pool = await create_redis_pool()
    
    # NOTE: WBS 2.1.1.2.3 - Provider client registry initialization
    # Implementation deferred to Stage 3: Integration (WBS 3.x)
    # When providers are integrated: app.state.provider_registry = ProviderRegistry()
```

**Root Cause**:
- **Wishful thinking** - "I'll get to this later"
- **Incomplete planning** - work not tracked in issue tracker
- **Technical debt accumulation** - TODOs never addressed

**Prevention Strategy**:
1. **Convert TODO to NOTE** when work is planned for future phase
2. **Link to WBS/ticket** - make TODOs traceable
3. **Set expiration** - `# TODO(2024-Q2): Implement caching`
4. **Track in issue tracker** - GitHub issue > code comment
5. **Review in retrospective** - audit TODOs periodically

---

## Category 10: Race Conditions (NEW - December 2025 CodeRabbit)

### Anti-Pattern 10.1: Unprotected Read-Modify-Write in Async Code

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```python
# src/api/middleware/rate_limit.py - Token Bucket race condition
class InMemoryRateLimiter(RateLimiter):
    def __init__(self, requests_per_minute: int = 60, burst: int = 10):
        self._buckets: dict[str, tuple[float, float]] = {}  # No synchronization

    async def is_allowed(self, client_id: str) -> RateLimitResult:
        now = time.time()
        
        # RACE: Request 1 reads tokens=5
        # RACE: Request 2 reads tokens=5 (before Request 1 writes)
        if client_id in self._buckets:
            tokens, last_update = self._buckets[client_id]
        else:
            tokens = float(self.burst)
            last_update = now
        
        # Refill and consume
        tokens = min(self.burst, tokens + elapsed * self._refill_rate)
        if tokens >= 1:
            tokens -= 1
            # RACE: Request 1 writes tokens=4
            # RACE: Request 2 writes tokens=4 (should be 3!)
            self._buckets[client_id] = (tokens, now)
```

**Issue Type**: CodeRabbit MAJOR - `Race condition in concurrent token bucket updates`

**Post-Fix Pattern**:
```python
import asyncio

class InMemoryRateLimiter(RateLimiter):
    def __init__(self, requests_per_minute: int = 60, burst: int = 10):
        self._buckets: dict[str, tuple[float, float]] = {}
        self._locks: dict[str, asyncio.Lock] = {}  # Per-client locks

    def _get_or_create_lock(self, client_id: str) -> asyncio.Lock:
        """Atomically get or create lock for client."""
        if client_id not in self._locks:
            self._locks[client_id] = asyncio.Lock()
        return self._locks[client_id]

    async def is_allowed(self, client_id: str) -> RateLimitResult:
        lock = self._get_or_create_lock(client_id)
        
        async with lock:  # Atomic read-modify-write
            now = time.time()
            if client_id in self._buckets:
                tokens, last_update = self._buckets[client_id]
            else:
                tokens = float(self.burst)
                last_update = now
            
            tokens = min(self.burst, tokens + elapsed * self._refill_rate)
            if tokens >= 1:
                tokens -= 1
                self._buckets[client_id] = (tokens, now)
                return RateLimitResult(allowed=True, ...)
```

**Root Cause**:
- **Shared mutable state** - dict accessed by concurrent coroutines
- **Non-atomic operations** - read, compute, write are separate steps
- **Async context blindness** - developer assumes single-threaded execution

**Prevention Strategy**:
1. **Use asyncio.Lock()** for shared mutable state in async code
2. **Per-resource locks** - avoid global lock bottleneck (per-client, per-session)
3. **Atomic operations** - Redis INCR, database transactions
4. **Immutable data** - prefer functional patterns where possible
5. **Test with concurrency** - use `asyncio.gather()` to simulate races

---

### Anti-Pattern 10.2: State Mutation in Property Getter

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```python
# src/clients/circuit_breaker.py - Race in state property
class CircuitBreaker:
    @property
    def state(self) -> CircuitState:
        # RACE: Mutation inside getter!
        if self._state == CircuitState.OPEN and self._should_attempt_recovery():
            self._state = CircuitState.HALF_OPEN  # Two coroutines could both transition
        return self._state
```

**Issue Type**: CodeRabbit MAJOR - `State property has side effect that could cause race conditions`

**Post-Fix Pattern**:
```python
class CircuitBreaker:
    def __init__(self):
        self._lock = asyncio.Lock()
    
    @property
    def state(self) -> CircuitState:
        """Read-only state property."""
        return self._state  # No mutation in getter
    
    async def check_and_update_state(self) -> CircuitState:
        """Atomically check and update circuit state."""
        async with self._lock:
            if self._state == CircuitState.OPEN and self._should_attempt_recovery():
                self._state = CircuitState.HALF_OPEN
            return self._state
```

**Root Cause**:
- **Side effects in getters** - properties should be read-only
- **Convenience over correctness** - combining check+update seems simpler
- **Hidden state transitions** - caller doesn't expect property access to change state

**Prevention Strategy**:
1. **Properties should be pure** - no side effects, no mutations
2. **Separate read and update** - `state` property vs `update_state()` method
3. **Make mutations explicit** - callers should call async method intentionally
4. **Use locks for state transitions** - protect critical sections

---

### Anti-Pattern 10.3: Non-Atomic Redis Operations

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```python
# src/services/cost_tracker.py - Redis race condition
async def record_usage(self, model: str, tokens: int, cost: float):
    daily_key = self._get_daily_key()
    
    # RACE: Read current data
    current_data = await self._redis.get(daily_key)
    data = json.loads(current_data) if current_data else {}
    
    # RACE: Modify in Python
    data["tokens"] = data.get("tokens", 0) + tokens
    data["cost"] = data.get("cost", 0) + cost
    
    # RACE: Write back - another request may have updated in between
    await self._redis.set(daily_key, json.dumps(data))
```

**Issue Type**: CodeRabbit MAJOR - `Race condition: concurrent requests may lose usage data`

**Post-Fix Pattern**:
```python
async def record_usage(self, model: str, tokens: int, cost: float):
    daily_key = self._get_daily_key()
    
    # Atomic Redis operations - no race condition
    pipe = self._redis.pipeline()
    pipe.hincrby(daily_key, "tokens", tokens)
    pipe.hincrbyfloat(daily_key, "cost", cost)
    pipe.hincrby(daily_key, "request_count", 1)
    await pipe.execute()
```

**Root Cause**:
- **Application-level atomicity** - assuming Python code is atomic
- **Ignoring concurrency** - treating Redis like a single-user database
- **Unfamiliarity with Redis primitives** - not knowing about INCR/HINCR

**Prevention Strategy**:
1. **Use Redis atomic operations** - INCR, HINCRBY, HINCRBYFLOAT
2. **Use pipelines** for multiple atomic operations
3. **Use Lua scripts** for complex atomic logic
4. **Use WATCH/MULTI/EXEC** for optimistic locking
5. **Test under load** - concurrent requests reveal races

---

## Category 11: Secret Exposure (NEW - December 2025 CodeRabbit)

### Anti-Pattern 11.1: Secrets in Helm CLI Arguments

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```yaml
# .github/workflows/cd-prod.yml - Secrets visible in process list
- name: Deploy
  run: |
    helm upgrade llm-gateway ./deploy/helm \
      --set secrets.anthropicApiKey=${{ secrets.ANTHROPIC_API_KEY }} \
      --set secrets.openaiApiKey=${{ secrets.OPENAI_API_KEY }}
```

**Issue Type**: CodeRabbit MAJOR - `Secrets exposed via command-line arguments`

**Impact**: Secrets visible in `ps aux`, CI logs, and shell history.

**Post-Fix Pattern**:
```yaml
- name: Deploy
  env:
    HELM_ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
    HELM_OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  run: |
    helm upgrade llm-gateway ./deploy/helm \
      --set secrets.anthropicApiKey="$HELM_ANTHROPIC_API_KEY" \
      --set secrets.openaiApiKey="$HELM_OPENAI_API_KEY"
```

**Prevention Strategy**:
1. **Use environment variables** - GitHub Actions masks env vars in logs
2. **Use `--set-file`** with temporary files (cleaned up after)
3. **Use External Secrets Operator** for production
4. **Never echo secrets** in CI scripts

---

### Anti-Pattern 11.2: Secrets in Redis URL

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```yaml
# deploy/helm/llm-gateway/templates/_helpers.tpl
{{- define "llm-gateway.redisUrl" -}}
redis://{{ .Values.redis.auth.password }}@{{ .Values.redis.host }}:{{ .Values.redis.port }}
{{- end }}
# Password embedded in URL, visible in pod spec and logs
```

**Issue Type**: CodeRabbit MAJOR - `Redis password exposed in deployment spec`

**Post-Fix Pattern**:
```yaml
# _helpers.tpl - URL without password
{{- define "llm-gateway.redisUrl" -}}
redis://{{ .Values.redis.host }}:{{ .Values.redis.port }}
{{- end }}

# deployment.yaml - Password from Secret
- name: LLM_GATEWAY_REDIS_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ .Release.Name }}-secrets
      key: redis-password
```

**Prevention Strategy**:
1. **Separate URL and password** - password from secretKeyRef
2. **Use Kubernetes Secrets** for all credentials
3. **External Secrets Operator** for production (AWS Secrets Manager, HashiCorp Vault)
4. **Audit pod specs** - `kubectl get pod -o yaml` should not show secrets

---

## Category 12: Kubernetes/Helm Configuration (NEW - December 2025 CodeRabbit)

### Anti-Pattern 12.1: Environment Variable Prefix Mismatch

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```yaml
# deployment.yaml - Wrong env var names
- name: ENVIRONMENT
  value: "production"
- name: REDIS_URL
  value: "redis://redis:6379"

# config.py - Expects LLM_GATEWAY_ prefix
class Settings(BaseSettings):
    model_config = {"env_prefix": "LLM_GATEWAY_"}
    environment: str = "development"
    redis_url: str = "redis://localhost:6379"
```

**Issue Type**: CodeRabbit MAJOR - `Environment variable names don't match Settings prefix`

**Impact**: Helm values silently ignored, application uses code defaults.

**Post-Fix Pattern**:
```yaml
# deployment.yaml - Correct prefix
- name: LLM_GATEWAY_ENVIRONMENT
  value: "production"
- name: LLM_GATEWAY_REDIS_URL
  value: "redis://redis:6379"
```

**Prevention Strategy**:
1. **Document env prefix** in README and values.yaml
2. **Helm unit tests** to verify env var names
3. **Integration tests** that validate Settings loads correctly
4. **CI check** - grep for env vars without prefix

---

### Anti-Pattern 12.2: Probe Paths Don't Match Endpoints

**Pre-Fix Anti-Pattern** (from llm-gateway CodeRabbit PR #2):
```yaml
# deployment.yaml - Non-existent paths
livenessProbe:
  httpGet:
    path: /live    # Doesn't exist!
readinessProbe:
  httpGet:
    path: /ready   # Doesn't exist!
```

**Issue Type**: CodeRabbit CRITICAL - `Probe paths don't match actual endpoints`

**Impact**: Kubernetes marks pods unhealthy, causing restart loops.

**Post-Fix Pattern**:
```yaml
livenessProbe:
  httpGet:
    path: /health        # Actual endpoint
readinessProbe:
  httpGet:
    path: /health/ready  # Actual endpoint
```

**Prevention Strategy**:
1. **kubeconform validation** in CI - validates manifests
2. **Integration tests** - verify probe endpoints exist
3. **Document health endpoints** in API spec (OpenAPI)
4. **Helm unit tests** with helm-unittest plugin

---

## Category 13: Provider Abstraction Patterns (NEW - December 2025 TDD)

### Anti-Pattern 13.1: Missing Provider Implementation

**Pre-Fix Anti-Pattern** (from llm-gateway ARCHITECTURE.md gap analysis):
```python
# src/providers/__init__.py - Missing AnthropicProvider
from .openai import OpenAIProvider
from .ollama import OllamaProvider
# AnthropicProvider not implemented! Only AnthropicToolHandler exists.
```

**Issue Type**: Architecture gap - `AnthropicProvider not yet implemented`

**Impact**: Cannot route to Claude models, breaks provider abstraction.

**Post-Fix Pattern** (implemented with TDD - 26 tests):
```python
# src/providers/anthropic.py
class AnthropicProvider(LLMProvider):
    """Anthropic Claude provider implementing LLMProvider interface."""
    
    SUPPORTED_MODELS = [
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-5-sonnet-20241022",
        "claude-3-haiku-20240307",
    ]
    
    def __init__(self, api_key: str, max_retries: int = 3, retry_delay: float = 1.0):
        self._client = AsyncAnthropic(api_key=api_key)
        self._max_retries = max_retries
        self._retry_delay = retry_delay
    
    async def complete(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """Non-streaming chat completion."""
        response = await self._execute_with_retry(
            self._client.messages.create,
            model=request.model,
            messages=self._transform_messages(request.messages),
            max_tokens=request.max_tokens or 4096,
        )
        return self._transform_response(response)
    
    async def stream(self, request: ChatCompletionRequest) -> AsyncIterator[ChatCompletionChunk]:
        """Streaming chat completion."""
        async with self._client.messages.stream(...) as stream:
            async for event in stream:
                yield self._transform_chunk(event)
    
    def supports_model(self, model: str) -> bool:
        return model.startswith("claude-")
    
    def get_supported_models(self) -> list[str]:
        return self.SUPPORTED_MODELS.copy()
```

**Root Cause**:
- **Incremental development** - OpenAI implemented first, Anthropic deferred
- **Tool handler confusion** - AnthropicToolHandler existed but not full provider
- **Missing TDD** - no failing tests to drive implementation

**Prevention Strategy**:
1. **Architecture gap analysis** - review ARCHITECTURE.md periodically
2. **TDD for providers** - write tests before implementation
3. **Interface compliance tests** - verify all providers implement ABC
4. **Provider registry** - enumerate expected providers at startup

---

## Prevention Checklist

### Pre-Commit Checks

- [ ] **Run Mypy**: `mypy --strict workflows/`
- [ ] **Run Ruff**: `ruff check workflows/`
- [ ] **Run Bandit**: `bandit -r workflows/`
- [ ] **Run SonarQube**: Local analysis before push
- [ ] **Run Tests**: `pytest tests/`
- [ ] **Check Coverage**: `pytest --cov=workflows --cov-report=term-missing`

### Code Review Checklist

- [ ] **Type Annotations**: All parameters and return types annotated?
- [ ] **Optional Types**: Variables that can be `None` use `Optional[T]`?
- [ ] **Type Guards**: `if x is not None:` before dereferencing?
- [ ] **Exception Handling**: Specific exceptions caught, not generic `Exception`?
- [ ] **Exception Logging**: All caught exceptions logged with context?
- [ ] **Exception Hierarchy**: No redundant derived exceptions in catch clauses?
- [ ] **Unused Parameters**: All parameters actually used in function?
- [ ] **Framework Parameters**: Unused framework params prefixed with `_`?
- [ ] **Cognitive Complexity**: Functions <15 complexity (use helpers)?
- [ ] **Import Types**: Third-party imports have `# type: ignore[import-untyped]`?
- [ ] **Variable Shadowing**: No variable names reused in same scope?
- [ ] **Regex Patterns**: Length constraints on all quantifiers?
- [ ] **Async Functions**: Async without await documented with NOSONAR?
- [ ] **TODO Comments**: Converted to NOTE or linked to issue tracker?

### Project Setup

**pyproject.toml** (or **setup.cfg**):
```toml
[tool.mypy]
python_version = "3.9"
strict = true
warn_unused_ignores = true
warn_redundant_casts = true
warn_return_any = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_any_unimported = true

[tool.ruff]
line-length = 120
target-version = "py39"
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C90", # mccabe complexity
    "N",   # pep8-naming
    "UP",  # pyupgrade
    "B",   # flake8-bugbear
]
ignore = []

[tool.ruff.mccabe]
max-complexity = 15

[tool.bandit]
exclude_dirs = ["tests", "test_*"]
skips = ["B101", "B601"]  # Skip assert and shell=True warnings
```

**sonar-project.properties**:
```properties
sonar.projectKey=llm-document-enhancer
sonar.sources=workflows
sonar.python.version=3.9
sonar.python.coverage.reportPaths=coverage.xml
sonar.python.pylint.reportPaths=pylint-report.txt
sonar.python.bandit.reportPaths=bandit-report.json

# Thresholds
sonar.qualitygate.wait=true
sonar.issue.ignore.multicriteria=e1,e2
sonar.issue.ignore.multicriteria.e1.ruleKey=python:S1172
sonar.issue.ignore.multicriteria.e1.resourceKey=**/*_test.py
sonar.issue.ignore.multicriteria.e2.ruleKey=python:S1135
sonar.issue.ignore.multicriteria.e2.resourceKey=**/test_*.py
```

---

## Tool Comparison

| Tool | Strength | Weakness | When to Use |
|------|----------|----------|-------------|
| **Mypy** | Type checking, Optional detection | Requires type annotations | Always - catches type errors pre-runtime |
| **SonarQube** | Cognitive complexity, code smells | Slower, requires server | CI/CD quality gates, comprehensive analysis |
| **CodeRabbit** | AI-powered suggestions, context-aware | Requires GitHub/GitLab, paid | PR reviews, architectural feedback |
| **Bandit** | Security vulnerabilities | Limited scope (security only) | Security audits, pre-deployment checks |
| **Ruff** | Fast linting, auto-fixing | Less comprehensive than Pylint | Pre-commit hooks, local dev loops |

### Recommended Workflow

1. **Local Development** (pre-commit):
   ```bash
   ruff check --fix workflows/     # Fast linting
   mypy workflows/                 # Type checking
   pytest tests/                   # Test validation
   ```

2. **CI/CD Pipeline** (GitHub Actions):
   ```yaml
   - name: Lint
     run: ruff check workflows/
   
   - name: Type Check
     run: mypy --strict workflows/
   
   - name: Security Scan
     run: bandit -r workflows/ -f json -o bandit-report.json
   
   - name: SonarQube Scan
     run: sonar-scanner
   
   - name: Test
     run: pytest --cov=workflows --cov-report=xml
   ```

3. **Pull Request Review**:
   - **CodeRabbit** provides AI-powered review comments
   - **SonarQube** Quality Gate must pass (0 bugs, 0 vulnerabilities)
   - **Human reviewer** checks logic, architecture, readability

---

## Additional Coding Practices

Based on the analysis of 222 issues fixed across 21 commits, here are **additional coding practices** to prevent recurring anti-patterns:

### 1. Type Annotation Standards

**Practice 1.1: Explicit Optional for All Nullable Variables**
```python
# âŒ BAD - Mypy infers Any or causes errors
config = None
if should_load:
    config = load_config()

# âœ… GOOD - Explicit Optional[ConfigType]
config: Optional[ConfigType] = None
if should_load:
    config = load_config()
```

**Practice 1.2: Type Guards Before Dereferencing**
```python
# âŒ BAD - No type guard
def process_data(data: Optional[Dict[str, Any]]) -> str:
    return data['key']  # Mypy error: Optional not checked

# âœ… GOOD - Type guard narrows type
def process_data(data: Optional[Dict[str, Any]]) -> str:
    if data is None:
        return "N/A"
    return data['key']  # Mypy knows data is Dict here
```

**Practice 1.3: Annotate Empty Collections**
```python
# âŒ BAD - Mypy infers List[Never]
results = []
for item in items:
    results.append(process(item))

# âœ… GOOD - Explicit type annotation
results: List[ProcessedItem] = []
for item in items:
    results.append(process(item))
```

### 2. Exception Handling Standards

**Practice 2.1: Specific Exceptions with Logging**
```python
import logging

# âŒ BAD - Bare except
try:
    data = json.load(f)
except:
    pass

# âœ… GOOD - Specific exception with context
try:
    data = json.load(f)
except json.JSONDecodeError as e:
    logging.error(f"Failed to parse JSON from {filename}: {e}")
    raise
except FileNotFoundError:
    logging.warning(f"Config file {filename} not found, using defaults")
    data = DEFAULT_CONFIG
```

**Practice 2.2: Always Provide Fallback or Re-raise**
```python
# âŒ BAD - Silent failure
try:
    summary = generate_summary(text)
except Exception:
    pass

# âœ… GOOD - Fallback with logging
try:
    summary = generate_summary(text)
except (ValueError, RuntimeError) as e:
    logging.debug(f"Summary generation failed: {e}")
    summary = text[:500]  # Use truncated text as fallback
except Exception as e:
    logging.error(f"Unexpected error in generate_summary: {e}")
    raise
```

### 3. Function Complexity Standards

**Practice 3.1: Extract Method for Complex Logic**
```python
# âŒ BAD - Cognitive complexity 28
def process_taxonomy(taxonomy, metadata_dir):
    # 1. Load taxonomy
    if not taxonomy_path.exists():
        raise FileNotFoundError(...)
    with open(taxonomy_path) as f:
        taxonomy = json.load(f)
    
    # 2. Extract books
    book_set = set()
    if 'tiers' in taxonomy:
        for tier_name, tier_data in taxonomy['tiers'].items():
            if 'books' in tier_data:
                for book in tier_data['books']:
                    book_set.add(book)
    
    # 3. Scan directory
    if not book_set:
        for meta_file in metadata_dir.glob("*_metadata.json"):
            book_name = meta_file.stem.replace("_metadata", "")
            book_set.add(book_name)
    
    # ... more nested logic

# âœ… GOOD - Complexity 4 per function
def process_taxonomy(taxonomy_path, metadata_dir):
    taxonomy = _load_taxonomy(taxonomy_path)
    book_set = _extract_books(taxonomy)
    if not book_set:
        book_set = _scan_metadata_dir(metadata_dir)
    return _load_metadata(book_set, metadata_dir)

def _load_taxonomy(path):
    if not path.exists():
        raise FileNotFoundError(...)
    with open(path) as f:
        return json.load(f)

def _extract_books(taxonomy):
    # Single responsibility: extract books from taxonomy
    ...
```

**Practice 3.2: Use Dataclasses for Parameter Groups**
```python
from dataclasses import dataclass

# âŒ BAD - 6 parameters
def generate_cross_ref(book, pages, content, n, chapters, chapter_pages):
    ...

# âœ… GOOD - Grouped into context
@dataclass
class CrossRefContext:
    book: str
    pages: List[Dict[str, Any]]
    n: int
    chapters: Optional[List[Tuple[int, str, int, int]]] = None

def generate_cross_ref(context: CrossRefContext):
    # Access via context.book, context.pages, etc.
    ...
```

### 4. Import Standards

**Practice 4.1: Type Ignore with Specific Error Codes**
```python
# âŒ BAD - Generic type ignore
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore

# âœ… GOOD - Specific error code
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore[import-untyped]
```

**Practice 4.2: Install Type Stubs Where Available**
```bash
# Check for type stubs
pip search types-sklearn
pip search types-requests

# Install type stubs
pip install types-requests types-beautifulsoup4
```

**Practice 4.3: Prefer Absolute Imports**
```python
# âŒ FRAGILE - Relative imports break easily
from ..utils.helpers import process_data
from ...shared.models import DataModel

# âœ… ROBUST - Absolute imports
from workflows.metadata_extraction.utils.helpers import process_data
from workflows.shared.models import DataModel
```

### 5. Regex Standards

**Practice 5.1: Use Character Class Negation**
```python
# âŒ BAD - Catastrophic backtracking
pattern = r'Chapter (.+?): (.+?)(?=\n|$)'

# âœ… GOOD - Character class negation with length limits
pattern = r'Chapter ([0-9]{1,3}): ([^\n]{1,200})(?:\n|$)'
```

**Practice 5.2: Test Regex Performance**
```python
import re
import time

pattern = r'Chapter ([0-9]{1,3}): ([^\n]{1,200})(?:\n|$)'
test_text = "Chapter 1: Introduction\n" * 1000

start = time.time()
matches = re.findall(pattern, test_text)
elapsed = time.time() - start

assert elapsed < 0.1, f"Regex too slow: {elapsed:.3f}s"
```

### 6. Variable Naming Standards

**Practice 6.1: Avoid Generic Names in Nested Scopes**
```python
# âŒ BAD - Shadowing
tier_name, concepts, books = parse_tier(tier)
for concepts in chapter_data:  # Shadows outer 'concepts'
    process(concepts)

# âœ… GOOD - Descriptive names
tier_name, tier_concepts, tier_books = parse_tier(tier)
for chapter_concepts in chapter_data:
    process(chapter_concepts)
```

**Practice 6.2: Prefix Variables with Scope**
```python
# âŒ BAD - Unclear scope
def process_book(book_data):
    chapters = book_data['chapters']
    for chapter in chapters:
        sections = chapter['sections']
        for section in sections:
            # What scope am I in? Hard to tell
            ...

# âœ… GOOD - Scope-prefixed names
def process_book(book_data):
    book_chapters = book_data['chapters']
    for chapter in book_chapters:
        chapter_sections = chapter['sections']
        for section in chapter_sections:
            # Clear: section is from chapter_sections
            ...
```

### 7. Documentation Standards

**Practice 7.1: Document Type Ignore Reasons**
```python
# âœ… GOOD - Explain why type ignore is needed
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore[import-untyped]
# Reason: scikit-learn does not provide type stubs. See: https://github.com/scikit-learn/scikit-learn/issues/16705
```

**Practice 7.2: Document Unused Parameters**
```python
# âœ… GOOD - Explain why parameter exists
def process_data(data: Dict[str, Any], _legacy_format: Optional[str] = None) -> Result:
    """Process data dictionary.
    
    Args:
        data: Dictionary containing data to process
        _legacy_format: Kept for backward compatibility with v1 API. Not used in v2.
    """
    # Underscore prefix indicates intentionally unused
```

### 8. Testing Standards

**Practice 8.1: Test Edge Cases**
```python
def test_extract_concepts():
    # Normal case
    assert extract_concepts("Chapter 1: Intro") == ["Intro"]
    
    # Edge cases
    assert extract_concepts("") == []
    assert extract_concepts("No chapters here") == []
    assert extract_concepts("Chapter 999: Long title " * 100) == ["Long title..."]
    
    # Error cases
    with pytest.raises(ValueError):
        extract_concepts(None)
```

**Practice 8.2: Test Type Guards**
```python
def test_process_data_with_none():
    # Test that type guards handle None correctly
    result = process_data(None)
    assert result == "N/A"  # Fallback value
    
def test_process_data_with_valid_data():
    result = process_data({"key": "value"})
    assert result == "value"
```

---

## CI/CD Integration

### GitHub Actions Workflow

**.github/workflows/quality-checks.yml**:
```yaml
name: Code Quality Checks

on:
  pull_request:
    paths:
      - 'workflows/**'
  push:
    branches:
      - main
      - develop

jobs:
  quality:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install mypy ruff bandit pytest pytest-cov
          pip install types-requests types-beautifulsoup4  # Type stubs
      
      - name: Run Ruff (fast linting)
        run: ruff check workflows/
      
      - name: Run Mypy (type checking)
        run: mypy --strict workflows/
      
      - name: Run Bandit (security)
        run: bandit -r workflows/ -f json -o bandit-report.json
      
      - name: Run Tests with Coverage
        run: pytest --cov=workflows --cov-report=xml --cov-report=term-missing
      
      - name: SonarQube Scan
        uses: sonarsource/sonarqube-scan-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
      
      - name: SonarQube Quality Gate
        uses: sonarsource/sonarqube-quality-gate-action@master
        timeout-minutes: 5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
      
      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
```

### Pre-Commit Hooks

**.pre-commit-config.yaml**:
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
  
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
        args: [--strict]
        additional_dependencies: [types-requests, types-beautifulsoup4]
  
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.5
    hooks:
      - id: bandit
        args: [-r, workflows/]
  
  - repo: local
    hooks:
      - id: pytest
        name: pytest
        entry: pytest
        language: system
        pass_filenames: false
        always_run: true
```

Install:
```bash
pip install pre-commit
pre-commit install
```

---

## Summary Statistics

### Issues Fixed by Tool

| Tool | Issues | % | Category |
|------|--------|---|----------|
| **Mypy** | 86 | 30% | Type annotations (62), Imports (24) |
| **SonarQube/SonarLint** | 123 | 43% | Complexity (48), Exceptions (39), Unused params (26), Async (5), TODOs (5) |
| **CodeRabbit** | 59 | 21% | Regex (18), Race conditions (3), Secrets (3), K8s/Helm (35) |
| **Ruff** | 11 | 4% | Unused imports (11) |
| **Bandit** | 7 | 2% | Bare except (7) |
| **Total** | **286** | **100%** | - |

### December 2025 Update: llm-gateway Full CodeRabbit Analysis (PR #2)

| Batch | Severity | Issues | Key Categories |
|-------|----------|--------|----------------|
| Batch 1 | ðŸ”´ Critical | 8 | Security (pydantic-settings), broken probes, missing deps |
| Batch 2 | ðŸŸ  Major | 10 | Race conditions (3), connection pooling, env prefix mismatch |
| Batch 3 | ðŸŸ  Major | 10 | Secret exposure (3), dependency pinning, Redis URL parsing |
| Batch 4 | ðŸŸ¡ Minor | 13 | Documentation, placeholders, TDD tests |
| **Total** | | **41** | âœ… All resolved (December 4, 2025) |

**New Pattern Categories Discovered**:
- **Race Conditions (3)**: Token bucket, circuit breaker state, Redis read-modify-write
- **Secret Exposure (3)**: Helm CLI args, Redis URL password, env var logging
- **K8s/Helm Config (8)**: Env prefix mismatch, probe paths, placeholder checksums
- **Connection Pooling (1)**: httpx.AsyncClient per request
- **Provider Abstraction (1)**: AnthropicProvider implementation (26 TDD tests)

### Issues by Severity

| Severity | Count | % | Action Required |
|----------|-------|---|-----------------|
| **CRITICAL** | 8 | 3% | Immediate fix (blocking) |
| **MAJOR** | 150 | 52% | Fix before merge |
| **MEDIUM** | 79 | 28% | Fix during refactor |
| **LOW** | 41 | 14% | Optional improvement |
| **INFO** | 8 | 3% | Document as technical debt |

### Time to Fix

| Commit/Batch | Issues | Files | Time (est) |
|--------|--------|-------|------------|
| 7132fcf0 | 25 | 12 | ~2 hours |
| 223126db | 39 | 6 | ~3 hours |
| da505377 | 20 | 5 | ~1.5 hours |
| 6c7e80a0 | 9 | 5 | ~1 hour |
| d1680a04 | 14 | 7 | ~1 hour |
| 99f6a16f | 19 | 16 | ~2 hours |
| 655880a5 | 5 | 8 | ~30 min |
| CL-028/CL-029 | 23 | 13 | ~45 min |
| CodeRabbit PR #2 | 41 | 31 | ~8 hours |
| **Total** | **195** | **103** | **~20 hours** |

**Average**: ~6 minutes per issue fix (includes testing and TDD)

---

## Conclusion

This analysis of 30+ commits fixing 286 issues across `workflows/` and `llm-gateway/src` directories reveals **13 major anti-pattern categories**:

1. **Type Annotation Issues** (28%) - Missing Optional, no type guards
2. **Cognitive Complexity** (22%) - Functions doing too much
3. **Exception Handling** (17%) - Bare except, no logging, redundant hierarchy
4. **Unused Parameters** (14%) - Refactoring debt, framework-required params
5. **Import Problems** (11%) - Untyped imports, relative paths, unused imports
6. **Regex Patterns** (8%) - Catastrophic backtracking
7. **Variable Shadowing** (5%) - Generic names reused
8. **Async/Await Patterns** - Async stubs without await, async generators
9. **TODO/FIXME Comments** - Indefinite TODOs without tracking
10. **Race Conditions** (NEW) - Token buckets, circuit breakers, Redis operations
11. **Secret Exposure** (NEW) - CLI args, URLs, env vars
12. **K8s/Helm Configuration** (NEW) - Env prefix mismatch, probe paths
13. **Provider Abstraction** (NEW) - Missing implementations, interface compliance

**Key Takeaways**:
- **Mypy catches 30%** of issues - run it locally before commit
- **SonarQube/SonarLint catches 43%** - integrate into CI/CD pipeline
- **CodeRabbit catches 21%** - invaluable for architecture, security, and race conditions
- **Ruff auto-fixes 4%** - unused imports fixed instantly with `ruff --fix`
- **Prevention > Cure** - 20 hours to fix 286 issues, but pre-commit hooks catch them instantly
- **Tool synergy** - Mypy + SonarQube + CodeRabbit + Bandit + Ruff = comprehensive coverage
- **NOSONAR for intentional patterns** - document async stubs that will be implemented later
- **TDD for providers** - 26 tests drove AnthropicProvider implementation

**New Patterns Discovered (December 2025 CodeRabbit)**:
- **Race conditions in async code** - Use asyncio.Lock() for shared mutable state
- **Secret exposure** - Never pass secrets via CLI args, use env vars or secretKeyRef
- **K8s env var prefix mismatch** - Match Helm env vars to pydantic Settings prefix
- **Redis non-atomic operations** - Use HINCRBY/HINCRBYFLOAT instead of GET/SET
- **Provider abstraction gaps** - TDD to ensure all providers implement interface

**Recommended Workflow**:
1. **Local**: Ruff (fast) + Mypy (types) + Pytest (tests) - before commit
2. **CI/CD**: All tools + SonarQube Quality Gate - before merge
3. **Review**: CodeRabbit AI + human review - architecture, security, and logic

**ROI**: Investing 30 minutes to set up pre-commit hooks saves 20+ hours of manual fixing.

---

**Document Metadata**:
- **Generated**: November 26, 2025
- **Updated**: December 4, 2025
- **Commits Analyzed**: 30+ (June 2024 - December 2025)
- **Issues Fixed**: 286
- **Files Affected**: 103 (workflows/ + llm-gateway/src directories)
- **Tools Used**: Mypy, SonarQube, SonarLint, CodeRabbit, Bandit, Ruff
- **Next Review**: After 50 new commits or 3 months
